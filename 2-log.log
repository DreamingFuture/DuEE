nohup: ignoring input
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 5, 
    "event_type": "role", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 1e-05, 
    "max_len": 150, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 30.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 48, 
    "per_gpu_train_batch_size": 64, 
    "seed": 66, 
    "stride": 100, 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/13915 [00:00<?, ?it/s]tokenizing...:   2%|▏         | 302/13915 [00:00<00:04, 3018.94it/s]tokenizing...:   4%|▍         | 604/13915 [00:00<00:04, 2948.30it/s]tokenizing...:   7%|▋         | 914/13915 [00:00<00:04, 3016.04it/s]tokenizing...:   9%|▉         | 1230/13915 [00:00<00:04, 3069.24it/s]tokenizing...:  11%|█         | 1538/13915 [00:00<00:04, 2944.22it/s]tokenizing...:  13%|█▎        | 1842/13915 [00:00<00:04, 2972.99it/s]tokenizing...:  15%|█▌        | 2140/13915 [00:00<00:04, 2843.63it/s]tokenizing...:  17%|█▋        | 2434/13915 [00:00<00:04, 2866.70it/s]tokenizing...:  20%|█▉        | 2722/13915 [00:00<00:03, 2832.88it/s]tokenizing...:  22%|██▏       | 3006/13915 [00:01<00:03, 2811.26it/s]tokenizing...:  24%|██▎       | 3288/13915 [00:01<00:03, 2748.45it/s]tokenizing...:  26%|██▌       | 3592/13915 [00:01<00:03, 2833.17it/s]tokenizing...:  28%|██▊       | 3891/13915 [00:01<00:03, 2876.84it/s]tokenizing...:  30%|███       | 4180/13915 [00:01<00:04, 2150.90it/s]tokenizing...:  32%|███▏      | 4493/13915 [00:01<00:03, 2384.81it/s]tokenizing...:  34%|███▍      | 4789/13915 [00:01<00:03, 2530.94it/s]tokenizing...:  37%|███▋      | 5088/13915 [00:01<00:03, 2652.41it/s]tokenizing...:  39%|███▊      | 5369/13915 [00:01<00:03, 2681.36it/s]tokenizing...:  41%|████      | 5704/13915 [00:02<00:02, 2868.79it/s]tokenizing...:  43%|████▎     | 6000/13915 [00:02<00:02, 2842.61it/s]tokenizing...:  45%|████▌     | 6291/13915 [00:02<00:02, 2832.16it/s]tokenizing...:  47%|████▋     | 6588/13915 [00:02<00:02, 2870.74it/s]tokenizing...:  50%|████▉     | 6915/13915 [00:02<00:02, 2984.32it/s]tokenizing...:  52%|█████▏    | 7216/13915 [00:02<00:02, 2990.47it/s]tokenizing...:  54%|█████▍    | 7517/13915 [00:02<00:02, 2893.21it/s]tokenizing...:  56%|█████▋    | 7837/13915 [00:02<00:02, 2980.78it/s]tokenizing...:  59%|█████▉    | 8178/13915 [00:02<00:01, 3102.61it/s]tokenizing...:  61%|██████    | 8490/13915 [00:02<00:01, 3096.37it/s]tokenizing...:  63%|██████▎   | 8804/13915 [00:03<00:01, 3107.81it/s]tokenizing...:  66%|██████▌   | 9117/13915 [00:03<00:01, 3113.96it/s]tokenizing...:  68%|██████▊   | 9429/13915 [00:03<00:01, 2872.93it/s]tokenizing...:  70%|███████   | 9750/13915 [00:03<00:01, 2966.74it/s]tokenizing...:  72%|███████▏  | 10051/13915 [00:03<00:01, 2861.71it/s]tokenizing...:  74%|███████▍  | 10359/13915 [00:03<00:01, 2922.16it/s]tokenizing...:  77%|███████▋  | 10655/13915 [00:03<00:01, 2929.93it/s]tokenizing...:  79%|███████▉  | 10966/13915 [00:03<00:00, 2982.27it/s]tokenizing...:  81%|████████  | 11266/13915 [00:03<00:00, 2768.63it/s]tokenizing...:  83%|████████▎ | 11547/13915 [00:04<00:00, 2618.04it/s]tokenizing...:  85%|████████▍ | 11813/13915 [00:04<00:00, 2517.29it/s]tokenizing...:  87%|████████▋ | 12068/13915 [00:04<00:00, 2485.41it/s]tokenizing...:  89%|████████▊ | 12319/13915 [00:04<00:00, 2407.97it/s]tokenizing...:  90%|█████████ | 12562/13915 [00:04<00:00, 2361.13it/s]tokenizing...:  92%|█████████▏| 12811/13915 [00:04<00:00, 2396.00it/s]tokenizing...:  94%|█████████▍| 13052/13915 [00:04<00:00, 2388.63it/s]tokenizing...:  96%|█████████▌| 13292/13915 [00:04<00:00, 2322.55it/s]tokenizing...:  98%|█████████▊| 13580/13915 [00:04<00:00, 2479.84it/s]tokenizing...: 100%|█████████▉| 13904/13915 [00:05<00:00, 2695.25it/s]tokenizing...: 100%|██████████| 13915/13915 [00:05<00:00, 2759.47it/s]
tokenizing...:   0%|          | 0/1790 [00:00<?, ?it/s]tokenizing...:  15%|█▌        | 277/1790 [00:00<00:00, 2766.00it/s]tokenizing...:  32%|███▏      | 579/1790 [00:00<00:00, 2911.28it/s]tokenizing...:  49%|████▊     | 871/1790 [00:00<00:00, 2910.37it/s]tokenizing...:  65%|██████▍   | 1163/1790 [00:00<00:00, 1646.67it/s]tokenizing...:  81%|████████  | 1449/1790 [00:00<00:00, 1938.59it/s]tokenizing...:  94%|█████████▍| 1690/1790 [00:00<00:00, 2046.35it/s]tokenizing...: 100%|██████████| 1790/1790 [00:00<00:00, 2142.11it/s]
08/22/2022 16:16:22 - INFO - root -   The nums of the train_dataset features is 13915
08/22/2022 16:16:22 - INFO - root -   The nums of the eval_dataset features is 1790
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
08/22/2022 16:16:26 - INFO - root -   ***** Running train *****
********************************************************************************
/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:52  batch_loss: 5.9157 [Training] 2/218 [..............................] - ETA: 1:50  batch_loss: 5.9196 [Training] 3/218 [..............................] - ETA: 1:29  batch_loss: 5.9267 [Training] 4/218 [..............................] - ETA: 1:20  batch_loss: 5.9167 [Training] 5/218 [..............................] - ETA: 1:14  batch_loss: 5.8900 [Training] 6/218 [..............................] - ETA: 1:10  batch_loss: 5.8387 [Training] 7/218 [..............................] - ETA: 1:08  batch_loss: 5.7852 [Training] 8/218 [>.............................] - ETA: 1:05  batch_loss: 5.7138 [Training] 9/218 [>.............................] - ETA: 1:03  batch_loss: 5.6493 [Training] 10/218 [>.............................] - ETA: 1:02  batch_loss: 5.5819 [Training] 11/218 [>.............................] - ETA: 1:00  batch_loss: 5.5071 [Training] 12/218 [>.............................] - ETA: 59s  batch_loss: 5.4280 [Training] 13/218 [>.............................] - ETA: 58s  batch_loss: 5.3552 [Training] 14/218 [>.............................] - ETA: 57s  batch_loss: 5.2696 [Training] 15/218 [=>............................] - ETA: 56s  batch_loss: 5.1797 [Training] 16/218 [=>............................] - ETA: 56s  batch_loss: 5.1006 [Training] 17/218 [=>............................] - ETA: 55s  batch_loss: 5.0047 [Training] 18/218 [=>............................] - ETA: 54s  batch_loss: 4.9154 [Training] 19/218 [=>............................] - ETA: 54s  batch_loss: 4.8328 [Training] 20/218 [=>............................] - ETA: 53s  batch_loss: 4.7264 [Training] 21/218 [=>............................] - ETA: 53s  batch_loss: 4.6308 [Training] 22/218 [==>...........................] - ETA: 52s  batch_loss: 4.5516 [Training] 23/218 [==>...........................] - ETA: 52s  batch_loss: 4.4659 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 4.3902 [Training] 25/218 [==>...........................] - ETA: 51s  batch_loss: 4.3049 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 4.2225 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 4.1545 [Training] 28/218 [==>...........................] - ETA: 50s  batch_loss: 4.0961 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 4.0396 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 3.9772 [Training] 31/218 [===>..........................] - ETA: 48s  batch_loss: 3.9219 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 3.8657 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 3.8160 [Training] 34/218 [===>..........................] - ETA: 47s  batch_loss: 3.7651 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 3.7332 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 3.6930 [Training] 37/218 [====>.........................] - ETA: 46s  batch_loss: 3.6540 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 3.6221 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 3.5962 [Training] 40/218 [====>.........................] - ETA: 45s  batch_loss: 3.5569 [Training] 41/218 [====>.........................] - ETA: 45s  batch_loss: 3.5274 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 3.4931 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 3.4585 [Training] 44/218 [=====>........................] - ETA: 44s  batch_loss: 3.4309 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 3.4075 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 3.3813 [Training] 47/218 [=====>........................] - ETA: 43s  batch_loss: 3.3514 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 3.3349 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 3.3101 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 3.2965 [Training] 51/218 [======>.......................] - ETA: 42s  batch_loss: 3.2740 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 3.2520 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 3.2292 [Training] 54/218 [======>.......................] - ETA: 41s  batch_loss: 3.2009 [Training] 55/218 [======>.......................] - ETA: 41s  batch_loss: 3.1827 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 3.1666 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 3.1509 [Training] 58/218 [======>.......................] - ETA: 40s  batch_loss: 3.1345 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 3.1151 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 3.1003 [Training] 61/218 [=======>......................] - ETA: 39s  batch_loss: 3.0836 [Training] 62/218 [=======>......................] - ETA: 39s  batch_loss: 3.0711 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 3.0566 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 3.0428 [Training] 65/218 [=======>......................] - ETA: 38s  batch_loss: 3.0248 [Training] 66/218 [========>.....................] - ETA: 38s  batch_loss: 3.0140 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 3.0034 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 2.9915 [Training] 69/218 [========>.....................] - ETA: 37s  batch_loss: 2.9746 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 2.9577 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 2.9423 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 2.9298 [Training] 73/218 [=========>....................] - ETA: 36s  batch_loss: 2.9188 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 2.9068 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 2.8960 [Training] 76/218 [=========>....................] - ETA: 35s  batch_loss: 2.8870 [Training] 77/218 [=========>....................] - ETA: 35s  batch_loss: 2.8782 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 2.8664 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 2.8574 [Training] 80/218 [==========>...................] - ETA: 34s  batch_loss: 2.8463 [Training] 81/218 [==========>...................] - ETA: 34s  batch_loss: 2.8359 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 2.8246 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 2.8122 [Training] 84/218 [==========>...................] - ETA: 33s  batch_loss: 2.8025 [Training] 85/218 [==========>...................] - ETA: 33s  batch_loss: 2.7902 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 2.7783 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 2.7676 [Training] 88/218 [===========>..................] - ETA: 32s  batch_loss: 2.7633 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 2.7571 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 2.7453 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 2.7362 [Training] 92/218 [===========>..................] - ETA: 31s  batch_loss: 2.7276 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 2.7238 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 2.7151 [Training] 95/218 [============>.................] - ETA: 30s  batch_loss: 2.7057 [Training] 96/218 [============>.................] - ETA: 30s  batch_loss: 2.6972 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 2.6936 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 2.6871 [Training] 99/218 [============>.................] - ETA: 29s  batch_loss: 2.6788 [Training] 100/218 [============>.................] - ETA: 29s  batch_loss: 2.6697 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 2.6620 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 2.6563 [Training] 103/218 [=============>................] - ETA: 28s  batch_loss: 2.6481 [Training] 104/218 [=============>................] - ETA: 28s  batch_loss: 2.6398 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 2.6318 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 2.6229 [Training] 107/218 [=============>................] - ETA: 27s  batch_loss: 2.6152 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 2.6117 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 2.6064 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 2.5994 [Training] 111/218 [==============>...............] - ETA: 26s  batch_loss: 2.5918 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 2.5856 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 2.5775 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 2.5699 [Training] 115/218 [==============>...............] - ETA: 25s  batch_loss: 2.5667 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 2.5593 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 2.5538 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 2.5477 [Training] 119/218 [===============>..............] - ETA: 24s  batch_loss: 2.5408 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 2.5353 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 2.5299 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 2.5236 [Training] 123/218 [===============>..............] - ETA: 23s  batch_loss: 2.5169 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 2.5098 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 2.5044 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 2.4986 [Training] 127/218 [================>.............] - ETA: 22s  batch_loss: 2.4920 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 2.4859 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 2.4824 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 2.4781 [Training] 131/218 [=================>............] - ETA: 21s  batch_loss: 2.4733 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 2.4676 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 2.4632 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 2.4565 [Training] 135/218 [=================>............] - ETA: 20s  batch_loss: 2.4508 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 2.4440 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 2.4390 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 2.4349 [Training] 139/218 [==================>...........] - ETA: 19s  batch_loss: 2.4311 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 2.4262 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 2.4218 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 2.4177 [Training] 143/218 [==================>...........] - ETA: 18s  batch_loss: 2.4142 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 2.4104 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 2.4059 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 2.4019 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 2.3965 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 2.3931 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 2.3886 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 2.3834 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 2.3793 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 2.3731 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 2.3683 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 2.3633 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 2.3583 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 2.3537 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 2.3518 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 2.3457 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 2.3423 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 2.3369 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 2.3317 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 2.3284 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 2.3230 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 2.3193 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 2.3162 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 2.3113 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 2.3074 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 2.3035 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 2.3012 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 2.2972 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 2.2934 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 2.2899 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 2.2852 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 2.2819 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 2.2782 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 2.2737 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 2.2687 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 2.2652 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 2.2609 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 2.2571 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 2.2530 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 2.2487 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 2.2443 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 2.2409 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 2.2361 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 2.2330 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 2.2283 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 2.2233 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 2.2200 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 2.2179 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 2.2138 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 2.2090 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 2.2047 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 2.2011 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 2.1977 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 2.1943 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 2.1905 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 2.1859 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 2.1818 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 2.1782 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 2.1746 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 2.1705 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 2.1672 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 2.1637 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 2.1592 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 2.1554 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 2.1512 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 2.1486 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 2.1455 [Training] 210/218 [===========================>..] - ETA: 1s  batch_loss: 2.1408 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 2.1375 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 2.1334 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 2.1293 [Training] 214/218 [============================>.] - ETA: 0s  batch_loss: 2.1256 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 2.1218 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 2.1186 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 2.1157 [Training] 218/218 [==============================] 249.5ms/step  batch_loss: 2.1121 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:17:27 - INFO - root -   The F1-score is 0.1565906838453915
08/22/2022 16:17:27 - INFO - root -   the best eval f1 is 0.1566, saving model !!
08/22/2022 16:17:29 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:08  batch_loss: 1.5011 [Training] 2/218 [..............................] - ETA: 1:32  batch_loss: 1.3896 [Training] 3/218 [..............................] - ETA: 1:21  batch_loss: 1.4170 [Training] 4/218 [..............................] - ETA: 1:14  batch_loss: 1.3478 [Training] 5/218 [..............................] - ETA: 1:10  batch_loss: 1.3547 [Training] 6/218 [..............................] - ETA: 1:07  batch_loss: 1.3467 [Training] 7/218 [..............................] - ETA: 1:05  batch_loss: 1.3674 [Training] 8/218 [>.............................] - ETA: 1:04  batch_loss: 1.3510 [Training] 9/218 [>.............................] - ETA: 1:02  batch_loss: 1.3580 [Training] 10/218 [>.............................] - ETA: 1:01  batch_loss: 1.3795 [Training] 11/218 [>.............................] - ETA: 1:00  batch_loss: 1.3839 [Training] 12/218 [>.............................] - ETA: 59s  batch_loss: 1.3554 [Training] 13/218 [>.............................] - ETA: 58s  batch_loss: 1.3666 [Training] 14/218 [>.............................] - ETA: 58s  batch_loss: 1.3723 [Training] 15/218 [=>............................] - ETA: 57s  batch_loss: 1.3598 [Training] 16/218 [=>............................] - ETA: 56s  batch_loss: 1.3602 [Training] 17/218 [=>............................] - ETA: 55s  batch_loss: 1.3507 [Training] 18/218 [=>............................] - ETA: 55s  batch_loss: 1.3426 [Training] 19/218 [=>............................] - ETA: 54s  batch_loss: 1.3479 [Training] 20/218 [=>............................] - ETA: 54s  batch_loss: 1.3457 [Training] 21/218 [=>............................] - ETA: 53s  batch_loss: 1.3390 [Training] 22/218 [==>...........................] - ETA: 53s  batch_loss: 1.3302 [Training] 23/218 [==>...........................] - ETA: 53s  batch_loss: 1.3308 [Training] 24/218 [==>...........................] - ETA: 52s  batch_loss: 1.3255 [Training] 25/218 [==>...........................] - ETA: 52s  batch_loss: 1.3227 [Training] 26/218 [==>...........................] - ETA: 51s  batch_loss: 1.3168 [Training] 27/218 [==>...........................] - ETA: 51s  batch_loss: 1.3264 [Training] 28/218 [==>...........................] - ETA: 51s  batch_loss: 1.3253 [Training] 29/218 [==>...........................] - ETA: 50s  batch_loss: 1.3251 [Training] 30/218 [===>..........................] - ETA: 50s  batch_loss: 1.3208 [Training] 31/218 [===>..........................] - ETA: 50s  batch_loss: 1.3248 [Training] 32/218 [===>..........................] - ETA: 49s  batch_loss: 1.3270 [Training] 33/218 [===>..........................] - ETA: 49s  batch_loss: 1.3242 [Training] 34/218 [===>..........................] - ETA: 49s  batch_loss: 1.3226 [Training] 35/218 [===>..........................] - ETA: 48s  batch_loss: 1.3151 [Training] 36/218 [===>..........................] - ETA: 48s  batch_loss: 1.3073 [Training] 37/218 [====>.........................] - ETA: 48s  batch_loss: 1.3093 [Training] 38/218 [====>.........................] - ETA: 47s  batch_loss: 1.3070 [Training] 39/218 [====>.........................] - ETA: 47s  batch_loss: 1.3042 [Training] 40/218 [====>.........................] - ETA: 47s  batch_loss: 1.2995 [Training] 41/218 [====>.........................] - ETA: 46s  batch_loss: 1.2934 [Training] 42/218 [====>.........................] - ETA: 46s  batch_loss: 1.2938 [Training] 43/218 [====>.........................] - ETA: 46s  batch_loss: 1.2915 [Training] 44/218 [=====>........................] - ETA: 45s  batch_loss: 1.2893 [Training] 45/218 [=====>........................] - ETA: 45s  batch_loss: 1.2884 [Training] 46/218 [=====>........................] - ETA: 45s  batch_loss: 1.2878 [Training] 47/218 [=====>........................] - ETA: 44s  batch_loss: 1.2861 [Training] 48/218 [=====>........................] - ETA: 44s  batch_loss: 1.2832 [Training] 49/218 [=====>........................] - ETA: 44s  batch_loss: 1.2814 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 1.2817 [Training] 51/218 [======>.......................] - ETA: 43s  batch_loss: 1.2790 [Training] 52/218 [======>.......................] - ETA: 43s  batch_loss: 1.2745 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 1.2731 [Training] 54/218 [======>.......................] - ETA: 42s  batch_loss: 1.2704 [Training] 55/218 [======>.......................] - ETA: 42s  batch_loss: 1.2689 [Training] 56/218 [======>.......................] - ETA: 42s  batch_loss: 1.2689 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 1.2721 [Training] 58/218 [======>.......................] - ETA: 41s  batch_loss: 1.2716 [Training] 59/218 [=======>......................] - ETA: 41s  batch_loss: 1.2714 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 1.2684 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 1.2700 [Training] 62/218 [=======>......................] - ETA: 40s  batch_loss: 1.2697 [Training] 63/218 [=======>......................] - ETA: 40s  batch_loss: 1.2664 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 1.2677 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 1.2650 [Training] 66/218 [========>.....................] - ETA: 39s  batch_loss: 1.2597 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 1.2600 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 1.2582 [Training] 69/218 [========>.....................] - ETA: 38s  batch_loss: 1.2581 [Training] 70/218 [========>.....................] - ETA: 38s  batch_loss: 1.2538 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 1.2522 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 1.2509 [Training] 73/218 [=========>....................] - ETA: 37s  batch_loss: 1.2487 [Training] 74/218 [=========>....................] - ETA: 37s  batch_loss: 1.2461 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 1.2461 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 1.2473 [Training] 77/218 [=========>....................] - ETA: 36s  batch_loss: 1.2470 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 1.2456 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 1.2416 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 1.2432 [Training] 81/218 [==========>...................] - ETA: 35s  batch_loss: 1.2406 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 1.2394 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 1.2394 [Training] 84/218 [==========>...................] - ETA: 34s  batch_loss: 1.2371 [Training] 85/218 [==========>...................] - ETA: 34s  batch_loss: 1.2373 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 1.2381 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 1.2371 [Training] 88/218 [===========>..................] - ETA: 33s  batch_loss: 1.2349 [Training] 89/218 [===========>..................] - ETA: 33s  batch_loss: 1.2346 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 1.2331 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 1.2319 [Training] 92/218 [===========>..................] - ETA: 32s  batch_loss: 1.2292 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 1.2289 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 1.2290 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 1.2265 [Training] 96/218 [============>.................] - ETA: 31s  batch_loss: 1.2256 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 1.2245 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 1.2232 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 1.2253 [Training] 100/218 [============>.................] - ETA: 30s  batch_loss: 1.2268 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 1.2272 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 1.2267 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 1.2271 [Training] 104/218 [=============>................] - ETA: 29s  batch_loss: 1.2256 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 1.2236 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 1.2239 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 1.2215 [Training] 108/218 [=============>................] - ETA: 28s  batch_loss: 1.2212 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 1.2199 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 1.2184 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 1.2163 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 1.2158 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 1.2135 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 1.2126 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 1.2128 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 1.2118 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 1.2112 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 1.2110 [Training] 119/218 [===============>..............] - ETA: 25s  batch_loss: 1.2090 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 1.2075 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 1.2063 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 1.2036 [Training] 123/218 [===============>..............] - ETA: 24s  batch_loss: 1.2033 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 1.2008 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 1.2007 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 1.1993 [Training] 127/218 [================>.............] - ETA: 23s  batch_loss: 1.1980 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 1.1969 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 1.1959 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 1.1965 [Training] 131/218 [=================>............] - ETA: 22s  batch_loss: 1.1946 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 1.1946 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 1.1926 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 1.1917 [Training] 135/218 [=================>............] - ETA: 21s  batch_loss: 1.1900 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 1.1905 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 1.1892 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 1.1884 [Training] 139/218 [==================>...........] - ETA: 20s  batch_loss: 1.1871 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 1.1866 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 1.1852 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 1.1838 [Training] 143/218 [==================>...........] - ETA: 18s  batch_loss: 1.1834 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 1.1835 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 1.1812 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 1.1801 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 1.1792 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 1.1774 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 1.1766 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 1.1756 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 1.1744 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 1.1741 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 1.1742 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 1.1736 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 1.1727 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 1.1706 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 1.1688 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 1.1680 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 1.1669 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 1.1651 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 1.1636 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 1.1619 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 1.1602 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 1.1603 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 1.1590 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 1.1602 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 1.1588 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 1.1572 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 1.1565 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 1.1552 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 1.1546 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 1.1529 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 1.1524 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 1.1507 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 1.1488 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 1.1484 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 1.1478 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 1.1484 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 1.1475 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 1.1466 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 1.1456 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 1.1447 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 1.1442 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 1.1432 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 1.1424 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 1.1423 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 1.1407 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 1.1392 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 1.1389 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 1.1379 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 1.1361 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 1.1353 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 1.1345 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 1.1330 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 1.1320 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 1.1311 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 1.1303 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 1.1294 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 1.1277 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 1.1269 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 1.1268 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 1.1255 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 1.1246 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 1.1233 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 1.1228 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 1.1214 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 1.1205 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 1.1200 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 1.1191 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 1.1178 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 1.1166 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 1.1147 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 1.1130 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 1.1117 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 1.1111 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 1.1101 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 1.1093 [Training] 218/218 [==============================] 250.9ms/step  batch_loss: 1.1096 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:18:29 - INFO - root -   The F1-score is 0.38471654003506717
08/22/2022 16:18:29 - INFO - root -   the best eval f1 is 0.3847, saving model !!
08/22/2022 16:18:31 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:10  batch_loss: 0.8410 [Training] 2/218 [..............................] - ETA: 1:32  batch_loss: 0.8291 [Training] 3/218 [..............................] - ETA: 1:19  batch_loss: 0.8278 [Training] 4/218 [..............................] - ETA: 1:13  batch_loss: 0.8171 [Training] 5/218 [..............................] - ETA: 1:08  batch_loss: 0.8129 [Training] 6/218 [..............................] - ETA: 1:06  batch_loss: 0.8418 [Training] 7/218 [..............................] - ETA: 1:04  batch_loss: 0.8545 [Training] 8/218 [>.............................] - ETA: 1:03  batch_loss: 0.8322 [Training] 9/218 [>.............................] - ETA: 1:01  batch_loss: 0.8408 [Training] 10/218 [>.............................] - ETA: 1:00  batch_loss: 0.8369 [Training] 11/218 [>.............................] - ETA: 59s  batch_loss: 0.8390 [Training] 12/218 [>.............................] - ETA: 58s  batch_loss: 0.8445 [Training] 13/218 [>.............................] - ETA: 58s  batch_loss: 0.8514 [Training] 14/218 [>.............................] - ETA: 57s  batch_loss: 0.8480 [Training] 15/218 [=>............................] - ETA: 56s  batch_loss: 0.8499 [Training] 16/218 [=>............................] - ETA: 55s  batch_loss: 0.8584 [Training] 17/218 [=>............................] - ETA: 55s  batch_loss: 0.8539 [Training] 18/218 [=>............................] - ETA: 55s  batch_loss: 0.8491 [Training] 19/218 [=>............................] - ETA: 54s  batch_loss: 0.8533 [Training] 20/218 [=>............................] - ETA: 54s  batch_loss: 0.8540 [Training] 21/218 [=>............................] - ETA: 53s  batch_loss: 0.8510 [Training] 22/218 [==>...........................] - ETA: 53s  batch_loss: 0.8538 [Training] 23/218 [==>...........................] - ETA: 53s  batch_loss: 0.8524 [Training] 24/218 [==>...........................] - ETA: 52s  batch_loss: 0.8546 [Training] 25/218 [==>...........................] - ETA: 52s  batch_loss: 0.8536 [Training] 26/218 [==>...........................] - ETA: 51s  batch_loss: 0.8550 [Training] 27/218 [==>...........................] - ETA: 51s  batch_loss: 0.8536 [Training] 28/218 [==>...........................] - ETA: 51s  batch_loss: 0.8533 [Training] 29/218 [==>...........................] - ETA: 50s  batch_loss: 0.8549 [Training] 30/218 [===>..........................] - ETA: 50s  batch_loss: 0.8562 [Training] 31/218 [===>..........................] - ETA: 50s  batch_loss: 0.8537 [Training] 32/218 [===>..........................] - ETA: 49s  batch_loss: 0.8571 [Training] 33/218 [===>..........................] - ETA: 49s  batch_loss: 0.8582 [Training] 34/218 [===>..........................] - ETA: 49s  batch_loss: 0.8553 [Training] 35/218 [===>..........................] - ETA: 49s  batch_loss: 0.8546 [Training] 36/218 [===>..........................] - ETA: 48s  batch_loss: 0.8530 [Training] 37/218 [====>.........................] - ETA: 48s  batch_loss: 0.8512 [Training] 38/218 [====>.........................] - ETA: 48s  batch_loss: 0.8514 [Training] 39/218 [====>.........................] - ETA: 47s  batch_loss: 0.8527 [Training] 40/218 [====>.........................] - ETA: 47s  batch_loss: 0.8496 [Training] 41/218 [====>.........................] - ETA: 47s  batch_loss: 0.8547 [Training] 42/218 [====>.........................] - ETA: 47s  batch_loss: 0.8531 [Training] 43/218 [====>.........................] - ETA: 46s  batch_loss: 0.8502 [Training] 44/218 [=====>........................] - ETA: 46s  batch_loss: 0.8477 [Training] 45/218 [=====>........................] - ETA: 46s  batch_loss: 0.8465 [Training] 46/218 [=====>........................] - ETA: 45s  batch_loss: 0.8460 [Training] 47/218 [=====>........................] - ETA: 45s  batch_loss: 0.8445 [Training] 48/218 [=====>........................] - ETA: 45s  batch_loss: 0.8436 [Training] 49/218 [=====>........................] - ETA: 45s  batch_loss: 0.8408 [Training] 50/218 [=====>........................] - ETA: 44s  batch_loss: 0.8433 [Training] 51/218 [======>.......................] - ETA: 44s  batch_loss: 0.8435 [Training] 52/218 [======>.......................] - ETA: 44s  batch_loss: 0.8444 [Training] 53/218 [======>.......................] - ETA: 43s  batch_loss: 0.8447 [Training] 54/218 [======>.......................] - ETA: 43s  batch_loss: 0.8442 [Training] 55/218 [======>.......................] - ETA: 43s  batch_loss: 0.8435 [Training] 56/218 [======>.......................] - ETA: 43s  batch_loss: 0.8415 [Training] 57/218 [======>.......................] - ETA: 42s  batch_loss: 0.8425 [Training] 58/218 [======>.......................] - ETA: 42s  batch_loss: 0.8431 [Training] 59/218 [=======>......................] - ETA: 42s  batch_loss: 0.8428 [Training] 60/218 [=======>......................] - ETA: 41s  batch_loss: 0.8447 [Training] 61/218 [=======>......................] - ETA: 41s  batch_loss: 0.8450 [Training] 62/218 [=======>......................] - ETA: 41s  batch_loss: 0.8461 [Training] 63/218 [=======>......................] - ETA: 41s  batch_loss: 0.8453 [Training] 64/218 [=======>......................] - ETA: 40s  batch_loss: 0.8443 [Training] 65/218 [=======>......................] - ETA: 40s  batch_loss: 0.8442 [Training] 66/218 [========>.....................] - ETA: 40s  batch_loss: 0.8453 [Training] 67/218 [========>.....................] - ETA: 40s  batch_loss: 0.8446 [Training] 68/218 [========>.....................] - ETA: 39s  batch_loss: 0.8423 [Training] 69/218 [========>.....................] - ETA: 39s  batch_loss: 0.8435 [Training] 70/218 [========>.....................] - ETA: 39s  batch_loss: 0.8458 [Training] 71/218 [========>.....................] - ETA: 38s  batch_loss: 0.8475 [Training] 72/218 [========>.....................] - ETA: 38s  batch_loss: 0.8480 [Training] 73/218 [=========>....................] - ETA: 38s  batch_loss: 0.8486 [Training] 74/218 [=========>....................] - ETA: 38s  batch_loss: 0.8479 [Training] 75/218 [=========>....................] - ETA: 37s  batch_loss: 0.8471 [Training] 76/218 [=========>....................] - ETA: 37s  batch_loss: 0.8461 [Training] 77/218 [=========>....................] - ETA: 37s  batch_loss: 0.8451 [Training] 78/218 [=========>....................] - ETA: 36s  batch_loss: 0.8449 [Training] 79/218 [=========>....................] - ETA: 36s  batch_loss: 0.8466 [Training] 80/218 [==========>...................] - ETA: 36s  batch_loss: 0.8470 [Training] 81/218 [==========>...................] - ETA: 36s  batch_loss: 0.8473 [Training] 82/218 [==========>...................] - ETA: 35s  batch_loss: 0.8467 [Training] 83/218 [==========>...................] - ETA: 35s  batch_loss: 0.8459 [Training] 84/218 [==========>...................] - ETA: 35s  batch_loss: 0.8458 [Training] 85/218 [==========>...................] - ETA: 34s  batch_loss: 0.8447 [Training] 86/218 [==========>...................] - ETA: 34s  batch_loss: 0.8449 [Training] 87/218 [==========>...................] - ETA: 34s  batch_loss: 0.8454 [Training] 88/218 [===========>..................] - ETA: 34s  batch_loss: 0.8453 [Training] 89/218 [===========>..................] - ETA: 33s  batch_loss: 0.8453 [Training] 90/218 [===========>..................] - ETA: 33s  batch_loss: 0.8434 [Training] 91/218 [===========>..................] - ETA: 33s  batch_loss: 0.8421 [Training] 92/218 [===========>..................] - ETA: 32s  batch_loss: 0.8409 [Training] 93/218 [===========>..................] - ETA: 32s  batch_loss: 0.8407 [Training] 94/218 [===========>..................] - ETA: 32s  batch_loss: 0.8399 [Training] 95/218 [============>.................] - ETA: 32s  batch_loss: 0.8391 [Training] 96/218 [============>.................] - ETA: 31s  batch_loss: 0.8382 [Training] 97/218 [============>.................] - ETA: 31s  batch_loss: 0.8392 [Training] 98/218 [============>.................] - ETA: 31s  batch_loss: 0.8374 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.8377 [Training] 100/218 [============>.................] - ETA: 30s  batch_loss: 0.8359 [Training] 101/218 [============>.................] - ETA: 30s  batch_loss: 0.8343 [Training] 102/218 [=============>................] - ETA: 30s  batch_loss: 0.8345 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.8352 [Training] 104/218 [=============>................] - ETA: 29s  batch_loss: 0.8350 [Training] 105/218 [=============>................] - ETA: 29s  batch_loss: 0.8334 [Training] 106/218 [=============>................] - ETA: 29s  batch_loss: 0.8323 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.8322 [Training] 108/218 [=============>................] - ETA: 28s  batch_loss: 0.8336 [Training] 109/218 [==============>...............] - ETA: 28s  batch_loss: 0.8339 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.8329 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.8323 [Training] 112/218 [==============>...............] - ETA: 27s  batch_loss: 0.8318 [Training] 113/218 [==============>...............] - ETA: 27s  batch_loss: 0.8305 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.8293 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.8289 [Training] 116/218 [==============>...............] - ETA: 26s  batch_loss: 0.8295 [Training] 117/218 [===============>..............] - ETA: 26s  batch_loss: 0.8287 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.8277 [Training] 119/218 [===============>..............] - ETA: 25s  batch_loss: 0.8272 [Training] 120/218 [===============>..............] - ETA: 25s  batch_loss: 0.8249 [Training] 121/218 [===============>..............] - ETA: 25s  batch_loss: 0.8243 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.8226 [Training] 123/218 [===============>..............] - ETA: 24s  batch_loss: 0.8219 [Training] 124/218 [================>.............] - ETA: 24s  batch_loss: 0.8205 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.8199 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.8191 [Training] 127/218 [================>.............] - ETA: 23s  batch_loss: 0.8191 [Training] 128/218 [================>.............] - ETA: 23s  batch_loss: 0.8191 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.8182 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.8165 [Training] 131/218 [=================>............] - ETA: 22s  batch_loss: 0.8167 [Training] 132/218 [=================>............] - ETA: 22s  batch_loss: 0.8165 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.8146 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.8133 [Training] 135/218 [=================>............] - ETA: 21s  batch_loss: 0.8124 [Training] 136/218 [=================>............] - ETA: 21s  batch_loss: 0.8119 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.8115 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.8103 [Training] 139/218 [==================>...........] - ETA: 20s  batch_loss: 0.8099 [Training] 140/218 [==================>...........] - ETA: 20s  batch_loss: 0.8092 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.8090 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.8081 [Training] 143/218 [==================>...........] - ETA: 19s  batch_loss: 0.8086 [Training] 144/218 [==================>...........] - ETA: 19s  batch_loss: 0.8075 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.8072 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.8069 [Training] 147/218 [===================>..........] - ETA: 18s  batch_loss: 0.8062 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.8052 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.8048 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.8041 [Training] 151/218 [===================>..........] - ETA: 17s  batch_loss: 0.8033 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.8023 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.8021 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.8010 [Training] 155/218 [====================>.........] - ETA: 16s  batch_loss: 0.8005 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.7999 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.7991 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.7984 [Training] 159/218 [====================>.........] - ETA: 15s  batch_loss: 0.7988 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.7980 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.7972 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.7962 [Training] 163/218 [=====================>........] - ETA: 14s  batch_loss: 0.7958 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.7956 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.7952 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.7942 [Training] 167/218 [=====================>........] - ETA: 13s  batch_loss: 0.7935 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.7937 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.7929 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.7929 [Training] 171/218 [======================>.......] - ETA: 12s  batch_loss: 0.7916 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.7914 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.7916 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.7903 [Training] 175/218 [=======================>......] - ETA: 11s  batch_loss: 0.7909 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.7908 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.7901 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.7897 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.7898 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.7896 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.7890 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.7888 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.7883 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.7882 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.7878 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.7869 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.7864 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.7856 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.7850 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.7845 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.7844 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.7842 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.7836 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.7829 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.7826 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.7821 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.7811 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.7809 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.7802 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.7803 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.7799 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.7795 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.7788 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.7786 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.7785 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 0.7782 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.7773 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.7766 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.7765 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 0.7759 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.7752 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.7747 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.7744 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 0.7742 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.7735 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.7731 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.7722 [Training] 218/218 [==============================] 254.7ms/step  batch_loss: 0.7717 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:19:33 - INFO - root -   The F1-score is 0.47487099738460453
08/22/2022 16:19:33 - INFO - root -   the best eval f1 is 0.4749, saving model !!
08/22/2022 16:19:36 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:09  batch_loss: 0.6524 [Training] 2/218 [..............................] - ETA: 1:31  batch_loss: 0.7036 [Training] 3/218 [..............................] - ETA: 1:18  batch_loss: 0.6772 [Training] 4/218 [..............................] - ETA: 1:11  batch_loss: 0.6618 [Training] 5/218 [..............................] - ETA: 1:08  batch_loss: 0.6798 [Training] 6/218 [..............................] - ETA: 1:05  batch_loss: 0.6581 [Training] 7/218 [..............................] - ETA: 1:03  batch_loss: 0.6589 [Training] 8/218 [>.............................] - ETA: 1:02  batch_loss: 0.6453 [Training] 9/218 [>.............................] - ETA: 1:00  batch_loss: 0.6533 [Training] 10/218 [>.............................] - ETA: 59s  batch_loss: 0.6577 [Training] 11/218 [>.............................] - ETA: 58s  batch_loss: 0.6544 [Training] 12/218 [>.............................] - ETA: 57s  batch_loss: 0.6540 [Training] 13/218 [>.............................] - ETA: 56s  batch_loss: 0.6544 [Training] 14/218 [>.............................] - ETA: 56s  batch_loss: 0.6459 [Training] 15/218 [=>............................] - ETA: 55s  batch_loss: 0.6443 [Training] 16/218 [=>............................] - ETA: 54s  batch_loss: 0.6405 [Training] 17/218 [=>............................] - ETA: 54s  batch_loss: 0.6326 [Training] 18/218 [=>............................] - ETA: 53s  batch_loss: 0.6284 [Training] 19/218 [=>............................] - ETA: 53s  batch_loss: 0.6257 [Training] 20/218 [=>............................] - ETA: 53s  batch_loss: 0.6234 [Training] 21/218 [=>............................] - ETA: 52s  batch_loss: 0.6276 [Training] 22/218 [==>...........................] - ETA: 52s  batch_loss: 0.6299 [Training] 23/218 [==>...........................] - ETA: 51s  batch_loss: 0.6329 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.6380 [Training] 25/218 [==>...........................] - ETA: 51s  batch_loss: 0.6404 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 0.6495 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 0.6499 [Training] 28/218 [==>...........................] - ETA: 50s  batch_loss: 0.6481 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 0.6496 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.6511 [Training] 31/218 [===>..........................] - ETA: 49s  batch_loss: 0.6522 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.6507 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.6491 [Training] 34/218 [===>..........................] - ETA: 48s  batch_loss: 0.6482 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.6459 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.6458 [Training] 37/218 [====>.........................] - ETA: 47s  batch_loss: 0.6455 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.6468 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.6464 [Training] 40/218 [====>.........................] - ETA: 46s  batch_loss: 0.6457 [Training] 41/218 [====>.........................] - ETA: 46s  batch_loss: 0.6464 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.6491 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.6474 [Training] 44/218 [=====>........................] - ETA: 45s  batch_loss: 0.6495 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.6493 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.6495 [Training] 47/218 [=====>........................] - ETA: 44s  batch_loss: 0.6488 [Training] 48/218 [=====>........................] - ETA: 44s  batch_loss: 0.6477 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.6473 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.6472 [Training] 51/218 [======>.......................] - ETA: 43s  batch_loss: 0.6470 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.6444 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.6465 [Training] 54/218 [======>.......................] - ETA: 42s  batch_loss: 0.6457 [Training] 55/218 [======>.......................] - ETA: 42s  batch_loss: 0.6459 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.6445 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.6462 [Training] 58/218 [======>.......................] - ETA: 41s  batch_loss: 0.6453 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 0.6467 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.6449 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.6433 [Training] 62/218 [=======>......................] - ETA: 40s  batch_loss: 0.6442 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.6450 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.6452 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 0.6447 [Training] 66/218 [========>.....................] - ETA: 39s  batch_loss: 0.6442 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.6428 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.6415 [Training] 69/218 [========>.....................] - ETA: 38s  batch_loss: 0.6422 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 0.6405 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.6402 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.6408 [Training] 73/218 [=========>....................] - ETA: 37s  batch_loss: 0.6404 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.6399 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.6396 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.6411 [Training] 77/218 [=========>....................] - ETA: 36s  batch_loss: 0.6413 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.6420 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.6423 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.6422 [Training] 81/218 [==========>...................] - ETA: 35s  batch_loss: 0.6426 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.6406 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.6393 [Training] 84/218 [==========>...................] - ETA: 34s  batch_loss: 0.6377 [Training] 85/218 [==========>...................] - ETA: 34s  batch_loss: 0.6363 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.6357 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.6353 [Training] 88/218 [===========>..................] - ETA: 33s  batch_loss: 0.6360 [Training] 89/218 [===========>..................] - ETA: 33s  batch_loss: 0.6356 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.6347 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.6352 [Training] 92/218 [===========>..................] - ETA: 32s  batch_loss: 0.6342 [Training] 93/218 [===========>..................] - ETA: 32s  batch_loss: 0.6348 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.6344 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.6342 [Training] 96/218 [============>.................] - ETA: 31s  batch_loss: 0.6334 [Training] 97/218 [============>.................] - ETA: 31s  batch_loss: 0.6335 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.6329 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.6324 [Training] 100/218 [============>.................] - ETA: 30s  batch_loss: 0.6317 [Training] 101/218 [============>.................] - ETA: 30s  batch_loss: 0.6311 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.6308 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.6306 [Training] 104/218 [=============>................] - ETA: 29s  batch_loss: 0.6296 [Training] 105/218 [=============>................] - ETA: 29s  batch_loss: 0.6296 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.6289 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.6290 [Training] 108/218 [=============>................] - ETA: 28s  batch_loss: 0.6288 [Training] 109/218 [==============>...............] - ETA: 28s  batch_loss: 0.6290 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.6297 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.6294 [Training] 112/218 [==============>...............] - ETA: 27s  batch_loss: 0.6294 [Training] 113/218 [==============>...............] - ETA: 27s  batch_loss: 0.6287 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.6286 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.6277 [Training] 116/218 [==============>...............] - ETA: 26s  batch_loss: 0.6281 [Training] 117/218 [===============>..............] - ETA: 26s  batch_loss: 0.6275 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.6274 [Training] 119/218 [===============>..............] - ETA: 25s  batch_loss: 0.6282 [Training] 120/218 [===============>..............] - ETA: 25s  batch_loss: 0.6270 [Training] 121/218 [===============>..............] - ETA: 25s  batch_loss: 0.6264 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.6261 [Training] 123/218 [===============>..............] - ETA: 24s  batch_loss: 0.6268 [Training] 124/218 [================>.............] - ETA: 24s  batch_loss: 0.6273 [Training] 125/218 [================>.............] - ETA: 24s  batch_loss: 0.6264 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.6255 [Training] 127/218 [================>.............] - ETA: 23s  batch_loss: 0.6255 [Training] 128/218 [================>.............] - ETA: 23s  batch_loss: 0.6254 [Training] 129/218 [================>.............] - ETA: 23s  batch_loss: 0.6250 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.6245 [Training] 131/218 [=================>............] - ETA: 22s  batch_loss: 0.6236 [Training] 132/218 [=================>............] - ETA: 22s  batch_loss: 0.6231 [Training] 133/218 [=================>............] - ETA: 22s  batch_loss: 0.6226 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.6225 [Training] 135/218 [=================>............] - ETA: 21s  batch_loss: 0.6222 [Training] 136/218 [=================>............] - ETA: 21s  batch_loss: 0.6225 [Training] 137/218 [=================>............] - ETA: 21s  batch_loss: 0.6227 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.6221 [Training] 139/218 [==================>...........] - ETA: 20s  batch_loss: 0.6223 [Training] 140/218 [==================>...........] - ETA: 20s  batch_loss: 0.6219 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.6207 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.6205 [Training] 143/218 [==================>...........] - ETA: 19s  batch_loss: 0.6192 [Training] 144/218 [==================>...........] - ETA: 19s  batch_loss: 0.6189 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.6178 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.6178 [Training] 147/218 [===================>..........] - ETA: 18s  batch_loss: 0.6180 [Training] 148/218 [===================>..........] - ETA: 18s  batch_loss: 0.6177 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.6181 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.6173 [Training] 151/218 [===================>..........] - ETA: 17s  batch_loss: 0.6165 [Training] 152/218 [===================>..........] - ETA: 17s  batch_loss: 0.6161 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.6159 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.6161 [Training] 155/218 [====================>.........] - ETA: 16s  batch_loss: 0.6154 [Training] 156/218 [====================>.........] - ETA: 16s  batch_loss: 0.6149 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.6150 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.6151 [Training] 159/218 [====================>.........] - ETA: 15s  batch_loss: 0.6142 [Training] 160/218 [=====================>........] - ETA: 15s  batch_loss: 0.6136 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.6130 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.6130 [Training] 163/218 [=====================>........] - ETA: 14s  batch_loss: 0.6127 [Training] 164/218 [=====================>........] - ETA: 14s  batch_loss: 0.6123 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.6116 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.6120 [Training] 167/218 [=====================>........] - ETA: 13s  batch_loss: 0.6111 [Training] 168/218 [======================>.......] - ETA: 13s  batch_loss: 0.6103 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.6105 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.6101 [Training] 171/218 [======================>.......] - ETA: 12s  batch_loss: 0.6097 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.6087 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.6084 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.6085 [Training] 175/218 [=======================>......] - ETA: 11s  batch_loss: 0.6082 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.6079 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.6077 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.6073 [Training] 179/218 [=======================>......] - ETA: 10s  batch_loss: 0.6067 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.6067 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.6059 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.6050 [Training] 183/218 [========================>.....] - ETA: 9s  batch_loss: 0.6048 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.6046 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.6040 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.6039 [Training] 187/218 [========================>.....] - ETA: 8s  batch_loss: 0.6039 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.6042 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.6035 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.6034 [Training] 191/218 [=========================>....] - ETA: 7s  batch_loss: 0.6035 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.6030 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.6026 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.6025 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.6021 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.6019 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.6025 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.6028 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.6028 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.6021 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.6014 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.6011 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.6008 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.6009 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.6008 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 0.6005 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.5999 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.5992 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.5988 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 0.5988 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.5991 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.5990 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.5985 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 0.5982 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.5983 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.5980 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.5982 [Training] 218/218 [==============================] 258.4ms/step  batch_loss: 0.5984 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:20:39 - INFO - root -   The F1-score is 0.5360514434259133
08/22/2022 16:20:39 - INFO - root -   the best eval f1 is 0.5361, saving model !!
08/22/2022 16:20:41 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:10  batch_loss: 0.5508 [Training] 2/218 [..............................] - ETA: 1:32  batch_loss: 0.5077 [Training] 3/218 [..............................] - ETA: 1:18  batch_loss: 0.5369 [Training] 4/218 [..............................] - ETA: 1:11  batch_loss: 0.5472 [Training] 5/218 [..............................] - ETA: 1:08  batch_loss: 0.5554 [Training] 6/218 [..............................] - ETA: 1:05  batch_loss: 0.5349 [Training] 7/218 [..............................] - ETA: 1:03  batch_loss: 0.5430 [Training] 8/218 [>.............................] - ETA: 1:01  batch_loss: 0.5390 [Training] 9/218 [>.............................] - ETA: 1:00  batch_loss: 0.5333 [Training] 10/218 [>.............................] - ETA: 59s  batch_loss: 0.5328 [Training] 11/218 [>.............................] - ETA: 58s  batch_loss: 0.5296 [Training] 12/218 [>.............................] - ETA: 57s  batch_loss: 0.5375 [Training] 13/218 [>.............................] - ETA: 56s  batch_loss: 0.5369 [Training] 14/218 [>.............................] - ETA: 56s  batch_loss: 0.5386 [Training] 15/218 [=>............................] - ETA: 55s  batch_loss: 0.5418 [Training] 16/218 [=>............................] - ETA: 54s  batch_loss: 0.5363 [Training] 17/218 [=>............................] - ETA: 54s  batch_loss: 0.5312 [Training] 18/218 [=>............................] - ETA: 53s  batch_loss: 0.5402 [Training] 19/218 [=>............................] - ETA: 53s  batch_loss: 0.5409 [Training] 20/218 [=>............................] - ETA: 52s  batch_loss: 0.5344 [Training] 21/218 [=>............................] - ETA: 52s  batch_loss: 0.5389 [Training] 22/218 [==>...........................] - ETA: 52s  batch_loss: 0.5408 [Training] 23/218 [==>...........................] - ETA: 51s  batch_loss: 0.5412 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.5395 [Training] 25/218 [==>...........................] - ETA: 50s  batch_loss: 0.5406 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 0.5384 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 0.5362 [Training] 28/218 [==>...........................] - ETA: 49s  batch_loss: 0.5330 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 0.5337 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.5303 [Training] 31/218 [===>..........................] - ETA: 48s  batch_loss: 0.5315 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.5333 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.5324 [Training] 34/218 [===>..........................] - ETA: 47s  batch_loss: 0.5323 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.5302 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.5280 [Training] 37/218 [====>.........................] - ETA: 47s  batch_loss: 0.5284 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.5278 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.5261 [Training] 40/218 [====>.........................] - ETA: 46s  batch_loss: 0.5255 [Training] 41/218 [====>.........................] - ETA: 45s  batch_loss: 0.5251 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.5243 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.5264 [Training] 44/218 [=====>........................] - ETA: 44s  batch_loss: 0.5253 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.5247 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.5213 [Training] 47/218 [=====>........................] - ETA: 44s  batch_loss: 0.5202 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 0.5197 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.5195 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.5210 [Training] 51/218 [======>.......................] - ETA: 42s  batch_loss: 0.5204 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.5198 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.5189 [Training] 54/218 [======>.......................] - ETA: 42s  batch_loss: 0.5187 [Training] 55/218 [======>.......................] - ETA: 41s  batch_loss: 0.5184 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.5191 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.5186 [Training] 58/218 [======>.......................] - ETA: 40s  batch_loss: 0.5169 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 0.5175 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.5157 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.5148 [Training] 62/218 [=======>......................] - ETA: 39s  batch_loss: 0.5147 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.5143 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.5136 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 0.5131 [Training] 66/218 [========>.....................] - ETA: 38s  batch_loss: 0.5126 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.5143 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.5128 [Training] 69/218 [========>.....................] - ETA: 37s  batch_loss: 0.5134 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 0.5135 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.5116 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.5126 [Training] 73/218 [=========>....................] - ETA: 36s  batch_loss: 0.5109 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.5110 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.5093 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.5092 [Training] 77/218 [=========>....................] - ETA: 35s  batch_loss: 0.5094 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.5081 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.5070 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.5051 [Training] 81/218 [==========>...................] - ETA: 34s  batch_loss: 0.5066 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.5051 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.5057 [Training] 84/218 [==========>...................] - ETA: 33s  batch_loss: 0.5054 [Training] 85/218 [==========>...................] - ETA: 33s  batch_loss: 0.5037 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.5033 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.5029 [Training] 88/218 [===========>..................] - ETA: 32s  batch_loss: 0.5027 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.5023 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.5016 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.5020 [Training] 92/218 [===========>..................] - ETA: 31s  batch_loss: 0.5011 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.5011 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.5020 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.5012 [Training] 96/218 [============>.................] - ETA: 30s  batch_loss: 0.5002 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.5002 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.5002 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.5003 [Training] 100/218 [============>.................] - ETA: 29s  batch_loss: 0.5006 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.5007 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.4998 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.4992 [Training] 104/218 [=============>................] - ETA: 28s  batch_loss: 0.4998 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.5000 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.5005 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.5008 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 0.5010 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.5009 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.5010 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.5010 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 0.5001 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.5005 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.5006 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.5009 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.5008 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.5008 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.5006 [Training] 119/218 [===============>..............] - ETA: 24s  batch_loss: 0.5001 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.5004 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.5006 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.4998 [Training] 123/218 [===============>..............] - ETA: 23s  batch_loss: 0.4992 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.4986 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.4985 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.4990 [Training] 127/218 [================>.............] - ETA: 22s  batch_loss: 0.4980 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.4980 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.4976 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.4968 [Training] 131/218 [=================>............] - ETA: 21s  batch_loss: 0.4972 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.4973 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.4971 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.4967 [Training] 135/218 [=================>............] - ETA: 20s  batch_loss: 0.4966 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.4965 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.4962 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.4966 [Training] 139/218 [==================>...........] - ETA: 19s  batch_loss: 0.4967 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.4963 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.4961 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.4962 [Training] 143/218 [==================>...........] - ETA: 18s  batch_loss: 0.4963 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.4967 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.4964 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.4965 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.4970 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.4970 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.4971 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.4971 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.4967 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.4966 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.4969 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.4969 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.4972 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.4969 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.4968 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.4967 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.4960 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.4954 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.4959 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.4958 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.4958 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.4960 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.4960 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.4964 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.4958 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.4954 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.4951 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.4947 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.4951 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.4943 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.4941 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.4934 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.4927 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.4921 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.4919 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.4917 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.4918 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.4916 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.4915 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.4917 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.4914 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.4915 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.4914 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.4914 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.4918 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.4920 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.4913 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.4909 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.4905 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.4903 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.4906 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.4900 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.4903 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.4903 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.4900 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.4898 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.4898 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.4896 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.4893 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.4886 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.4885 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.4882 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.4882 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 0.4877 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.4878 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.4880 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.4880 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 0.4881 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.4878 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.4877 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.4881 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 0.4879 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.4878 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.4873 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.4872 [Training] 218/218 [==============================] 250.8ms/step  batch_loss: 0.4871 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:21:41 - INFO - root -   The F1-score is 0.5680022182171081
08/22/2022 16:21:41 - INFO - root -   the best eval f1 is 0.5680, saving model !!
08/22/2022 16:21:43 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:12  batch_loss: 0.3907 [Training] 2/218 [..............................] - ETA: 1:34  batch_loss: 0.3881 [Training] 3/218 [..............................] - ETA: 1:20  batch_loss: 0.3913 [Training] 4/218 [..............................] - ETA: 1:13  batch_loss: 0.4141 [Training] 5/218 [..............................] - ETA: 1:09  batch_loss: 0.3970 [Training] 6/218 [..............................] - ETA: 1:06  batch_loss: 0.4138 [Training] 7/218 [..............................] - ETA: 1:03  batch_loss: 0.4083 [Training] 8/218 [>.............................] - ETA: 1:02  batch_loss: 0.4133 [Training] 9/218 [>.............................] - ETA: 1:00  batch_loss: 0.4182 [Training] 10/218 [>.............................] - ETA: 59s  batch_loss: 0.4247 [Training] 11/218 [>.............................] - ETA: 58s  batch_loss: 0.4262 [Training] 12/218 [>.............................] - ETA: 57s  batch_loss: 0.4302 [Training] 13/218 [>.............................] - ETA: 56s  batch_loss: 0.4267 [Training] 14/218 [>.............................] - ETA: 56s  batch_loss: 0.4339 [Training] 15/218 [=>............................] - ETA: 55s  batch_loss: 0.4341 [Training] 16/218 [=>............................] - ETA: 54s  batch_loss: 0.4345 [Training] 17/218 [=>............................] - ETA: 54s  batch_loss: 0.4295 [Training] 18/218 [=>............................] - ETA: 53s  batch_loss: 0.4266 [Training] 19/218 [=>............................] - ETA: 53s  batch_loss: 0.4265 [Training] 20/218 [=>............................] - ETA: 52s  batch_loss: 0.4242 [Training] 21/218 [=>............................] - ETA: 52s  batch_loss: 0.4269 [Training] 22/218 [==>...........................] - ETA: 52s  batch_loss: 0.4303 [Training] 23/218 [==>...........................] - ETA: 51s  batch_loss: 0.4295 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.4311 [Training] 25/218 [==>...........................] - ETA: 50s  batch_loss: 0.4287 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 0.4329 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 0.4319 [Training] 28/218 [==>...........................] - ETA: 49s  batch_loss: 0.4332 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 0.4312 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.4322 [Training] 31/218 [===>..........................] - ETA: 48s  batch_loss: 0.4320 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.4338 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.4318 [Training] 34/218 [===>..........................] - ETA: 47s  batch_loss: 0.4304 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.4315 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.4310 [Training] 37/218 [====>.........................] - ETA: 46s  batch_loss: 0.4289 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.4273 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.4278 [Training] 40/218 [====>.........................] - ETA: 45s  batch_loss: 0.4265 [Training] 41/218 [====>.........................] - ETA: 45s  batch_loss: 0.4265 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.4260 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.4256 [Training] 44/218 [=====>........................] - ETA: 44s  batch_loss: 0.4253 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.4241 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.4235 [Training] 47/218 [=====>........................] - ETA: 43s  batch_loss: 0.4242 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 0.4246 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.4266 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.4281 [Training] 51/218 [======>.......................] - ETA: 42s  batch_loss: 0.4276 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.4278 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.4275 [Training] 54/218 [======>.......................] - ETA: 41s  batch_loss: 0.4277 [Training] 55/218 [======>.......................] - ETA: 41s  batch_loss: 0.4271 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.4279 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.4277 [Training] 58/218 [======>.......................] - ETA: 40s  batch_loss: 0.4279 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 0.4275 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.4279 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.4300 [Training] 62/218 [=======>......................] - ETA: 39s  batch_loss: 0.4298 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.4291 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.4289 [Training] 65/218 [=======>......................] - ETA: 38s  batch_loss: 0.4280 [Training] 66/218 [========>.....................] - ETA: 38s  batch_loss: 0.4287 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.4287 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.4284 [Training] 69/218 [========>.....................] - ETA: 37s  batch_loss: 0.4276 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 0.4267 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.4254 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.4257 [Training] 73/218 [=========>....................] - ETA: 36s  batch_loss: 0.4246 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.4244 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.4249 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.4258 [Training] 77/218 [=========>....................] - ETA: 35s  batch_loss: 0.4260 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.4259 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.4257 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.4250 [Training] 81/218 [==========>...................] - ETA: 34s  batch_loss: 0.4248 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.4243 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.4240 [Training] 84/218 [==========>...................] - ETA: 33s  batch_loss: 0.4235 [Training] 85/218 [==========>...................] - ETA: 33s  batch_loss: 0.4235 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.4229 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.4226 [Training] 88/218 [===========>..................] - ETA: 32s  batch_loss: 0.4225 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.4226 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.4223 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.4225 [Training] 92/218 [===========>..................] - ETA: 31s  batch_loss: 0.4238 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.4234 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.4233 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.4227 [Training] 96/218 [============>.................] - ETA: 30s  batch_loss: 0.4224 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.4227 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.4227 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.4227 [Training] 100/218 [============>.................] - ETA: 29s  batch_loss: 0.4233 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.4230 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.4230 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.4234 [Training] 104/218 [=============>................] - ETA: 28s  batch_loss: 0.4229 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.4226 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.4222 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.4215 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 0.4218 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.4216 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.4214 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.4209 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 0.4212 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.4217 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.4219 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.4215 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.4209 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.4206 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.4204 [Training] 119/218 [===============>..............] - ETA: 25s  batch_loss: 0.4196 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.4192 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.4188 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.4186 [Training] 123/218 [===============>..............] - ETA: 23s  batch_loss: 0.4184 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.4183 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.4185 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.4179 [Training] 127/218 [================>.............] - ETA: 22s  batch_loss: 0.4183 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.4182 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.4177 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.4169 [Training] 131/218 [=================>............] - ETA: 21s  batch_loss: 0.4170 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.4168 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.4174 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.4175 [Training] 135/218 [=================>............] - ETA: 20s  batch_loss: 0.4171 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.4168 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.4166 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.4161 [Training] 139/218 [==================>...........] - ETA: 19s  batch_loss: 0.4165 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.4163 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.4159 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.4155 [Training] 143/218 [==================>...........] - ETA: 18s  batch_loss: 0.4148 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.4146 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.4142 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.4139 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.4138 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.4141 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.4139 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.4136 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.4140 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.4141 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.4136 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.4135 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.4135 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.4129 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.4128 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.4127 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.4125 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.4126 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.4122 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.4121 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.4121 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.4126 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.4128 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.4125 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.4128 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.4128 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.4129 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.4126 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.4125 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.4124 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.4125 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.4124 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.4122 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.4126 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.4121 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.4123 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.4124 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.4124 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.4122 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.4122 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.4126 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.4122 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.4120 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.4121 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.4126 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.4127 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.4126 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.4128 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.4126 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.4129 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.4128 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.4132 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.4132 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.4132 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.4136 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.4134 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.4135 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.4133 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.4130 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.4129 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.4127 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.4124 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.4123 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 0.4122 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.4123 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.4125 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.4125 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 0.4125 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.4124 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.4129 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.4126 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 0.4124 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.4125 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.4122 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.4124 [Training] 218/218 [==============================] 251.2ms/step  batch_loss: 0.4124 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:22:44 - INFO - root -   The F1-score is 0.5924154424325248
08/22/2022 16:22:44 - INFO - root -   the best eval f1 is 0.5924, saving model !!
08/22/2022 16:22:46 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:13  batch_loss: 0.3644 [Training] 2/218 [..............................] - ETA: 1:35  batch_loss: 0.3759 [Training] 3/218 [..............................] - ETA: 1:21  batch_loss: 0.3854 [Training] 4/218 [..............................] - ETA: 1:13  batch_loss: 0.3700 [Training] 5/218 [..............................] - ETA: 1:09  batch_loss: 0.3580 [Training] 6/218 [..............................] - ETA: 1:06  batch_loss: 0.3656 [Training] 7/218 [..............................] - ETA: 1:04  batch_loss: 0.3623 [Training] 8/218 [>.............................] - ETA: 1:02  batch_loss: 0.3592 [Training] 9/218 [>.............................] - ETA: 1:00  batch_loss: 0.3628 [Training] 10/218 [>.............................] - ETA: 59s  batch_loss: 0.3690 [Training] 11/218 [>.............................] - ETA: 58s  batch_loss: 0.3632 [Training] 12/218 [>.............................] - ETA: 57s  batch_loss: 0.3676 [Training] 13/218 [>.............................] - ETA: 57s  batch_loss: 0.3666 [Training] 14/218 [>.............................] - ETA: 56s  batch_loss: 0.3664 [Training] 15/218 [=>............................] - ETA: 55s  batch_loss: 0.3659 [Training] 16/218 [=>............................] - ETA: 55s  batch_loss: 0.3631 [Training] 17/218 [=>............................] - ETA: 54s  batch_loss: 0.3660 [Training] 18/218 [=>............................] - ETA: 54s  batch_loss: 0.3681 [Training] 19/218 [=>............................] - ETA: 53s  batch_loss: 0.3685 [Training] 20/218 [=>............................] - ETA: 53s  batch_loss: 0.3690 [Training] 21/218 [=>............................] - ETA: 52s  batch_loss: 0.3681 [Training] 22/218 [==>...........................] - ETA: 52s  batch_loss: 0.3693 [Training] 23/218 [==>...........................] - ETA: 51s  batch_loss: 0.3695 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.3679 [Training] 25/218 [==>...........................] - ETA: 51s  batch_loss: 0.3667 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 0.3678 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 0.3672 [Training] 28/218 [==>...........................] - ETA: 50s  batch_loss: 0.3652 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 0.3656 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.3653 [Training] 31/218 [===>..........................] - ETA: 49s  batch_loss: 0.3671 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.3674 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.3684 [Training] 34/218 [===>..........................] - ETA: 48s  batch_loss: 0.3677 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.3665 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.3656 [Training] 37/218 [====>.........................] - ETA: 47s  batch_loss: 0.3674 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.3689 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.3686 [Training] 40/218 [====>.........................] - ETA: 46s  batch_loss: 0.3672 [Training] 41/218 [====>.........................] - ETA: 45s  batch_loss: 0.3658 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.3655 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.3660 [Training] 44/218 [=====>........................] - ETA: 44s  batch_loss: 0.3656 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.3644 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.3654 [Training] 47/218 [=====>........................] - ETA: 44s  batch_loss: 0.3658 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 0.3660 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.3660 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.3662 [Training] 51/218 [======>.......................] - ETA: 43s  batch_loss: 0.3653 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.3648 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.3632 [Training] 54/218 [======>.......................] - ETA: 42s  batch_loss: 0.3639 [Training] 55/218 [======>.......................] - ETA: 41s  batch_loss: 0.3643 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.3643 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.3630 [Training] 58/218 [======>.......................] - ETA: 40s  batch_loss: 0.3629 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 0.3625 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.3630 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.3626 [Training] 62/218 [=======>......................] - ETA: 39s  batch_loss: 0.3633 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.3630 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.3638 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 0.3627 [Training] 66/218 [========>.....................] - ETA: 38s  batch_loss: 0.3637 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.3642 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.3643 [Training] 69/218 [========>.....................] - ETA: 37s  batch_loss: 0.3642 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 0.3638 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.3640 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.3640 [Training] 73/218 [=========>....................] - ETA: 36s  batch_loss: 0.3639 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.3646 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.3638 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.3639 [Training] 77/218 [=========>....................] - ETA: 35s  batch_loss: 0.3630 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.3626 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.3624 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.3621 [Training] 81/218 [==========>...................] - ETA: 34s  batch_loss: 0.3619 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.3618 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.3626 [Training] 84/218 [==========>...................] - ETA: 33s  batch_loss: 0.3623 [Training] 85/218 [==========>...................] - ETA: 33s  batch_loss: 0.3633 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.3636 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.3638 [Training] 88/218 [===========>..................] - ETA: 32s  batch_loss: 0.3635 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.3631 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.3626 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.3615 [Training] 92/218 [===========>..................] - ETA: 31s  batch_loss: 0.3614 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.3619 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.3625 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.3630 [Training] 96/218 [============>.................] - ETA: 30s  batch_loss: 0.3634 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.3636 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.3641 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.3638 [Training] 100/218 [============>.................] - ETA: 29s  batch_loss: 0.3639 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.3633 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.3632 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.3630 [Training] 104/218 [=============>................] - ETA: 28s  batch_loss: 0.3631 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.3625 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.3626 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.3625 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 0.3620 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.3623 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.3628 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.3626 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 0.3627 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.3634 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.3632 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.3631 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.3634 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.3627 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.3625 [Training] 119/218 [===============>..............] - ETA: 24s  batch_loss: 0.3619 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.3620 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.3617 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.3615 [Training] 123/218 [===============>..............] - ETA: 23s  batch_loss: 0.3611 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.3615 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.3616 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.3613 [Training] 127/218 [================>.............] - ETA: 22s  batch_loss: 0.3613 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.3611 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.3606 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.3606 [Training] 131/218 [=================>............] - ETA: 21s  batch_loss: 0.3607 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.3602 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.3597 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.3592 [Training] 135/218 [=================>............] - ETA: 20s  batch_loss: 0.3596 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.3596 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.3598 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.3599 [Training] 139/218 [==================>...........] - ETA: 19s  batch_loss: 0.3595 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.3596 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.3591 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.3591 [Training] 143/218 [==================>...........] - ETA: 18s  batch_loss: 0.3588 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.3593 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.3588 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.3589 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.3589 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.3588 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.3587 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.3589 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.3586 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.3586 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.3585 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.3583 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.3581 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.3585 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.3587 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.3581 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.3575 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.3574 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.3574 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.3569 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.3568 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.3564 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.3564 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.3565 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.3563 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.3565 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.3567 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.3569 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.3571 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.3571 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.3569 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.3567 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.3568 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.3569 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.3570 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.3570 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.3569 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.3568 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.3565 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.3568 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.3567 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.3566 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.3565 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.3564 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.3565 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.3565 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.3566 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.3565 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.3565 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.3563 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.3563 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.3560 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.3562 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.3561 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.3559 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.3561 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.3562 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.3563 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.3566 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.3563 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.3561 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.3564 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.3566 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 0.3568 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.3568 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.3567 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.3562 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 0.3561 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.3565 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.3565 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.3567 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 0.3564 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.3563 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.3565 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.3562 [Training] 218/218 [==============================] 251.4ms/step  batch_loss: 0.3562 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:23:47 - INFO - root -   The F1-score is 0.6200292592100013
08/22/2022 16:23:47 - INFO - root -   the best eval f1 is 0.6200, saving model !!
08/22/2022 16:23:49 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:21  batch_loss: 0.3386 [Training] 2/218 [..............................] - ETA: 1:38  batch_loss: 0.3294 [Training] 3/218 [..............................] - ETA: 1:22  batch_loss: 0.3233 [Training] 4/218 [..............................] - ETA: 1:15  batch_loss: 0.3211 [Training] 5/218 [..............................] - ETA: 1:10  batch_loss: 0.3154 [Training] 6/218 [..............................] - ETA: 1:07  batch_loss: 0.3125 [Training] 7/218 [..............................] - ETA: 1:05  batch_loss: 0.3182 [Training] 8/218 [>.............................] - ETA: 1:03  batch_loss: 0.3263 [Training] 9/218 [>.............................] - ETA: 1:01  batch_loss: 0.3257 [Training] 10/218 [>.............................] - ETA: 1:00  batch_loss: 0.3233 [Training] 11/218 [>.............................] - ETA: 59s  batch_loss: 0.3226 [Training] 12/218 [>.............................] - ETA: 58s  batch_loss: 0.3211 [Training] 13/218 [>.............................] - ETA: 57s  batch_loss: 0.3244 [Training] 14/218 [>.............................] - ETA: 56s  batch_loss: 0.3209 [Training] 15/218 [=>............................] - ETA: 56s  batch_loss: 0.3207 [Training] 16/218 [=>............................] - ETA: 55s  batch_loss: 0.3251 [Training] 17/218 [=>............................] - ETA: 54s  batch_loss: 0.3277 [Training] 18/218 [=>............................] - ETA: 54s  batch_loss: 0.3260 [Training] 19/218 [=>............................] - ETA: 53s  batch_loss: 0.3255 [Training] 20/218 [=>............................] - ETA: 53s  batch_loss: 0.3262 [Training] 21/218 [=>............................] - ETA: 52s  batch_loss: 0.3268 [Training] 22/218 [==>...........................] - ETA: 52s  batch_loss: 0.3286 [Training] 23/218 [==>...........................] - ETA: 52s  batch_loss: 0.3308 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.3301 [Training] 25/218 [==>...........................] - ETA: 51s  batch_loss: 0.3278 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 0.3302 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 0.3346 [Training] 28/218 [==>...........................] - ETA: 50s  batch_loss: 0.3352 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 0.3352 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.3346 [Training] 31/218 [===>..........................] - ETA: 49s  batch_loss: 0.3332 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.3334 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.3325 [Training] 34/218 [===>..........................] - ETA: 48s  batch_loss: 0.3328 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.3321 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.3338 [Training] 37/218 [====>.........................] - ETA: 47s  batch_loss: 0.3350 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.3339 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.3344 [Training] 40/218 [====>.........................] - ETA: 46s  batch_loss: 0.3327 [Training] 41/218 [====>.........................] - ETA: 45s  batch_loss: 0.3311 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.3320 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.3332 [Training] 44/218 [=====>........................] - ETA: 45s  batch_loss: 0.3325 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.3318 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.3314 [Training] 47/218 [=====>........................] - ETA: 44s  batch_loss: 0.3324 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 0.3314 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.3315 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.3313 [Training] 51/218 [======>.......................] - ETA: 43s  batch_loss: 0.3309 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.3287 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.3276 [Training] 54/218 [======>.......................] - ETA: 42s  batch_loss: 0.3272 [Training] 55/218 [======>.......................] - ETA: 41s  batch_loss: 0.3259 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.3261 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.3250 [Training] 58/218 [======>.......................] - ETA: 41s  batch_loss: 0.3249 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 0.3243 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.3235 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.3227 [Training] 62/218 [=======>......................] - ETA: 40s  batch_loss: 0.3219 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.3227 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.3228 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 0.3224 [Training] 66/218 [========>.....................] - ETA: 38s  batch_loss: 0.3219 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.3225 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.3219 [Training] 69/218 [========>.....................] - ETA: 38s  batch_loss: 0.3212 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 0.3208 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.3207 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.3202 [Training] 73/218 [=========>....................] - ETA: 37s  batch_loss: 0.3197 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.3197 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.3206 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.3205 [Training] 77/218 [=========>....................] - ETA: 35s  batch_loss: 0.3205 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.3209 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.3215 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.3211 [Training] 81/218 [==========>...................] - ETA: 34s  batch_loss: 0.3215 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.3213 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.3213 [Training] 84/218 [==========>...................] - ETA: 34s  batch_loss: 0.3215 [Training] 85/218 [==========>...................] - ETA: 33s  batch_loss: 0.3212 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.3212 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.3206 [Training] 88/218 [===========>..................] - ETA: 33s  batch_loss: 0.3200 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.3215 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.3214 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.3209 [Training] 92/218 [===========>..................] - ETA: 32s  batch_loss: 0.3209 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.3203 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.3204 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.3206 [Training] 96/218 [============>.................] - ETA: 31s  batch_loss: 0.3205 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.3206 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.3207 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.3205 [Training] 100/218 [============>.................] - ETA: 29s  batch_loss: 0.3208 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.3201 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.3209 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.3202 [Training] 104/218 [=============>................] - ETA: 28s  batch_loss: 0.3199 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.3203 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.3202 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.3200 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 0.3202 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.3198 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.3198 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.3198 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 0.3196 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.3191 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.3194 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.3191 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.3187 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.3188 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.3189 [Training] 119/218 [===============>..............] - ETA: 25s  batch_loss: 0.3187 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.3189 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.3193 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.3193 [Training] 123/218 [===============>..............] - ETA: 24s  batch_loss: 0.3189 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.3185 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.3190 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.3183 [Training] 127/218 [================>.............] - ETA: 23s  batch_loss: 0.3180 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.3176 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.3174 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.3180 [Training] 131/218 [=================>............] - ETA: 22s  batch_loss: 0.3182 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.3186 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.3188 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.3187 [Training] 135/218 [=================>............] - ETA: 21s  batch_loss: 0.3183 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.3187 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.3193 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.3192 [Training] 139/218 [==================>...........] - ETA: 19s  batch_loss: 0.3188 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.3183 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.3178 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.3177 [Training] 143/218 [==================>...........] - ETA: 18s  batch_loss: 0.3178 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.3178 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.3175 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.3176 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.3172 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.3171 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.3167 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.3165 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.3164 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.3160 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.3161 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.3159 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.3158 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.3159 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.3162 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.3159 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.3161 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.3163 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.3164 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.3163 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.3162 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.3163 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.3162 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.3161 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.3159 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.3163 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.3165 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.3159 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.3156 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.3150 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.3146 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.3146 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.3141 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.3144 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.3142 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.3140 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.3145 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.3144 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.3144 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.3145 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.3147 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.3148 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.3146 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.3145 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.3146 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.3146 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.3146 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.3144 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.3140 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.3140 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.3140 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.3139 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.3141 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.3143 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.3141 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.3140 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.3139 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.3139 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.3139 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.3139 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.3137 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.3136 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.3135 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 0.3135 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.3133 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.3134 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.3134 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 0.3134 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.3130 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.3131 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.3129 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 0.3132 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.3130 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.3129 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.3131 [Training] 218/218 [==============================] 251.3ms/step  batch_loss: 0.3131 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:24:50 - INFO - root -   The F1-score is 0.6474673202614378
08/22/2022 16:24:50 - INFO - root -   the best eval f1 is 0.6475, saving model !!
08/22/2022 16:24:51 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:24  batch_loss: 0.2629 [Training] 2/218 [..............................] - ETA: 1:38  batch_loss: 0.2801 [Training] 3/218 [..............................] - ETA: 1:22  batch_loss: 0.3012 [Training] 4/218 [..............................] - ETA: 1:14  batch_loss: 0.2915 [Training] 5/218 [..............................] - ETA: 1:09  batch_loss: 0.2886 [Training] 6/218 [..............................] - ETA: 1:06  batch_loss: 0.2920 [Training] 7/218 [..............................] - ETA: 1:04  batch_loss: 0.2983 [Training] 8/218 [>.............................] - ETA: 1:02  batch_loss: 0.2998 [Training] 9/218 [>.............................] - ETA: 1:01  batch_loss: 0.2990 [Training] 10/218 [>.............................] - ETA: 59s  batch_loss: 0.2975 [Training] 11/218 [>.............................] - ETA: 58s  batch_loss: 0.2957 [Training] 12/218 [>.............................] - ETA: 57s  batch_loss: 0.2958 [Training] 13/218 [>.............................] - ETA: 57s  batch_loss: 0.2949 [Training] 14/218 [>.............................] - ETA: 56s  batch_loss: 0.2927 [Training] 15/218 [=>............................] - ETA: 55s  batch_loss: 0.2928 [Training] 16/218 [=>............................] - ETA: 54s  batch_loss: 0.2930 [Training] 17/218 [=>............................] - ETA: 54s  batch_loss: 0.2972 [Training] 18/218 [=>............................] - ETA: 53s  batch_loss: 0.2978 [Training] 19/218 [=>............................] - ETA: 53s  batch_loss: 0.2996 [Training] 20/218 [=>............................] - ETA: 52s  batch_loss: 0.2989 [Training] 21/218 [=>............................] - ETA: 52s  batch_loss: 0.3000 [Training] 22/218 [==>...........................] - ETA: 52s  batch_loss: 0.3007 [Training] 23/218 [==>...........................] - ETA: 51s  batch_loss: 0.3033 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.3007 [Training] 25/218 [==>...........................] - ETA: 50s  batch_loss: 0.2971 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 0.2967 [Training] 27/218 [==>...........................] - ETA: 49s  batch_loss: 0.2946 [Training] 28/218 [==>...........................] - ETA: 49s  batch_loss: 0.2930 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 0.2912 [Training] 30/218 [===>..........................] - ETA: 48s  batch_loss: 0.2908 [Training] 31/218 [===>..........................] - ETA: 48s  batch_loss: 0.2907 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.2905 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.2902 [Training] 34/218 [===>..........................] - ETA: 47s  batch_loss: 0.2878 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.2883 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.2871 [Training] 37/218 [====>.........................] - ETA: 46s  batch_loss: 0.2850 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.2848 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.2854 [Training] 40/218 [====>.........................] - ETA: 45s  batch_loss: 0.2834 [Training] 41/218 [====>.........................] - ETA: 45s  batch_loss: 0.2826 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.2815 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.2805 [Training] 44/218 [=====>........................] - ETA: 44s  batch_loss: 0.2801 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.2801 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.2830 [Training] 47/218 [=====>........................] - ETA: 43s  batch_loss: 0.2836 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 0.2832 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.2828 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.2833 [Training] 51/218 [======>.......................] - ETA: 42s  batch_loss: 0.2839 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.2846 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.2846 [Training] 54/218 [======>.......................] - ETA: 42s  batch_loss: 0.2849 [Training] 55/218 [======>.......................] - ETA: 41s  batch_loss: 0.2840 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.2845 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.2836 [Training] 58/218 [======>.......................] - ETA: 40s  batch_loss: 0.2833 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 0.2836 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.2838 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.2831 [Training] 62/218 [=======>......................] - ETA: 39s  batch_loss: 0.2830 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.2830 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.2831 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 0.2825 [Training] 66/218 [========>.....................] - ETA: 38s  batch_loss: 0.2827 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.2834 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.2835 [Training] 69/218 [========>.....................] - ETA: 37s  batch_loss: 0.2823 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 0.2826 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.2828 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.2824 [Training] 73/218 [=========>....................] - ETA: 36s  batch_loss: 0.2829 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.2833 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.2826 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.2832 [Training] 77/218 [=========>....................] - ETA: 35s  batch_loss: 0.2851 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.2854 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.2850 [Training] 80/218 [==========>...................] - ETA: 34s  batch_loss: 0.2850 [Training] 81/218 [==========>...................] - ETA: 34s  batch_loss: 0.2847 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.2858 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.2856 [Training] 84/218 [==========>...................] - ETA: 33s  batch_loss: 0.2853 [Training] 85/218 [==========>...................] - ETA: 33s  batch_loss: 0.2848 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.2847 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.2850 [Training] 88/218 [===========>..................] - ETA: 32s  batch_loss: 0.2860 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.2852 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.2853 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.2851 [Training] 92/218 [===========>..................] - ETA: 31s  batch_loss: 0.2849 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.2852 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.2857 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.2853 [Training] 96/218 [============>.................] - ETA: 30s  batch_loss: 0.2846 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.2843 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.2844 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.2843 [Training] 100/218 [============>.................] - ETA: 29s  batch_loss: 0.2841 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.2840 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.2839 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.2843 [Training] 104/218 [=============>................] - ETA: 28s  batch_loss: 0.2838 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.2833 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.2841 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.2840 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 0.2841 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.2840 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.2839 [Training] 111/218 [==============>...............] - ETA: 26s  batch_loss: 0.2838 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 0.2836 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.2836 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.2841 [Training] 115/218 [==============>...............] - ETA: 25s  batch_loss: 0.2843 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.2840 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.2839 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.2839 [Training] 119/218 [===============>..............] - ETA: 24s  batch_loss: 0.2840 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.2843 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.2846 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.2843 [Training] 123/218 [===============>..............] - ETA: 24s  batch_loss: 0.2841 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.2843 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.2841 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.2837 [Training] 127/218 [================>.............] - ETA: 23s  batch_loss: 0.2835 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.2835 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.2833 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.2835 [Training] 131/218 [=================>............] - ETA: 22s  batch_loss: 0.2836 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.2838 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.2837 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.2840 [Training] 135/218 [=================>............] - ETA: 21s  batch_loss: 0.2836 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.2834 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.2834 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.2836 [Training] 139/218 [==================>...........] - ETA: 20s  batch_loss: 0.2835 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.2835 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.2838 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.2839 [Training] 143/218 [==================>...........] - ETA: 19s  batch_loss: 0.2840 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.2844 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.2845 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.2844 [Training] 147/218 [===================>..........] - ETA: 18s  batch_loss: 0.2849 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.2855 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.2852 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.2851 [Training] 151/218 [===================>..........] - ETA: 17s  batch_loss: 0.2850 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.2849 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.2845 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.2845 [Training] 155/218 [====================>.........] - ETA: 16s  batch_loss: 0.2845 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.2842 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.2840 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.2838 [Training] 159/218 [====================>.........] - ETA: 15s  batch_loss: 0.2838 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.2834 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.2834 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.2835 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.2831 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.2832 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.2829 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.2828 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.2830 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.2829 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.2829 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.2828 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.2833 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.2831 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.2834 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.2834 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.2834 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.2833 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.2833 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.2830 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.2829 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.2825 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.2824 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.2825 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.2823 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.2821 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.2824 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.2821 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.2819 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.2821 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.2818 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.2821 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.2819 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.2823 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.2821 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.2818 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.2820 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.2820 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.2824 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.2823 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.2823 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.2821 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.2820 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.2818 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.2817 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.2814 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.2813 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 0.2812 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.2810 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.2812 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.2810 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 0.2811 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.2810 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.2810 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.2809 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 0.2809 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.2807 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.2806 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.2807 [Training] 218/218 [==============================] 252.9ms/step  batch_loss: 0.2805 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:25:53 - INFO - root -   The F1-score is 0.658604905376785
08/22/2022 16:25:53 - INFO - root -   the best eval f1 is 0.6586, saving model !!
08/22/2022 16:25:55 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:14  batch_loss: 0.2256 [Training] 2/218 [..............................] - ETA: 1:34  batch_loss: 0.2435 [Training] 3/218 [..............................] - ETA: 1:20  batch_loss: 0.2469 [Training] 4/218 [..............................] - ETA: 1:13  batch_loss: 0.2339 [Training] 5/218 [..............................] - ETA: 1:09  batch_loss: 0.2340 [Training] 6/218 [..............................] - ETA: 1:06  batch_loss: 0.2334 [Training] 7/218 [..............................] - ETA: 1:04  batch_loss: 0.2412 [Training] 8/218 [>.............................] - ETA: 1:02  batch_loss: 0.2379 [Training] 9/218 [>.............................] - ETA: 1:01  batch_loss: 0.2431 [Training] 10/218 [>.............................] - ETA: 1:00  batch_loss: 0.2422 [Training] 11/218 [>.............................] - ETA: 59s  batch_loss: 0.2467 [Training] 12/218 [>.............................] - ETA: 58s  batch_loss: 0.2510 [Training] 13/218 [>.............................] - ETA: 57s  batch_loss: 0.2546 [Training] 14/218 [>.............................] - ETA: 56s  batch_loss: 0.2545 [Training] 15/218 [=>............................] - ETA: 56s  batch_loss: 0.2535 [Training] 16/218 [=>............................] - ETA: 55s  batch_loss: 0.2548 [Training] 17/218 [=>............................] - ETA: 54s  batch_loss: 0.2507 [Training] 18/218 [=>............................] - ETA: 54s  batch_loss: 0.2527 [Training] 19/218 [=>............................] - ETA: 53s  batch_loss: 0.2522 [Training] 20/218 [=>............................] - ETA: 53s  batch_loss: 0.2563 [Training] 21/218 [=>............................] - ETA: 53s  batch_loss: 0.2598 [Training] 22/218 [==>...........................] - ETA: 52s  batch_loss: 0.2573 [Training] 23/218 [==>...........................] - ETA: 52s  batch_loss: 0.2579 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.2547 [Training] 25/218 [==>...........................] - ETA: 51s  batch_loss: 0.2570 [Training] 26/218 [==>...........................] - ETA: 51s  batch_loss: 0.2577 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 0.2571 [Training] 28/218 [==>...........................] - ETA: 50s  batch_loss: 0.2565 [Training] 29/218 [==>...........................] - ETA: 50s  batch_loss: 0.2559 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.2550 [Training] 31/218 [===>..........................] - ETA: 49s  batch_loss: 0.2537 [Training] 32/218 [===>..........................] - ETA: 49s  batch_loss: 0.2540 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.2555 [Training] 34/218 [===>..........................] - ETA: 48s  batch_loss: 0.2570 [Training] 35/218 [===>..........................] - ETA: 48s  batch_loss: 0.2569 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.2570 [Training] 37/218 [====>.........................] - ETA: 47s  batch_loss: 0.2580 [Training] 38/218 [====>.........................] - ETA: 47s  batch_loss: 0.2570 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.2557 [Training] 40/218 [====>.........................] - ETA: 46s  batch_loss: 0.2546 [Training] 41/218 [====>.........................] - ETA: 46s  batch_loss: 0.2529 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.2534 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.2528 [Training] 44/218 [=====>........................] - ETA: 45s  batch_loss: 0.2537 [Training] 45/218 [=====>........................] - ETA: 45s  batch_loss: 0.2533 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.2543 [Training] 47/218 [=====>........................] - ETA: 44s  batch_loss: 0.2547 [Training] 48/218 [=====>........................] - ETA: 44s  batch_loss: 0.2545 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.2543 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.2535 [Training] 51/218 [======>.......................] - ETA: 43s  batch_loss: 0.2543 [Training] 52/218 [======>.......................] - ETA: 43s  batch_loss: 0.2539 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.2540 [Training] 54/218 [======>.......................] - ETA: 42s  batch_loss: 0.2556 [Training] 55/218 [======>.......................] - ETA: 42s  batch_loss: 0.2560 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.2552 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.2550 [Training] 58/218 [======>.......................] - ETA: 41s  batch_loss: 0.2549 [Training] 59/218 [=======>......................] - ETA: 41s  batch_loss: 0.2554 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.2552 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.2544 [Training] 62/218 [=======>......................] - ETA: 40s  batch_loss: 0.2535 [Training] 63/218 [=======>......................] - ETA: 40s  batch_loss: 0.2533 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.2532 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 0.2535 [Training] 66/218 [========>.....................] - ETA: 39s  batch_loss: 0.2528 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.2520 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.2523 [Training] 69/218 [========>.....................] - ETA: 38s  batch_loss: 0.2523 [Training] 70/218 [========>.....................] - ETA: 38s  batch_loss: 0.2519 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.2521 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.2522 [Training] 73/218 [=========>....................] - ETA: 37s  batch_loss: 0.2516 [Training] 74/218 [=========>....................] - ETA: 37s  batch_loss: 0.2514 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.2513 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.2523 [Training] 77/218 [=========>....................] - ETA: 36s  batch_loss: 0.2519 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.2517 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.2512 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.2516 [Training] 81/218 [==========>...................] - ETA: 35s  batch_loss: 0.2519 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.2520 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.2522 [Training] 84/218 [==========>...................] - ETA: 34s  batch_loss: 0.2516 [Training] 85/218 [==========>...................] - ETA: 34s  batch_loss: 0.2514 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.2519 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.2521 [Training] 88/218 [===========>..................] - ETA: 33s  batch_loss: 0.2522 [Training] 89/218 [===========>..................] - ETA: 33s  batch_loss: 0.2526 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.2524 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.2520 [Training] 92/218 [===========>..................] - ETA: 32s  batch_loss: 0.2527 [Training] 93/218 [===========>..................] - ETA: 32s  batch_loss: 0.2527 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.2528 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.2526 [Training] 96/218 [============>.................] - ETA: 31s  batch_loss: 0.2525 [Training] 97/218 [============>.................] - ETA: 31s  batch_loss: 0.2520 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.2513 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.2510 [Training] 100/218 [============>.................] - ETA: 30s  batch_loss: 0.2511 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.2512 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.2513 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.2514 [Training] 104/218 [=============>................] - ETA: 29s  batch_loss: 0.2513 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.2509 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.2508 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.2504 [Training] 108/218 [=============>................] - ETA: 28s  batch_loss: 0.2507 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.2504 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.2506 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.2502 [Training] 112/218 [==============>...............] - ETA: 27s  batch_loss: 0.2505 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.2503 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.2500 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.2501 [Training] 116/218 [==============>...............] - ETA: 26s  batch_loss: 0.2496 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.2494 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.2491 [Training] 119/218 [===============>..............] - ETA: 25s  batch_loss: 0.2487 [Training] 120/218 [===============>..............] - ETA: 25s  batch_loss: 0.2489 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.2492 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.2494 [Training] 123/218 [===============>..............] - ETA: 24s  batch_loss: 0.2494 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.2495 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.2496 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.2496 [Training] 127/218 [================>.............] - ETA: 23s  batch_loss: 0.2496 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.2503 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.2502 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.2504 [Training] 131/218 [=================>............] - ETA: 22s  batch_loss: 0.2503 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.2503 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.2504 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.2507 [Training] 135/218 [=================>............] - ETA: 21s  batch_loss: 0.2504 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.2506 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.2511 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.2509 [Training] 139/218 [==================>...........] - ETA: 20s  batch_loss: 0.2508 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.2513 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.2515 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.2516 [Training] 143/218 [==================>...........] - ETA: 19s  batch_loss: 0.2520 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.2524 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.2523 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.2522 [Training] 147/218 [===================>..........] - ETA: 18s  batch_loss: 0.2521 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.2519 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.2525 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.2526 [Training] 151/218 [===================>..........] - ETA: 17s  batch_loss: 0.2523 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.2523 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.2520 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.2523 [Training] 155/218 [====================>.........] - ETA: 16s  batch_loss: 0.2525 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.2526 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.2528 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.2529 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.2529 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.2529 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.2528 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.2528 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.2526 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.2524 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.2523 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.2523 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.2525 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.2523 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.2519 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.2518 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.2517 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.2516 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.2515 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.2512 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.2514 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.2513 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.2514 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.2516 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.2513 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.2510 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.2510 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.2508 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.2512 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.2510 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.2513 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.2516 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.2516 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.2517 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.2518 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.2514 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.2513 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.2512 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.2513 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.2513 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.2511 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.2511 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.2511 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.2511 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.2514 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.2514 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.2513 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.2516 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.2518 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.2516 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.2517 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 0.2517 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.2517 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.2517 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.2516 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 0.2517 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.2518 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.2516 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.2514 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 0.2512 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.2514 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.2511 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.2509 [Training] 218/218 [==============================] 252.7ms/step  batch_loss: 0.2513 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:26:56 - INFO - root -   The F1-score is 0.6624309392265194
08/22/2022 16:26:56 - INFO - root -   the best eval f1 is 0.6624, saving model !!
08/22/2022 16:26:58 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:20  batch_loss: 0.2019 [Training] 2/218 [..............................] - ETA: 1:36  batch_loss: 0.2160 [Training] 3/218 [..............................] - ETA: 1:21  batch_loss: 0.2244 [Training] 4/218 [..............................] - ETA: 1:14  batch_loss: 0.2224 [Training] 5/218 [..............................] - ETA: 1:09  batch_loss: 0.2206 [Training] 6/218 [..............................] - ETA: 1:06  batch_loss: 0.2340 [Training] 7/218 [..............................] - ETA: 1:04  batch_loss: 0.2372 [Training] 8/218 [>.............................] - ETA: 1:02  batch_loss: 0.2329 [Training] 9/218 [>.............................] - ETA: 1:00  batch_loss: 0.2259 [Training] 10/218 [>.............................] - ETA: 59s  batch_loss: 0.2294 [Training] 11/218 [>.............................] - ETA: 58s  batch_loss: 0.2287 [Training] 12/218 [>.............................] - ETA: 57s  batch_loss: 0.2330 [Training] 13/218 [>.............................] - ETA: 56s  batch_loss: 0.2278 [Training] 14/218 [>.............................] - ETA: 56s  batch_loss: 0.2257 [Training] 15/218 [=>............................] - ETA: 55s  batch_loss: 0.2254 [Training] 16/218 [=>............................] - ETA: 54s  batch_loss: 0.2295 [Training] 17/218 [=>............................] - ETA: 54s  batch_loss: 0.2302 [Training] 18/218 [=>............................] - ETA: 53s  batch_loss: 0.2300 [Training] 19/218 [=>............................] - ETA: 53s  batch_loss: 0.2264 [Training] 20/218 [=>............................] - ETA: 53s  batch_loss: 0.2314 [Training] 21/218 [=>............................] - ETA: 52s  batch_loss: 0.2314 [Training] 22/218 [==>...........................] - ETA: 52s  batch_loss: 0.2309 [Training] 23/218 [==>...........................] - ETA: 51s  batch_loss: 0.2311 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.2292 [Training] 25/218 [==>...........................] - ETA: 51s  batch_loss: 0.2301 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 0.2309 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 0.2321 [Training] 28/218 [==>...........................] - ETA: 50s  batch_loss: 0.2336 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 0.2329 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.2337 [Training] 31/218 [===>..........................] - ETA: 48s  batch_loss: 0.2349 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.2353 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.2359 [Training] 34/218 [===>..........................] - ETA: 47s  batch_loss: 0.2372 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.2362 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.2350 [Training] 37/218 [====>.........................] - ETA: 47s  batch_loss: 0.2338 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.2340 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.2354 [Training] 40/218 [====>.........................] - ETA: 46s  batch_loss: 0.2354 [Training] 41/218 [====>.........................] - ETA: 45s  batch_loss: 0.2359 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.2353 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.2345 [Training] 44/218 [=====>........................] - ETA: 45s  batch_loss: 0.2350 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.2338 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.2340 [Training] 47/218 [=====>........................] - ETA: 44s  batch_loss: 0.2345 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 0.2334 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.2332 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.2328 [Training] 51/218 [======>.......................] - ETA: 42s  batch_loss: 0.2326 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.2324 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.2332 [Training] 54/218 [======>.......................] - ETA: 42s  batch_loss: 0.2328 [Training] 55/218 [======>.......................] - ETA: 41s  batch_loss: 0.2321 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.2331 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.2327 [Training] 58/218 [======>.......................] - ETA: 41s  batch_loss: 0.2316 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 0.2316 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.2317 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.2328 [Training] 62/218 [=======>......................] - ETA: 39s  batch_loss: 0.2330 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.2326 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.2331 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 0.2334 [Training] 66/218 [========>.....................] - ETA: 38s  batch_loss: 0.2326 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.2316 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.2320 [Training] 69/218 [========>.....................] - ETA: 38s  batch_loss: 0.2326 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 0.2325 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.2328 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.2327 [Training] 73/218 [=========>....................] - ETA: 37s  batch_loss: 0.2322 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.2326 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.2325 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.2330 [Training] 77/218 [=========>....................] - ETA: 35s  batch_loss: 0.2324 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.2319 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.2317 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.2320 [Training] 81/218 [==========>...................] - ETA: 34s  batch_loss: 0.2327 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.2320 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.2327 [Training] 84/218 [==========>...................] - ETA: 34s  batch_loss: 0.2324 [Training] 85/218 [==========>...................] - ETA: 33s  batch_loss: 0.2320 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.2318 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.2314 [Training] 88/218 [===========>..................] - ETA: 33s  batch_loss: 0.2311 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.2311 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.2309 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.2305 [Training] 92/218 [===========>..................] - ETA: 32s  batch_loss: 0.2310 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.2305 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.2304 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.2301 [Training] 96/218 [============>.................] - ETA: 31s  batch_loss: 0.2297 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.2291 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.2289 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.2297 [Training] 100/218 [============>.................] - ETA: 29s  batch_loss: 0.2295 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.2292 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.2291 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.2295 [Training] 104/218 [=============>................] - ETA: 28s  batch_loss: 0.2293 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.2298 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.2303 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.2304 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 0.2307 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.2311 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.2311 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.2307 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 0.2306 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.2308 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.2312 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.2312 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.2311 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.2307 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.2307 [Training] 119/218 [===============>..............] - ETA: 25s  batch_loss: 0.2308 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.2309 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.2307 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.2308 [Training] 123/218 [===============>..............] - ETA: 24s  batch_loss: 0.2308 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.2309 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.2312 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.2309 [Training] 127/218 [================>.............] - ETA: 23s  batch_loss: 0.2305 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.2303 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.2302 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.2300 [Training] 131/218 [=================>............] - ETA: 22s  batch_loss: 0.2297 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.2295 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.2295 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.2293 [Training] 135/218 [=================>............] - ETA: 21s  batch_loss: 0.2296 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.2296 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.2297 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.2299 [Training] 139/218 [==================>...........] - ETA: 20s  batch_loss: 0.2300 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.2302 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.2305 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.2309 [Training] 143/218 [==================>...........] - ETA: 18s  batch_loss: 0.2304 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.2302 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.2306 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.2305 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.2302 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.2301 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.2301 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.2301 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.2299 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.2302 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.2308 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.2306 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.2305 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.2302 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.2303 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.2303 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.2308 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.2307 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.2307 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.2306 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.2309 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.2306 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.2308 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.2308 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.2308 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.2308 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.2309 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.2310 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.2306 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.2303 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.2303 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.2304 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.2303 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.2302 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.2299 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.2300 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.2298 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.2297 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.2297 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.2297 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.2296 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.2296 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.2296 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.2294 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.2293 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.2293 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.2294 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.2295 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.2294 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.2293 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.2292 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.2291 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.2291 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.2291 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.2293 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.2292 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.2292 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.2292 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.2291 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.2290 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.2290 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.2291 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.2291 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 0.2289 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.2288 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.2288 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.2289 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 0.2290 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.2293 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.2294 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.2295 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 0.2295 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.2295 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.2297 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.2298 [Training] 218/218 [==============================] 251.7ms/step  batch_loss: 0.2301 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:27:59 - INFO - root -   The F1-score is 0.6812681673764619
08/22/2022 16:27:59 - INFO - root -   the best eval f1 is 0.6813, saving model !!
08/22/2022 16:28:01 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:13  batch_loss: 0.2026 [Training] 2/218 [..............................] - ETA: 1:33  batch_loss: 0.1955 [Training] 3/218 [..............................] - ETA: 1:19  batch_loss: 0.2075 [Training] 4/218 [..............................] - ETA: 1:12  batch_loss: 0.2038 [Training] 5/218 [..............................] - ETA: 1:08  batch_loss: 0.2020 [Training] 6/218 [..............................] - ETA: 1:05  batch_loss: 0.2003 [Training] 7/218 [..............................] - ETA: 1:02  batch_loss: 0.2020 [Training] 8/218 [>.............................] - ETA: 1:01  batch_loss: 0.2011 [Training] 9/218 [>.............................] - ETA: 59s  batch_loss: 0.2004 [Training] 10/218 [>.............................] - ETA: 58s  batch_loss: 0.2083 [Training] 11/218 [>.............................] - ETA: 57s  batch_loss: 0.2072 [Training] 12/218 [>.............................] - ETA: 56s  batch_loss: 0.2016 [Training] 13/218 [>.............................] - ETA: 56s  batch_loss: 0.2019 [Training] 14/218 [>.............................] - ETA: 55s  batch_loss: 0.2038 [Training] 15/218 [=>............................] - ETA: 54s  batch_loss: 0.2028 [Training] 16/218 [=>............................] - ETA: 54s  batch_loss: 0.2053 [Training] 17/218 [=>............................] - ETA: 54s  batch_loss: 0.2038 [Training] 18/218 [=>............................] - ETA: 53s  batch_loss: 0.2016 [Training] 19/218 [=>............................] - ETA: 53s  batch_loss: 0.2047 [Training] 20/218 [=>............................] - ETA: 52s  batch_loss: 0.2086 [Training] 21/218 [=>............................] - ETA: 52s  batch_loss: 0.2061 [Training] 22/218 [==>...........................] - ETA: 51s  batch_loss: 0.2083 [Training] 23/218 [==>...........................] - ETA: 51s  batch_loss: 0.2071 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.2079 [Training] 25/218 [==>...........................] - ETA: 50s  batch_loss: 0.2058 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 0.2035 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 0.2031 [Training] 28/218 [==>...........................] - ETA: 49s  batch_loss: 0.2036 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 0.2023 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.2022 [Training] 31/218 [===>..........................] - ETA: 48s  batch_loss: 0.2033 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.2037 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.2036 [Training] 34/218 [===>..........................] - ETA: 47s  batch_loss: 0.2052 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.2071 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.2063 [Training] 37/218 [====>.........................] - ETA: 46s  batch_loss: 0.2063 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.2066 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.2057 [Training] 40/218 [====>.........................] - ETA: 46s  batch_loss: 0.2054 [Training] 41/218 [====>.........................] - ETA: 45s  batch_loss: 0.2055 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.2062 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.2056 [Training] 44/218 [=====>........................] - ETA: 44s  batch_loss: 0.2049 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.2062 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.2051 [Training] 47/218 [=====>........................] - ETA: 44s  batch_loss: 0.2053 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 0.2061 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.2063 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.2064 [Training] 51/218 [======>.......................] - ETA: 42s  batch_loss: 0.2078 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.2076 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.2079 [Training] 54/218 [======>.......................] - ETA: 42s  batch_loss: 0.2074 [Training] 55/218 [======>.......................] - ETA: 41s  batch_loss: 0.2082 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.2084 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.2081 [Training] 58/218 [======>.......................] - ETA: 41s  batch_loss: 0.2077 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 0.2071 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.2076 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.2079 [Training] 62/218 [=======>......................] - ETA: 39s  batch_loss: 0.2079 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.2082 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.2084 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 0.2092 [Training] 66/218 [========>.....................] - ETA: 38s  batch_loss: 0.2090 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.2088 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.2084 [Training] 69/218 [========>.....................] - ETA: 38s  batch_loss: 0.2090 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 0.2090 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.2094 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.2098 [Training] 73/218 [=========>....................] - ETA: 37s  batch_loss: 0.2092 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.2086 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.2087 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.2083 [Training] 77/218 [=========>....................] - ETA: 36s  batch_loss: 0.2090 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.2094 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.2100 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.2099 [Training] 81/218 [==========>...................] - ETA: 34s  batch_loss: 0.2102 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.2105 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.2110 [Training] 84/218 [==========>...................] - ETA: 34s  batch_loss: 0.2115 [Training] 85/218 [==========>...................] - ETA: 33s  batch_loss: 0.2115 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.2123 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.2125 [Training] 88/218 [===========>..................] - ETA: 33s  batch_loss: 0.2125 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.2121 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.2123 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.2121 [Training] 92/218 [===========>..................] - ETA: 32s  batch_loss: 0.2120 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.2121 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.2126 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.2125 [Training] 96/218 [============>.................] - ETA: 31s  batch_loss: 0.2127 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.2124 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.2127 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.2123 [Training] 100/218 [============>.................] - ETA: 30s  batch_loss: 0.2125 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.2126 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.2123 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.2121 [Training] 104/218 [=============>................] - ETA: 28s  batch_loss: 0.2121 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.2119 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.2118 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.2118 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 0.2119 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.2115 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.2109 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.2106 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 0.2110 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.2113 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.2113 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.2112 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.2113 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.2111 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.2114 [Training] 119/218 [===============>..............] - ETA: 25s  batch_loss: 0.2113 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.2111 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.2110 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.2113 [Training] 123/218 [===============>..............] - ETA: 24s  batch_loss: 0.2116 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.2115 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.2115 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.2118 [Training] 127/218 [================>.............] - ETA: 23s  batch_loss: 0.2118 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.2121 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.2121 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.2117 [Training] 131/218 [=================>............] - ETA: 22s  batch_loss: 0.2118 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.2119 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.2121 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.2123 [Training] 135/218 [=================>............] - ETA: 21s  batch_loss: 0.2124 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.2123 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.2121 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.2120 [Training] 139/218 [==================>...........] - ETA: 20s  batch_loss: 0.2120 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.2120 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.2117 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.2113 [Training] 143/218 [==================>...........] - ETA: 19s  batch_loss: 0.2112 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.2111 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.2106 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.2111 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.2109 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.2109 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.2109 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.2113 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.2114 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.2113 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.2115 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.2112 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.2111 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.2109 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.2108 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.2107 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.2113 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.2116 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.2114 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.2116 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.2115 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.2116 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.2119 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.2120 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.2117 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.2114 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.2111 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.2111 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.2108 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.2107 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.2106 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.2107 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.2107 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.2106 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.2108 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.2105 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.2104 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.2104 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.2105 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.2104 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.2103 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.2102 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.2101 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.2104 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.2103 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.2101 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.2100 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.2099 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.2098 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.2099 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.2099 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.2096 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.2096 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.2098 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.2099 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.2100 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.2100 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.2098 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.2098 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.2098 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.2101 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.2101 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.2100 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 0.2098 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.2098 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.2101 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.2102 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 0.2101 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.2100 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.2100 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.2102 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 0.2102 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.2102 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.2103 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.2102 [Training] 218/218 [==============================] 252.3ms/step  batch_loss: 0.2098 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:29:01 - INFO - root -   The F1-score is 0.6844432533833579
08/22/2022 16:29:02 - INFO - root -   the best eval f1 is 0.6844, saving model !!
08/22/2022 16:29:04 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:25  batch_loss: 0.2104 [Training] 2/218 [..............................] - ETA: 1:38  batch_loss: 0.1981 [Training] 3/218 [..............................] - ETA: 1:23  batch_loss: 0.1858 [Training] 4/218 [..............................] - ETA: 1:15  batch_loss: 0.1922 [Training] 5/218 [..............................] - ETA: 1:11  batch_loss: 0.1950 [Training] 6/218 [..............................] - ETA: 1:07  batch_loss: 0.1953 [Training] 7/218 [..............................] - ETA: 1:05  batch_loss: 0.1963 [Training] 8/218 [>.............................] - ETA: 1:04  batch_loss: 0.1945 [Training] 9/218 [>.............................] - ETA: 1:02  batch_loss: 0.1898 [Training] 10/218 [>.............................] - ETA: 1:01  batch_loss: 0.1866 [Training] 11/218 [>.............................] - ETA: 1:00  batch_loss: 0.1874 [Training] 12/218 [>.............................] - ETA: 59s  batch_loss: 0.1903 [Training] 13/218 [>.............................] - ETA: 58s  batch_loss: 0.1936 [Training] 14/218 [>.............................] - ETA: 57s  batch_loss: 0.1934 [Training] 15/218 [=>............................] - ETA: 56s  batch_loss: 0.1928 [Training] 16/218 [=>............................] - ETA: 56s  batch_loss: 0.1928 [Training] 17/218 [=>............................] - ETA: 55s  batch_loss: 0.1931 [Training] 18/218 [=>............................] - ETA: 55s  batch_loss: 0.1961 [Training] 19/218 [=>............................] - ETA: 54s  batch_loss: 0.1947 [Training] 20/218 [=>............................] - ETA: 54s  batch_loss: 0.1934 [Training] 21/218 [=>............................] - ETA: 53s  batch_loss: 0.1930 [Training] 22/218 [==>...........................] - ETA: 53s  batch_loss: 0.1954 [Training] 23/218 [==>...........................] - ETA: 52s  batch_loss: 0.1955 [Training] 24/218 [==>...........................] - ETA: 52s  batch_loss: 0.1946 [Training] 25/218 [==>...........................] - ETA: 51s  batch_loss: 0.1966 [Training] 26/218 [==>...........................] - ETA: 51s  batch_loss: 0.1972 [Training] 27/218 [==>...........................] - ETA: 51s  batch_loss: 0.1977 [Training] 28/218 [==>...........................] - ETA: 50s  batch_loss: 0.1968 [Training] 29/218 [==>...........................] - ETA: 50s  batch_loss: 0.1960 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.1975 [Training] 31/218 [===>..........................] - ETA: 49s  batch_loss: 0.1963 [Training] 32/218 [===>..........................] - ETA: 49s  batch_loss: 0.1957 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.1951 [Training] 34/218 [===>..........................] - ETA: 48s  batch_loss: 0.1939 [Training] 35/218 [===>..........................] - ETA: 48s  batch_loss: 0.1935 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.1935 [Training] 37/218 [====>.........................] - ETA: 47s  batch_loss: 0.1929 [Training] 38/218 [====>.........................] - ETA: 47s  batch_loss: 0.1923 [Training] 39/218 [====>.........................] - ETA: 47s  batch_loss: 0.1927 [Training] 40/218 [====>.........................] - ETA: 46s  batch_loss: 0.1922 [Training] 41/218 [====>.........................] - ETA: 46s  batch_loss: 0.1927 [Training] 42/218 [====>.........................] - ETA: 46s  batch_loss: 0.1919 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.1921 [Training] 44/218 [=====>........................] - ETA: 45s  batch_loss: 0.1926 [Training] 45/218 [=====>........................] - ETA: 45s  batch_loss: 0.1922 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.1928 [Training] 47/218 [=====>........................] - ETA: 44s  batch_loss: 0.1935 [Training] 48/218 [=====>........................] - ETA: 44s  batch_loss: 0.1938 [Training] 49/218 [=====>........................] - ETA: 44s  batch_loss: 0.1941 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.1946 [Training] 51/218 [======>.......................] - ETA: 43s  batch_loss: 0.1948 [Training] 52/218 [======>.......................] - ETA: 43s  batch_loss: 0.1942 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.1937 [Training] 54/218 [======>.......................] - ETA: 42s  batch_loss: 0.1932 [Training] 55/218 [======>.......................] - ETA: 42s  batch_loss: 0.1931 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.1935 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.1939 [Training] 58/218 [======>.......................] - ETA: 41s  batch_loss: 0.1947 [Training] 59/218 [=======>......................] - ETA: 41s  batch_loss: 0.1954 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.1951 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.1951 [Training] 62/218 [=======>......................] - ETA: 40s  batch_loss: 0.1944 [Training] 63/218 [=======>......................] - ETA: 40s  batch_loss: 0.1937 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.1937 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 0.1937 [Training] 66/218 [========>.....................] - ETA: 39s  batch_loss: 0.1933 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.1939 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.1937 [Training] 69/218 [========>.....................] - ETA: 38s  batch_loss: 0.1938 [Training] 70/218 [========>.....................] - ETA: 38s  batch_loss: 0.1938 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.1931 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.1929 [Training] 73/218 [=========>....................] - ETA: 37s  batch_loss: 0.1932 [Training] 74/218 [=========>....................] - ETA: 37s  batch_loss: 0.1931 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.1925 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.1928 [Training] 77/218 [=========>....................] - ETA: 36s  batch_loss: 0.1930 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.1930 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.1935 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.1939 [Training] 81/218 [==========>...................] - ETA: 35s  batch_loss: 0.1946 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.1950 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.1947 [Training] 84/218 [==========>...................] - ETA: 34s  batch_loss: 0.1950 [Training] 85/218 [==========>...................] - ETA: 34s  batch_loss: 0.1948 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.1946 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.1942 [Training] 88/218 [===========>..................] - ETA: 33s  batch_loss: 0.1938 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.1944 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.1953 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.1951 [Training] 92/218 [===========>..................] - ETA: 32s  batch_loss: 0.1948 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.1947 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.1946 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.1945 [Training] 96/218 [============>.................] - ETA: 31s  batch_loss: 0.1941 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.1941 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.1940 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.1938 [Training] 100/218 [============>.................] - ETA: 30s  batch_loss: 0.1939 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.1937 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.1940 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.1941 [Training] 104/218 [=============>................] - ETA: 28s  batch_loss: 0.1944 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.1943 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.1941 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.1938 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 0.1940 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.1936 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.1933 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.1934 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 0.1933 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.1928 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.1931 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.1925 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.1925 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.1925 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.1923 [Training] 119/218 [===============>..............] - ETA: 24s  batch_loss: 0.1923 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.1923 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.1923 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.1926 [Training] 123/218 [===============>..............] - ETA: 23s  batch_loss: 0.1928 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.1931 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.1930 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.1931 [Training] 127/218 [================>.............] - ETA: 22s  batch_loss: 0.1930 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.1930 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.1932 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.1930 [Training] 131/218 [=================>............] - ETA: 21s  batch_loss: 0.1934 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.1932 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.1931 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.1929 [Training] 135/218 [=================>............] - ETA: 20s  batch_loss: 0.1930 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.1928 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.1927 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.1926 [Training] 139/218 [==================>...........] - ETA: 19s  batch_loss: 0.1927 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.1929 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.1929 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.1928 [Training] 143/218 [==================>...........] - ETA: 18s  batch_loss: 0.1924 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.1922 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.1926 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.1928 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.1929 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.1928 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.1926 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.1927 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.1929 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.1928 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.1929 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.1928 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.1929 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.1929 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.1928 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.1928 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.1927 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.1924 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.1922 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.1927 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.1928 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.1929 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.1932 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.1932 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.1931 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.1932 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.1933 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.1931 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.1930 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.1931 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.1930 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.1930 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.1933 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.1933 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.1934 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.1934 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.1937 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.1937 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.1935 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.1936 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.1934 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.1932 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.1935 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.1934 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.1932 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.1933 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.1934 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.1931 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.1933 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.1930 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.1931 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.1933 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.1932 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.1933 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.1934 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.1937 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.1940 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.1940 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.1941 [Training] 202/218 [==========================>...] - ETA: 3s  batch_loss: 0.1944 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.1943 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.1945 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.1946 [Training] 206/218 [===========================>..] - ETA: 2s  batch_loss: 0.1946 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.1945 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.1945 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.1949 [Training] 210/218 [===========================>..] - ETA: 1s  batch_loss: 0.1952 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.1950 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.1951 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.1950 [Training] 214/218 [============================>.] - ETA: 0s  batch_loss: 0.1951 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.1951 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.1951 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.1950 [Training] 218/218 [==============================] 249.0ms/step  batch_loss: 0.1951 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:30:04 - INFO - root -   The F1-score is 0.6850038202403278
08/22/2022 16:30:04 - INFO - root -   the best eval f1 is 0.6850, saving model !!
08/22/2022 16:30:06 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:19  batch_loss: 0.1625 [Training] 2/218 [..............................] - ETA: 1:36  batch_loss: 0.1800 [Training] 3/218 [..............................] - ETA: 1:22  batch_loss: 0.1671 [Training] 4/218 [..............................] - ETA: 1:14  batch_loss: 0.1710 [Training] 5/218 [..............................] - ETA: 1:10  batch_loss: 0.1655 [Training] 6/218 [..............................] - ETA: 1:07  batch_loss: 0.1693 [Training] 7/218 [..............................] - ETA: 1:04  batch_loss: 0.1777 [Training] 8/218 [>.............................] - ETA: 1:02  batch_loss: 0.1827 [Training] 9/218 [>.............................] - ETA: 1:01  batch_loss: 0.1837 [Training] 10/218 [>.............................] - ETA: 59s  batch_loss: 0.1828 [Training] 11/218 [>.............................] - ETA: 58s  batch_loss: 0.1831 [Training] 12/218 [>.............................] - ETA: 57s  batch_loss: 0.1833 [Training] 13/218 [>.............................] - ETA: 57s  batch_loss: 0.1823 [Training] 14/218 [>.............................] - ETA: 56s  batch_loss: 0.1851 [Training] 15/218 [=>............................] - ETA: 55s  batch_loss: 0.1846 [Training] 16/218 [=>............................] - ETA: 55s  batch_loss: 0.1835 [Training] 17/218 [=>............................] - ETA: 54s  batch_loss: 0.1815 [Training] 18/218 [=>............................] - ETA: 54s  batch_loss: 0.1811 [Training] 19/218 [=>............................] - ETA: 53s  batch_loss: 0.1821 [Training] 20/218 [=>............................] - ETA: 53s  batch_loss: 0.1822 [Training] 21/218 [=>............................] - ETA: 52s  batch_loss: 0.1807 [Training] 22/218 [==>...........................] - ETA: 52s  batch_loss: 0.1800 [Training] 23/218 [==>...........................] - ETA: 51s  batch_loss: 0.1825 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.1835 [Training] 25/218 [==>...........................] - ETA: 51s  batch_loss: 0.1822 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 0.1854 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 0.1857 [Training] 28/218 [==>...........................] - ETA: 50s  batch_loss: 0.1852 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 0.1840 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.1829 [Training] 31/218 [===>..........................] - ETA: 48s  batch_loss: 0.1831 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.1838 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.1834 [Training] 34/218 [===>..........................] - ETA: 48s  batch_loss: 0.1839 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.1833 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.1838 [Training] 37/218 [====>.........................] - ETA: 47s  batch_loss: 0.1830 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.1827 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.1835 [Training] 40/218 [====>.........................] - ETA: 46s  batch_loss: 0.1849 [Training] 41/218 [====>.........................] - ETA: 45s  batch_loss: 0.1837 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.1840 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.1841 [Training] 44/218 [=====>........................] - ETA: 45s  batch_loss: 0.1832 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.1833 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.1838 [Training] 47/218 [=====>........................] - ETA: 44s  batch_loss: 0.1829 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 0.1830 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.1825 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.1828 [Training] 51/218 [======>.......................] - ETA: 43s  batch_loss: 0.1833 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.1836 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.1836 [Training] 54/218 [======>.......................] - ETA: 42s  batch_loss: 0.1831 [Training] 55/218 [======>.......................] - ETA: 42s  batch_loss: 0.1829 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.1835 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.1837 [Training] 58/218 [======>.......................] - ETA: 41s  batch_loss: 0.1840 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 0.1840 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.1842 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.1839 [Training] 62/218 [=======>......................] - ETA: 40s  batch_loss: 0.1831 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.1832 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.1826 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 0.1822 [Training] 66/218 [========>.....................] - ETA: 39s  batch_loss: 0.1818 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.1816 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.1818 [Training] 69/218 [========>.....................] - ETA: 38s  batch_loss: 0.1813 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 0.1811 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.1819 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.1815 [Training] 73/218 [=========>....................] - ETA: 37s  batch_loss: 0.1811 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.1814 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.1817 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.1818 [Training] 77/218 [=========>....................] - ETA: 36s  batch_loss: 0.1817 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.1812 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.1806 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.1804 [Training] 81/218 [==========>...................] - ETA: 35s  batch_loss: 0.1801 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.1796 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.1794 [Training] 84/218 [==========>...................] - ETA: 34s  batch_loss: 0.1800 [Training] 85/218 [==========>...................] - ETA: 34s  batch_loss: 0.1794 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.1794 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.1793 [Training] 88/218 [===========>..................] - ETA: 33s  batch_loss: 0.1793 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.1800 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.1807 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.1807 [Training] 92/218 [===========>..................] - ETA: 32s  batch_loss: 0.1808 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.1810 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.1816 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.1814 [Training] 96/218 [============>.................] - ETA: 31s  batch_loss: 0.1810 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.1805 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.1805 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.1806 [Training] 100/218 [============>.................] - ETA: 30s  batch_loss: 0.1808 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.1803 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.1800 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.1804 [Training] 104/218 [=============>................] - ETA: 29s  batch_loss: 0.1807 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.1808 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.1808 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.1808 [Training] 108/218 [=============>................] - ETA: 28s  batch_loss: 0.1806 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.1810 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.1810 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.1807 [Training] 112/218 [==============>...............] - ETA: 27s  batch_loss: 0.1808 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.1809 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.1809 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.1812 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.1809 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.1807 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.1809 [Training] 119/218 [===============>..............] - ETA: 25s  batch_loss: 0.1815 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.1811 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.1811 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.1808 [Training] 123/218 [===============>..............] - ETA: 24s  batch_loss: 0.1806 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.1807 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.1808 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.1811 [Training] 127/218 [================>.............] - ETA: 23s  batch_loss: 0.1807 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.1805 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.1803 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.1806 [Training] 131/218 [=================>............] - ETA: 22s  batch_loss: 0.1806 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.1804 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.1804 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.1804 [Training] 135/218 [=================>............] - ETA: 21s  batch_loss: 0.1805 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.1806 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.1804 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.1806 [Training] 139/218 [==================>...........] - ETA: 19s  batch_loss: 0.1807 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.1809 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.1808 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.1809 [Training] 143/218 [==================>...........] - ETA: 18s  batch_loss: 0.1812 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.1809 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.1806 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.1806 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.1810 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.1810 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.1810 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.1809 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.1809 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.1807 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.1807 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.1807 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.1806 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.1809 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.1810 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.1810 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.1810 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.1810 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.1811 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.1811 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.1810 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.1813 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.1811 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.1811 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.1810 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.1813 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.1812 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.1812 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.1813 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.1813 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.1814 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.1814 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.1814 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.1816 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.1816 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.1816 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.1816 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.1815 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.1815 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.1817 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.1815 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.1814 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.1816 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.1818 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.1815 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.1813 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.1813 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.1812 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.1812 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.1814 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.1814 [Training] 194/218 [=========================>....] - ETA: 5s  batch_loss: 0.1813 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.1812 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.1814 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.1813 [Training] 198/218 [==========================>...] - ETA: 4s  batch_loss: 0.1817 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.1816 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.1816 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.1814 [Training] 202/218 [==========================>...] - ETA: 3s  batch_loss: 0.1815 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.1815 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.1813 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.1813 [Training] 206/218 [===========================>..] - ETA: 2s  batch_loss: 0.1814 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.1814 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.1817 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.1817 [Training] 210/218 [===========================>..] - ETA: 1s  batch_loss: 0.1818 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.1819 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.1817 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.1818 [Training] 214/218 [============================>.] - ETA: 0s  batch_loss: 0.1818 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.1817 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.1819 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.1818 [Training] 218/218 [==============================] 248.8ms/step  batch_loss: 0.1816 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:31:06 - INFO - root -   The F1-score is 0.6952068824252355
08/22/2022 16:31:06 - INFO - root -   the best eval f1 is 0.6952, saving model !!
08/22/2022 16:31:08 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:24  batch_loss: 0.1757 [Training] 2/218 [..............................] - ETA: 1:38  batch_loss: 0.1765 [Training] 3/218 [..............................] - ETA: 1:24  batch_loss: 0.1695 [Training] 4/218 [..............................] - ETA: 1:16  batch_loss: 0.1879 [Training] 5/218 [..............................] - ETA: 1:11  batch_loss: 0.1854 [Training] 6/218 [..............................] - ETA: 1:07  batch_loss: 0.1911 [Training] 7/218 [..............................] - ETA: 1:03  batch_loss: 0.1874 [Training] 8/218 [>.............................] - ETA: 1:01  batch_loss: 0.1859 [Training] 9/218 [>.............................] - ETA: 1:00  batch_loss: 0.1851 [Training] 10/218 [>.............................] - ETA: 59s  batch_loss: 0.1823 [Training] 11/218 [>.............................] - ETA: 58s  batch_loss: 0.1823 [Training] 12/218 [>.............................] - ETA: 57s  batch_loss: 0.1778 [Training] 13/218 [>.............................] - ETA: 56s  batch_loss: 0.1775 [Training] 14/218 [>.............................] - ETA: 55s  batch_loss: 0.1807 [Training] 15/218 [=>............................] - ETA: 54s  batch_loss: 0.1817 [Training] 16/218 [=>............................] - ETA: 54s  batch_loss: 0.1817 [Training] 17/218 [=>............................] - ETA: 53s  batch_loss: 0.1790 [Training] 18/218 [=>............................] - ETA: 53s  batch_loss: 0.1780 [Training] 19/218 [=>............................] - ETA: 52s  batch_loss: 0.1769 [Training] 20/218 [=>............................] - ETA: 52s  batch_loss: 0.1771 [Training] 21/218 [=>............................] - ETA: 52s  batch_loss: 0.1757 [Training] 22/218 [==>...........................] - ETA: 51s  batch_loss: 0.1748 [Training] 23/218 [==>...........................] - ETA: 51s  batch_loss: 0.1725 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.1724 [Training] 25/218 [==>...........................] - ETA: 50s  batch_loss: 0.1711 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 0.1717 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 0.1716 [Training] 28/218 [==>...........................] - ETA: 49s  batch_loss: 0.1705 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 0.1698 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.1689 [Training] 31/218 [===>..........................] - ETA: 48s  batch_loss: 0.1683 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.1686 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.1679 [Training] 34/218 [===>..........................] - ETA: 47s  batch_loss: 0.1670 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.1667 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.1667 [Training] 37/218 [====>.........................] - ETA: 46s  batch_loss: 0.1675 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.1665 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.1664 [Training] 40/218 [====>.........................] - ETA: 45s  batch_loss: 0.1670 [Training] 41/218 [====>.........................] - ETA: 45s  batch_loss: 0.1691 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.1684 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.1673 [Training] 44/218 [=====>........................] - ETA: 44s  batch_loss: 0.1670 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.1668 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.1670 [Training] 47/218 [=====>........................] - ETA: 43s  batch_loss: 0.1665 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 0.1659 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.1668 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.1675 [Training] 51/218 [======>.......................] - ETA: 42s  batch_loss: 0.1673 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.1666 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.1672 [Training] 54/218 [======>.......................] - ETA: 42s  batch_loss: 0.1674 [Training] 55/218 [======>.......................] - ETA: 41s  batch_loss: 0.1668 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.1667 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.1660 [Training] 58/218 [======>.......................] - ETA: 40s  batch_loss: 0.1657 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 0.1655 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.1654 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.1663 [Training] 62/218 [=======>......................] - ETA: 39s  batch_loss: 0.1661 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.1653 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.1652 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 0.1651 [Training] 66/218 [========>.....................] - ETA: 38s  batch_loss: 0.1652 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.1654 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.1658 [Training] 69/218 [========>.....................] - ETA: 38s  batch_loss: 0.1656 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 0.1658 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.1653 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.1649 [Training] 73/218 [=========>....................] - ETA: 36s  batch_loss: 0.1653 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.1649 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.1650 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.1650 [Training] 77/218 [=========>....................] - ETA: 35s  batch_loss: 0.1657 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.1657 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.1660 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.1656 [Training] 81/218 [==========>...................] - ETA: 34s  batch_loss: 0.1657 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.1659 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.1657 [Training] 84/218 [==========>...................] - ETA: 34s  batch_loss: 0.1656 [Training] 85/218 [==========>...................] - ETA: 33s  batch_loss: 0.1656 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.1656 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.1659 [Training] 88/218 [===========>..................] - ETA: 33s  batch_loss: 0.1658 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.1657 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.1652 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.1651 [Training] 92/218 [===========>..................] - ETA: 32s  batch_loss: 0.1652 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.1656 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.1654 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.1660 [Training] 96/218 [============>.................] - ETA: 31s  batch_loss: 0.1664 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.1663 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.1660 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.1658 [Training] 100/218 [============>.................] - ETA: 29s  batch_loss: 0.1657 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.1664 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.1665 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.1668 [Training] 104/218 [=============>................] - ETA: 28s  batch_loss: 0.1667 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.1665 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.1665 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.1664 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 0.1664 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.1661 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.1659 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.1660 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 0.1659 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.1660 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.1660 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.1659 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.1658 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.1654 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.1653 [Training] 119/218 [===============>..............] - ETA: 25s  batch_loss: 0.1651 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.1651 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.1654 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.1654 [Training] 123/218 [===============>..............] - ETA: 24s  batch_loss: 0.1658 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.1660 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.1659 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.1658 [Training] 127/218 [================>.............] - ETA: 23s  batch_loss: 0.1656 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.1656 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.1655 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.1653 [Training] 131/218 [=================>............] - ETA: 22s  batch_loss: 0.1652 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.1654 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.1654 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.1655 [Training] 135/218 [=================>............] - ETA: 21s  batch_loss: 0.1657 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.1656 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.1660 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.1661 [Training] 139/218 [==================>...........] - ETA: 20s  batch_loss: 0.1661 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.1660 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.1658 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.1658 [Training] 143/218 [==================>...........] - ETA: 18s  batch_loss: 0.1658 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.1660 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.1660 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.1661 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.1660 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.1662 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.1661 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.1662 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.1661 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.1662 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.1659 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.1660 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.1662 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.1663 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.1665 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.1667 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.1667 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.1667 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.1666 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.1670 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.1669 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.1669 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.1670 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.1673 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.1672 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.1670 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.1671 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.1671 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.1675 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.1676 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.1675 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.1673 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.1673 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.1671 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.1672 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.1671 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.1670 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.1671 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.1671 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.1672 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.1673 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.1673 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.1676 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.1676 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.1676 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.1676 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.1678 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.1678 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.1678 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.1677 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.1681 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.1680 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.1681 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.1681 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.1682 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.1681 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.1681 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.1682 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.1682 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.1683 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.1683 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.1682 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.1681 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 0.1681 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.1680 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.1680 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.1680 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 0.1680 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.1683 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.1686 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.1687 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 0.1688 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.1690 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.1692 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.1691 [Training] 218/218 [==============================] 251.9ms/step  batch_loss: 0.1691 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:32:09 - INFO - root -   The F1-score is 0.6914001227579623
08/22/2022 16:32:09 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:16  batch_loss: 0.1659 [Training] 2/218 [..............................] - ETA: 1:34  batch_loss: 0.1596 [Training] 3/218 [..............................] - ETA: 1:20  batch_loss: 0.1619 [Training] 4/218 [..............................] - ETA: 1:13  batch_loss: 0.1544 [Training] 5/218 [..............................] - ETA: 1:09  batch_loss: 0.1543 [Training] 6/218 [..............................] - ETA: 1:06  batch_loss: 0.1531 [Training] 7/218 [..............................] - ETA: 1:04  batch_loss: 0.1552 [Training] 8/218 [>.............................] - ETA: 1:01  batch_loss: 0.1549 [Training] 9/218 [>.............................] - ETA: 1:00  batch_loss: 0.1514 [Training] 10/218 [>.............................] - ETA: 59s  batch_loss: 0.1521 [Training] 11/218 [>.............................] - ETA: 58s  batch_loss: 0.1523 [Training] 12/218 [>.............................] - ETA: 57s  batch_loss: 0.1571 [Training] 13/218 [>.............................] - ETA: 56s  batch_loss: 0.1558 [Training] 14/218 [>.............................] - ETA: 56s  batch_loss: 0.1582 [Training] 15/218 [=>............................] - ETA: 55s  batch_loss: 0.1573 [Training] 16/218 [=>............................] - ETA: 55s  batch_loss: 0.1568 [Training] 17/218 [=>............................] - ETA: 54s  batch_loss: 0.1570 [Training] 18/218 [=>............................] - ETA: 54s  batch_loss: 0.1575 [Training] 19/218 [=>............................] - ETA: 53s  batch_loss: 0.1555 [Training] 20/218 [=>............................] - ETA: 53s  batch_loss: 0.1545 [Training] 21/218 [=>............................] - ETA: 52s  batch_loss: 0.1547 [Training] 22/218 [==>...........................] - ETA: 52s  batch_loss: 0.1542 [Training] 23/218 [==>...........................] - ETA: 51s  batch_loss: 0.1550 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.1563 [Training] 25/218 [==>...........................] - ETA: 51s  batch_loss: 0.1563 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 0.1546 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 0.1534 [Training] 28/218 [==>...........................] - ETA: 49s  batch_loss: 0.1547 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 0.1546 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.1549 [Training] 31/218 [===>..........................] - ETA: 48s  batch_loss: 0.1544 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.1548 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.1553 [Training] 34/218 [===>..........................] - ETA: 47s  batch_loss: 0.1546 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.1538 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.1546 [Training] 37/218 [====>.........................] - ETA: 47s  batch_loss: 0.1539 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.1538 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.1532 [Training] 40/218 [====>.........................] - ETA: 46s  batch_loss: 0.1537 [Training] 41/218 [====>.........................] - ETA: 45s  batch_loss: 0.1527 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.1521 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.1512 [Training] 44/218 [=====>........................] - ETA: 44s  batch_loss: 0.1510 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.1498 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.1505 [Training] 47/218 [=====>........................] - ETA: 44s  batch_loss: 0.1513 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 0.1516 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.1510 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.1516 [Training] 51/218 [======>.......................] - ETA: 42s  batch_loss: 0.1522 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.1523 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.1529 [Training] 54/218 [======>.......................] - ETA: 42s  batch_loss: 0.1523 [Training] 55/218 [======>.......................] - ETA: 41s  batch_loss: 0.1523 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.1532 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.1528 [Training] 58/218 [======>.......................] - ETA: 41s  batch_loss: 0.1522 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 0.1525 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.1526 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.1526 [Training] 62/218 [=======>......................] - ETA: 40s  batch_loss: 0.1526 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.1529 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.1528 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 0.1528 [Training] 66/218 [========>.....................] - ETA: 38s  batch_loss: 0.1529 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.1533 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.1531 [Training] 69/218 [========>.....................] - ETA: 38s  batch_loss: 0.1531 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 0.1534 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.1536 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.1535 [Training] 73/218 [=========>....................] - ETA: 36s  batch_loss: 0.1530 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.1537 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.1543 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.1544 [Training] 77/218 [=========>....................] - ETA: 35s  batch_loss: 0.1543 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.1546 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.1543 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.1543 [Training] 81/218 [==========>...................] - ETA: 34s  batch_loss: 0.1543 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.1544 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.1541 [Training] 84/218 [==========>...................] - ETA: 34s  batch_loss: 0.1540 [Training] 85/218 [==========>...................] - ETA: 33s  batch_loss: 0.1536 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.1540 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.1542 [Training] 88/218 [===========>..................] - ETA: 33s  batch_loss: 0.1541 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.1540 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.1543 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.1539 [Training] 92/218 [===========>..................] - ETA: 31s  batch_loss: 0.1537 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.1540 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.1541 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.1543 [Training] 96/218 [============>.................] - ETA: 30s  batch_loss: 0.1548 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.1546 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.1546 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.1546 [Training] 100/218 [============>.................] - ETA: 29s  batch_loss: 0.1546 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.1544 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.1545 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.1542 [Training] 104/218 [=============>................] - ETA: 28s  batch_loss: 0.1546 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.1545 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.1541 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.1544 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 0.1542 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.1540 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.1543 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.1543 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 0.1545 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.1546 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.1547 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.1546 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.1549 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.1547 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.1547 [Training] 119/218 [===============>..............] - ETA: 25s  batch_loss: 0.1545 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.1548 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.1549 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.1551 [Training] 123/218 [===============>..............] - ETA: 24s  batch_loss: 0.1551 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.1552 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.1550 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.1552 [Training] 127/218 [================>.............] - ETA: 22s  batch_loss: 0.1552 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.1551 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.1550 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.1554 [Training] 131/218 [=================>............] - ETA: 21s  batch_loss: 0.1553 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.1555 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.1556 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.1554 [Training] 135/218 [=================>............] - ETA: 20s  batch_loss: 0.1555 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.1556 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.1560 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.1560 [Training] 139/218 [==================>...........] - ETA: 19s  batch_loss: 0.1565 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.1566 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.1564 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.1566 [Training] 143/218 [==================>...........] - ETA: 18s  batch_loss: 0.1569 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.1571 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.1569 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.1568 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.1567 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.1565 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.1566 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.1566 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.1565 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.1567 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.1566 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.1566 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.1568 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.1569 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.1569 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.1570 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.1571 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.1572 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.1577 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.1580 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.1580 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.1579 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.1578 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.1580 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.1579 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.1579 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.1578 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.1579 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.1580 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.1581 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.1583 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.1582 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.1581 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.1581 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.1580 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.1580 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.1581 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.1579 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.1577 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.1575 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.1575 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.1575 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.1575 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.1574 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.1576 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.1576 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.1578 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.1577 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.1581 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.1584 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.1583 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.1585 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.1583 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.1583 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.1585 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.1586 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.1585 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.1584 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.1585 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.1586 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.1586 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.1587 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.1588 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 0.1589 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.1587 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.1586 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.1585 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 0.1585 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.1586 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.1586 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.1584 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 0.1584 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.1584 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.1583 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.1588 [Training] 218/218 [==============================] 250.8ms/step  batch_loss: 0.1587 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:33:10 - INFO - root -   The F1-score is 0.6983973473335177
08/22/2022 16:33:10 - INFO - root -   the best eval f1 is 0.6984, saving model !!
08/22/2022 16:33:11 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:16  batch_loss: 0.1320 [Training] 2/218 [..............................] - ETA: 1:36  batch_loss: 0.1344 [Training] 3/218 [..............................] - ETA: 1:22  batch_loss: 0.1480 [Training] 4/218 [..............................] - ETA: 1:14  batch_loss: 0.1442 [Training] 5/218 [..............................] - ETA: 1:09  batch_loss: 0.1429 [Training] 6/218 [..............................] - ETA: 1:06  batch_loss: 0.1461 [Training] 7/218 [..............................] - ETA: 1:04  batch_loss: 0.1477 [Training] 8/218 [>.............................] - ETA: 1:02  batch_loss: 0.1428 [Training] 9/218 [>.............................] - ETA: 1:01  batch_loss: 0.1438 [Training] 10/218 [>.............................] - ETA: 59s  batch_loss: 0.1413 [Training] 11/218 [>.............................] - ETA: 58s  batch_loss: 0.1401 [Training] 12/218 [>.............................] - ETA: 58s  batch_loss: 0.1426 [Training] 13/218 [>.............................] - ETA: 57s  batch_loss: 0.1479 [Training] 14/218 [>.............................] - ETA: 56s  batch_loss: 0.1460 [Training] 15/218 [=>............................] - ETA: 55s  batch_loss: 0.1459 [Training] 16/218 [=>............................] - ETA: 55s  batch_loss: 0.1444 [Training] 17/218 [=>............................] - ETA: 54s  batch_loss: 0.1440 [Training] 18/218 [=>............................] - ETA: 54s  batch_loss: 0.1451 [Training] 19/218 [=>............................] - ETA: 53s  batch_loss: 0.1441 [Training] 20/218 [=>............................] - ETA: 53s  batch_loss: 0.1425 [Training] 21/218 [=>............................] - ETA: 52s  batch_loss: 0.1419 [Training] 22/218 [==>...........................] - ETA: 52s  batch_loss: 0.1427 [Training] 23/218 [==>...........................] - ETA: 51s  batch_loss: 0.1417 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.1417 [Training] 25/218 [==>...........................] - ETA: 51s  batch_loss: 0.1418 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 0.1436 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 0.1434 [Training] 28/218 [==>...........................] - ETA: 49s  batch_loss: 0.1433 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 0.1447 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.1459 [Training] 31/218 [===>..........................] - ETA: 48s  batch_loss: 0.1461 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.1465 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.1453 [Training] 34/218 [===>..........................] - ETA: 48s  batch_loss: 0.1448 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.1449 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.1443 [Training] 37/218 [====>.........................] - ETA: 47s  batch_loss: 0.1452 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.1449 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.1442 [Training] 40/218 [====>.........................] - ETA: 46s  batch_loss: 0.1437 [Training] 41/218 [====>.........................] - ETA: 45s  batch_loss: 0.1441 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.1452 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.1466 [Training] 44/218 [=====>........................] - ETA: 45s  batch_loss: 0.1466 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.1472 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.1467 [Training] 47/218 [=====>........................] - ETA: 44s  batch_loss: 0.1469 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 0.1475 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.1476 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.1471 [Training] 51/218 [======>.......................] - ETA: 43s  batch_loss: 0.1470 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.1470 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.1470 [Training] 54/218 [======>.......................] - ETA: 42s  batch_loss: 0.1462 [Training] 55/218 [======>.......................] - ETA: 41s  batch_loss: 0.1460 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.1454 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.1454 [Training] 58/218 [======>.......................] - ETA: 41s  batch_loss: 0.1451 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 0.1451 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.1449 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.1449 [Training] 62/218 [=======>......................] - ETA: 40s  batch_loss: 0.1457 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.1457 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.1462 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 0.1465 [Training] 66/218 [========>.....................] - ETA: 38s  batch_loss: 0.1461 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.1460 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.1461 [Training] 69/218 [========>.....................] - ETA: 38s  batch_loss: 0.1460 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 0.1457 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.1462 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.1463 [Training] 73/218 [=========>....................] - ETA: 37s  batch_loss: 0.1476 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.1483 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.1479 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.1476 [Training] 77/218 [=========>....................] - ETA: 36s  batch_loss: 0.1485 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.1486 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.1487 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.1485 [Training] 81/218 [==========>...................] - ETA: 34s  batch_loss: 0.1492 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.1493 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.1489 [Training] 84/218 [==========>...................] - ETA: 34s  batch_loss: 0.1488 [Training] 85/218 [==========>...................] - ETA: 33s  batch_loss: 0.1489 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.1487 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.1493 [Training] 88/218 [===========>..................] - ETA: 33s  batch_loss: 0.1496 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.1494 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.1494 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.1496 [Training] 92/218 [===========>..................] - ETA: 32s  batch_loss: 0.1494 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.1492 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.1487 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.1486 [Training] 96/218 [============>.................] - ETA: 31s  batch_loss: 0.1489 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.1490 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.1487 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.1484 [Training] 100/218 [============>.................] - ETA: 30s  batch_loss: 0.1489 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.1488 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.1487 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.1489 [Training] 104/218 [=============>................] - ETA: 29s  batch_loss: 0.1493 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.1493 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.1495 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.1496 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 0.1499 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.1497 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.1496 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.1493 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 0.1493 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.1494 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.1494 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.1489 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.1490 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.1489 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.1488 [Training] 119/218 [===============>..............] - ETA: 25s  batch_loss: 0.1489 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.1492 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.1490 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.1491 [Training] 123/218 [===============>..............] - ETA: 24s  batch_loss: 0.1490 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.1492 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.1490 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.1488 [Training] 127/218 [================>.............] - ETA: 23s  batch_loss: 0.1492 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.1492 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.1494 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.1493 [Training] 131/218 [=================>............] - ETA: 22s  batch_loss: 0.1492 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.1494 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.1496 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.1495 [Training] 135/218 [=================>............] - ETA: 21s  batch_loss: 0.1495 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.1496 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.1496 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.1496 [Training] 139/218 [==================>...........] - ETA: 19s  batch_loss: 0.1498 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.1498 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.1500 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.1499 [Training] 143/218 [==================>...........] - ETA: 18s  batch_loss: 0.1495 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.1494 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.1495 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.1498 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.1498 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.1495 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.1496 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.1498 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.1498 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.1496 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.1493 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.1493 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.1495 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.1494 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.1492 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.1493 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.1492 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.1493 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.1492 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.1492 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.1493 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.1493 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.1492 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.1492 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.1490 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.1489 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.1488 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.1487 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.1485 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.1487 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.1489 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.1489 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.1491 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.1490 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.1490 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.1492 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.1491 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.1492 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.1490 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.1491 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.1490 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.1493 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.1493 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.1495 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.1496 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.1499 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.1497 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.1498 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.1497 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.1495 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.1496 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.1497 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.1495 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.1495 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.1496 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.1496 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.1495 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.1493 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.1492 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.1493 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.1492 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.1493 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.1493 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 0.1493 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.1492 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.1495 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.1495 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 0.1495 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.1495 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.1495 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.1497 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 0.1498 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.1496 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.1497 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.1498 [Training] 218/218 [==============================] 251.6ms/step  batch_loss: 0.1496 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:34:12 - INFO - root -   The F1-score is 0.7057852376693955
08/22/2022 16:34:12 - INFO - root -   the best eval f1 is 0.7058, saving model !!
08/22/2022 16:34:14 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:20  batch_loss: 0.1141 [Training] 2/218 [..............................] - ETA: 1:36  batch_loss: 0.1396 [Training] 3/218 [..............................] - ETA: 1:21  batch_loss: 0.1408 [Training] 4/218 [..............................] - ETA: 1:14  batch_loss: 0.1378 [Training] 5/218 [..............................] - ETA: 1:09  batch_loss: 0.1426 [Training] 6/218 [..............................] - ETA: 1:06  batch_loss: 0.1470 [Training] 7/218 [..............................] - ETA: 1:04  batch_loss: 0.1466 [Training] 8/218 [>.............................] - ETA: 1:02  batch_loss: 0.1454 [Training] 9/218 [>.............................] - ETA: 1:01  batch_loss: 0.1467 [Training] 10/218 [>.............................] - ETA: 59s  batch_loss: 0.1411 [Training] 11/218 [>.............................] - ETA: 58s  batch_loss: 0.1408 [Training] 12/218 [>.............................] - ETA: 57s  batch_loss: 0.1405 [Training] 13/218 [>.............................] - ETA: 57s  batch_loss: 0.1428 [Training] 14/218 [>.............................] - ETA: 56s  batch_loss: 0.1412 [Training] 15/218 [=>............................] - ETA: 55s  batch_loss: 0.1438 [Training] 16/218 [=>............................] - ETA: 55s  batch_loss: 0.1424 [Training] 17/218 [=>............................] - ETA: 54s  batch_loss: 0.1409 [Training] 18/218 [=>............................] - ETA: 54s  batch_loss: 0.1403 [Training] 19/218 [=>............................] - ETA: 53s  batch_loss: 0.1412 [Training] 20/218 [=>............................] - ETA: 53s  batch_loss: 0.1402 [Training] 21/218 [=>............................] - ETA: 52s  batch_loss: 0.1413 [Training] 22/218 [==>...........................] - ETA: 52s  batch_loss: 0.1407 [Training] 23/218 [==>...........................] - ETA: 51s  batch_loss: 0.1423 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.1421 [Training] 25/218 [==>...........................] - ETA: 51s  batch_loss: 0.1422 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 0.1434 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 0.1425 [Training] 28/218 [==>...........................] - ETA: 50s  batch_loss: 0.1429 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 0.1449 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.1436 [Training] 31/218 [===>..........................] - ETA: 49s  batch_loss: 0.1440 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.1451 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.1447 [Training] 34/218 [===>..........................] - ETA: 48s  batch_loss: 0.1435 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.1434 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.1443 [Training] 37/218 [====>.........................] - ETA: 47s  batch_loss: 0.1445 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.1439 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.1444 [Training] 40/218 [====>.........................] - ETA: 46s  batch_loss: 0.1442 [Training] 41/218 [====>.........................] - ETA: 45s  batch_loss: 0.1438 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.1439 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.1435 [Training] 44/218 [=====>........................] - ETA: 45s  batch_loss: 0.1437 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.1435 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.1429 [Training] 47/218 [=====>........................] - ETA: 44s  batch_loss: 0.1431 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 0.1429 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.1428 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.1422 [Training] 51/218 [======>.......................] - ETA: 43s  batch_loss: 0.1422 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.1418 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.1422 [Training] 54/218 [======>.......................] - ETA: 42s  batch_loss: 0.1422 [Training] 55/218 [======>.......................] - ETA: 41s  batch_loss: 0.1417 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.1414 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.1414 [Training] 58/218 [======>.......................] - ETA: 41s  batch_loss: 0.1411 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 0.1416 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.1414 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.1415 [Training] 62/218 [=======>......................] - ETA: 40s  batch_loss: 0.1414 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.1412 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.1407 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 0.1402 [Training] 66/218 [========>.....................] - ETA: 38s  batch_loss: 0.1397 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.1398 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.1400 [Training] 69/218 [========>.....................] - ETA: 38s  batch_loss: 0.1397 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 0.1394 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.1395 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.1393 [Training] 73/218 [=========>....................] - ETA: 37s  batch_loss: 0.1398 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.1398 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.1395 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.1401 [Training] 77/218 [=========>....................] - ETA: 36s  batch_loss: 0.1398 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.1398 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.1397 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.1395 [Training] 81/218 [==========>...................] - ETA: 35s  batch_loss: 0.1395 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.1394 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.1394 [Training] 84/218 [==========>...................] - ETA: 34s  batch_loss: 0.1390 [Training] 85/218 [==========>...................] - ETA: 33s  batch_loss: 0.1392 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.1391 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.1391 [Training] 88/218 [===========>..................] - ETA: 33s  batch_loss: 0.1393 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.1392 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.1393 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.1394 [Training] 92/218 [===========>..................] - ETA: 32s  batch_loss: 0.1393 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.1389 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.1392 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.1389 [Training] 96/218 [============>.................] - ETA: 31s  batch_loss: 0.1389 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.1383 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.1386 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.1383 [Training] 100/218 [============>.................] - ETA: 30s  batch_loss: 0.1381 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.1375 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.1375 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.1374 [Training] 104/218 [=============>................] - ETA: 29s  batch_loss: 0.1375 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.1373 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.1373 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.1375 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 0.1372 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.1371 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.1374 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.1371 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 0.1369 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.1368 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.1368 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.1367 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.1369 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.1370 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.1368 [Training] 119/218 [===============>..............] - ETA: 25s  batch_loss: 0.1369 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.1367 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.1366 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.1365 [Training] 123/218 [===============>..............] - ETA: 24s  batch_loss: 0.1363 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.1361 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.1364 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.1364 [Training] 127/218 [================>.............] - ETA: 23s  batch_loss: 0.1365 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.1366 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.1364 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.1366 [Training] 131/218 [=================>............] - ETA: 22s  batch_loss: 0.1367 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.1369 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.1370 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.1371 [Training] 135/218 [=================>............] - ETA: 21s  batch_loss: 0.1373 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.1374 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.1374 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.1376 [Training] 139/218 [==================>...........] - ETA: 20s  batch_loss: 0.1377 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.1380 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.1379 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.1381 [Training] 143/218 [==================>...........] - ETA: 19s  batch_loss: 0.1382 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.1383 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.1382 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.1380 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.1382 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.1381 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.1383 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.1383 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.1385 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.1385 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.1385 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.1386 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.1386 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.1387 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.1389 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.1389 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.1388 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.1390 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.1389 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.1386 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.1387 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.1385 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.1386 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.1387 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.1389 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.1390 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.1392 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.1393 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.1391 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.1392 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.1394 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.1396 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.1396 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.1396 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.1396 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.1397 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.1397 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.1397 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.1395 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.1397 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.1400 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.1399 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.1399 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.1399 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.1399 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.1400 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.1401 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.1401 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.1401 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.1400 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.1401 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.1404 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.1406 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.1407 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.1407 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.1408 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.1407 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.1407 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.1409 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.1409 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.1410 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.1408 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.1408 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 0.1413 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.1411 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.1414 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.1415 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 0.1415 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.1417 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.1416 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.1416 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 0.1415 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.1413 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.1412 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.1413 [Training] 218/218 [==============================] 252.5ms/step  batch_loss: 0.1413 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:35:15 - INFO - root -   The F1-score is 0.6877076411960132
08/22/2022 16:35:15 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:08  batch_loss: 0.1656 [Training] 2/218 [..............................] - ETA: 1:31  batch_loss: 0.1473 [Training] 3/218 [..............................] - ETA: 1:18  batch_loss: 0.1414 [Training] 4/218 [..............................] - ETA: 1:12  batch_loss: 0.1332 [Training] 5/218 [..............................] - ETA: 1:08  batch_loss: 0.1328 [Training] 6/218 [..............................] - ETA: 1:05  batch_loss: 0.1302 [Training] 7/218 [..............................] - ETA: 1:03  batch_loss: 0.1261 [Training] 8/218 [>.............................] - ETA: 1:02  batch_loss: 0.1290 [Training] 9/218 [>.............................] - ETA: 1:00  batch_loss: 0.1282 [Training] 10/218 [>.............................] - ETA: 59s  batch_loss: 0.1310 [Training] 11/218 [>.............................] - ETA: 58s  batch_loss: 0.1282 [Training] 12/218 [>.............................] - ETA: 57s  batch_loss: 0.1278 [Training] 13/218 [>.............................] - ETA: 57s  batch_loss: 0.1260 [Training] 14/218 [>.............................] - ETA: 56s  batch_loss: 0.1293 [Training] 15/218 [=>............................] - ETA: 55s  batch_loss: 0.1275 [Training] 16/218 [=>............................] - ETA: 55s  batch_loss: 0.1279 [Training] 17/218 [=>............................] - ETA: 54s  batch_loss: 0.1263 [Training] 18/218 [=>............................] - ETA: 54s  batch_loss: 0.1242 [Training] 19/218 [=>............................] - ETA: 53s  batch_loss: 0.1270 [Training] 20/218 [=>............................] - ETA: 53s  batch_loss: 0.1276 [Training] 21/218 [=>............................] - ETA: 52s  batch_loss: 0.1276 [Training] 22/218 [==>...........................] - ETA: 52s  batch_loss: 0.1275 [Training] 23/218 [==>...........................] - ETA: 51s  batch_loss: 0.1284 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.1283 [Training] 25/218 [==>...........................] - ETA: 51s  batch_loss: 0.1285 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 0.1283 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 0.1311 [Training] 28/218 [==>...........................] - ETA: 50s  batch_loss: 0.1302 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 0.1295 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.1300 [Training] 31/218 [===>..........................] - ETA: 48s  batch_loss: 0.1296 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.1287 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.1306 [Training] 34/218 [===>..........................] - ETA: 48s  batch_loss: 0.1318 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.1314 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.1312 [Training] 37/218 [====>.........................] - ETA: 47s  batch_loss: 0.1302 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.1297 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.1296 [Training] 40/218 [====>.........................] - ETA: 46s  batch_loss: 0.1303 [Training] 41/218 [====>.........................] - ETA: 45s  batch_loss: 0.1301 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.1295 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.1294 [Training] 44/218 [=====>........................] - ETA: 45s  batch_loss: 0.1293 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.1295 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.1297 [Training] 47/218 [=====>........................] - ETA: 44s  batch_loss: 0.1288 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 0.1294 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.1287 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.1289 [Training] 51/218 [======>.......................] - ETA: 42s  batch_loss: 0.1289 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.1290 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.1287 [Training] 54/218 [======>.......................] - ETA: 42s  batch_loss: 0.1287 [Training] 55/218 [======>.......................] - ETA: 41s  batch_loss: 0.1289 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.1285 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.1288 [Training] 58/218 [======>.......................] - ETA: 41s  batch_loss: 0.1286 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 0.1287 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.1291 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.1295 [Training] 62/218 [=======>......................] - ETA: 39s  batch_loss: 0.1291 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.1298 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.1297 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 0.1305 [Training] 66/218 [========>.....................] - ETA: 38s  batch_loss: 0.1309 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.1311 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.1313 [Training] 69/218 [========>.....................] - ETA: 38s  batch_loss: 0.1313 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 0.1314 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.1316 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.1311 [Training] 73/218 [=========>....................] - ETA: 37s  batch_loss: 0.1316 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.1313 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.1310 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.1312 [Training] 77/218 [=========>....................] - ETA: 35s  batch_loss: 0.1312 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.1309 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.1307 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.1306 [Training] 81/218 [==========>...................] - ETA: 34s  batch_loss: 0.1308 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.1304 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.1305 [Training] 84/218 [==========>...................] - ETA: 34s  batch_loss: 0.1311 [Training] 85/218 [==========>...................] - ETA: 33s  batch_loss: 0.1308 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.1315 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.1314 [Training] 88/218 [===========>..................] - ETA: 33s  batch_loss: 0.1312 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.1314 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.1311 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.1312 [Training] 92/218 [===========>..................] - ETA: 32s  batch_loss: 0.1310 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.1314 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.1316 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.1316 [Training] 96/218 [============>.................] - ETA: 31s  batch_loss: 0.1318 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.1319 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.1316 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.1317 [Training] 100/218 [============>.................] - ETA: 30s  batch_loss: 0.1317 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.1319 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.1316 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.1316 [Training] 104/218 [=============>................] - ETA: 29s  batch_loss: 0.1315 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.1314 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.1315 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.1318 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 0.1318 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.1317 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.1316 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.1315 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 0.1315 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.1313 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.1314 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.1315 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.1320 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.1317 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.1320 [Training] 119/218 [===============>..............] - ETA: 25s  batch_loss: 0.1322 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.1320 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.1321 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.1319 [Training] 123/218 [===============>..............] - ETA: 24s  batch_loss: 0.1319 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.1320 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.1319 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.1321 [Training] 127/218 [================>.............] - ETA: 23s  batch_loss: 0.1320 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.1321 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.1318 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.1316 [Training] 131/218 [=================>............] - ETA: 22s  batch_loss: 0.1317 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.1320 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.1323 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.1321 [Training] 135/218 [=================>............] - ETA: 21s  batch_loss: 0.1321 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.1322 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.1321 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.1323 [Training] 139/218 [==================>...........] - ETA: 20s  batch_loss: 0.1320 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.1320 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.1320 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.1321 [Training] 143/218 [==================>...........] - ETA: 18s  batch_loss: 0.1320 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.1326 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.1326 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.1327 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.1328 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.1331 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.1331 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.1329 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.1329 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.1328 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.1328 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.1330 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.1331 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.1336 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.1338 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.1341 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.1342 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.1341 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.1341 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.1342 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.1342 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.1342 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.1340 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.1338 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.1338 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.1339 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.1338 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.1338 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.1337 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.1338 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.1337 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.1338 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.1343 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.1342 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.1342 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.1342 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.1343 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.1343 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.1342 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.1341 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.1342 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.1343 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.1345 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.1345 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.1344 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.1345 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.1344 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.1344 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.1345 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.1345 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.1346 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.1349 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.1350 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.1348 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.1349 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.1349 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.1348 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.1347 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.1347 [Training] 202/218 [==========================>...] - ETA: 3s  batch_loss: 0.1349 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.1347 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.1348 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.1347 [Training] 206/218 [===========================>..] - ETA: 2s  batch_loss: 0.1347 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.1347 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.1347 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.1346 [Training] 210/218 [===========================>..] - ETA: 1s  batch_loss: 0.1346 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.1345 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.1344 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.1344 [Training] 214/218 [============================>.] - ETA: 0s  batch_loss: 0.1343 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.1343 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.1343 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.1343 [Training] 218/218 [==============================] 248.9ms/step  batch_loss: 0.1346 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:36:15 - INFO - root -   The F1-score is 0.6931158418578932
08/22/2022 16:36:15 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:15  batch_loss: 0.1102 [Training] 2/218 [..............................] - ETA: 1:36  batch_loss: 0.0933 [Training] 3/218 [..............................] - ETA: 1:21  batch_loss: 0.1054 [Training] 4/218 [..............................] - ETA: 1:14  batch_loss: 0.1089 [Training] 5/218 [..............................] - ETA: 1:10  batch_loss: 0.1045 [Training] 6/218 [..............................] - ETA: 1:07  batch_loss: 0.1044 [Training] 7/218 [..............................] - ETA: 1:04  batch_loss: 0.1139 [Training] 8/218 [>.............................] - ETA: 1:02  batch_loss: 0.1142 [Training] 9/218 [>.............................] - ETA: 1:01  batch_loss: 0.1129 [Training] 10/218 [>.............................] - ETA: 1:00  batch_loss: 0.1098 [Training] 11/218 [>.............................] - ETA: 59s  batch_loss: 0.1145 [Training] 12/218 [>.............................] - ETA: 58s  batch_loss: 0.1181 [Training] 13/218 [>.............................] - ETA: 57s  batch_loss: 0.1213 [Training] 14/218 [>.............................] - ETA: 56s  batch_loss: 0.1224 [Training] 15/218 [=>............................] - ETA: 55s  batch_loss: 0.1234 [Training] 16/218 [=>............................] - ETA: 55s  batch_loss: 0.1229 [Training] 17/218 [=>............................] - ETA: 54s  batch_loss: 0.1235 [Training] 18/218 [=>............................] - ETA: 53s  batch_loss: 0.1235 [Training] 19/218 [=>............................] - ETA: 53s  batch_loss: 0.1239 [Training] 20/218 [=>............................] - ETA: 52s  batch_loss: 0.1241 [Training] 21/218 [=>............................] - ETA: 52s  batch_loss: 0.1239 [Training] 22/218 [==>...........................] - ETA: 52s  batch_loss: 0.1247 [Training] 23/218 [==>...........................] - ETA: 51s  batch_loss: 0.1254 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.1254 [Training] 25/218 [==>...........................] - ETA: 50s  batch_loss: 0.1250 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 0.1245 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 0.1244 [Training] 28/218 [==>...........................] - ETA: 49s  batch_loss: 0.1243 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 0.1241 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.1229 [Training] 31/218 [===>..........................] - ETA: 48s  batch_loss: 0.1240 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.1245 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.1235 [Training] 34/218 [===>..........................] - ETA: 47s  batch_loss: 0.1244 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.1253 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.1268 [Training] 37/218 [====>.........................] - ETA: 47s  batch_loss: 0.1262 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.1258 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.1256 [Training] 40/218 [====>.........................] - ETA: 46s  batch_loss: 0.1252 [Training] 41/218 [====>.........................] - ETA: 45s  batch_loss: 0.1250 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.1250 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.1247 [Training] 44/218 [=====>........................] - ETA: 45s  batch_loss: 0.1253 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.1253 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.1262 [Training] 47/218 [=====>........................] - ETA: 44s  batch_loss: 0.1255 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 0.1252 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.1254 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.1248 [Training] 51/218 [======>.......................] - ETA: 42s  batch_loss: 0.1246 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.1243 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.1239 [Training] 54/218 [======>.......................] - ETA: 42s  batch_loss: 0.1242 [Training] 55/218 [======>.......................] - ETA: 41s  batch_loss: 0.1242 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.1241 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.1239 [Training] 58/218 [======>.......................] - ETA: 41s  batch_loss: 0.1241 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 0.1241 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.1240 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.1247 [Training] 62/218 [=======>......................] - ETA: 39s  batch_loss: 0.1242 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.1239 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.1243 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 0.1246 [Training] 66/218 [========>.....................] - ETA: 38s  batch_loss: 0.1248 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.1245 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.1246 [Training] 69/218 [========>.....................] - ETA: 38s  batch_loss: 0.1249 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 0.1258 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.1264 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.1261 [Training] 73/218 [=========>....................] - ETA: 36s  batch_loss: 0.1260 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.1259 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.1260 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.1262 [Training] 77/218 [=========>....................] - ETA: 35s  batch_loss: 0.1262 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.1266 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.1265 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.1264 [Training] 81/218 [==========>...................] - ETA: 34s  batch_loss: 0.1261 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.1262 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.1261 [Training] 84/218 [==========>...................] - ETA: 34s  batch_loss: 0.1266 [Training] 85/218 [==========>...................] - ETA: 33s  batch_loss: 0.1265 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.1264 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.1262 [Training] 88/218 [===========>..................] - ETA: 32s  batch_loss: 0.1263 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.1265 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.1264 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.1261 [Training] 92/218 [===========>..................] - ETA: 31s  batch_loss: 0.1265 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.1261 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.1260 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.1262 [Training] 96/218 [============>.................] - ETA: 30s  batch_loss: 0.1261 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.1261 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.1262 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.1262 [Training] 100/218 [============>.................] - ETA: 29s  batch_loss: 0.1258 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.1258 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.1260 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.1261 [Training] 104/218 [=============>................] - ETA: 28s  batch_loss: 0.1261 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.1261 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.1263 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.1262 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 0.1262 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.1258 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.1263 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.1262 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 0.1260 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.1259 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.1258 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.1264 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.1264 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.1268 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.1267 [Training] 119/218 [===============>..............] - ETA: 25s  batch_loss: 0.1266 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.1266 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.1264 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.1263 [Training] 123/218 [===============>..............] - ETA: 23s  batch_loss: 0.1264 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.1264 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.1266 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.1266 [Training] 127/218 [================>.............] - ETA: 22s  batch_loss: 0.1263 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.1262 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.1262 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.1266 [Training] 131/218 [=================>............] - ETA: 21s  batch_loss: 0.1264 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.1265 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.1265 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.1267 [Training] 135/218 [=================>............] - ETA: 20s  batch_loss: 0.1267 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.1265 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.1263 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.1263 [Training] 139/218 [==================>...........] - ETA: 19s  batch_loss: 0.1261 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.1260 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.1260 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.1262 [Training] 143/218 [==================>...........] - ETA: 18s  batch_loss: 0.1263 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.1263 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.1264 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.1265 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.1266 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.1265 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.1266 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.1264 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.1264 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.1264 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.1262 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.1263 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.1261 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.1260 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.1264 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.1265 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.1266 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.1267 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.1267 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.1269 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.1268 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.1267 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.1268 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.1268 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.1271 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.1273 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.1276 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.1274 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.1275 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.1276 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.1277 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.1276 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.1277 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.1277 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.1278 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.1276 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.1274 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.1275 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.1273 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.1272 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.1272 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.1272 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.1270 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.1269 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.1272 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.1271 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.1272 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.1272 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.1275 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.1274 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.1274 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.1272 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.1272 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.1272 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.1273 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.1274 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.1275 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.1276 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.1276 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.1276 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.1274 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.1276 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.1278 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 0.1278 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.1278 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.1278 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.1279 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 0.1277 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.1276 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.1276 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.1275 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 0.1273 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.1273 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.1274 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.1275 [Training] 218/218 [==============================] 251.2ms/step  batch_loss: 0.1275 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:37:16 - INFO - root -   The F1-score is 0.7057842046718577
08/22/2022 16:37:16 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:12  batch_loss: 0.1364 [Training] 2/218 [..............................] - ETA: 1:34  batch_loss: 0.1083 [Training] 3/218 [..............................] - ETA: 1:20  batch_loss: 0.1066 [Training] 4/218 [..............................] - ETA: 1:13  batch_loss: 0.1141 [Training] 5/218 [..............................] - ETA: 1:08  batch_loss: 0.1115 [Training] 6/218 [..............................] - ETA: 1:06  batch_loss: 0.1088 [Training] 7/218 [..............................] - ETA: 1:03  batch_loss: 0.1094 [Training] 8/218 [>.............................] - ETA: 1:02  batch_loss: 0.1090 [Training] 9/218 [>.............................] - ETA: 1:00  batch_loss: 0.1092 [Training] 10/218 [>.............................] - ETA: 59s  batch_loss: 0.1119 [Training] 11/218 [>.............................] - ETA: 58s  batch_loss: 0.1118 [Training] 12/218 [>.............................] - ETA: 57s  batch_loss: 0.1134 [Training] 13/218 [>.............................] - ETA: 57s  batch_loss: 0.1123 [Training] 14/218 [>.............................] - ETA: 56s  batch_loss: 0.1154 [Training] 15/218 [=>............................] - ETA: 55s  batch_loss: 0.1164 [Training] 16/218 [=>............................] - ETA: 55s  batch_loss: 0.1190 [Training] 17/218 [=>............................] - ETA: 54s  batch_loss: 0.1200 [Training] 18/218 [=>............................] - ETA: 54s  batch_loss: 0.1187 [Training] 19/218 [=>............................] - ETA: 53s  batch_loss: 0.1195 [Training] 20/218 [=>............................] - ETA: 53s  batch_loss: 0.1205 [Training] 21/218 [=>............................] - ETA: 52s  batch_loss: 0.1190 [Training] 22/218 [==>...........................] - ETA: 52s  batch_loss: 0.1185 [Training] 23/218 [==>...........................] - ETA: 52s  batch_loss: 0.1197 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.1187 [Training] 25/218 [==>...........................] - ETA: 51s  batch_loss: 0.1177 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 0.1179 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 0.1171 [Training] 28/218 [==>...........................] - ETA: 50s  batch_loss: 0.1171 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 0.1169 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.1176 [Training] 31/218 [===>..........................] - ETA: 49s  batch_loss: 0.1191 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.1201 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.1197 [Training] 34/218 [===>..........................] - ETA: 48s  batch_loss: 0.1200 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.1210 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.1207 [Training] 37/218 [====>.........................] - ETA: 47s  batch_loss: 0.1209 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.1205 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.1206 [Training] 40/218 [====>.........................] - ETA: 46s  batch_loss: 0.1207 [Training] 41/218 [====>.........................] - ETA: 46s  batch_loss: 0.1203 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.1200 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.1200 [Training] 44/218 [=====>........................] - ETA: 45s  batch_loss: 0.1201 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.1201 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.1198 [Training] 47/218 [=====>........................] - ETA: 44s  batch_loss: 0.1199 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 0.1192 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.1185 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.1184 [Training] 51/218 [======>.......................] - ETA: 42s  batch_loss: 0.1183 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.1178 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.1180 [Training] 54/218 [======>.......................] - ETA: 42s  batch_loss: 0.1174 [Training] 55/218 [======>.......................] - ETA: 41s  batch_loss: 0.1175 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.1173 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.1172 [Training] 58/218 [======>.......................] - ETA: 40s  batch_loss: 0.1174 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 0.1175 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.1176 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.1172 [Training] 62/218 [=======>......................] - ETA: 39s  batch_loss: 0.1169 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.1174 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.1181 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 0.1180 [Training] 66/218 [========>.....................] - ETA: 38s  batch_loss: 0.1178 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.1177 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.1181 [Training] 69/218 [========>.....................] - ETA: 38s  batch_loss: 0.1182 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 0.1181 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.1185 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.1190 [Training] 73/218 [=========>....................] - ETA: 36s  batch_loss: 0.1192 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.1190 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.1190 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.1193 [Training] 77/218 [=========>....................] - ETA: 35s  batch_loss: 0.1193 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.1198 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.1196 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.1193 [Training] 81/218 [==========>...................] - ETA: 34s  batch_loss: 0.1192 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.1190 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.1191 [Training] 84/218 [==========>...................] - ETA: 34s  batch_loss: 0.1194 [Training] 85/218 [==========>...................] - ETA: 33s  batch_loss: 0.1194 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.1191 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.1195 [Training] 88/218 [===========>..................] - ETA: 32s  batch_loss: 0.1196 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.1197 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.1199 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.1201 [Training] 92/218 [===========>..................] - ETA: 31s  batch_loss: 0.1201 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.1204 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.1203 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.1208 [Training] 96/218 [============>.................] - ETA: 30s  batch_loss: 0.1207 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.1205 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.1202 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.1205 [Training] 100/218 [============>.................] - ETA: 29s  batch_loss: 0.1206 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.1207 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.1209 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.1211 [Training] 104/218 [=============>................] - ETA: 28s  batch_loss: 0.1212 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.1210 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.1208 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.1207 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 0.1209 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.1207 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.1212 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.1212 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 0.1215 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.1216 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.1214 [Training] 115/218 [==============>...............] - ETA: 25s  batch_loss: 0.1209 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.1210 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.1213 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.1213 [Training] 119/218 [===============>..............] - ETA: 24s  batch_loss: 0.1211 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.1213 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.1213 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.1211 [Training] 123/218 [===============>..............] - ETA: 23s  batch_loss: 0.1211 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.1217 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.1216 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.1219 [Training] 127/218 [================>.............] - ETA: 22s  batch_loss: 0.1219 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.1218 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.1217 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.1218 [Training] 131/218 [=================>............] - ETA: 21s  batch_loss: 0.1220 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.1218 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.1217 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.1215 [Training] 135/218 [=================>............] - ETA: 20s  batch_loss: 0.1218 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.1220 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.1220 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.1220 [Training] 139/218 [==================>...........] - ETA: 19s  batch_loss: 0.1218 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.1219 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.1220 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.1219 [Training] 143/218 [==================>...........] - ETA: 18s  batch_loss: 0.1217 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.1215 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.1215 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.1214 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.1213 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.1216 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.1215 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.1215 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.1217 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.1220 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.1222 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.1221 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.1221 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.1219 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.1216 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.1217 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.1217 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.1216 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.1217 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.1216 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.1218 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.1219 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.1218 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.1218 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.1216 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.1216 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.1216 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.1218 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.1218 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.1218 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.1218 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.1218 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.1219 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.1220 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.1218 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.1218 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.1218 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.1217 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.1219 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.1218 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.1217 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.1219 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.1217 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.1216 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.1216 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.1216 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.1218 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.1220 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.1221 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.1220 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.1219 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.1220 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.1221 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.1224 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.1223 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.1226 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.1226 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.1227 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.1225 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.1227 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.1228 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.1228 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.1228 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 0.1228 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.1227 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.1228 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.1231 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 0.1232 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.1234 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.1236 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.1235 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 0.1234 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.1232 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.1234 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.1233 [Training] 218/218 [==============================] 251.1ms/step  batch_loss: 0.1233 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:38:17 - INFO - root -   The F1-score is 0.7105974587726412
08/22/2022 16:38:17 - INFO - root -   the best eval f1 is 0.7106, saving model !!
08/22/2022 16:38:19 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:20  batch_loss: 0.1263 [Training] 2/218 [..............................] - ETA: 1:36  batch_loss: 0.1184 [Training] 3/218 [..............................] - ETA: 1:22  batch_loss: 0.1102 [Training] 4/218 [..............................] - ETA: 1:15  batch_loss: 0.1071 [Training] 5/218 [..............................] - ETA: 1:10  batch_loss: 0.1120 [Training] 6/218 [..............................] - ETA: 1:07  batch_loss: 0.1186 [Training] 7/218 [..............................] - ETA: 1:05  batch_loss: 0.1177 [Training] 8/218 [>.............................] - ETA: 1:03  batch_loss: 0.1184 [Training] 9/218 [>.............................] - ETA: 1:01  batch_loss: 0.1171 [Training] 10/218 [>.............................] - ETA: 1:00  batch_loss: 0.1208 [Training] 11/218 [>.............................] - ETA: 59s  batch_loss: 0.1171 [Training] 12/218 [>.............................] - ETA: 58s  batch_loss: 0.1177 [Training] 13/218 [>.............................] - ETA: 57s  batch_loss: 0.1156 [Training] 14/218 [>.............................] - ETA: 57s  batch_loss: 0.1168 [Training] 15/218 [=>............................] - ETA: 56s  batch_loss: 0.1149 [Training] 16/218 [=>............................] - ETA: 55s  batch_loss: 0.1170 [Training] 17/218 [=>............................] - ETA: 55s  batch_loss: 0.1149 [Training] 18/218 [=>............................] - ETA: 54s  batch_loss: 0.1157 [Training] 19/218 [=>............................] - ETA: 54s  batch_loss: 0.1147 [Training] 20/218 [=>............................] - ETA: 53s  batch_loss: 0.1158 [Training] 21/218 [=>............................] - ETA: 53s  batch_loss: 0.1148 [Training] 22/218 [==>...........................] - ETA: 52s  batch_loss: 0.1155 [Training] 23/218 [==>...........................] - ETA: 52s  batch_loss: 0.1167 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.1168 [Training] 25/218 [==>...........................] - ETA: 51s  batch_loss: 0.1163 [Training] 26/218 [==>...........................] - ETA: 51s  batch_loss: 0.1163 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 0.1156 [Training] 28/218 [==>...........................] - ETA: 50s  batch_loss: 0.1156 [Training] 29/218 [==>...........................] - ETA: 50s  batch_loss: 0.1149 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.1142 [Training] 31/218 [===>..........................] - ETA: 49s  batch_loss: 0.1139 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.1134 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.1133 [Training] 34/218 [===>..........................] - ETA: 48s  batch_loss: 0.1133 [Training] 35/218 [===>..........................] - ETA: 48s  batch_loss: 0.1137 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.1136 [Training] 37/218 [====>.........................] - ETA: 47s  batch_loss: 0.1131 [Training] 38/218 [====>.........................] - ETA: 47s  batch_loss: 0.1124 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.1117 [Training] 40/218 [====>.........................] - ETA: 46s  batch_loss: 0.1118 [Training] 41/218 [====>.........................] - ETA: 46s  batch_loss: 0.1110 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.1109 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.1109 [Training] 44/218 [=====>........................] - ETA: 45s  batch_loss: 0.1105 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.1106 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.1112 [Training] 47/218 [=====>........................] - ETA: 44s  batch_loss: 0.1114 [Training] 48/218 [=====>........................] - ETA: 44s  batch_loss: 0.1119 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.1110 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.1109 [Training] 51/218 [======>.......................] - ETA: 43s  batch_loss: 0.1106 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.1107 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.1109 [Training] 54/218 [======>.......................] - ETA: 42s  batch_loss: 0.1108 [Training] 55/218 [======>.......................] - ETA: 42s  batch_loss: 0.1112 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.1108 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.1108 [Training] 58/218 [======>.......................] - ETA: 41s  batch_loss: 0.1107 [Training] 59/218 [=======>......................] - ETA: 41s  batch_loss: 0.1109 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.1108 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.1115 [Training] 62/218 [=======>......................] - ETA: 40s  batch_loss: 0.1118 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.1124 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.1118 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 0.1116 [Training] 66/218 [========>.....................] - ETA: 39s  batch_loss: 0.1115 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.1115 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.1115 [Training] 69/218 [========>.....................] - ETA: 38s  batch_loss: 0.1112 [Training] 70/218 [========>.....................] - ETA: 38s  batch_loss: 0.1111 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.1122 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.1123 [Training] 73/218 [=========>....................] - ETA: 37s  batch_loss: 0.1121 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.1126 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.1122 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.1127 [Training] 77/218 [=========>....................] - ETA: 36s  batch_loss: 0.1130 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.1130 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.1138 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.1139 [Training] 81/218 [==========>...................] - ETA: 35s  batch_loss: 0.1143 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.1146 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.1152 [Training] 84/218 [==========>...................] - ETA: 34s  batch_loss: 0.1151 [Training] 85/218 [==========>...................] - ETA: 34s  batch_loss: 0.1153 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.1150 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.1149 [Training] 88/218 [===========>..................] - ETA: 33s  batch_loss: 0.1147 [Training] 89/218 [===========>..................] - ETA: 33s  batch_loss: 0.1149 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.1152 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.1155 [Training] 92/218 [===========>..................] - ETA: 32s  batch_loss: 0.1154 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.1151 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.1149 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.1151 [Training] 96/218 [============>.................] - ETA: 31s  batch_loss: 0.1155 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.1152 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.1151 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.1152 [Training] 100/218 [============>.................] - ETA: 30s  batch_loss: 0.1149 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.1149 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.1151 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.1150 [Training] 104/218 [=============>................] - ETA: 29s  batch_loss: 0.1149 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.1153 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.1154 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.1153 [Training] 108/218 [=============>................] - ETA: 28s  batch_loss: 0.1158 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.1156 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.1156 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.1157 [Training] 112/218 [==============>...............] - ETA: 27s  batch_loss: 0.1156 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.1155 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.1155 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.1154 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.1155 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.1155 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.1153 [Training] 119/218 [===============>..............] - ETA: 25s  batch_loss: 0.1153 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.1151 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.1151 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.1151 [Training] 123/218 [===============>..............] - ETA: 24s  batch_loss: 0.1150 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.1152 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.1153 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.1154 [Training] 127/218 [================>.............] - ETA: 23s  batch_loss: 0.1151 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.1152 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.1151 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.1155 [Training] 131/218 [=================>............] - ETA: 22s  batch_loss: 0.1155 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.1154 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.1154 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.1156 [Training] 135/218 [=================>............] - ETA: 21s  batch_loss: 0.1157 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.1156 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.1154 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.1154 [Training] 139/218 [==================>...........] - ETA: 20s  batch_loss: 0.1152 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.1152 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.1151 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.1149 [Training] 143/218 [==================>...........] - ETA: 19s  batch_loss: 0.1150 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.1147 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.1149 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.1147 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.1146 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.1144 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.1143 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.1144 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.1144 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.1147 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.1148 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.1154 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.1154 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.1154 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.1153 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.1152 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.1153 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.1154 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.1155 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.1153 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.1154 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.1153 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.1153 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.1153 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.1151 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.1151 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.1151 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.1152 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.1153 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.1153 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.1154 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.1154 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.1153 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.1154 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.1155 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.1155 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.1156 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.1157 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.1157 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.1158 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.1160 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.1160 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.1161 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.1162 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.1162 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.1161 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.1161 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.1162 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.1161 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.1161 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.1161 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.1163 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.1163 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.1163 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.1163 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.1162 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.1162 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.1160 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.1159 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.1161 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.1162 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.1160 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.1160 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 0.1160 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.1161 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.1161 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.1162 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 0.1164 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.1164 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.1164 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.1165 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 0.1165 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.1165 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.1164 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.1163 [Training] 218/218 [==============================] 251.9ms/step  batch_loss: 0.1161 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:39:20 - INFO - root -   The F1-score is 0.7013816565993196
08/22/2022 16:39:20 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:17  batch_loss: 0.0889 [Training] 2/218 [..............................] - ETA: 1:34  batch_loss: 0.0979 [Training] 3/218 [..............................] - ETA: 1:20  batch_loss: 0.1154 [Training] 4/218 [..............................] - ETA: 1:12  batch_loss: 0.1056 [Training] 5/218 [..............................] - ETA: 1:08  batch_loss: 0.1067 [Training] 6/218 [..............................] - ETA: 1:05  batch_loss: 0.1086 [Training] 7/218 [..............................] - ETA: 1:02  batch_loss: 0.1090 [Training] 8/218 [>.............................] - ETA: 1:01  batch_loss: 0.1069 [Training] 9/218 [>.............................] - ETA: 59s  batch_loss: 0.1048 [Training] 10/218 [>.............................] - ETA: 58s  batch_loss: 0.1025 [Training] 11/218 [>.............................] - ETA: 57s  batch_loss: 0.1037 [Training] 12/218 [>.............................] - ETA: 56s  batch_loss: 0.1019 [Training] 13/218 [>.............................] - ETA: 55s  batch_loss: 0.1026 [Training] 14/218 [>.............................] - ETA: 55s  batch_loss: 0.1014 [Training] 15/218 [=>............................] - ETA: 54s  batch_loss: 0.1019 [Training] 16/218 [=>............................] - ETA: 53s  batch_loss: 0.1011 [Training] 17/218 [=>............................] - ETA: 53s  batch_loss: 0.1032 [Training] 18/218 [=>............................] - ETA: 52s  batch_loss: 0.1029 [Training] 19/218 [=>............................] - ETA: 52s  batch_loss: 0.1027 [Training] 20/218 [=>............................] - ETA: 52s  batch_loss: 0.1036 [Training] 21/218 [=>............................] - ETA: 51s  batch_loss: 0.1039 [Training] 22/218 [==>...........................] - ETA: 51s  batch_loss: 0.1038 [Training] 23/218 [==>...........................] - ETA: 51s  batch_loss: 0.1043 [Training] 24/218 [==>...........................] - ETA: 50s  batch_loss: 0.1035 [Training] 25/218 [==>...........................] - ETA: 50s  batch_loss: 0.1033 [Training] 26/218 [==>...........................] - ETA: 49s  batch_loss: 0.1064 [Training] 27/218 [==>...........................] - ETA: 49s  batch_loss: 0.1058 [Training] 28/218 [==>...........................] - ETA: 49s  batch_loss: 0.1069 [Training] 29/218 [==>...........................] - ETA: 48s  batch_loss: 0.1063 [Training] 30/218 [===>..........................] - ETA: 48s  batch_loss: 0.1062 [Training] 31/218 [===>..........................] - ETA: 48s  batch_loss: 0.1059 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.1076 [Training] 33/218 [===>..........................] - ETA: 47s  batch_loss: 0.1074 [Training] 34/218 [===>..........................] - ETA: 47s  batch_loss: 0.1072 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.1070 [Training] 36/218 [===>..........................] - ETA: 46s  batch_loss: 0.1076 [Training] 37/218 [====>.........................] - ETA: 46s  batch_loss: 0.1070 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.1074 [Training] 39/218 [====>.........................] - ETA: 45s  batch_loss: 0.1067 [Training] 40/218 [====>.........................] - ETA: 45s  batch_loss: 0.1072 [Training] 41/218 [====>.........................] - ETA: 45s  batch_loss: 0.1078 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.1071 [Training] 43/218 [====>.........................] - ETA: 44s  batch_loss: 0.1074 [Training] 44/218 [=====>........................] - ETA: 44s  batch_loss: 0.1071 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.1072 [Training] 46/218 [=====>........................] - ETA: 43s  batch_loss: 0.1080 [Training] 47/218 [=====>........................] - ETA: 43s  batch_loss: 0.1074 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 0.1070 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.1064 [Training] 50/218 [=====>........................] - ETA: 42s  batch_loss: 0.1061 [Training] 51/218 [======>.......................] - ETA: 42s  batch_loss: 0.1055 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.1057 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.1056 [Training] 54/218 [======>.......................] - ETA: 41s  batch_loss: 0.1052 [Training] 55/218 [======>.......................] - ETA: 41s  batch_loss: 0.1052 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.1050 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.1047 [Training] 58/218 [======>.......................] - ETA: 40s  batch_loss: 0.1049 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 0.1048 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.1044 [Training] 61/218 [=======>......................] - ETA: 39s  batch_loss: 0.1044 [Training] 62/218 [=======>......................] - ETA: 39s  batch_loss: 0.1046 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.1040 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.1038 [Training] 65/218 [=======>......................] - ETA: 38s  batch_loss: 0.1035 [Training] 66/218 [========>.....................] - ETA: 38s  batch_loss: 0.1039 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.1037 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.1037 [Training] 69/218 [========>.....................] - ETA: 37s  batch_loss: 0.1039 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 0.1040 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.1036 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.1036 [Training] 73/218 [=========>....................] - ETA: 36s  batch_loss: 0.1037 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.1039 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.1040 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.1040 [Training] 77/218 [=========>....................] - ETA: 35s  batch_loss: 0.1041 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.1040 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.1042 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.1043 [Training] 81/218 [==========>...................] - ETA: 34s  batch_loss: 0.1043 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.1052 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.1054 [Training] 84/218 [==========>...................] - ETA: 33s  batch_loss: 0.1050 [Training] 85/218 [==========>...................] - ETA: 33s  batch_loss: 0.1050 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.1058 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.1057 [Training] 88/218 [===========>..................] - ETA: 32s  batch_loss: 0.1057 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.1058 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.1060 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.1061 [Training] 92/218 [===========>..................] - ETA: 31s  batch_loss: 0.1066 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.1065 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.1068 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.1069 [Training] 96/218 [============>.................] - ETA: 30s  batch_loss: 0.1067 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.1067 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.1070 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.1070 [Training] 100/218 [============>.................] - ETA: 29s  batch_loss: 0.1074 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.1076 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.1077 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.1082 [Training] 104/218 [=============>................] - ETA: 28s  batch_loss: 0.1082 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.1086 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.1084 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.1086 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 0.1087 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.1089 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.1094 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.1092 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 0.1092 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.1095 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.1094 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.1092 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.1094 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.1091 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.1094 [Training] 119/218 [===============>..............] - ETA: 25s  batch_loss: 0.1093 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.1094 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.1096 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.1096 [Training] 123/218 [===============>..............] - ETA: 23s  batch_loss: 0.1097 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.1097 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.1099 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.1097 [Training] 127/218 [================>.............] - ETA: 22s  batch_loss: 0.1101 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.1103 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.1103 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.1102 [Training] 131/218 [=================>............] - ETA: 21s  batch_loss: 0.1101 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.1101 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.1102 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.1103 [Training] 135/218 [=================>............] - ETA: 20s  batch_loss: 0.1106 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.1107 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.1108 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.1108 [Training] 139/218 [==================>...........] - ETA: 19s  batch_loss: 0.1107 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.1107 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.1107 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.1111 [Training] 143/218 [==================>...........] - ETA: 18s  batch_loss: 0.1110 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.1109 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.1108 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.1111 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.1110 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.1111 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.1110 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.1111 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.1108 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.1108 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.1113 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.1113 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.1114 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.1114 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.1114 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.1115 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.1115 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.1113 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.1113 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.1111 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.1109 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.1109 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.1108 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.1109 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.1108 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.1108 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.1108 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.1109 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.1113 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.1112 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.1110 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.1110 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.1109 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.1109 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.1109 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.1110 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.1111 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.1110 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.1110 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.1109 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.1110 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.1111 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.1111 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.1113 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.1113 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.1112 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.1114 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.1113 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.1116 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.1116 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.1116 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.1117 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.1118 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.1117 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.1117 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.1117 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.1117 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.1117 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.1117 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.1118 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.1119 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.1119 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.1122 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 0.1123 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.1125 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.1124 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.1125 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 0.1126 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.1124 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.1124 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.1124 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 0.1125 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.1126 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.1126 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.1127 [Training] 218/218 [==============================] 251.2ms/step  batch_loss: 0.1127 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:40:21 - INFO - root -   The F1-score is 0.7063033305807872
08/22/2022 16:40:21 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:16  batch_loss: 0.0811 [Training] 2/218 [..............................] - ETA: 1:35  batch_loss: 0.1033 [Training] 3/218 [..............................] - ETA: 1:21  batch_loss: 0.1016 [Training] 4/218 [..............................] - ETA: 1:14  batch_loss: 0.1017 [Training] 5/218 [..............................] - ETA: 1:09  batch_loss: 0.1044 [Training] 6/218 [..............................] - ETA: 1:06  batch_loss: 0.1000 [Training] 7/218 [..............................] - ETA: 1:04  batch_loss: 0.0971 [Training] 8/218 [>.............................] - ETA: 1:02  batch_loss: 0.0966 [Training] 9/218 [>.............................] - ETA: 1:01  batch_loss: 0.0990 [Training] 10/218 [>.............................] - ETA: 59s  batch_loss: 0.0976 [Training] 11/218 [>.............................] - ETA: 58s  batch_loss: 0.0958 [Training] 12/218 [>.............................] - ETA: 57s  batch_loss: 0.0930 [Training] 13/218 [>.............................] - ETA: 57s  batch_loss: 0.0951 [Training] 14/218 [>.............................] - ETA: 55s  batch_loss: 0.0941 [Training] 15/218 [=>............................] - ETA: 55s  batch_loss: 0.0938 [Training] 16/218 [=>............................] - ETA: 54s  batch_loss: 0.0963 [Training] 17/218 [=>............................] - ETA: 54s  batch_loss: 0.0956 [Training] 18/218 [=>............................] - ETA: 53s  batch_loss: 0.0950 [Training] 19/218 [=>............................] - ETA: 52s  batch_loss: 0.0955 [Training] 20/218 [=>............................] - ETA: 52s  batch_loss: 0.0954 [Training] 21/218 [=>............................] - ETA: 52s  batch_loss: 0.0954 [Training] 22/218 [==>...........................] - ETA: 51s  batch_loss: 0.0956 [Training] 23/218 [==>...........................] - ETA: 51s  batch_loss: 0.0982 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.0988 [Training] 25/218 [==>...........................] - ETA: 50s  batch_loss: 0.0985 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 0.0998 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 0.0984 [Training] 28/218 [==>...........................] - ETA: 49s  batch_loss: 0.0983 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 0.0975 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.0972 [Training] 31/218 [===>..........................] - ETA: 48s  batch_loss: 0.0967 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.0971 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.0975 [Training] 34/218 [===>..........................] - ETA: 47s  batch_loss: 0.0975 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.0977 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.0978 [Training] 37/218 [====>.........................] - ETA: 46s  batch_loss: 0.0975 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.0978 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.0983 [Training] 40/218 [====>.........................] - ETA: 45s  batch_loss: 0.0985 [Training] 41/218 [====>.........................] - ETA: 45s  batch_loss: 0.0988 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.0989 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.0990 [Training] 44/218 [=====>........................] - ETA: 44s  batch_loss: 0.0987 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.0983 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.0979 [Training] 47/218 [=====>........................] - ETA: 43s  batch_loss: 0.0974 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 0.0974 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.0980 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.0975 [Training] 51/218 [======>.......................] - ETA: 42s  batch_loss: 0.0979 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.0983 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.0982 [Training] 54/218 [======>.......................] - ETA: 41s  batch_loss: 0.0984 [Training] 55/218 [======>.......................] - ETA: 41s  batch_loss: 0.0983 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.0982 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.0979 [Training] 58/218 [======>.......................] - ETA: 40s  batch_loss: 0.0976 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 0.0970 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.0972 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.0984 [Training] 62/218 [=======>......................] - ETA: 39s  batch_loss: 0.0988 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.0999 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.1006 [Training] 65/218 [=======>......................] - ETA: 38s  batch_loss: 0.1007 [Training] 66/218 [========>.....................] - ETA: 38s  batch_loss: 0.1006 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.1011 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.1012 [Training] 69/218 [========>.....................] - ETA: 37s  batch_loss: 0.1011 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 0.1013 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.1010 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.1010 [Training] 73/218 [=========>....................] - ETA: 36s  batch_loss: 0.1005 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.1003 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.1004 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.1001 [Training] 77/218 [=========>....................] - ETA: 35s  batch_loss: 0.0999 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.1003 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.1003 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.1002 [Training] 81/218 [==========>...................] - ETA: 34s  batch_loss: 0.1003 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.1006 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.1004 [Training] 84/218 [==========>...................] - ETA: 34s  batch_loss: 0.1001 [Training] 85/218 [==========>...................] - ETA: 33s  batch_loss: 0.1003 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.1002 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.1001 [Training] 88/218 [===========>..................] - ETA: 32s  batch_loss: 0.1005 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.1003 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.1004 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.1007 [Training] 92/218 [===========>..................] - ETA: 31s  batch_loss: 0.1007 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.1005 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.1006 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.1007 [Training] 96/218 [============>.................] - ETA: 30s  batch_loss: 0.1008 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.1006 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.1006 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.1007 [Training] 100/218 [============>.................] - ETA: 29s  batch_loss: 0.1018 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.1017 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.1018 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.1022 [Training] 104/218 [=============>................] - ETA: 28s  batch_loss: 0.1024 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.1023 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.1024 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.1025 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 0.1028 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.1031 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.1037 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.1040 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 0.1038 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.1038 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.1038 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.1041 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.1043 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.1041 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.1042 [Training] 119/218 [===============>..............] - ETA: 25s  batch_loss: 0.1040 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.1039 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.1041 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.1042 [Training] 123/218 [===============>..............] - ETA: 24s  batch_loss: 0.1043 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.1042 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.1039 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.1038 [Training] 127/218 [================>.............] - ETA: 22s  batch_loss: 0.1040 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.1043 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.1043 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.1043 [Training] 131/218 [=================>............] - ETA: 21s  batch_loss: 0.1041 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.1041 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.1041 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.1040 [Training] 135/218 [=================>............] - ETA: 20s  batch_loss: 0.1042 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.1043 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.1042 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.1045 [Training] 139/218 [==================>...........] - ETA: 19s  batch_loss: 0.1048 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.1046 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.1051 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.1051 [Training] 143/218 [==================>...........] - ETA: 18s  batch_loss: 0.1053 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.1054 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.1057 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.1057 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.1058 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.1060 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.1058 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.1060 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.1061 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.1061 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.1060 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.1061 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.1063 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.1062 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.1063 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.1063 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.1064 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.1064 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.1065 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.1066 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.1065 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.1064 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.1064 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.1064 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.1064 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.1064 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.1064 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.1065 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.1064 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.1065 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.1065 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.1069 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.1068 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.1067 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.1066 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.1065 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.1064 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.1064 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.1063 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.1062 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.1063 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.1067 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.1068 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.1067 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.1066 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.1067 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.1067 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.1066 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.1067 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.1066 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.1067 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.1067 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.1067 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.1067 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.1067 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.1069 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.1073 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.1073 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.1073 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.1074 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.1074 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.1073 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.1073 [Training] 206/218 [===========================>..] - ETA: 2s  batch_loss: 0.1073 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.1076 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.1076 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.1077 [Training] 210/218 [===========================>..] - ETA: 1s  batch_loss: 0.1077 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.1077 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.1076 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.1078 [Training] 214/218 [============================>.] - ETA: 0s  batch_loss: 0.1078 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.1077 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.1078 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.1077 [Training] 218/218 [==============================] 249.2ms/step  batch_loss: 0.1079 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:41:21 - INFO - root -   The F1-score is 0.6996171725458027
08/22/2022 16:41:21 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:13  batch_loss: 0.0930 [Training] 2/218 [..............................] - ETA: 1:33  batch_loss: 0.0904 [Training] 3/218 [..............................] - ETA: 1:19  batch_loss: 0.1061 [Training] 4/218 [..............................] - ETA: 1:12  batch_loss: 0.0998 [Training] 5/218 [..............................] - ETA: 1:08  batch_loss: 0.0958 [Training] 6/218 [..............................] - ETA: 1:05  batch_loss: 0.0970 [Training] 7/218 [..............................] - ETA: 1:03  batch_loss: 0.0986 [Training] 8/218 [>.............................] - ETA: 1:01  batch_loss: 0.0954 [Training] 9/218 [>.............................] - ETA: 1:00  batch_loss: 0.0952 [Training] 10/218 [>.............................] - ETA: 59s  batch_loss: 0.0949 [Training] 11/218 [>.............................] - ETA: 58s  batch_loss: 0.0968 [Training] 12/218 [>.............................] - ETA: 57s  batch_loss: 0.0993 [Training] 13/218 [>.............................] - ETA: 56s  batch_loss: 0.0978 [Training] 14/218 [>.............................] - ETA: 56s  batch_loss: 0.0988 [Training] 15/218 [=>............................] - ETA: 55s  batch_loss: 0.0982 [Training] 16/218 [=>............................] - ETA: 54s  batch_loss: 0.0990 [Training] 17/218 [=>............................] - ETA: 54s  batch_loss: 0.0988 [Training] 18/218 [=>............................] - ETA: 53s  batch_loss: 0.1007 [Training] 19/218 [=>............................] - ETA: 52s  batch_loss: 0.1012 [Training] 20/218 [=>............................] - ETA: 52s  batch_loss: 0.1017 [Training] 21/218 [=>............................] - ETA: 52s  batch_loss: 0.1021 [Training] 22/218 [==>...........................] - ETA: 51s  batch_loss: 0.1010 [Training] 23/218 [==>...........................] - ETA: 51s  batch_loss: 0.1021 [Training] 24/218 [==>...........................] - ETA: 50s  batch_loss: 0.1015 [Training] 25/218 [==>...........................] - ETA: 50s  batch_loss: 0.1010 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 0.1021 [Training] 27/218 [==>...........................] - ETA: 49s  batch_loss: 0.1006 [Training] 28/218 [==>...........................] - ETA: 49s  batch_loss: 0.1009 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 0.1014 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.1008 [Training] 31/218 [===>..........................] - ETA: 48s  batch_loss: 0.1000 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.1002 [Training] 33/218 [===>..........................] - ETA: 47s  batch_loss: 0.1002 [Training] 34/218 [===>..........................] - ETA: 47s  batch_loss: 0.1008 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.1001 [Training] 36/218 [===>..........................] - ETA: 46s  batch_loss: 0.1000 [Training] 37/218 [====>.........................] - ETA: 46s  batch_loss: 0.0994 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.0993 [Training] 39/218 [====>.........................] - ETA: 45s  batch_loss: 0.1001 [Training] 40/218 [====>.........................] - ETA: 45s  batch_loss: 0.1004 [Training] 41/218 [====>.........................] - ETA: 45s  batch_loss: 0.0996 [Training] 42/218 [====>.........................] - ETA: 44s  batch_loss: 0.0995 [Training] 43/218 [====>.........................] - ETA: 44s  batch_loss: 0.0997 [Training] 44/218 [=====>........................] - ETA: 44s  batch_loss: 0.0998 [Training] 45/218 [=====>........................] - ETA: 43s  batch_loss: 0.0988 [Training] 46/218 [=====>........................] - ETA: 43s  batch_loss: 0.0992 [Training] 47/218 [=====>........................] - ETA: 43s  batch_loss: 0.0987 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 0.0988 [Training] 49/218 [=====>........................] - ETA: 42s  batch_loss: 0.0985 [Training] 50/218 [=====>........................] - ETA: 42s  batch_loss: 0.0983 [Training] 51/218 [======>.......................] - ETA: 42s  batch_loss: 0.0983 [Training] 52/218 [======>.......................] - ETA: 41s  batch_loss: 0.0984 [Training] 53/218 [======>.......................] - ETA: 41s  batch_loss: 0.0984 [Training] 54/218 [======>.......................] - ETA: 41s  batch_loss: 0.0987 [Training] 55/218 [======>.......................] - ETA: 41s  batch_loss: 0.0983 [Training] 56/218 [======>.......................] - ETA: 40s  batch_loss: 0.0989 [Training] 57/218 [======>.......................] - ETA: 40s  batch_loss: 0.0986 [Training] 58/218 [======>.......................] - ETA: 40s  batch_loss: 0.0987 [Training] 59/218 [=======>......................] - ETA: 39s  batch_loss: 0.0982 [Training] 60/218 [=======>......................] - ETA: 39s  batch_loss: 0.0977 [Training] 61/218 [=======>......................] - ETA: 39s  batch_loss: 0.0975 [Training] 62/218 [=======>......................] - ETA: 39s  batch_loss: 0.0972 [Training] 63/218 [=======>......................] - ETA: 38s  batch_loss: 0.0972 [Training] 64/218 [=======>......................] - ETA: 38s  batch_loss: 0.0972 [Training] 65/218 [=======>......................] - ETA: 38s  batch_loss: 0.0976 [Training] 66/218 [========>.....................] - ETA: 38s  batch_loss: 0.0976 [Training] 67/218 [========>.....................] - ETA: 37s  batch_loss: 0.0973 [Training] 68/218 [========>.....................] - ETA: 37s  batch_loss: 0.0974 [Training] 69/218 [========>.....................] - ETA: 37s  batch_loss: 0.0973 [Training] 70/218 [========>.....................] - ETA: 36s  batch_loss: 0.0975 [Training] 71/218 [========>.....................] - ETA: 36s  batch_loss: 0.0975 [Training] 72/218 [========>.....................] - ETA: 36s  batch_loss: 0.0974 [Training] 73/218 [=========>....................] - ETA: 36s  batch_loss: 0.0976 [Training] 74/218 [=========>....................] - ETA: 35s  batch_loss: 0.0976 [Training] 75/218 [=========>....................] - ETA: 35s  batch_loss: 0.0978 [Training] 76/218 [=========>....................] - ETA: 35s  batch_loss: 0.0978 [Training] 77/218 [=========>....................] - ETA: 35s  batch_loss: 0.0979 [Training] 78/218 [=========>....................] - ETA: 34s  batch_loss: 0.0979 [Training] 79/218 [=========>....................] - ETA: 34s  batch_loss: 0.0983 [Training] 80/218 [==========>...................] - ETA: 34s  batch_loss: 0.0984 [Training] 81/218 [==========>...................] - ETA: 34s  batch_loss: 0.0981 [Training] 82/218 [==========>...................] - ETA: 33s  batch_loss: 0.0979 [Training] 83/218 [==========>...................] - ETA: 33s  batch_loss: 0.0978 [Training] 84/218 [==========>...................] - ETA: 33s  batch_loss: 0.0978 [Training] 85/218 [==========>...................] - ETA: 33s  batch_loss: 0.0976 [Training] 86/218 [==========>...................] - ETA: 32s  batch_loss: 0.0975 [Training] 87/218 [==========>...................] - ETA: 32s  batch_loss: 0.0974 [Training] 88/218 [===========>..................] - ETA: 32s  batch_loss: 0.0976 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.0973 [Training] 90/218 [===========>..................] - ETA: 31s  batch_loss: 0.0973 [Training] 91/218 [===========>..................] - ETA: 31s  batch_loss: 0.0972 [Training] 92/218 [===========>..................] - ETA: 31s  batch_loss: 0.0969 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.0970 [Training] 94/218 [===========>..................] - ETA: 30s  batch_loss: 0.0969 [Training] 95/218 [============>.................] - ETA: 30s  batch_loss: 0.0969 [Training] 96/218 [============>.................] - ETA: 30s  batch_loss: 0.0965 [Training] 97/218 [============>.................] - ETA: 29s  batch_loss: 0.0965 [Training] 98/218 [============>.................] - ETA: 29s  batch_loss: 0.0966 [Training] 99/218 [============>.................] - ETA: 29s  batch_loss: 0.0967 [Training] 100/218 [============>.................] - ETA: 29s  batch_loss: 0.0967 [Training] 101/218 [============>.................] - ETA: 28s  batch_loss: 0.0968 [Training] 102/218 [=============>................] - ETA: 28s  batch_loss: 0.0969 [Training] 103/218 [=============>................] - ETA: 28s  batch_loss: 0.0970 [Training] 104/218 [=============>................] - ETA: 28s  batch_loss: 0.0967 [Training] 105/218 [=============>................] - ETA: 27s  batch_loss: 0.0970 [Training] 106/218 [=============>................] - ETA: 27s  batch_loss: 0.0972 [Training] 107/218 [=============>................] - ETA: 27s  batch_loss: 0.0976 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 0.0975 [Training] 109/218 [==============>...............] - ETA: 26s  batch_loss: 0.0975 [Training] 110/218 [==============>...............] - ETA: 26s  batch_loss: 0.0976 [Training] 111/218 [==============>...............] - ETA: 26s  batch_loss: 0.0976 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 0.0974 [Training] 113/218 [==============>...............] - ETA: 25s  batch_loss: 0.0975 [Training] 114/218 [==============>...............] - ETA: 25s  batch_loss: 0.0976 [Training] 115/218 [==============>...............] - ETA: 25s  batch_loss: 0.0977 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.0977 [Training] 117/218 [===============>..............] - ETA: 24s  batch_loss: 0.0977 [Training] 118/218 [===============>..............] - ETA: 24s  batch_loss: 0.0981 [Training] 119/218 [===============>..............] - ETA: 24s  batch_loss: 0.0982 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.0982 [Training] 121/218 [===============>..............] - ETA: 23s  batch_loss: 0.0982 [Training] 122/218 [===============>..............] - ETA: 23s  batch_loss: 0.0984 [Training] 123/218 [===============>..............] - ETA: 23s  batch_loss: 0.0984 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.0987 [Training] 125/218 [================>.............] - ETA: 22s  batch_loss: 0.0989 [Training] 126/218 [================>.............] - ETA: 22s  batch_loss: 0.0989 [Training] 127/218 [================>.............] - ETA: 22s  batch_loss: 0.0988 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.0986 [Training] 129/218 [================>.............] - ETA: 21s  batch_loss: 0.0987 [Training] 130/218 [================>.............] - ETA: 21s  batch_loss: 0.0988 [Training] 131/218 [=================>............] - ETA: 21s  batch_loss: 0.0988 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.0985 [Training] 133/218 [=================>............] - ETA: 20s  batch_loss: 0.0984 [Training] 134/218 [=================>............] - ETA: 20s  batch_loss: 0.0985 [Training] 135/218 [=================>............] - ETA: 20s  batch_loss: 0.0988 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.0990 [Training] 137/218 [=================>............] - ETA: 19s  batch_loss: 0.0991 [Training] 138/218 [=================>............] - ETA: 19s  batch_loss: 0.0987 [Training] 139/218 [==================>...........] - ETA: 19s  batch_loss: 0.0985 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.0988 [Training] 141/218 [==================>...........] - ETA: 18s  batch_loss: 0.0988 [Training] 142/218 [==================>...........] - ETA: 18s  batch_loss: 0.0988 [Training] 143/218 [==================>...........] - ETA: 18s  batch_loss: 0.0990 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.0988 [Training] 145/218 [==================>...........] - ETA: 17s  batch_loss: 0.0987 [Training] 146/218 [===================>..........] - ETA: 17s  batch_loss: 0.0987 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.0988 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.0988 [Training] 149/218 [===================>..........] - ETA: 16s  batch_loss: 0.0987 [Training] 150/218 [===================>..........] - ETA: 16s  batch_loss: 0.0990 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.0991 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.0991 [Training] 153/218 [====================>.........] - ETA: 15s  batch_loss: 0.0992 [Training] 154/218 [====================>.........] - ETA: 15s  batch_loss: 0.0995 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.0995 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.0996 [Training] 157/218 [====================>.........] - ETA: 14s  batch_loss: 0.0995 [Training] 158/218 [====================>.........] - ETA: 14s  batch_loss: 0.0996 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.0999 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.1000 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.1003 [Training] 162/218 [=====================>........] - ETA: 13s  batch_loss: 0.1001 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.1003 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.1006 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.1007 [Training] 166/218 [=====================>........] - ETA: 12s  batch_loss: 0.1007 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.1007 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.1008 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.1010 [Training] 170/218 [======================>.......] - ETA: 11s  batch_loss: 0.1009 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.1012 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.1012 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.1013 [Training] 174/218 [======================>.......] - ETA: 10s  batch_loss: 0.1012 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.1013 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.1013 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.1016 [Training] 178/218 [=======================>......] - ETA: 9s  batch_loss: 0.1015 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.1019 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.1019 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.1021 [Training] 182/218 [========================>.....] - ETA: 8s  batch_loss: 0.1020 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.1019 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.1020 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.1023 [Training] 186/218 [========================>.....] - ETA: 7s  batch_loss: 0.1023 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.1023 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.1023 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.1023 [Training] 190/218 [=========================>....] - ETA: 6s  batch_loss: 0.1024 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.1024 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.1024 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.1026 [Training] 194/218 [=========================>....] - ETA: 5s  batch_loss: 0.1025 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.1024 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.1027 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.1026 [Training] 198/218 [==========================>...] - ETA: 4s  batch_loss: 0.1026 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.1026 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.1027 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.1028 [Training] 202/218 [==========================>...] - ETA: 3s  batch_loss: 0.1026 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.1027 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.1028 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.1028 [Training] 206/218 [===========================>..] - ETA: 2s  batch_loss: 0.1028 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.1028 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.1029 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.1030 [Training] 210/218 [===========================>..] - ETA: 1s  batch_loss: 0.1030 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.1032 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.1031 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.1031 [Training] 214/218 [============================>.] - ETA: 0s  batch_loss: 0.1033 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.1032 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.1030 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.1033 [Training] 218/218 [==============================] 244.4ms/step  batch_loss: 0.1031 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:42:20 - INFO - root -   The F1-score is 0.7013964950711938
08/22/2022 16:42:20 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/218 [..............................] - ETA: 2:08  batch_loss: 0.0979 [Training] 2/218 [..............................] - ETA: 1:30  batch_loss: 0.0914 [Training] 3/218 [..............................] - ETA: 1:18  batch_loss: 0.0954 [Training] 4/218 [..............................] - ETA: 1:11  batch_loss: 0.0998 [Training] 5/218 [..............................] - ETA: 1:07  batch_loss: 0.1036 [Training] 6/218 [..............................] - ETA: 1:05  batch_loss: 0.1018 [Training] 7/218 [..............................] - ETA: 1:03  batch_loss: 0.1026 [Training] 8/218 [>.............................] - ETA: 1:01  batch_loss: 0.1019 [Training] 9/218 [>.............................] - ETA: 1:00  batch_loss: 0.1014 [Training] 10/218 [>.............................] - ETA: 59s  batch_loss: 0.1024 [Training] 11/218 [>.............................] - ETA: 58s  batch_loss: 0.0999 [Training] 12/218 [>.............................] - ETA: 57s  batch_loss: 0.0997 [Training] 13/218 [>.............................] - ETA: 56s  batch_loss: 0.0992 [Training] 14/218 [>.............................] - ETA: 55s  batch_loss: 0.0974 [Training] 15/218 [=>............................] - ETA: 55s  batch_loss: 0.0976 [Training] 16/218 [=>............................] - ETA: 54s  batch_loss: 0.0968 [Training] 17/218 [=>............................] - ETA: 54s  batch_loss: 0.0967 [Training] 18/218 [=>............................] - ETA: 53s  batch_loss: 0.0963 [Training] 19/218 [=>............................] - ETA: 53s  batch_loss: 0.0972 [Training] 20/218 [=>............................] - ETA: 52s  batch_loss: 0.0978 [Training] 21/218 [=>............................] - ETA: 52s  batch_loss: 0.1005 [Training] 22/218 [==>...........................] - ETA: 51s  batch_loss: 0.0998 [Training] 23/218 [==>...........................] - ETA: 51s  batch_loss: 0.0994 [Training] 24/218 [==>...........................] - ETA: 51s  batch_loss: 0.0991 [Training] 25/218 [==>...........................] - ETA: 50s  batch_loss: 0.0992 [Training] 26/218 [==>...........................] - ETA: 50s  batch_loss: 0.0986 [Training] 27/218 [==>...........................] - ETA: 50s  batch_loss: 0.0982 [Training] 28/218 [==>...........................] - ETA: 49s  batch_loss: 0.0986 [Training] 29/218 [==>...........................] - ETA: 49s  batch_loss: 0.0989 [Training] 30/218 [===>..........................] - ETA: 49s  batch_loss: 0.0985 [Training] 31/218 [===>..........................] - ETA: 48s  batch_loss: 0.0985 [Training] 32/218 [===>..........................] - ETA: 48s  batch_loss: 0.0983 [Training] 33/218 [===>..........................] - ETA: 48s  batch_loss: 0.0982 [Training] 34/218 [===>..........................] - ETA: 47s  batch_loss: 0.0982 [Training] 35/218 [===>..........................] - ETA: 47s  batch_loss: 0.0983 [Training] 36/218 [===>..........................] - ETA: 47s  batch_loss: 0.0993 [Training] 37/218 [====>.........................] - ETA: 46s  batch_loss: 0.0996 [Training] 38/218 [====>.........................] - ETA: 46s  batch_loss: 0.0997 [Training] 39/218 [====>.........................] - ETA: 46s  batch_loss: 0.0995 [Training] 40/218 [====>.........................] - ETA: 45s  batch_loss: 0.0988 [Training] 41/218 [====>.........................] - ETA: 45s  batch_loss: 0.0986 [Training] 42/218 [====>.........................] - ETA: 45s  batch_loss: 0.0980 [Training] 43/218 [====>.........................] - ETA: 45s  batch_loss: 0.0983 [Training] 44/218 [=====>........................] - ETA: 44s  batch_loss: 0.0981 [Training] 45/218 [=====>........................] - ETA: 44s  batch_loss: 0.0981 [Training] 46/218 [=====>........................] - ETA: 44s  batch_loss: 0.0972 [Training] 47/218 [=====>........................] - ETA: 43s  batch_loss: 0.0969 [Training] 48/218 [=====>........................] - ETA: 43s  batch_loss: 0.0968 [Training] 49/218 [=====>........................] - ETA: 43s  batch_loss: 0.0965 [Training] 50/218 [=====>........................] - ETA: 43s  batch_loss: 0.0962 [Training] 51/218 [======>.......................] - ETA: 42s  batch_loss: 0.0962 [Training] 52/218 [======>.......................] - ETA: 42s  batch_loss: 0.0967 [Training] 53/218 [======>.......................] - ETA: 42s  batch_loss: 0.0967 [Training] 54/218 [======>.......................] - ETA: 41s  batch_loss: 0.0964 [Training] 55/218 [======>.......................] - ETA: 41s  batch_loss: 0.0968 [Training] 56/218 [======>.......................] - ETA: 41s  batch_loss: 0.0968 [Training] 57/218 [======>.......................] - ETA: 41s  batch_loss: 0.0967 [Training] 58/218 [======>.......................] - ETA: 40s  batch_loss: 0.0966 [Training] 59/218 [=======>......................] - ETA: 40s  batch_loss: 0.0966 [Training] 60/218 [=======>......................] - ETA: 40s  batch_loss: 0.0965 [Training] 61/218 [=======>......................] - ETA: 40s  batch_loss: 0.0959 [Training] 62/218 [=======>......................] - ETA: 39s  batch_loss: 0.0956 [Training] 63/218 [=======>......................] - ETA: 39s  batch_loss: 0.0956 [Training] 64/218 [=======>......................] - ETA: 39s  batch_loss: 0.0955 [Training] 65/218 [=======>......................] - ETA: 39s  batch_loss: 0.0954 [Training] 66/218 [========>.....................] - ETA: 38s  batch_loss: 0.0957 [Training] 67/218 [========>.....................] - ETA: 38s  batch_loss: 0.0957 [Training] 68/218 [========>.....................] - ETA: 38s  batch_loss: 0.0954 [Training] 69/218 [========>.....................] - ETA: 37s  batch_loss: 0.0956 [Training] 70/218 [========>.....................] - ETA: 37s  batch_loss: 0.0956 [Training] 71/218 [========>.....................] - ETA: 37s  batch_loss: 0.0959 [Training] 72/218 [========>.....................] - ETA: 37s  batch_loss: 0.0958 [Training] 73/218 [=========>....................] - ETA: 36s  batch_loss: 0.0956 [Training] 74/218 [=========>....................] - ETA: 36s  batch_loss: 0.0955 [Training] 75/218 [=========>....................] - ETA: 36s  batch_loss: 0.0956 [Training] 76/218 [=========>....................] - ETA: 36s  batch_loss: 0.0956 [Training] 77/218 [=========>....................] - ETA: 35s  batch_loss: 0.0957 [Training] 78/218 [=========>....................] - ETA: 35s  batch_loss: 0.0955 [Training] 79/218 [=========>....................] - ETA: 35s  batch_loss: 0.0956 [Training] 80/218 [==========>...................] - ETA: 35s  batch_loss: 0.0956 [Training] 81/218 [==========>...................] - ETA: 34s  batch_loss: 0.0954 [Training] 82/218 [==========>...................] - ETA: 34s  batch_loss: 0.0957 [Training] 83/218 [==========>...................] - ETA: 34s  batch_loss: 0.0956 [Training] 84/218 [==========>...................] - ETA: 33s  batch_loss: 0.0960 [Training] 85/218 [==========>...................] - ETA: 33s  batch_loss: 0.0957 [Training] 86/218 [==========>...................] - ETA: 33s  batch_loss: 0.0963 [Training] 87/218 [==========>...................] - ETA: 33s  batch_loss: 0.0960 [Training] 88/218 [===========>..................] - ETA: 32s  batch_loss: 0.0959 [Training] 89/218 [===========>..................] - ETA: 32s  batch_loss: 0.0957 [Training] 90/218 [===========>..................] - ETA: 32s  batch_loss: 0.0955 [Training] 91/218 [===========>..................] - ETA: 32s  batch_loss: 0.0952 [Training] 92/218 [===========>..................] - ETA: 31s  batch_loss: 0.0954 [Training] 93/218 [===========>..................] - ETA: 31s  batch_loss: 0.0953 [Training] 94/218 [===========>..................] - ETA: 31s  batch_loss: 0.0951 [Training] 95/218 [============>.................] - ETA: 31s  batch_loss: 0.0954 [Training] 96/218 [============>.................] - ETA: 30s  batch_loss: 0.0953 [Training] 97/218 [============>.................] - ETA: 30s  batch_loss: 0.0955 [Training] 98/218 [============>.................] - ETA: 30s  batch_loss: 0.0952 [Training] 99/218 [============>.................] - ETA: 30s  batch_loss: 0.0954 [Training] 100/218 [============>.................] - ETA: 29s  batch_loss: 0.0951 [Training] 101/218 [============>.................] - ETA: 29s  batch_loss: 0.0952 [Training] 102/218 [=============>................] - ETA: 29s  batch_loss: 0.0952 [Training] 103/218 [=============>................] - ETA: 29s  batch_loss: 0.0950 [Training] 104/218 [=============>................] - ETA: 28s  batch_loss: 0.0949 [Training] 105/218 [=============>................] - ETA: 28s  batch_loss: 0.0948 [Training] 106/218 [=============>................] - ETA: 28s  batch_loss: 0.0950 [Training] 107/218 [=============>................] - ETA: 28s  batch_loss: 0.0947 [Training] 108/218 [=============>................] - ETA: 27s  batch_loss: 0.0948 [Training] 109/218 [==============>...............] - ETA: 27s  batch_loss: 0.0947 [Training] 110/218 [==============>...............] - ETA: 27s  batch_loss: 0.0946 [Training] 111/218 [==============>...............] - ETA: 27s  batch_loss: 0.0948 [Training] 112/218 [==============>...............] - ETA: 26s  batch_loss: 0.0948 [Training] 113/218 [==============>...............] - ETA: 26s  batch_loss: 0.0951 [Training] 114/218 [==============>...............] - ETA: 26s  batch_loss: 0.0958 [Training] 115/218 [==============>...............] - ETA: 26s  batch_loss: 0.0958 [Training] 116/218 [==============>...............] - ETA: 25s  batch_loss: 0.0958 [Training] 117/218 [===============>..............] - ETA: 25s  batch_loss: 0.0958 [Training] 118/218 [===============>..............] - ETA: 25s  batch_loss: 0.0962 [Training] 119/218 [===============>..............] - ETA: 24s  batch_loss: 0.0959 [Training] 120/218 [===============>..............] - ETA: 24s  batch_loss: 0.0959 [Training] 121/218 [===============>..............] - ETA: 24s  batch_loss: 0.0958 [Training] 122/218 [===============>..............] - ETA: 24s  batch_loss: 0.0960 [Training] 123/218 [===============>..............] - ETA: 23s  batch_loss: 0.0958 [Training] 124/218 [================>.............] - ETA: 23s  batch_loss: 0.0957 [Training] 125/218 [================>.............] - ETA: 23s  batch_loss: 0.0957 [Training] 126/218 [================>.............] - ETA: 23s  batch_loss: 0.0960 [Training] 127/218 [================>.............] - ETA: 22s  batch_loss: 0.0961 [Training] 128/218 [================>.............] - ETA: 22s  batch_loss: 0.0963 [Training] 129/218 [================>.............] - ETA: 22s  batch_loss: 0.0966 [Training] 130/218 [================>.............] - ETA: 22s  batch_loss: 0.0964 [Training] 131/218 [=================>............] - ETA: 21s  batch_loss: 0.0966 [Training] 132/218 [=================>............] - ETA: 21s  batch_loss: 0.0971 [Training] 133/218 [=================>............] - ETA: 21s  batch_loss: 0.0969 [Training] 134/218 [=================>............] - ETA: 21s  batch_loss: 0.0970 [Training] 135/218 [=================>............] - ETA: 20s  batch_loss: 0.0968 [Training] 136/218 [=================>............] - ETA: 20s  batch_loss: 0.0968 [Training] 137/218 [=================>............] - ETA: 20s  batch_loss: 0.0968 [Training] 138/218 [=================>............] - ETA: 20s  batch_loss: 0.0965 [Training] 139/218 [==================>...........] - ETA: 19s  batch_loss: 0.0965 [Training] 140/218 [==================>...........] - ETA: 19s  batch_loss: 0.0965 [Training] 141/218 [==================>...........] - ETA: 19s  batch_loss: 0.0967 [Training] 142/218 [==================>...........] - ETA: 19s  batch_loss: 0.0965 [Training] 143/218 [==================>...........] - ETA: 18s  batch_loss: 0.0969 [Training] 144/218 [==================>...........] - ETA: 18s  batch_loss: 0.0968 [Training] 145/218 [==================>...........] - ETA: 18s  batch_loss: 0.0969 [Training] 146/218 [===================>..........] - ETA: 18s  batch_loss: 0.0968 [Training] 147/218 [===================>..........] - ETA: 17s  batch_loss: 0.0968 [Training] 148/218 [===================>..........] - ETA: 17s  batch_loss: 0.0970 [Training] 149/218 [===================>..........] - ETA: 17s  batch_loss: 0.0969 [Training] 150/218 [===================>..........] - ETA: 17s  batch_loss: 0.0969 [Training] 151/218 [===================>..........] - ETA: 16s  batch_loss: 0.0970 [Training] 152/218 [===================>..........] - ETA: 16s  batch_loss: 0.0973 [Training] 153/218 [====================>.........] - ETA: 16s  batch_loss: 0.0973 [Training] 154/218 [====================>.........] - ETA: 16s  batch_loss: 0.0973 [Training] 155/218 [====================>.........] - ETA: 15s  batch_loss: 0.0972 [Training] 156/218 [====================>.........] - ETA: 15s  batch_loss: 0.0974 [Training] 157/218 [====================>.........] - ETA: 15s  batch_loss: 0.0976 [Training] 158/218 [====================>.........] - ETA: 15s  batch_loss: 0.0974 [Training] 159/218 [====================>.........] - ETA: 14s  batch_loss: 0.0973 [Training] 160/218 [=====================>........] - ETA: 14s  batch_loss: 0.0973 [Training] 161/218 [=====================>........] - ETA: 14s  batch_loss: 0.0974 [Training] 162/218 [=====================>........] - ETA: 14s  batch_loss: 0.0974 [Training] 163/218 [=====================>........] - ETA: 13s  batch_loss: 0.0975 [Training] 164/218 [=====================>........] - ETA: 13s  batch_loss: 0.0977 [Training] 165/218 [=====================>........] - ETA: 13s  batch_loss: 0.0975 [Training] 166/218 [=====================>........] - ETA: 13s  batch_loss: 0.0978 [Training] 167/218 [=====================>........] - ETA: 12s  batch_loss: 0.0977 [Training] 168/218 [======================>.......] - ETA: 12s  batch_loss: 0.0978 [Training] 169/218 [======================>.......] - ETA: 12s  batch_loss: 0.0982 [Training] 170/218 [======================>.......] - ETA: 12s  batch_loss: 0.0981 [Training] 171/218 [======================>.......] - ETA: 11s  batch_loss: 0.0983 [Training] 172/218 [======================>.......] - ETA: 11s  batch_loss: 0.0982 [Training] 173/218 [======================>.......] - ETA: 11s  batch_loss: 0.0983 [Training] 174/218 [======================>.......] - ETA: 11s  batch_loss: 0.0982 [Training] 175/218 [=======================>......] - ETA: 10s  batch_loss: 0.0983 [Training] 176/218 [=======================>......] - ETA: 10s  batch_loss: 0.0986 [Training] 177/218 [=======================>......] - ETA: 10s  batch_loss: 0.0985 [Training] 178/218 [=======================>......] - ETA: 10s  batch_loss: 0.0987 [Training] 179/218 [=======================>......] - ETA: 9s  batch_loss: 0.0987 [Training] 180/218 [=======================>......] - ETA: 9s  batch_loss: 0.0987 [Training] 181/218 [=======================>......] - ETA: 9s  batch_loss: 0.0987 [Training] 182/218 [========================>.....] - ETA: 9s  batch_loss: 0.0989 [Training] 183/218 [========================>.....] - ETA: 8s  batch_loss: 0.0990 [Training] 184/218 [========================>.....] - ETA: 8s  batch_loss: 0.0991 [Training] 185/218 [========================>.....] - ETA: 8s  batch_loss: 0.0992 [Training] 186/218 [========================>.....] - ETA: 8s  batch_loss: 0.0992 [Training] 187/218 [========================>.....] - ETA: 7s  batch_loss: 0.0990 [Training] 188/218 [========================>.....] - ETA: 7s  batch_loss: 0.0993 [Training] 189/218 [=========================>....] - ETA: 7s  batch_loss: 0.0992 [Training] 190/218 [=========================>....] - ETA: 7s  batch_loss: 0.0993 [Training] 191/218 [=========================>....] - ETA: 6s  batch_loss: 0.0992 [Training] 192/218 [=========================>....] - ETA: 6s  batch_loss: 0.0991 [Training] 193/218 [=========================>....] - ETA: 6s  batch_loss: 0.0991 [Training] 194/218 [=========================>....] - ETA: 6s  batch_loss: 0.0991 [Training] 195/218 [=========================>....] - ETA: 5s  batch_loss: 0.0990 [Training] 196/218 [=========================>....] - ETA: 5s  batch_loss: 0.0992 [Training] 197/218 [==========================>...] - ETA: 5s  batch_loss: 0.0991 [Training] 198/218 [==========================>...] - ETA: 5s  batch_loss: 0.0991 [Training] 199/218 [==========================>...] - ETA: 4s  batch_loss: 0.0991 [Training] 200/218 [==========================>...] - ETA: 4s  batch_loss: 0.0991 [Training] 201/218 [==========================>...] - ETA: 4s  batch_loss: 0.0990 [Training] 202/218 [==========================>...] - ETA: 4s  batch_loss: 0.0989 [Training] 203/218 [==========================>...] - ETA: 3s  batch_loss: 0.0992 [Training] 204/218 [===========================>..] - ETA: 3s  batch_loss: 0.0991 [Training] 205/218 [===========================>..] - ETA: 3s  batch_loss: 0.0991 [Training] 206/218 [===========================>..] - ETA: 3s  batch_loss: 0.0990 [Training] 207/218 [===========================>..] - ETA: 2s  batch_loss: 0.0991 [Training] 208/218 [===========================>..] - ETA: 2s  batch_loss: 0.0991 [Training] 209/218 [===========================>..] - ETA: 2s  batch_loss: 0.0991 [Training] 210/218 [===========================>..] - ETA: 2s  batch_loss: 0.0992 [Training] 211/218 [============================>.] - ETA: 1s  batch_loss: 0.0992 [Training] 212/218 [============================>.] - ETA: 1s  batch_loss: 0.0994 [Training] 213/218 [============================>.] - ETA: 1s  batch_loss: 0.0993 [Training] 214/218 [============================>.] - ETA: 1s  batch_loss: 0.0993 [Training] 215/218 [============================>.] - ETA: 0s  batch_loss: 0.0994 [Training] 216/218 [============================>.] - ETA: 0s  batch_loss: 0.0995 [Training] 217/218 [============================>.] - ETA: 0s  batch_loss: 0.0995 [Training] 218/218 [==============================] 251.8ms/step  batch_loss: 0.0996 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/22/2022 16:43:21 - INFO - root -   The F1-score is 0.7056838126096936
08/22/2022 16:43:21 - INFO - root -   Early stop in 25 epoch!
