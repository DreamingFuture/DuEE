nohup: ignoring input
********** train start *************
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 5, 
    "event_type": "role", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 1e-05, 
    "max_len": 150, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 30.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 48, 
    "per_gpu_train_batch_size": 64, 
    "seed": 66, 
    "stride": 100, 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/6770 [00:00<?, ?it/s]tokenizing...:   4%|▎         | 244/6770 [00:00<00:02, 2437.92it/s]tokenizing...:   7%|▋         | 488/6770 [00:00<00:02, 2222.11it/s]tokenizing...:  11%|█▏        | 773/6770 [00:00<00:02, 2493.77it/s]tokenizing...:  16%|█▌        | 1074/6770 [00:00<00:02, 2686.48it/s]tokenizing...:  20%|█▉        | 1349/6770 [00:00<00:02, 2706.16it/s]tokenizing...:  24%|██▍       | 1621/6770 [00:00<00:01, 2681.20it/s]tokenizing...:  28%|██▊       | 1904/6770 [00:00<00:01, 2727.90it/s]tokenizing...:  32%|███▏      | 2178/6770 [00:00<00:01, 2729.63it/s]tokenizing...:  36%|███▌      | 2452/6770 [00:00<00:01, 2699.51it/s]tokenizing...:  40%|████      | 2729/6770 [00:01<00:01, 2720.13it/s]tokenizing...:  45%|████▍     | 3013/6770 [00:01<00:01, 2753.76it/s]tokenizing...:  49%|████▊     | 3289/6770 [00:01<00:01, 2745.82it/s]tokenizing...:  53%|█████▎    | 3564/6770 [00:01<00:01, 2654.66it/s]tokenizing...:  57%|█████▋    | 3836/6770 [00:01<00:01, 2671.66it/s]tokenizing...:  61%|██████    | 4109/6770 [00:01<00:00, 2683.31it/s]tokenizing...:  65%|██████▍   | 4379/6770 [00:01<00:00, 2684.66it/s]tokenizing...:  69%|██████▉   | 4672/6770 [00:01<00:00, 2756.89it/s]tokenizing...:  73%|███████▎  | 4963/6770 [00:01<00:00, 2801.46it/s]tokenizing...:  77%|███████▋  | 5244/6770 [00:01<00:00, 2646.93it/s]tokenizing...:  81%|████████▏ | 5511/6770 [00:02<00:00, 2628.20it/s]tokenizing...:  85%|████████▌ | 5776/6770 [00:02<00:00, 1937.75it/s]tokenizing...:  89%|████████▉ | 6053/6770 [00:02<00:00, 2130.12it/s]tokenizing...:  93%|█████████▎| 6291/6770 [00:02<00:00, 2140.11it/s]tokenizing...:  97%|█████████▋| 6546/6770 [00:02<00:00, 2244.58it/s]tokenizing...: 100%|██████████| 6770/6770 [00:02<00:00, 2530.98it/s]
tokenizing...:   0%|          | 0/1840 [00:00<?, ?it/s]tokenizing...:  16%|█▌        | 287/1840 [00:00<00:00, 2863.13it/s]tokenizing...:  31%|███       | 574/1840 [00:00<00:00, 2866.36it/s]tokenizing...:  47%|████▋     | 861/1840 [00:00<00:00, 2776.33it/s]tokenizing...:  62%|██████▏   | 1139/1840 [00:00<00:00, 2754.75it/s]tokenizing...:  77%|███████▋  | 1415/1840 [00:00<00:00, 2672.31it/s]tokenizing...:  91%|█████████▏| 1683/1840 [00:00<00:00, 2515.79it/s]tokenizing...: 100%|██████████| 1840/1840 [00:00<00:00, 2634.44it/s]
09/04/2022 06:19:03 - INFO - root -   The nums of the train_dataset features is 6770
09/04/2022 06:19:03 - INFO - root -   The nums of the eval_dataset features is 1840
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
09/04/2022 06:19:08 - INFO - root -   ***** Running train *****
********************************************************************************
/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:52  batch_loss: 6.0310 [Training] 2/106 [..............................] - ETA: 1:07  batch_loss: 6.0293 [Training] 3/106 [..............................] - ETA: 52s  batch_loss: 6.0202 [Training] 4/106 [>.............................] - ETA: 45s  batch_loss: 6.0169 [Training] 5/106 [>.............................] - ETA: 41s  batch_loss: 5.9943 [Training] 6/106 [>.............................] - ETA: 38s  batch_loss: 5.9554 [Training] 7/106 [>.............................] - ETA: 36s  batch_loss: 5.9045 [Training] 8/106 [=>............................] - ETA: 34s  batch_loss: 5.8451 [Training] 9/106 [=>............................] - ETA: 33s  batch_loss: 5.7778 [Training] 10/106 [=>............................] - ETA: 31s  batch_loss: 5.7126 [Training] 11/106 [==>...........................] - ETA: 30s  batch_loss: 5.6517 [Training] 12/106 [==>...........................] - ETA: 29s  batch_loss: 5.5785 [Training] 13/106 [==>...........................] - ETA: 29s  batch_loss: 5.5080 [Training] 14/106 [==>...........................] - ETA: 28s  batch_loss: 5.4452 [Training] 15/106 [===>..........................] - ETA: 27s  batch_loss: 5.3740 [Training] 16/106 [===>..........................] - ETA: 26s  batch_loss: 5.2999 [Training] 17/106 [===>..........................] - ETA: 26s  batch_loss: 5.2221 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 5.1458 [Training] 19/106 [====>.........................] - ETA: 25s  batch_loss: 5.0635 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 4.9841 [Training] 21/106 [====>.........................] - ETA: 24s  batch_loss: 4.9054 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 4.8081 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 4.7254 [Training] 24/106 [=====>........................] - ETA: 23s  batch_loss: 4.6468 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 4.5647 [Training] 26/106 [======>.......................] - ETA: 22s  batch_loss: 4.4957 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 4.4197 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 4.3424 [Training] 29/106 [=======>......................] - ETA: 21s  batch_loss: 4.2678 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 4.1943 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 4.1293 [Training] 32/106 [========>.....................] - ETA: 20s  batch_loss: 4.0845 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 4.0255 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 3.9793 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 3.9305 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 3.8922 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 3.8439 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 3.8097 [Training] 39/106 [==========>...................] - ETA: 17s  batch_loss: 3.7704 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 3.7463 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 3.7084 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 3.6678 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 3.6328 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 3.6041 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 3.5776 [Training] 46/106 [============>.................] - ETA: 15s  batch_loss: 3.5468 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 3.5245 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 3.4946 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 3.4680 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 3.4405 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 3.4224 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 3.4026 [Training] 53/106 [==============>...............] - ETA: 13s  batch_loss: 3.3781 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 3.3579 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 3.3378 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 3.3246 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 3.3041 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 3.2811 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 3.2666 [Training] 60/106 [===============>..............] - ETA: 11s  batch_loss: 3.2528 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 3.2340 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 3.2189 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 3.2075 [Training] 64/106 [=================>............] - ETA: 10s  batch_loss: 3.1882 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 3.1772 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 3.1635 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 3.1522 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 3.1362 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 3.1206 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 3.1032 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 3.0887 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 3.0732 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 3.0631 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 3.0513 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 3.0378 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 3.0256 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 3.0106 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 2.9979 [Training] 79/106 [=====================>........] - ETA: 6s  batch_loss: 2.9842 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 2.9733 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 2.9611 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 2.9501 [Training] 83/106 [======================>.......] - ETA: 5s  batch_loss: 2.9383 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 2.9259 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 2.9146 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 2.9022 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 2.8943 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 2.8833 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 2.8734 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 2.8651 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 2.8567 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 2.8510 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 2.8393 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 2.8324 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 2.8247 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 2.8156 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 2.8095 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 2.8040 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 2.7979 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 2.7882 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 2.7809 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 2.7736 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 2.7660 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 2.7587 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 2.7505 [Training] 106/106 [==============================] 255.8ms/step  batch_loss: 2.7408 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:19:41 - INFO - root -   The F1-score is 0.015779092702169626
09/04/2022 06:19:41 - INFO - root -   the best eval f1 is 0.0158, saving model !!
09/04/2022 06:19:44 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:24  batch_loss: 1.7560 [Training] 2/106 [..............................] - ETA: 54s  batch_loss: 1.8191 [Training] 3/106 [..............................] - ETA: 44s  batch_loss: 1.8137 [Training] 4/106 [>.............................] - ETA: 39s  batch_loss: 1.8119 [Training] 5/106 [>.............................] - ETA: 36s  batch_loss: 1.8527 [Training] 6/106 [>.............................] - ETA: 34s  batch_loss: 1.8676 [Training] 7/106 [>.............................] - ETA: 32s  batch_loss: 1.8788 [Training] 8/106 [=>............................] - ETA: 31s  batch_loss: 1.9000 [Training] 9/106 [=>............................] - ETA: 30s  batch_loss: 1.9144 [Training] 10/106 [=>............................] - ETA: 29s  batch_loss: 1.9015 [Training] 11/106 [==>...........................] - ETA: 28s  batch_loss: 1.9023 [Training] 12/106 [==>...........................] - ETA: 27s  batch_loss: 1.8972 [Training] 13/106 [==>...........................] - ETA: 27s  batch_loss: 1.8931 [Training] 14/106 [==>...........................] - ETA: 26s  batch_loss: 1.8834 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 1.8857 [Training] 16/106 [===>..........................] - ETA: 25s  batch_loss: 1.8850 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 1.8847 [Training] 18/106 [====>.........................] - ETA: 24s  batch_loss: 1.8789 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 1.8738 [Training] 20/106 [====>.........................] - ETA: 23s  batch_loss: 1.8903 [Training] 21/106 [====>.........................] - ETA: 23s  batch_loss: 1.9024 [Training] 22/106 [=====>........................] - ETA: 22s  batch_loss: 1.9047 [Training] 23/106 [=====>........................] - ETA: 22s  batch_loss: 1.9036 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 1.9004 [Training] 25/106 [======>.......................] - ETA: 21s  batch_loss: 1.8980 [Training] 26/106 [======>.......................] - ETA: 21s  batch_loss: 1.9006 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 1.9005 [Training] 28/106 [======>.......................] - ETA: 20s  batch_loss: 1.8954 [Training] 29/106 [=======>......................] - ETA: 20s  batch_loss: 1.8836 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 1.8755 [Training] 31/106 [=======>......................] - ETA: 19s  batch_loss: 1.8755 [Training] 32/106 [========>.....................] - ETA: 19s  batch_loss: 1.8722 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 1.8756 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 1.8813 [Training] 35/106 [========>.....................] - ETA: 18s  batch_loss: 1.8805 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 1.8783 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 1.8716 [Training] 38/106 [=========>....................] - ETA: 17s  batch_loss: 1.8681 [Training] 39/106 [==========>...................] - ETA: 17s  batch_loss: 1.8666 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 1.8635 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 1.8615 [Training] 42/106 [==========>...................] - ETA: 16s  batch_loss: 1.8533 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 1.8484 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 1.8469 [Training] 45/106 [===========>..................] - ETA: 15s  batch_loss: 1.8436 [Training] 46/106 [============>.................] - ETA: 15s  batch_loss: 1.8369 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 1.8290 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 1.8291 [Training] 49/106 [============>.................] - ETA: 14s  batch_loss: 1.8264 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 1.8260 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 1.8256 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 1.8218 [Training] 53/106 [==============>...............] - ETA: 13s  batch_loss: 1.8189 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 1.8123 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 1.8079 [Training] 56/106 [==============>...............] - ETA: 12s  batch_loss: 1.8051 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 1.8014 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 1.7993 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 1.8029 [Training] 60/106 [===============>..............] - ETA: 11s  batch_loss: 1.7968 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 1.7926 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 1.7932 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 1.7906 [Training] 64/106 [=================>............] - ETA: 10s  batch_loss: 1.7883 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 1.7863 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 1.7835 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 1.7800 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 1.7787 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 1.7787 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 1.7764 [Training] 71/106 [===================>..........] - ETA: 8s  batch_loss: 1.7775 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 1.7749 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 1.7755 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 1.7722 [Training] 75/106 [====================>.........] - ETA: 7s  batch_loss: 1.7674 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 1.7689 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 1.7680 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 1.7658 [Training] 79/106 [=====================>........] - ETA: 6s  batch_loss: 1.7666 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 1.7638 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 1.7614 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 1.7583 [Training] 83/106 [======================>.......] - ETA: 5s  batch_loss: 1.7555 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 1.7507 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 1.7496 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 1.7462 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 1.7425 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 1.7384 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 1.7368 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 1.7372 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 1.7345 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 1.7321 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 1.7319 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 1.7292 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 1.7252 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 1.7228 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 1.7200 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 1.7193 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 1.7155 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 1.7154 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 1.7134 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 1.7121 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 1.7074 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 1.7043 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 1.7019 [Training] 106/106 [==============================] 254.5ms/step  batch_loss: 1.7013 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:20:17 - INFO - root -   The F1-score is 0.07202908429902295
09/04/2022 06:20:17 - INFO - root -   the best eval f1 is 0.0720, saving model !!
09/04/2022 06:20:20 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:28  batch_loss: 1.4162 [Training] 2/106 [..............................] - ETA: 56s  batch_loss: 1.4322 [Training] 3/106 [..............................] - ETA: 46s  batch_loss: 1.4224 [Training] 4/106 [>.............................] - ETA: 40s  batch_loss: 1.4285 [Training] 5/106 [>.............................] - ETA: 37s  batch_loss: 1.4152 [Training] 6/106 [>.............................] - ETA: 34s  batch_loss: 1.4319 [Training] 7/106 [>.............................] - ETA: 33s  batch_loss: 1.4281 [Training] 8/106 [=>............................] - ETA: 31s  batch_loss: 1.4417 [Training] 9/106 [=>............................] - ETA: 30s  batch_loss: 1.4493 [Training] 10/106 [=>............................] - ETA: 29s  batch_loss: 1.4294 [Training] 11/106 [==>...........................] - ETA: 28s  batch_loss: 1.4220 [Training] 12/106 [==>...........................] - ETA: 28s  batch_loss: 1.4259 [Training] 13/106 [==>...........................] - ETA: 27s  batch_loss: 1.4210 [Training] 14/106 [==>...........................] - ETA: 26s  batch_loss: 1.4246 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 1.4175 [Training] 16/106 [===>..........................] - ETA: 25s  batch_loss: 1.4181 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 1.4223 [Training] 18/106 [====>.........................] - ETA: 24s  batch_loss: 1.4207 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 1.4213 [Training] 20/106 [====>.........................] - ETA: 23s  batch_loss: 1.4180 [Training] 21/106 [====>.........................] - ETA: 23s  batch_loss: 1.4261 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 1.4317 [Training] 23/106 [=====>........................] - ETA: 22s  batch_loss: 1.4223 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 1.4161 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 1.4155 [Training] 26/106 [======>.......................] - ETA: 21s  batch_loss: 1.4135 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 1.4126 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 1.4100 [Training] 29/106 [=======>......................] - ETA: 20s  batch_loss: 1.4045 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 1.4078 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 1.4057 [Training] 32/106 [========>.....................] - ETA: 19s  batch_loss: 1.4021 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 1.4020 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 1.3990 [Training] 35/106 [========>.....................] - ETA: 18s  batch_loss: 1.3957 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 1.3992 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 1.3955 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 1.3926 [Training] 39/106 [==========>...................] - ETA: 17s  batch_loss: 1.3984 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 1.3964 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 1.3957 [Training] 42/106 [==========>...................] - ETA: 16s  batch_loss: 1.3886 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 1.3902 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 1.3859 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 1.3830 [Training] 46/106 [============>.................] - ETA: 15s  batch_loss: 1.3844 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 1.3886 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 1.3849 [Training] 49/106 [============>.................] - ETA: 14s  batch_loss: 1.3877 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 1.3860 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 1.3853 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 1.3841 [Training] 53/106 [==============>...............] - ETA: 13s  batch_loss: 1.3793 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 1.3759 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 1.3756 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 1.3751 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 1.3707 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 1.3670 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 1.3675 [Training] 60/106 [===============>..............] - ETA: 11s  batch_loss: 1.3676 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 1.3664 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 1.3646 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 1.3631 [Training] 64/106 [=================>............] - ETA: 10s  batch_loss: 1.3619 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 1.3626 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 1.3652 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 1.3619 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 1.3624 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 1.3645 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 1.3623 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 1.3607 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 1.3613 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 1.3608 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 1.3579 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 1.3564 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 1.3570 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 1.3558 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 1.3534 [Training] 79/106 [=====================>........] - ETA: 6s  batch_loss: 1.3492 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 1.3487 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 1.3479 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 1.3477 [Training] 83/106 [======================>.......] - ETA: 5s  batch_loss: 1.3478 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 1.3477 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 1.3485 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 1.3468 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 1.3455 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 1.3431 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 1.3420 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 1.3400 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 1.3385 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 1.3373 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 1.3358 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 1.3356 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 1.3335 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 1.3327 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 1.3322 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 1.3313 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 1.3302 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 1.3290 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 1.3288 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 1.3275 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 1.3269 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 1.3254 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 1.3242 [Training] 106/106 [==============================] 255.9ms/step  batch_loss: 1.3237 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:20:53 - INFO - root -   The F1-score is 0.20319176768117203
09/04/2022 06:20:53 - INFO - root -   the best eval f1 is 0.2032, saving model !!
09/04/2022 06:20:56 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:24  batch_loss: 1.1391 [Training] 2/106 [..............................] - ETA: 55s  batch_loss: 1.1528 [Training] 3/106 [..............................] - ETA: 45s  batch_loss: 1.1318 [Training] 4/106 [>.............................] - ETA: 39s  batch_loss: 1.1554 [Training] 5/106 [>.............................] - ETA: 36s  batch_loss: 1.1515 [Training] 6/106 [>.............................] - ETA: 33s  batch_loss: 1.1550 [Training] 7/106 [>.............................] - ETA: 32s  batch_loss: 1.1618 [Training] 8/106 [=>............................] - ETA: 31s  batch_loss: 1.1630 [Training] 9/106 [=>............................] - ETA: 30s  batch_loss: 1.1774 [Training] 10/106 [=>............................] - ETA: 29s  batch_loss: 1.1752 [Training] 11/106 [==>...........................] - ETA: 28s  batch_loss: 1.1706 [Training] 12/106 [==>...........................] - ETA: 27s  batch_loss: 1.1721 [Training] 13/106 [==>...........................] - ETA: 27s  batch_loss: 1.1714 [Training] 14/106 [==>...........................] - ETA: 26s  batch_loss: 1.1588 [Training] 15/106 [===>..........................] - ETA: 25s  batch_loss: 1.1633 [Training] 16/106 [===>..........................] - ETA: 25s  batch_loss: 1.1577 [Training] 17/106 [===>..........................] - ETA: 24s  batch_loss: 1.1558 [Training] 18/106 [====>.........................] - ETA: 24s  batch_loss: 1.1569 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 1.1500 [Training] 20/106 [====>.........................] - ETA: 23s  batch_loss: 1.1546 [Training] 21/106 [====>.........................] - ETA: 23s  batch_loss: 1.1482 [Training] 22/106 [=====>........................] - ETA: 22s  batch_loss: 1.1514 [Training] 23/106 [=====>........................] - ETA: 22s  batch_loss: 1.1495 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 1.1467 [Training] 25/106 [======>.......................] - ETA: 21s  batch_loss: 1.1422 [Training] 26/106 [======>.......................] - ETA: 21s  batch_loss: 1.1355 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 1.1353 [Training] 28/106 [======>.......................] - ETA: 20s  batch_loss: 1.1289 [Training] 29/106 [=======>......................] - ETA: 20s  batch_loss: 1.1334 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 1.1284 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 1.1251 [Training] 32/106 [========>.....................] - ETA: 19s  batch_loss: 1.1259 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 1.1273 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 1.1316 [Training] 35/106 [========>.....................] - ETA: 18s  batch_loss: 1.1353 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 1.1321 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 1.1305 [Training] 38/106 [=========>....................] - ETA: 17s  batch_loss: 1.1274 [Training] 39/106 [==========>...................] - ETA: 17s  batch_loss: 1.1227 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 1.1223 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 1.1223 [Training] 42/106 [==========>...................] - ETA: 16s  batch_loss: 1.1245 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 1.1218 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 1.1219 [Training] 45/106 [===========>..................] - ETA: 15s  batch_loss: 1.1216 [Training] 46/106 [============>.................] - ETA: 15s  batch_loss: 1.1219 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 1.1233 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 1.1205 [Training] 49/106 [============>.................] - ETA: 14s  batch_loss: 1.1201 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 1.1186 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 1.1187 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 1.1151 [Training] 53/106 [==============>...............] - ETA: 13s  batch_loss: 1.1145 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 1.1178 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 1.1136 [Training] 56/106 [==============>...............] - ETA: 12s  batch_loss: 1.1121 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 1.1100 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 1.1105 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 1.1114 [Training] 60/106 [===============>..............] - ETA: 11s  batch_loss: 1.1099 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 1.1099 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 1.1090 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 1.1087 [Training] 64/106 [=================>............] - ETA: 10s  batch_loss: 1.1055 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 1.1071 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 1.1052 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 1.1033 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 1.1040 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 1.1043 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 1.1015 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 1.1003 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 1.0992 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 1.0980 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 1.0966 [Training] 75/106 [====================>.........] - ETA: 7s  batch_loss: 1.0965 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 1.0941 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 1.0926 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 1.0933 [Training] 79/106 [=====================>........] - ETA: 6s  batch_loss: 1.0939 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 1.0939 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 1.0935 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 1.0953 [Training] 83/106 [======================>.......] - ETA: 5s  batch_loss: 1.0959 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 1.0955 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 1.0955 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 1.0994 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 1.1000 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 1.0983 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 1.0979 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 1.0981 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 1.0966 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 1.0969 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 1.0968 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 1.0978 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 1.0974 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 1.0985 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 1.1002 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 1.1003 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 1.1000 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 1.0985 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 1.0984 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 1.0967 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 1.0958 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 1.0949 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 1.0950 [Training] 106/106 [==============================] 255.5ms/step  batch_loss: 1.0941 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:21:29 - INFO - root -   The F1-score is 0.3505308808135188
09/04/2022 06:21:29 - INFO - root -   the best eval f1 is 0.3505, saving model !!
09/04/2022 06:21:32 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:27  batch_loss: 1.0097 [Training] 2/106 [..............................] - ETA: 56s  batch_loss: 1.0062 [Training] 3/106 [..............................] - ETA: 46s  batch_loss: 0.9929 [Training] 4/106 [>.............................] - ETA: 40s  batch_loss: 0.9704 [Training] 5/106 [>.............................] - ETA: 37s  batch_loss: 0.9834 [Training] 6/106 [>.............................] - ETA: 34s  batch_loss: 0.9840 [Training] 7/106 [>.............................] - ETA: 33s  batch_loss: 0.9951 [Training] 8/106 [=>............................] - ETA: 31s  batch_loss: 1.0037 [Training] 9/106 [=>............................] - ETA: 30s  batch_loss: 1.0073 [Training] 10/106 [=>............................] - ETA: 29s  batch_loss: 0.9988 [Training] 11/106 [==>...........................] - ETA: 28s  batch_loss: 1.0137 [Training] 12/106 [==>...........................] - ETA: 28s  batch_loss: 1.0020 [Training] 13/106 [==>...........................] - ETA: 27s  batch_loss: 0.9959 [Training] 14/106 [==>...........................] - ETA: 27s  batch_loss: 0.9880 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 0.9788 [Training] 16/106 [===>..........................] - ETA: 25s  batch_loss: 0.9820 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.9813 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 0.9776 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 0.9720 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.9771 [Training] 21/106 [====>.........................] - ETA: 23s  batch_loss: 0.9727 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.9732 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 0.9777 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.9736 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.9752 [Training] 26/106 [======>.......................] - ETA: 21s  batch_loss: 0.9801 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.9803 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.9749 [Training] 29/106 [=======>......................] - ETA: 20s  batch_loss: 0.9776 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.9755 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.9764 [Training] 32/106 [========>.....................] - ETA: 19s  batch_loss: 0.9783 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.9744 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.9754 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.9765 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.9743 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.9749 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.9775 [Training] 39/106 [==========>...................] - ETA: 17s  batch_loss: 0.9753 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.9763 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.9734 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 0.9749 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.9760 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.9707 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.9696 [Training] 46/106 [============>.................] - ETA: 15s  batch_loss: 0.9680 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.9645 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.9632 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.9613 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.9602 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.9605 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.9565 [Training] 53/106 [==============>...............] - ETA: 13s  batch_loss: 0.9561 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.9589 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.9587 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.9561 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.9544 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.9512 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.9524 [Training] 60/106 [===============>..............] - ETA: 11s  batch_loss: 0.9509 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.9490 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.9497 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.9492 [Training] 64/106 [=================>............] - ETA: 10s  batch_loss: 0.9473 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.9486 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.9484 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.9494 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.9494 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.9501 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.9481 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.9455 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.9450 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.9445 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.9446 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.9450 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.9434 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.9435 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.9417 [Training] 79/106 [=====================>........] - ETA: 6s  batch_loss: 0.9405 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.9394 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.9378 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.9384 [Training] 83/106 [======================>.......] - ETA: 5s  batch_loss: 0.9386 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.9374 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.9351 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.9362 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.9366 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.9369 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.9364 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.9342 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.9326 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.9322 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.9309 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.9294 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.9292 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.9276 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.9270 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.9273 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.9277 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.9268 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.9254 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.9247 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.9233 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.9236 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.9238 [Training] 106/106 [==============================] 257.0ms/step  batch_loss: 0.9232 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:22:06 - INFO - root -   The F1-score is 0.4033295497540673
09/04/2022 06:22:06 - INFO - root -   the best eval f1 is 0.4033, saving model !!
09/04/2022 06:22:08 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:29  batch_loss: 0.7895 [Training] 2/106 [..............................] - ETA: 57s  batch_loss: 0.8450 [Training] 3/106 [..............................] - ETA: 46s  batch_loss: 0.8273 [Training] 4/106 [>.............................] - ETA: 40s  batch_loss: 0.8012 [Training] 5/106 [>.............................] - ETA: 37s  batch_loss: 0.7838 [Training] 6/106 [>.............................] - ETA: 34s  batch_loss: 0.8083 [Training] 7/106 [>.............................] - ETA: 33s  batch_loss: 0.8133 [Training] 8/106 [=>............................] - ETA: 31s  batch_loss: 0.8141 [Training] 9/106 [=>............................] - ETA: 30s  batch_loss: 0.8097 [Training] 10/106 [=>............................] - ETA: 29s  batch_loss: 0.8104 [Training] 11/106 [==>...........................] - ETA: 28s  batch_loss: 0.8026 [Training] 12/106 [==>...........................] - ETA: 28s  batch_loss: 0.8143 [Training] 13/106 [==>...........................] - ETA: 27s  batch_loss: 0.8103 [Training] 14/106 [==>...........................] - ETA: 26s  batch_loss: 0.8126 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 0.8135 [Training] 16/106 [===>..........................] - ETA: 25s  batch_loss: 0.8164 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.8166 [Training] 18/106 [====>.........................] - ETA: 24s  batch_loss: 0.8147 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 0.8140 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.8177 [Training] 21/106 [====>.........................] - ETA: 23s  batch_loss: 0.8160 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.8134 [Training] 23/106 [=====>........................] - ETA: 22s  batch_loss: 0.8153 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.8099 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.8048 [Training] 26/106 [======>.......................] - ETA: 21s  batch_loss: 0.8051 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.8041 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.8025 [Training] 29/106 [=======>......................] - ETA: 20s  batch_loss: 0.8031 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.8035 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.8021 [Training] 32/106 [========>.....................] - ETA: 19s  batch_loss: 0.8020 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.8063 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.8045 [Training] 35/106 [========>.....................] - ETA: 18s  batch_loss: 0.8050 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.8063 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.8049 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.8048 [Training] 39/106 [==========>...................] - ETA: 17s  batch_loss: 0.8071 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.8067 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.8085 [Training] 42/106 [==========>...................] - ETA: 16s  batch_loss: 0.8062 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.8070 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.8065 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.8049 [Training] 46/106 [============>.................] - ETA: 15s  batch_loss: 0.8081 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.8064 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.8056 [Training] 49/106 [============>.................] - ETA: 14s  batch_loss: 0.8075 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.8080 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.8060 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.8057 [Training] 53/106 [==============>...............] - ETA: 13s  batch_loss: 0.8067 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.8066 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.8036 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.8045 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.8040 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.8045 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.8059 [Training] 60/106 [===============>..............] - ETA: 11s  batch_loss: 0.8071 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.8066 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.8042 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.8028 [Training] 64/106 [=================>............] - ETA: 10s  batch_loss: 0.8034 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.8022 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.8041 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.8024 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.8016 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.8030 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.8016 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.8011 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.8006 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.8019 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.8004 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.8009 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.8003 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.8011 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.8014 [Training] 79/106 [=====================>........] - ETA: 6s  batch_loss: 0.8014 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.8009 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.8005 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.7995 [Training] 83/106 [======================>.......] - ETA: 5s  batch_loss: 0.7996 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.8000 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.7993 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.7982 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.7979 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.7976 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.7974 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.7966 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.7959 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.7966 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.7963 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.7957 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.7955 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.7945 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.7938 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.7931 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.7923 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.7916 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.7920 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.7913 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.7903 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.7886 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.7883 [Training] 106/106 [==============================] 257.2ms/step  batch_loss: 0.7886 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:22:42 - INFO - root -   The F1-score is 0.47314222098524916
09/04/2022 06:22:42 - INFO - root -   the best eval f1 is 0.4731, saving model !!
09/04/2022 06:22:45 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:36  batch_loss: 0.7803 [Training] 2/106 [..............................] - ETA: 1:00  batch_loss: 0.6741 [Training] 3/106 [..............................] - ETA: 48s  batch_loss: 0.6857 [Training] 4/106 [>.............................] - ETA: 42s  batch_loss: 0.6947 [Training] 5/106 [>.............................] - ETA: 38s  batch_loss: 0.6718 [Training] 6/106 [>.............................] - ETA: 36s  batch_loss: 0.6679 [Training] 7/106 [>.............................] - ETA: 34s  batch_loss: 0.6598 [Training] 8/106 [=>............................] - ETA: 33s  batch_loss: 0.6685 [Training] 9/106 [=>............................] - ETA: 31s  batch_loss: 0.6656 [Training] 10/106 [=>............................] - ETA: 30s  batch_loss: 0.6696 [Training] 11/106 [==>...........................] - ETA: 29s  batch_loss: 0.6643 [Training] 12/106 [==>...........................] - ETA: 28s  batch_loss: 0.6682 [Training] 13/106 [==>...........................] - ETA: 28s  batch_loss: 0.6681 [Training] 14/106 [==>...........................] - ETA: 27s  batch_loss: 0.6670 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 0.6714 [Training] 16/106 [===>..........................] - ETA: 26s  batch_loss: 0.6730 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.6690 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 0.6684 [Training] 19/106 [====>.........................] - ETA: 25s  batch_loss: 0.6664 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.6664 [Training] 21/106 [====>.........................] - ETA: 24s  batch_loss: 0.6666 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.6706 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 0.6760 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.6786 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.6836 [Training] 26/106 [======>.......................] - ETA: 22s  batch_loss: 0.6956 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.6923 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.6930 [Training] 29/106 [=======>......................] - ETA: 21s  batch_loss: 0.6969 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.6936 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.6945 [Training] 32/106 [========>.....................] - ETA: 20s  batch_loss: 0.6915 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.6947 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.6992 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.7000 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.7007 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.6999 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.6983 [Training] 39/106 [==========>...................] - ETA: 18s  batch_loss: 0.6991 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.6971 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.6948 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 0.6930 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.6923 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.6930 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.6934 [Training] 46/106 [============>.................] - ETA: 16s  batch_loss: 0.6919 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.6921 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.6910 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.6901 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.6898 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.6890 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.6879 [Training] 53/106 [==============>...............] - ETA: 14s  batch_loss: 0.6895 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.6901 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.6895 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.6875 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.6859 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.6855 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.6868 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.6893 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.6909 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.6893 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.6913 [Training] 64/106 [=================>............] - ETA: 11s  batch_loss: 0.6921 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.6933 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.6935 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.6919 [Training] 68/106 [==================>...........] - ETA: 10s  batch_loss: 0.6925 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.6911 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.6906 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.6905 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.6902 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.6881 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.6893 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.6897 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.6885 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.6886 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.6879 [Training] 79/106 [=====================>........] - ETA: 7s  batch_loss: 0.6876 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.6868 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.6867 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.6857 [Training] 83/106 [======================>.......] - ETA: 6s  batch_loss: 0.6864 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.6890 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.6878 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.6876 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.6872 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.6863 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.6853 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.6847 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.6842 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.6840 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.6825 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.6824 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.6826 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.6822 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.6824 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.6825 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.6822 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.6814 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.6801 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.6796 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.6799 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.6799 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.6798 [Training] 106/106 [==============================] 260.0ms/step  batch_loss: 0.6802 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:23:19 - INFO - root -   The F1-score is 0.4869376399538577
09/04/2022 06:23:19 - INFO - root -   the best eval f1 is 0.4869, saving model !!
09/04/2022 06:23:21 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:33  batch_loss: 0.5536 [Training] 2/106 [..............................] - ETA: 59s  batch_loss: 0.5872 [Training] 3/106 [..............................] - ETA: 48s  batch_loss: 0.6070 [Training] 4/106 [>.............................] - ETA: 41s  batch_loss: 0.6052 [Training] 5/106 [>.............................] - ETA: 38s  batch_loss: 0.5953 [Training] 6/106 [>.............................] - ETA: 35s  batch_loss: 0.6030 [Training] 7/106 [>.............................] - ETA: 33s  batch_loss: 0.6031 [Training] 8/106 [=>............................] - ETA: 32s  batch_loss: 0.6065 [Training] 9/106 [=>............................] - ETA: 31s  batch_loss: 0.6090 [Training] 10/106 [=>............................] - ETA: 30s  batch_loss: 0.6093 [Training] 11/106 [==>...........................] - ETA: 29s  batch_loss: 0.6119 [Training] 12/106 [==>...........................] - ETA: 28s  batch_loss: 0.6107 [Training] 13/106 [==>...........................] - ETA: 27s  batch_loss: 0.6118 [Training] 14/106 [==>...........................] - ETA: 27s  batch_loss: 0.6097 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 0.6070 [Training] 16/106 [===>..........................] - ETA: 26s  batch_loss: 0.6052 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.6056 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 0.6031 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 0.6009 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.6019 [Training] 21/106 [====>.........................] - ETA: 23s  batch_loss: 0.6029 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.6096 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 0.6087 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.6122 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.6115 [Training] 26/106 [======>.......................] - ETA: 22s  batch_loss: 0.6089 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.6095 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.6127 [Training] 29/106 [=======>......................] - ETA: 21s  batch_loss: 0.6107 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.6109 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.6109 [Training] 32/106 [========>.....................] - ETA: 20s  batch_loss: 0.6077 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.6064 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.6081 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.6068 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.6062 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.6056 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.6063 [Training] 39/106 [==========>...................] - ETA: 18s  batch_loss: 0.6056 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.6054 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.6061 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 0.6044 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.6042 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.6053 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.6042 [Training] 46/106 [============>.................] - ETA: 16s  batch_loss: 0.6035 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.6045 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.6069 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.6065 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.6069 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.6069 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.6084 [Training] 53/106 [==============>...............] - ETA: 14s  batch_loss: 0.6090 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.6102 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.6083 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.6086 [Training] 57/106 [===============>..............] - ETA: 13s  batch_loss: 0.6084 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.6056 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.6060 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.6062 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.6054 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.6043 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.6038 [Training] 64/106 [=================>............] - ETA: 11s  batch_loss: 0.6032 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.6039 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.6035 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.6030 [Training] 68/106 [==================>...........] - ETA: 10s  batch_loss: 0.6029 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.6023 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.6003 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.6003 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.6002 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.5995 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.5981 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.5962 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.5986 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.5986 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.5984 [Training] 79/106 [=====================>........] - ETA: 7s  batch_loss: 0.5983 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.5983 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.5969 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.5964 [Training] 83/106 [======================>.......] - ETA: 6s  batch_loss: 0.5958 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.5952 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.5949 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.5956 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.5954 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.5950 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.5950 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.5946 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.5935 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.5926 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.5927 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.5920 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.5916 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.5910 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.5902 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.5913 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.5910 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.5909 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.5896 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.5894 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.5894 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.5888 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.5888 [Training] 106/106 [==============================] 259.8ms/step  batch_loss: 0.5881 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:23:55 - INFO - root -   The F1-score is 0.5160215841716075
09/04/2022 06:23:55 - INFO - root -   the best eval f1 is 0.5160, saving model !!
09/04/2022 06:23:58 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:32  batch_loss: 0.4683 [Training] 2/106 [..............................] - ETA: 58s  batch_loss: 0.5516 [Training] 3/106 [..............................] - ETA: 47s  batch_loss: 0.5447 [Training] 4/106 [>.............................] - ETA: 41s  batch_loss: 0.5412 [Training] 5/106 [>.............................] - ETA: 38s  batch_loss: 0.5385 [Training] 6/106 [>.............................] - ETA: 35s  batch_loss: 0.5353 [Training] 7/106 [>.............................] - ETA: 33s  batch_loss: 0.5301 [Training] 8/106 [=>............................] - ETA: 32s  batch_loss: 0.5242 [Training] 9/106 [=>............................] - ETA: 31s  batch_loss: 0.5285 [Training] 10/106 [=>............................] - ETA: 30s  batch_loss: 0.5299 [Training] 11/106 [==>...........................] - ETA: 29s  batch_loss: 0.5276 [Training] 12/106 [==>...........................] - ETA: 28s  batch_loss: 0.5263 [Training] 13/106 [==>...........................] - ETA: 28s  batch_loss: 0.5288 [Training] 14/106 [==>...........................] - ETA: 27s  batch_loss: 0.5274 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 0.5198 [Training] 16/106 [===>..........................] - ETA: 26s  batch_loss: 0.5261 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.5322 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 0.5339 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 0.5315 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.5295 [Training] 21/106 [====>.........................] - ETA: 24s  batch_loss: 0.5274 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.5223 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 0.5199 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.5234 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.5235 [Training] 26/106 [======>.......................] - ETA: 22s  batch_loss: 0.5222 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.5264 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.5265 [Training] 29/106 [=======>......................] - ETA: 21s  batch_loss: 0.5270 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.5272 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.5256 [Training] 32/106 [========>.....................] - ETA: 20s  batch_loss: 0.5248 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.5245 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.5256 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.5272 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.5268 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.5255 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.5271 [Training] 39/106 [==========>...................] - ETA: 17s  batch_loss: 0.5275 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.5280 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.5288 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 0.5283 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.5284 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.5279 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.5276 [Training] 46/106 [============>.................] - ETA: 15s  batch_loss: 0.5267 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.5266 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.5275 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.5270 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.5253 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.5257 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.5254 [Training] 53/106 [==============>...............] - ETA: 14s  batch_loss: 0.5248 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.5237 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.5234 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.5249 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.5256 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.5253 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.5241 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.5231 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.5221 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.5212 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.5235 [Training] 64/106 [=================>............] - ETA: 11s  batch_loss: 0.5228 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.5239 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.5245 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.5246 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.5250 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.5250 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.5246 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.5252 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.5258 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.5266 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.5262 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.5260 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.5262 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.5255 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.5252 [Training] 79/106 [=====================>........] - ETA: 7s  batch_loss: 0.5251 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.5244 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.5236 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.5235 [Training] 83/106 [======================>.......] - ETA: 6s  batch_loss: 0.5225 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.5213 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.5214 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.5216 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.5205 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.5195 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.5190 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.5177 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.5175 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.5170 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.5168 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.5165 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.5157 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.5152 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.5150 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.5144 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.5148 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.5152 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.5153 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.5151 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.5152 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.5151 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.5161 [Training] 106/106 [==============================] 259.4ms/step  batch_loss: 0.5156 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:24:32 - INFO - root -   The F1-score is 0.5490728791686477
09/04/2022 06:24:32 - INFO - root -   the best eval f1 is 0.5491, saving model !!
09/04/2022 06:24:34 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 2:23  batch_loss: 0.4398 [Training] 2/106 [..............................] - ETA: 1:24  batch_loss: 0.4732 [Training] 3/106 [..............................] - ETA: 1:04  batch_loss: 0.4532 [Training] 4/106 [>.............................] - ETA: 54s  batch_loss: 0.4903 [Training] 5/106 [>.............................] - ETA: 48s  batch_loss: 0.4911 [Training] 6/106 [>.............................] - ETA: 43s  batch_loss: 0.5037 [Training] 7/106 [>.............................] - ETA: 40s  batch_loss: 0.4900 [Training] 8/106 [=>............................] - ETA: 38s  batch_loss: 0.4874 [Training] 9/106 [=>............................] - ETA: 36s  batch_loss: 0.4844 [Training] 10/106 [=>............................] - ETA: 34s  batch_loss: 0.4863 [Training] 11/106 [==>...........................] - ETA: 33s  batch_loss: 0.4818 [Training] 12/106 [==>...........................] - ETA: 32s  batch_loss: 0.4728 [Training] 13/106 [==>...........................] - ETA: 31s  batch_loss: 0.4716 [Training] 14/106 [==>...........................] - ETA: 30s  batch_loss: 0.4672 [Training] 15/106 [===>..........................] - ETA: 29s  batch_loss: 0.4680 [Training] 16/106 [===>..........................] - ETA: 28s  batch_loss: 0.4659 [Training] 17/106 [===>..........................] - ETA: 28s  batch_loss: 0.4626 [Training] 18/106 [====>.........................] - ETA: 27s  batch_loss: 0.4566 [Training] 19/106 [====>.........................] - ETA: 26s  batch_loss: 0.4584 [Training] 20/106 [====>.........................] - ETA: 26s  batch_loss: 0.4586 [Training] 21/106 [====>.........................] - ETA: 25s  batch_loss: 0.4610 [Training] 22/106 [=====>........................] - ETA: 25s  batch_loss: 0.4608 [Training] 23/106 [=====>........................] - ETA: 24s  batch_loss: 0.4600 [Training] 24/106 [=====>........................] - ETA: 24s  batch_loss: 0.4630 [Training] 25/106 [======>.......................] - ETA: 24s  batch_loss: 0.4661 [Training] 26/106 [======>.......................] - ETA: 23s  batch_loss: 0.4696 [Training] 27/106 [======>.......................] - ETA: 23s  batch_loss: 0.4686 [Training] 28/106 [======>.......................] - ETA: 22s  batch_loss: 0.4711 [Training] 29/106 [=======>......................] - ETA: 22s  batch_loss: 0.4728 [Training] 30/106 [=======>......................] - ETA: 22s  batch_loss: 0.4745 [Training] 31/106 [=======>......................] - ETA: 21s  batch_loss: 0.4735 [Training] 32/106 [========>.....................] - ETA: 21s  batch_loss: 0.4755 [Training] 33/106 [========>.....................] - ETA: 20s  batch_loss: 0.4760 [Training] 34/106 [========>.....................] - ETA: 20s  batch_loss: 0.4778 [Training] 35/106 [========>.....................] - ETA: 20s  batch_loss: 0.4783 [Training] 36/106 [=========>....................] - ETA: 19s  batch_loss: 0.4791 [Training] 37/106 [=========>....................] - ETA: 19s  batch_loss: 0.4770 [Training] 38/106 [=========>....................] - ETA: 19s  batch_loss: 0.4791 [Training] 39/106 [==========>...................] - ETA: 18s  batch_loss: 0.4801 [Training] 40/106 [==========>...................] - ETA: 18s  batch_loss: 0.4802 [Training] 41/106 [==========>...................] - ETA: 18s  batch_loss: 0.4804 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 0.4788 [Training] 43/106 [===========>..................] - ETA: 17s  batch_loss: 0.4784 [Training] 44/106 [===========>..................] - ETA: 17s  batch_loss: 0.4769 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.4755 [Training] 46/106 [============>.................] - ETA: 16s  batch_loss: 0.4751 [Training] 47/106 [============>.................] - ETA: 16s  batch_loss: 0.4724 [Training] 48/106 [============>.................] - ETA: 16s  batch_loss: 0.4721 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.4710 [Training] 50/106 [=============>................] - ETA: 15s  batch_loss: 0.4686 [Training] 51/106 [=============>................] - ETA: 15s  batch_loss: 0.4681 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.4680 [Training] 53/106 [==============>...............] - ETA: 14s  batch_loss: 0.4675 [Training] 54/106 [==============>...............] - ETA: 14s  batch_loss: 0.4672 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.4667 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.4681 [Training] 57/106 [===============>..............] - ETA: 13s  batch_loss: 0.4679 [Training] 58/106 [===============>..............] - ETA: 13s  batch_loss: 0.4678 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.4678 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.4690 [Training] 61/106 [================>.............] - ETA: 12s  batch_loss: 0.4683 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.4682 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.4682 [Training] 64/106 [=================>............] - ETA: 11s  batch_loss: 0.4668 [Training] 65/106 [=================>............] - ETA: 11s  batch_loss: 0.4663 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.4663 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.4660 [Training] 68/106 [==================>...........] - ETA: 10s  batch_loss: 0.4667 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.4662 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.4661 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.4654 [Training] 72/106 [===================>..........] - ETA: 9s  batch_loss: 0.4652 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.4648 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.4656 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.4655 [Training] 76/106 [====================>.........] - ETA: 8s  batch_loss: 0.4653 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.4653 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.4652 [Training] 79/106 [=====================>........] - ETA: 7s  batch_loss: 0.4644 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.4644 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.4628 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.4622 [Training] 83/106 [======================>.......] - ETA: 6s  batch_loss: 0.4618 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.4611 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.4614 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.4609 [Training] 87/106 [=======================>......] - ETA: 5s  batch_loss: 0.4606 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.4599 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.4598 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.4601 [Training] 91/106 [========================>.....] - ETA: 4s  batch_loss: 0.4604 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.4614 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.4622 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.4623 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.4620 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.4615 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.4617 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.4620 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.4620 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.4619 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.4619 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.4618 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.4615 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.4610 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.4611 [Training] 106/106 [==============================] 263.8ms/step  batch_loss: 0.4610 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:25:09 - INFO - root -   The F1-score is 0.5644040619989311
09/04/2022 06:25:09 - INFO - root -   the best eval f1 is 0.5644, saving model !!
09/04/2022 06:25:13 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:35  batch_loss: 0.4087 [Training] 2/106 [..............................] - ETA: 1:00  batch_loss: 0.3921 [Training] 3/106 [..............................] - ETA: 46s  batch_loss: 0.4032 [Training] 4/106 [>.............................] - ETA: 41s  batch_loss: 0.4130 [Training] 5/106 [>.............................] - ETA: 38s  batch_loss: 0.3995 [Training] 6/106 [>.............................] - ETA: 35s  batch_loss: 0.4038 [Training] 7/106 [>.............................] - ETA: 34s  batch_loss: 0.4138 [Training] 8/106 [=>............................] - ETA: 32s  batch_loss: 0.4060 [Training] 9/106 [=>............................] - ETA: 31s  batch_loss: 0.3993 [Training] 10/106 [=>............................] - ETA: 30s  batch_loss: 0.4010 [Training] 11/106 [==>...........................] - ETA: 29s  batch_loss: 0.4032 [Training] 12/106 [==>...........................] - ETA: 28s  batch_loss: 0.4009 [Training] 13/106 [==>...........................] - ETA: 28s  batch_loss: 0.4060 [Training] 14/106 [==>...........................] - ETA: 27s  batch_loss: 0.3976 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 0.3984 [Training] 16/106 [===>..........................] - ETA: 26s  batch_loss: 0.4033 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.4040 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 0.4078 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 0.4119 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.4117 [Training] 21/106 [====>.........................] - ETA: 24s  batch_loss: 0.4096 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.4101 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 0.4089 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.4085 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.4087 [Training] 26/106 [======>.......................] - ETA: 22s  batch_loss: 0.4101 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.4123 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.4157 [Training] 29/106 [=======>......................] - ETA: 21s  batch_loss: 0.4201 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.4209 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.4201 [Training] 32/106 [========>.....................] - ETA: 20s  batch_loss: 0.4222 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.4226 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.4223 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.4233 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.4220 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.4218 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.4217 [Training] 39/106 [==========>...................] - ETA: 18s  batch_loss: 0.4224 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.4205 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.4192 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 0.4189 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.4202 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.4197 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.4183 [Training] 46/106 [============>.................] - ETA: 16s  batch_loss: 0.4198 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.4205 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.4202 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.4199 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.4204 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.4194 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.4197 [Training] 53/106 [==============>...............] - ETA: 14s  batch_loss: 0.4202 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.4195 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.4205 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.4198 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.4208 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.4210 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.4209 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.4198 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.4201 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.4185 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.4185 [Training] 64/106 [=================>............] - ETA: 11s  batch_loss: 0.4183 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.4184 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.4187 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.4184 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.4199 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.4192 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.4191 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.4180 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.4181 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.4177 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.4172 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.4158 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.4163 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.4151 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.4149 [Training] 79/106 [=====================>........] - ETA: 7s  batch_loss: 0.4149 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.4149 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.4158 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.4147 [Training] 83/106 [======================>.......] - ETA: 6s  batch_loss: 0.4145 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.4138 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.4139 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.4135 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.4139 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.4139 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.4135 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.4147 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.4147 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.4145 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.4136 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.4140 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.4142 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.4145 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.4139 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.4149 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.4145 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.4154 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.4157 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.4148 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.4151 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.4145 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.4140 [Training] 106/106 [==============================] 261.3ms/step  batch_loss: 0.4141 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:25:47 - INFO - root -   The F1-score is 0.5794466927618921
09/04/2022 06:25:47 - INFO - root -   the best eval f1 is 0.5794, saving model !!
09/04/2022 06:25:51 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:44  batch_loss: 0.3903 [Training] 2/106 [..............................] - ETA: 1:05  batch_loss: 0.3834 [Training] 3/106 [..............................] - ETA: 50s  batch_loss: 0.3747 [Training] 4/106 [>.............................] - ETA: 43s  batch_loss: 0.3748 [Training] 5/106 [>.............................] - ETA: 40s  batch_loss: 0.3647 [Training] 6/106 [>.............................] - ETA: 37s  batch_loss: 0.3565 [Training] 7/106 [>.............................] - ETA: 35s  batch_loss: 0.3685 [Training] 8/106 [=>............................] - ETA: 33s  batch_loss: 0.3729 [Training] 9/106 [=>............................] - ETA: 32s  batch_loss: 0.3737 [Training] 10/106 [=>............................] - ETA: 31s  batch_loss: 0.3724 [Training] 11/106 [==>...........................] - ETA: 30s  batch_loss: 0.3719 [Training] 12/106 [==>...........................] - ETA: 29s  batch_loss: 0.3797 [Training] 13/106 [==>...........................] - ETA: 28s  batch_loss: 0.3808 [Training] 14/106 [==>...........................] - ETA: 28s  batch_loss: 0.3757 [Training] 15/106 [===>..........................] - ETA: 27s  batch_loss: 0.3737 [Training] 16/106 [===>..........................] - ETA: 26s  batch_loss: 0.3707 [Training] 17/106 [===>..........................] - ETA: 26s  batch_loss: 0.3728 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 0.3695 [Training] 19/106 [====>.........................] - ETA: 25s  batch_loss: 0.3686 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.3703 [Training] 21/106 [====>.........................] - ETA: 24s  batch_loss: 0.3702 [Training] 22/106 [=====>........................] - ETA: 24s  batch_loss: 0.3723 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 0.3734 [Training] 24/106 [=====>........................] - ETA: 23s  batch_loss: 0.3731 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.3738 [Training] 26/106 [======>.......................] - ETA: 22s  batch_loss: 0.3743 [Training] 27/106 [======>.......................] - ETA: 22s  batch_loss: 0.3717 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.3733 [Training] 29/106 [=======>......................] - ETA: 21s  batch_loss: 0.3740 [Training] 30/106 [=======>......................] - ETA: 21s  batch_loss: 0.3733 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.3714 [Training] 32/106 [========>.....................] - ETA: 20s  batch_loss: 0.3720 [Training] 33/106 [========>.....................] - ETA: 20s  batch_loss: 0.3701 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.3707 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.3713 [Training] 36/106 [=========>....................] - ETA: 19s  batch_loss: 0.3706 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.3698 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.3688 [Training] 39/106 [==========>...................] - ETA: 18s  batch_loss: 0.3706 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.3702 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.3700 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 0.3691 [Training] 43/106 [===========>..................] - ETA: 17s  batch_loss: 0.3694 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.3693 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.3687 [Training] 46/106 [============>.................] - ETA: 16s  batch_loss: 0.3690 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.3704 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.3704 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.3700 [Training] 50/106 [=============>................] - ETA: 15s  batch_loss: 0.3698 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.3703 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.3697 [Training] 53/106 [==============>...............] - ETA: 14s  batch_loss: 0.3704 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.3689 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.3686 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.3678 [Training] 57/106 [===============>..............] - ETA: 13s  batch_loss: 0.3681 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.3686 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.3693 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.3690 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.3683 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.3689 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.3692 [Training] 64/106 [=================>............] - ETA: 11s  batch_loss: 0.3702 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.3688 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.3689 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.3689 [Training] 68/106 [==================>...........] - ETA: 10s  batch_loss: 0.3702 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.3702 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.3695 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.3698 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.3696 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.3698 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.3695 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.3690 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.3682 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.3691 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.3690 [Training] 79/106 [=====================>........] - ETA: 7s  batch_loss: 0.3693 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.3699 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.3710 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.3709 [Training] 83/106 [======================>.......] - ETA: 6s  batch_loss: 0.3724 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.3729 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.3720 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.3723 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.3727 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.3732 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.3729 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.3726 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.3724 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.3732 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.3730 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.3725 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.3723 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.3725 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.3726 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.3722 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.3720 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.3716 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.3719 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.3719 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.3713 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.3708 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.3710 [Training] 106/106 [==============================] 261.1ms/step  batch_loss: 0.3715 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:26:25 - INFO - root -   The F1-score is 0.5930689048328144
09/04/2022 06:26:25 - INFO - root -   the best eval f1 is 0.5931, saving model !!
09/04/2022 06:26:29 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:28  batch_loss: 0.3781 [Training] 2/106 [..............................] - ETA: 58s  batch_loss: 0.3684 [Training] 3/106 [..............................] - ETA: 46s  batch_loss: 0.3627 [Training] 4/106 [>.............................] - ETA: 41s  batch_loss: 0.3627 [Training] 5/106 [>.............................] - ETA: 37s  batch_loss: 0.3546 [Training] 6/106 [>.............................] - ETA: 35s  batch_loss: 0.3611 [Training] 7/106 [>.............................] - ETA: 33s  batch_loss: 0.3542 [Training] 8/106 [=>............................] - ETA: 32s  batch_loss: 0.3455 [Training] 9/106 [=>............................] - ETA: 31s  batch_loss: 0.3489 [Training] 10/106 [=>............................] - ETA: 30s  batch_loss: 0.3483 [Training] 11/106 [==>...........................] - ETA: 29s  batch_loss: 0.3483 [Training] 12/106 [==>...........................] - ETA: 28s  batch_loss: 0.3587 [Training] 13/106 [==>...........................] - ETA: 27s  batch_loss: 0.3581 [Training] 14/106 [==>...........................] - ETA: 27s  batch_loss: 0.3577 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 0.3580 [Training] 16/106 [===>..........................] - ETA: 26s  batch_loss: 0.3591 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.3564 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 0.3576 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 0.3551 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.3563 [Training] 21/106 [====>.........................] - ETA: 24s  batch_loss: 0.3550 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.3572 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 0.3543 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.3537 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.3534 [Training] 26/106 [======>.......................] - ETA: 22s  batch_loss: 0.3514 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.3484 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.3492 [Training] 29/106 [=======>......................] - ETA: 21s  batch_loss: 0.3481 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.3470 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.3477 [Training] 32/106 [========>.....................] - ETA: 20s  batch_loss: 0.3483 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.3479 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.3481 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.3477 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.3457 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.3449 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.3451 [Training] 39/106 [==========>...................] - ETA: 18s  batch_loss: 0.3459 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.3457 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.3444 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 0.3432 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.3437 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.3435 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.3431 [Training] 46/106 [============>.................] - ETA: 16s  batch_loss: 0.3438 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.3431 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.3422 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.3438 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.3455 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.3456 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.3460 [Training] 53/106 [==============>...............] - ETA: 14s  batch_loss: 0.3466 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.3464 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.3473 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.3479 [Training] 57/106 [===============>..............] - ETA: 13s  batch_loss: 0.3477 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.3471 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.3462 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.3457 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.3452 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.3446 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.3458 [Training] 64/106 [=================>............] - ETA: 11s  batch_loss: 0.3461 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.3456 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.3452 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.3447 [Training] 68/106 [==================>...........] - ETA: 10s  batch_loss: 0.3438 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.3433 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.3428 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.3423 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.3424 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.3428 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.3417 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.3418 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.3411 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.3410 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.3398 [Training] 79/106 [=====================>........] - ETA: 7s  batch_loss: 0.3399 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.3396 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.3387 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.3380 [Training] 83/106 [======================>.......] - ETA: 6s  batch_loss: 0.3380 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.3378 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.3374 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.3376 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.3378 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.3379 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.3380 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.3378 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.3384 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.3378 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.3378 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.3381 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.3382 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.3392 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.3392 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.3391 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.3387 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.3394 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.3402 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.3400 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.3402 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.3402 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.3405 [Training] 106/106 [==============================] 259.7ms/step  batch_loss: 0.3402 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:27:03 - INFO - root -   The F1-score is 0.598951507208388
09/04/2022 06:27:03 - INFO - root -   the best eval f1 is 0.5990, saving model !!
09/04/2022 06:27:07 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:21  batch_loss: 0.3645 [Training] 2/106 [..............................] - ETA: 53s  batch_loss: 0.3215 [Training] 3/106 [..............................] - ETA: 43s  batch_loss: 0.3143 [Training] 4/106 [>.............................] - ETA: 39s  batch_loss: 0.3189 [Training] 5/106 [>.............................] - ETA: 35s  batch_loss: 0.3127 [Training] 6/106 [>.............................] - ETA: 33s  batch_loss: 0.3175 [Training] 7/106 [>.............................] - ETA: 32s  batch_loss: 0.3163 [Training] 8/106 [=>............................] - ETA: 31s  batch_loss: 0.3159 [Training] 9/106 [=>............................] - ETA: 30s  batch_loss: 0.3191 [Training] 10/106 [=>............................] - ETA: 29s  batch_loss: 0.3153 [Training] 11/106 [==>...........................] - ETA: 28s  batch_loss: 0.3261 [Training] 12/106 [==>...........................] - ETA: 27s  batch_loss: 0.3195 [Training] 13/106 [==>...........................] - ETA: 27s  batch_loss: 0.3176 [Training] 14/106 [==>...........................] - ETA: 26s  batch_loss: 0.3167 [Training] 15/106 [===>..........................] - ETA: 25s  batch_loss: 0.3128 [Training] 16/106 [===>..........................] - ETA: 25s  batch_loss: 0.3126 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.3136 [Training] 18/106 [====>.........................] - ETA: 24s  batch_loss: 0.3125 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 0.3112 [Training] 20/106 [====>.........................] - ETA: 23s  batch_loss: 0.3104 [Training] 21/106 [====>.........................] - ETA: 23s  batch_loss: 0.3128 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.3117 [Training] 23/106 [=====>........................] - ETA: 22s  batch_loss: 0.3129 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.3148 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.3134 [Training] 26/106 [======>.......................] - ETA: 21s  batch_loss: 0.3126 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.3137 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.3126 [Training] 29/106 [=======>......................] - ETA: 20s  batch_loss: 0.3119 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.3110 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.3103 [Training] 32/106 [========>.....................] - ETA: 19s  batch_loss: 0.3083 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.3082 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.3083 [Training] 35/106 [========>.....................] - ETA: 18s  batch_loss: 0.3107 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.3088 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.3086 [Training] 38/106 [=========>....................] - ETA: 17s  batch_loss: 0.3075 [Training] 39/106 [==========>...................] - ETA: 17s  batch_loss: 0.3070 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.3079 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.3075 [Training] 42/106 [==========>...................] - ETA: 16s  batch_loss: 0.3081 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.3079 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.3091 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.3096 [Training] 46/106 [============>.................] - ETA: 15s  batch_loss: 0.3094 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.3091 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.3104 [Training] 49/106 [============>.................] - ETA: 14s  batch_loss: 0.3096 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.3102 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.3117 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.3109 [Training] 53/106 [==============>...............] - ETA: 13s  batch_loss: 0.3101 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.3107 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.3096 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.3106 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.3100 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.3101 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.3089 [Training] 60/106 [===============>..............] - ETA: 11s  batch_loss: 0.3085 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.3075 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.3078 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.3074 [Training] 64/106 [=================>............] - ETA: 10s  batch_loss: 0.3089 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.3090 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.3086 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.3080 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.3080 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.3074 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.3071 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.3070 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.3062 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.3062 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.3059 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.3056 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.3060 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.3067 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.3068 [Training] 79/106 [=====================>........] - ETA: 6s  batch_loss: 0.3065 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.3063 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.3065 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.3073 [Training] 83/106 [======================>.......] - ETA: 5s  batch_loss: 0.3068 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.3070 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.3074 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.3066 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.3071 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.3071 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.3074 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.3074 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.3074 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.3080 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.3077 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.3078 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.3083 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.3083 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.3081 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.3084 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.3086 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.3085 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.3085 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.3085 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.3083 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.3084 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.3082 [Training] 106/106 [==============================] 255.8ms/step  batch_loss: 0.3078 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:27:41 - INFO - root -   The F1-score is 0.6091077683906863
09/04/2022 06:27:41 - INFO - root -   the best eval f1 is 0.6091, saving model !!
09/04/2022 06:27:45 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:23  batch_loss: 0.2816 [Training] 2/106 [..............................] - ETA: 54s  batch_loss: 0.2669 [Training] 3/106 [..............................] - ETA: 44s  batch_loss: 0.2786 [Training] 4/106 [>.............................] - ETA: 39s  batch_loss: 0.2664 [Training] 5/106 [>.............................] - ETA: 36s  batch_loss: 0.2704 [Training] 6/106 [>.............................] - ETA: 34s  batch_loss: 0.2763 [Training] 7/106 [>.............................] - ETA: 32s  batch_loss: 0.2802 [Training] 8/106 [=>............................] - ETA: 31s  batch_loss: 0.2805 [Training] 9/106 [=>............................] - ETA: 30s  batch_loss: 0.2848 [Training] 10/106 [=>............................] - ETA: 29s  batch_loss: 0.2818 [Training] 11/106 [==>...........................] - ETA: 29s  batch_loss: 0.2820 [Training] 12/106 [==>...........................] - ETA: 28s  batch_loss: 0.2829 [Training] 13/106 [==>...........................] - ETA: 27s  batch_loss: 0.2851 [Training] 14/106 [==>...........................] - ETA: 27s  batch_loss: 0.2881 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 0.2837 [Training] 16/106 [===>..........................] - ETA: 25s  batch_loss: 0.2822 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.2827 [Training] 18/106 [====>.........................] - ETA: 24s  batch_loss: 0.2832 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 0.2835 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.2853 [Training] 21/106 [====>.........................] - ETA: 23s  batch_loss: 0.2861 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.2858 [Training] 23/106 [=====>........................] - ETA: 22s  batch_loss: 0.2881 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.2870 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.2878 [Training] 26/106 [======>.......................] - ETA: 21s  batch_loss: 0.2852 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.2869 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.2868 [Training] 29/106 [=======>......................] - ETA: 20s  batch_loss: 0.2874 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.2862 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.2885 [Training] 32/106 [========>.....................] - ETA: 19s  batch_loss: 0.2879 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.2902 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.2902 [Training] 35/106 [========>.....................] - ETA: 18s  batch_loss: 0.2894 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.2881 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.2884 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.2889 [Training] 39/106 [==========>...................] - ETA: 17s  batch_loss: 0.2899 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.2893 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.2879 [Training] 42/106 [==========>...................] - ETA: 16s  batch_loss: 0.2876 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.2867 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.2855 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.2850 [Training] 46/106 [============>.................] - ETA: 15s  batch_loss: 0.2839 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.2850 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.2853 [Training] 49/106 [============>.................] - ETA: 14s  batch_loss: 0.2853 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.2847 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.2844 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.2842 [Training] 53/106 [==============>...............] - ETA: 13s  batch_loss: 0.2832 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.2824 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.2826 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.2824 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.2833 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.2831 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.2833 [Training] 60/106 [===============>..............] - ETA: 11s  batch_loss: 0.2838 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.2836 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.2846 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.2839 [Training] 64/106 [=================>............] - ETA: 10s  batch_loss: 0.2842 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.2841 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.2836 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.2842 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.2847 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.2846 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.2847 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.2850 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.2847 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.2844 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.2834 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.2836 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.2840 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.2835 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.2836 [Training] 79/106 [=====================>........] - ETA: 6s  batch_loss: 0.2839 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.2831 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.2823 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.2821 [Training] 83/106 [======================>.......] - ETA: 5s  batch_loss: 0.2819 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.2812 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.2815 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.2815 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.2812 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.2814 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.2809 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.2822 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.2830 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.2828 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.2837 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.2842 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.2840 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.2839 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.2840 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.2841 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.2839 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.2837 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.2839 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.2840 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.2843 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.2848 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.2852 [Training] 106/106 [==============================] 255.5ms/step  batch_loss: 0.2845 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:28:18 - INFO - root -   The F1-score is 0.6125358023046692
09/04/2022 06:28:18 - INFO - root -   the best eval f1 is 0.6125, saving model !!
09/04/2022 06:28:22 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:28  batch_loss: 0.2628 [Training] 2/106 [..............................] - ETA: 58s  batch_loss: 0.2584 [Training] 3/106 [..............................] - ETA: 47s  batch_loss: 0.2558 [Training] 4/106 [>.............................] - ETA: 41s  batch_loss: 0.2648 [Training] 5/106 [>.............................] - ETA: 38s  batch_loss: 0.2619 [Training] 6/106 [>.............................] - ETA: 35s  batch_loss: 0.2669 [Training] 7/106 [>.............................] - ETA: 34s  batch_loss: 0.2667 [Training] 8/106 [=>............................] - ETA: 32s  batch_loss: 0.2639 [Training] 9/106 [=>............................] - ETA: 31s  batch_loss: 0.2633 [Training] 10/106 [=>............................] - ETA: 30s  batch_loss: 0.2584 [Training] 11/106 [==>...........................] - ETA: 29s  batch_loss: 0.2634 [Training] 12/106 [==>...........................] - ETA: 28s  batch_loss: 0.2644 [Training] 13/106 [==>...........................] - ETA: 27s  batch_loss: 0.2666 [Training] 14/106 [==>...........................] - ETA: 27s  batch_loss: 0.2633 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 0.2660 [Training] 16/106 [===>..........................] - ETA: 26s  batch_loss: 0.2685 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.2658 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 0.2660 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 0.2665 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.2643 [Training] 21/106 [====>.........................] - ETA: 23s  batch_loss: 0.2631 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.2619 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 0.2629 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.2662 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.2699 [Training] 26/106 [======>.......................] - ETA: 21s  batch_loss: 0.2695 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.2709 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.2708 [Training] 29/106 [=======>......................] - ETA: 20s  batch_loss: 0.2703 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.2699 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.2699 [Training] 32/106 [========>.....................] - ETA: 20s  batch_loss: 0.2702 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.2703 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.2709 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.2706 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.2707 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.2705 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.2700 [Training] 39/106 [==========>...................] - ETA: 17s  batch_loss: 0.2709 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.2696 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.2681 [Training] 42/106 [==========>...................] - ETA: 16s  batch_loss: 0.2680 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.2684 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.2677 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.2676 [Training] 46/106 [============>.................] - ETA: 15s  batch_loss: 0.2680 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.2671 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.2658 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.2658 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.2647 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.2648 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.2650 [Training] 53/106 [==============>...............] - ETA: 13s  batch_loss: 0.2651 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.2652 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.2651 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.2644 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.2648 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.2648 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.2644 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.2653 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.2650 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.2651 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.2646 [Training] 64/106 [=================>............] - ETA: 10s  batch_loss: 0.2648 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.2644 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.2645 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.2646 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.2641 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.2649 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.2641 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.2637 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.2639 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.2636 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.2632 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.2631 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.2629 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.2629 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.2625 [Training] 79/106 [=====================>........] - ETA: 7s  batch_loss: 0.2621 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.2620 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.2627 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.2628 [Training] 83/106 [======================>.......] - ETA: 5s  batch_loss: 0.2631 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.2627 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.2630 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.2631 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.2629 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.2627 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.2623 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.2623 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.2619 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.2614 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.2613 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.2615 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.2612 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.2609 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.2611 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.2615 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.2618 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.2620 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.2622 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.2619 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.2626 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.2626 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.2626 [Training] 106/106 [==============================] 260.1ms/step  batch_loss: 0.2625 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:28:56 - INFO - root -   The F1-score is 0.6149321569413809
09/04/2022 06:28:56 - INFO - root -   the best eval f1 is 0.6149, saving model !!
09/04/2022 06:29:00 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:30  batch_loss: 0.2485 [Training] 2/106 [..............................] - ETA: 58s  batch_loss: 0.2353 [Training] 3/106 [..............................] - ETA: 47s  batch_loss: 0.2387 [Training] 4/106 [>.............................] - ETA: 42s  batch_loss: 0.2367 [Training] 5/106 [>.............................] - ETA: 38s  batch_loss: 0.2475 [Training] 6/106 [>.............................] - ETA: 36s  batch_loss: 0.2392 [Training] 7/106 [>.............................] - ETA: 34s  batch_loss: 0.2395 [Training] 8/106 [=>............................] - ETA: 33s  batch_loss: 0.2366 [Training] 9/106 [=>............................] - ETA: 31s  batch_loss: 0.2395 [Training] 10/106 [=>............................] - ETA: 30s  batch_loss: 0.2395 [Training] 11/106 [==>...........................] - ETA: 30s  batch_loss: 0.2438 [Training] 12/106 [==>...........................] - ETA: 29s  batch_loss: 0.2414 [Training] 13/106 [==>...........................] - ETA: 28s  batch_loss: 0.2426 [Training] 14/106 [==>...........................] - ETA: 28s  batch_loss: 0.2435 [Training] 15/106 [===>..........................] - ETA: 27s  batch_loss: 0.2445 [Training] 16/106 [===>..........................] - ETA: 26s  batch_loss: 0.2458 [Training] 17/106 [===>..........................] - ETA: 26s  batch_loss: 0.2507 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 0.2521 [Training] 19/106 [====>.........................] - ETA: 25s  batch_loss: 0.2493 [Training] 20/106 [====>.........................] - ETA: 25s  batch_loss: 0.2472 [Training] 21/106 [====>.........................] - ETA: 24s  batch_loss: 0.2473 [Training] 22/106 [=====>........................] - ETA: 24s  batch_loss: 0.2455 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 0.2442 [Training] 24/106 [=====>........................] - ETA: 23s  batch_loss: 0.2443 [Training] 25/106 [======>.......................] - ETA: 23s  batch_loss: 0.2449 [Training] 26/106 [======>.......................] - ETA: 22s  batch_loss: 0.2465 [Training] 27/106 [======>.......................] - ETA: 22s  batch_loss: 0.2438 [Training] 28/106 [======>.......................] - ETA: 22s  batch_loss: 0.2438 [Training] 29/106 [=======>......................] - ETA: 21s  batch_loss: 0.2432 [Training] 30/106 [=======>......................] - ETA: 21s  batch_loss: 0.2440 [Training] 31/106 [=======>......................] - ETA: 21s  batch_loss: 0.2448 [Training] 32/106 [========>.....................] - ETA: 20s  batch_loss: 0.2438 [Training] 33/106 [========>.....................] - ETA: 20s  batch_loss: 0.2446 [Training] 34/106 [========>.....................] - ETA: 20s  batch_loss: 0.2435 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.2436 [Training] 36/106 [=========>....................] - ETA: 19s  batch_loss: 0.2438 [Training] 37/106 [=========>....................] - ETA: 19s  batch_loss: 0.2448 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.2444 [Training] 39/106 [==========>...................] - ETA: 18s  batch_loss: 0.2437 [Training] 40/106 [==========>...................] - ETA: 18s  batch_loss: 0.2440 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.2446 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 0.2457 [Training] 43/106 [===========>..................] - ETA: 17s  batch_loss: 0.2458 [Training] 44/106 [===========>..................] - ETA: 17s  batch_loss: 0.2458 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.2457 [Training] 46/106 [============>.................] - ETA: 16s  batch_loss: 0.2460 [Training] 47/106 [============>.................] - ETA: 16s  batch_loss: 0.2463 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.2469 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.2464 [Training] 50/106 [=============>................] - ETA: 15s  batch_loss: 0.2461 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.2463 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.2471 [Training] 53/106 [==============>...............] - ETA: 14s  batch_loss: 0.2468 [Training] 54/106 [==============>...............] - ETA: 14s  batch_loss: 0.2459 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.2462 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.2464 [Training] 57/106 [===============>..............] - ETA: 13s  batch_loss: 0.2458 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.2457 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.2452 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.2455 [Training] 61/106 [================>.............] - ETA: 12s  batch_loss: 0.2455 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.2460 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.2470 [Training] 64/106 [=================>............] - ETA: 11s  batch_loss: 0.2461 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.2466 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.2462 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.2458 [Training] 68/106 [==================>...........] - ETA: 10s  batch_loss: 0.2452 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.2452 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.2452 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.2452 [Training] 72/106 [===================>..........] - ETA: 9s  batch_loss: 0.2444 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.2442 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.2440 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.2447 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.2460 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.2462 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.2461 [Training] 79/106 [=====================>........] - ETA: 7s  batch_loss: 0.2460 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.2462 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.2464 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.2462 [Training] 83/106 [======================>.......] - ETA: 6s  batch_loss: 0.2462 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.2465 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.2462 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.2458 [Training] 87/106 [=======================>......] - ETA: 5s  batch_loss: 0.2461 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.2465 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.2462 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.2461 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.2470 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.2474 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.2474 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.2473 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.2471 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.2470 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.2466 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.2463 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.2461 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.2456 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.2464 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.2464 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.2459 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.2458 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.2454 [Training] 106/106 [==============================] 261.8ms/step  batch_loss: 0.2453 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:29:34 - INFO - root -   The F1-score is 0.6272252001852956
09/04/2022 06:29:34 - INFO - root -   the best eval f1 is 0.6272, saving model !!
09/04/2022 06:29:37 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:25  batch_loss: 0.1897 [Training] 2/106 [..............................] - ETA: 55s  batch_loss: 0.2032 [Training] 3/106 [..............................] - ETA: 45s  batch_loss: 0.2119 [Training] 4/106 [>.............................] - ETA: 40s  batch_loss: 0.2196 [Training] 5/106 [>.............................] - ETA: 36s  batch_loss: 0.2239 [Training] 6/106 [>.............................] - ETA: 34s  batch_loss: 0.2225 [Training] 7/106 [>.............................] - ETA: 33s  batch_loss: 0.2177 [Training] 8/106 [=>............................] - ETA: 32s  batch_loss: 0.2154 [Training] 9/106 [=>............................] - ETA: 30s  batch_loss: 0.2126 [Training] 10/106 [=>............................] - ETA: 29s  batch_loss: 0.2193 [Training] 11/106 [==>...........................] - ETA: 29s  batch_loss: 0.2210 [Training] 12/106 [==>...........................] - ETA: 28s  batch_loss: 0.2242 [Training] 13/106 [==>...........................] - ETA: 27s  batch_loss: 0.2296 [Training] 14/106 [==>...........................] - ETA: 27s  batch_loss: 0.2336 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 0.2318 [Training] 16/106 [===>..........................] - ETA: 25s  batch_loss: 0.2323 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.2294 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 0.2303 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 0.2289 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.2271 [Training] 21/106 [====>.........................] - ETA: 23s  batch_loss: 0.2274 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.2255 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 0.2271 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.2264 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.2264 [Training] 26/106 [======>.......................] - ETA: 21s  batch_loss: 0.2250 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.2258 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.2248 [Training] 29/106 [=======>......................] - ETA: 20s  batch_loss: 0.2237 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.2243 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.2265 [Training] 32/106 [========>.....................] - ETA: 19s  batch_loss: 0.2252 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.2265 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.2255 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.2259 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.2261 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.2265 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.2272 [Training] 39/106 [==========>...................] - ETA: 17s  batch_loss: 0.2269 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.2264 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.2265 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 0.2255 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.2246 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.2240 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.2242 [Training] 46/106 [============>.................] - ETA: 15s  batch_loss: 0.2251 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.2252 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.2244 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.2241 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.2243 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.2241 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.2242 [Training] 53/106 [==============>...............] - ETA: 13s  batch_loss: 0.2241 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.2241 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.2240 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.2247 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.2245 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.2246 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.2249 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.2245 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.2248 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.2240 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.2232 [Training] 64/106 [=================>............] - ETA: 10s  batch_loss: 0.2235 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.2230 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.2228 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.2228 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.2225 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.2221 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.2221 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.2228 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.2234 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.2229 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.2226 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.2233 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.2236 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.2240 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.2254 [Training] 79/106 [=====================>........] - ETA: 6s  batch_loss: 0.2266 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.2261 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.2267 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.2268 [Training] 83/106 [======================>.......] - ETA: 5s  batch_loss: 0.2275 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.2268 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.2273 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.2276 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.2281 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.2281 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.2285 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.2280 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.2290 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.2287 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.2296 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.2295 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.2291 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.2291 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.2291 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.2292 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.2294 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.2300 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.2297 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.2296 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.2297 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.2290 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.2291 [Training] 106/106 [==============================] 257.8ms/step  batch_loss: 0.2287 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:30:11 - INFO - root -   The F1-score is 0.627142012999803
09/04/2022 06:30:11 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:26  batch_loss: 0.2194 [Training] 2/106 [..............................] - ETA: 56s  batch_loss: 0.2408 [Training] 3/106 [..............................] - ETA: 45s  batch_loss: 0.2218 [Training] 4/106 [>.............................] - ETA: 40s  batch_loss: 0.2274 [Training] 5/106 [>.............................] - ETA: 36s  batch_loss: 0.2207 [Training] 6/106 [>.............................] - ETA: 35s  batch_loss: 0.2245 [Training] 7/106 [>.............................] - ETA: 33s  batch_loss: 0.2229 [Training] 8/106 [=>............................] - ETA: 32s  batch_loss: 0.2149 [Training] 9/106 [=>............................] - ETA: 30s  batch_loss: 0.2116 [Training] 10/106 [=>............................] - ETA: 29s  batch_loss: 0.2102 [Training] 11/106 [==>...........................] - ETA: 29s  batch_loss: 0.2076 [Training] 12/106 [==>...........................] - ETA: 28s  batch_loss: 0.2054 [Training] 13/106 [==>...........................] - ETA: 27s  batch_loss: 0.2066 [Training] 14/106 [==>...........................] - ETA: 27s  batch_loss: 0.2044 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 0.2049 [Training] 16/106 [===>..........................] - ETA: 26s  batch_loss: 0.2052 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.2053 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 0.2026 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 0.2008 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.2020 [Training] 21/106 [====>.........................] - ETA: 23s  batch_loss: 0.2013 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.2025 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 0.2043 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.2065 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.2069 [Training] 26/106 [======>.......................] - ETA: 22s  batch_loss: 0.2058 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.2057 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.2071 [Training] 29/106 [=======>......................] - ETA: 21s  batch_loss: 0.2082 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.2094 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.2107 [Training] 32/106 [========>.....................] - ETA: 20s  batch_loss: 0.2092 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.2103 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.2100 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.2102 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.2105 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.2103 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.2095 [Training] 39/106 [==========>...................] - ETA: 17s  batch_loss: 0.2086 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.2096 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.2084 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 0.2102 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.2113 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.2115 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.2106 [Training] 46/106 [============>.................] - ETA: 15s  batch_loss: 0.2097 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.2113 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.2108 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.2109 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.2113 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.2116 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.2113 [Training] 53/106 [==============>...............] - ETA: 13s  batch_loss: 0.2117 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.2117 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.2126 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.2130 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.2132 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.2134 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.2135 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.2139 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.2145 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.2142 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.2147 [Training] 64/106 [=================>............] - ETA: 11s  batch_loss: 0.2148 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.2143 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.2154 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.2154 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.2149 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.2155 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.2152 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.2153 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.2153 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.2155 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.2161 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.2155 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.2153 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.2149 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.2148 [Training] 79/106 [=====================>........] - ETA: 6s  batch_loss: 0.2146 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.2144 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.2145 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.2139 [Training] 83/106 [======================>.......] - ETA: 5s  batch_loss: 0.2139 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.2134 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.2135 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.2131 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.2129 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.2137 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.2134 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.2130 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.2128 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.2130 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.2131 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.2132 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.2128 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.2128 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.2127 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.2128 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.2132 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.2134 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.2140 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.2140 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.2139 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.2139 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.2141 [Training] 106/106 [==============================] 256.0ms/step  batch_loss: 0.2146 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:30:44 - INFO - root -   The F1-score is 0.6284227820372399
09/04/2022 06:30:44 - INFO - root -   the best eval f1 is 0.6284, saving model !!
09/04/2022 06:30:47 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:30  batch_loss: 0.2094 [Training] 2/106 [..............................] - ETA: 58s  batch_loss: 0.1789 [Training] 3/106 [..............................] - ETA: 47s  batch_loss: 0.2003 [Training] 4/106 [>.............................] - ETA: 41s  batch_loss: 0.1903 [Training] 5/106 [>.............................] - ETA: 37s  batch_loss: 0.1865 [Training] 6/106 [>.............................] - ETA: 35s  batch_loss: 0.1953 [Training] 7/106 [>.............................] - ETA: 33s  batch_loss: 0.1955 [Training] 8/106 [=>............................] - ETA: 32s  batch_loss: 0.2110 [Training] 9/106 [=>............................] - ETA: 31s  batch_loss: 0.2162 [Training] 10/106 [=>............................] - ETA: 30s  batch_loss: 0.2215 [Training] 11/106 [==>...........................] - ETA: 29s  batch_loss: 0.2211 [Training] 12/106 [==>...........................] - ETA: 28s  batch_loss: 0.2211 [Training] 13/106 [==>...........................] - ETA: 28s  batch_loss: 0.2175 [Training] 14/106 [==>...........................] - ETA: 27s  batch_loss: 0.2133 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 0.2142 [Training] 16/106 [===>..........................] - ETA: 26s  batch_loss: 0.2150 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.2141 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 0.2131 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 0.2130 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.2101 [Training] 21/106 [====>.........................] - ETA: 24s  batch_loss: 0.2099 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.2106 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 0.2107 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.2099 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.2086 [Training] 26/106 [======>.......................] - ETA: 22s  batch_loss: 0.2079 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.2074 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.2084 [Training] 29/106 [=======>......................] - ETA: 21s  batch_loss: 0.2079 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.2083 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.2086 [Training] 32/106 [========>.....................] - ETA: 20s  batch_loss: 0.2092 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.2087 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.2075 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.2061 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.2071 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.2066 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.2062 [Training] 39/106 [==========>...................] - ETA: 18s  batch_loss: 0.2050 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.2051 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.2045 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 0.2041 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.2046 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.2050 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.2053 [Training] 46/106 [============>.................] - ETA: 16s  batch_loss: 0.2059 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.2063 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.2053 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.2047 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.2042 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.2036 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.2030 [Training] 53/106 [==============>...............] - ETA: 14s  batch_loss: 0.2037 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.2035 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.2041 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.2037 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.2038 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.2031 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.2037 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.2032 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.2034 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.2038 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.2037 [Training] 64/106 [=================>............] - ETA: 11s  batch_loss: 0.2041 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.2034 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.2029 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.2023 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.2020 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.2028 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.2024 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.2021 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.2021 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.2019 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.2023 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.2018 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.2015 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.2014 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.2007 [Training] 79/106 [=====================>........] - ETA: 7s  batch_loss: 0.2009 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.2012 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.2013 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.2018 [Training] 83/106 [======================>.......] - ETA: 6s  batch_loss: 0.2022 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.2019 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.2018 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.2020 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.2018 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.2020 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.2017 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.2018 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.2014 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.2011 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.2008 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.2008 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.2010 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.2007 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.2010 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.2012 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.2008 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.2011 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.2012 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.2006 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.2005 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.2001 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.2001 [Training] 106/106 [==============================] 260.4ms/step  batch_loss: 0.2001 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:31:21 - INFO - root -   The F1-score is 0.6323886639676113
09/04/2022 06:31:21 - INFO - root -   the best eval f1 is 0.6324, saving model !!
09/04/2022 06:31:23 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:26  batch_loss: 0.1611 [Training] 2/106 [..............................] - ETA: 55s  batch_loss: 0.1823 [Training] 3/106 [..............................] - ETA: 44s  batch_loss: 0.1774 [Training] 4/106 [>.............................] - ETA: 39s  batch_loss: 0.1886 [Training] 5/106 [>.............................] - ETA: 36s  batch_loss: 0.1877 [Training] 6/106 [>.............................] - ETA: 34s  batch_loss: 0.1947 [Training] 7/106 [>.............................] - ETA: 32s  batch_loss: 0.1919 [Training] 8/106 [=>............................] - ETA: 31s  batch_loss: 0.1915 [Training] 9/106 [=>............................] - ETA: 30s  batch_loss: 0.1906 [Training] 10/106 [=>............................] - ETA: 29s  batch_loss: 0.1890 [Training] 11/106 [==>...........................] - ETA: 28s  batch_loss: 0.1909 [Training] 12/106 [==>...........................] - ETA: 27s  batch_loss: 0.1917 [Training] 13/106 [==>...........................] - ETA: 27s  batch_loss: 0.1929 [Training] 14/106 [==>...........................] - ETA: 26s  batch_loss: 0.1903 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 0.1894 [Training] 16/106 [===>..........................] - ETA: 25s  batch_loss: 0.1886 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.1895 [Training] 18/106 [====>.........................] - ETA: 24s  batch_loss: 0.1893 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 0.1877 [Training] 20/106 [====>.........................] - ETA: 23s  batch_loss: 0.1880 [Training] 21/106 [====>.........................] - ETA: 23s  batch_loss: 0.1884 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.1896 [Training] 23/106 [=====>........................] - ETA: 22s  batch_loss: 0.1897 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.1916 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.1916 [Training] 26/106 [======>.......................] - ETA: 21s  batch_loss: 0.1908 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.1902 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.1898 [Training] 29/106 [=======>......................] - ETA: 20s  batch_loss: 0.1882 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.1880 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.1872 [Training] 32/106 [========>.....................] - ETA: 19s  batch_loss: 0.1891 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.1899 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.1892 [Training] 35/106 [========>.....................] - ETA: 18s  batch_loss: 0.1900 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.1897 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.1909 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.1899 [Training] 39/106 [==========>...................] - ETA: 17s  batch_loss: 0.1911 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.1918 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.1920 [Training] 42/106 [==========>...................] - ETA: 16s  batch_loss: 0.1920 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.1914 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.1917 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.1920 [Training] 46/106 [============>.................] - ETA: 15s  batch_loss: 0.1925 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.1922 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.1915 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.1923 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.1921 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.1918 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.1917 [Training] 53/106 [==============>...............] - ETA: 13s  batch_loss: 0.1910 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.1904 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.1902 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.1899 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.1898 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.1896 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.1887 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.1894 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.1888 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.1882 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.1881 [Training] 64/106 [=================>............] - ETA: 10s  batch_loss: 0.1878 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.1879 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.1890 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.1882 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.1877 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.1873 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.1877 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.1877 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.1878 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.1878 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.1880 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.1879 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.1881 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.1878 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.1889 [Training] 79/106 [=====================>........] - ETA: 7s  batch_loss: 0.1888 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.1894 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.1898 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.1897 [Training] 83/106 [======================>.......] - ETA: 5s  batch_loss: 0.1895 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.1898 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.1899 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.1896 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.1898 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.1896 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.1908 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.1910 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.1908 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.1910 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.1912 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.1909 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.1907 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.1904 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.1900 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.1900 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.1900 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.1898 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.1901 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.1903 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.1903 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.1902 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.1903 [Training] 106/106 [==============================] 256.2ms/step  batch_loss: 0.1901 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:31:57 - INFO - root -   The F1-score is 0.6391051219990842
09/04/2022 06:31:57 - INFO - root -   the best eval f1 is 0.6391, saving model !!
09/04/2022 06:31:59 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:28  batch_loss: 0.1515 [Training] 2/106 [..............................] - ETA: 57s  batch_loss: 0.1679 [Training] 3/106 [..............................] - ETA: 46s  batch_loss: 0.1809 [Training] 4/106 [>.............................] - ETA: 41s  batch_loss: 0.1701 [Training] 5/106 [>.............................] - ETA: 37s  batch_loss: 0.1654 [Training] 6/106 [>.............................] - ETA: 35s  batch_loss: 0.1648 [Training] 7/106 [>.............................] - ETA: 33s  batch_loss: 0.1630 [Training] 8/106 [=>............................] - ETA: 32s  batch_loss: 0.1632 [Training] 9/106 [=>............................] - ETA: 31s  batch_loss: 0.1655 [Training] 10/106 [=>............................] - ETA: 30s  batch_loss: 0.1621 [Training] 11/106 [==>...........................] - ETA: 29s  batch_loss: 0.1634 [Training] 12/106 [==>...........................] - ETA: 28s  batch_loss: 0.1643 [Training] 13/106 [==>...........................] - ETA: 28s  batch_loss: 0.1652 [Training] 14/106 [==>...........................] - ETA: 27s  batch_loss: 0.1650 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 0.1661 [Training] 16/106 [===>..........................] - ETA: 26s  batch_loss: 0.1681 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.1689 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 0.1704 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 0.1711 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.1720 [Training] 21/106 [====>.........................] - ETA: 24s  batch_loss: 0.1725 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.1725 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 0.1737 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.1732 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.1739 [Training] 26/106 [======>.......................] - ETA: 22s  batch_loss: 0.1728 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.1760 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.1760 [Training] 29/106 [=======>......................] - ETA: 21s  batch_loss: 0.1766 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.1782 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.1790 [Training] 32/106 [========>.....................] - ETA: 20s  batch_loss: 0.1787 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.1781 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.1786 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.1787 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.1789 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.1799 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.1810 [Training] 39/106 [==========>...................] - ETA: 17s  batch_loss: 0.1802 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.1798 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.1808 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 0.1796 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.1792 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.1800 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.1812 [Training] 46/106 [============>.................] - ETA: 15s  batch_loss: 0.1818 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.1820 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.1825 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.1822 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.1826 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.1828 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.1834 [Training] 53/106 [==============>...............] - ETA: 13s  batch_loss: 0.1823 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.1828 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.1841 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.1854 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.1851 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.1855 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.1858 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.1856 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.1852 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.1858 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.1855 [Training] 64/106 [=================>............] - ETA: 11s  batch_loss: 0.1855 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.1852 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.1849 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.1851 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.1853 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.1846 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.1846 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.1854 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.1853 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.1852 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.1853 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.1850 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.1847 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.1843 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.1836 [Training] 79/106 [=====================>........] - ETA: 7s  batch_loss: 0.1833 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.1832 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.1835 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.1833 [Training] 83/106 [======================>.......] - ETA: 5s  batch_loss: 0.1827 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.1826 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.1824 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.1823 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.1816 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.1812 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.1814 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.1812 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.1813 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.1815 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.1813 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.1813 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.1811 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.1810 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.1814 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.1815 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.1816 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.1816 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.1818 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.1815 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.1817 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.1826 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.1823 [Training] 106/106 [==============================] 259.4ms/step  batch_loss: 0.1826 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:32:33 - INFO - root -   The F1-score is 0.6386971382615106
09/04/2022 06:32:33 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:22  batch_loss: 0.1519 [Training] 2/106 [..............................] - ETA: 53s  batch_loss: 0.1550 [Training] 3/106 [..............................] - ETA: 44s  batch_loss: 0.1767 [Training] 4/106 [>.............................] - ETA: 39s  batch_loss: 0.1701 [Training] 5/106 [>.............................] - ETA: 36s  batch_loss: 0.1704 [Training] 6/106 [>.............................] - ETA: 34s  batch_loss: 0.1718 [Training] 7/106 [>.............................] - ETA: 32s  batch_loss: 0.1768 [Training] 8/106 [=>............................] - ETA: 31s  batch_loss: 0.1737 [Training] 9/106 [=>............................] - ETA: 30s  batch_loss: 0.1751 [Training] 10/106 [=>............................] - ETA: 29s  batch_loss: 0.1764 [Training] 11/106 [==>...........................] - ETA: 28s  batch_loss: 0.1711 [Training] 12/106 [==>...........................] - ETA: 27s  batch_loss: 0.1659 [Training] 13/106 [==>...........................] - ETA: 26s  batch_loss: 0.1678 [Training] 14/106 [==>...........................] - ETA: 26s  batch_loss: 0.1689 [Training] 15/106 [===>..........................] - ETA: 25s  batch_loss: 0.1683 [Training] 16/106 [===>..........................] - ETA: 25s  batch_loss: 0.1666 [Training] 17/106 [===>..........................] - ETA: 24s  batch_loss: 0.1687 [Training] 18/106 [====>.........................] - ETA: 24s  batch_loss: 0.1701 [Training] 19/106 [====>.........................] - ETA: 23s  batch_loss: 0.1724 [Training] 20/106 [====>.........................] - ETA: 23s  batch_loss: 0.1729 [Training] 21/106 [====>.........................] - ETA: 23s  batch_loss: 0.1728 [Training] 22/106 [=====>........................] - ETA: 22s  batch_loss: 0.1719 [Training] 23/106 [=====>........................] - ETA: 22s  batch_loss: 0.1722 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.1724 [Training] 25/106 [======>.......................] - ETA: 21s  batch_loss: 0.1728 [Training] 26/106 [======>.......................] - ETA: 21s  batch_loss: 0.1718 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.1716 [Training] 28/106 [======>.......................] - ETA: 20s  batch_loss: 0.1709 [Training] 29/106 [=======>......................] - ETA: 20s  batch_loss: 0.1706 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.1707 [Training] 31/106 [=======>......................] - ETA: 19s  batch_loss: 0.1722 [Training] 32/106 [========>.....................] - ETA: 19s  batch_loss: 0.1723 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.1716 [Training] 34/106 [========>.....................] - ETA: 18s  batch_loss: 0.1712 [Training] 35/106 [========>.....................] - ETA: 18s  batch_loss: 0.1708 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.1715 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.1702 [Training] 38/106 [=========>....................] - ETA: 17s  batch_loss: 0.1700 [Training] 39/106 [==========>...................] - ETA: 17s  batch_loss: 0.1704 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.1706 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.1701 [Training] 42/106 [==========>...................] - ETA: 16s  batch_loss: 0.1709 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.1716 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.1718 [Training] 45/106 [===========>..................] - ETA: 15s  batch_loss: 0.1720 [Training] 46/106 [============>.................] - ETA: 15s  batch_loss: 0.1717 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.1715 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.1715 [Training] 49/106 [============>.................] - ETA: 14s  batch_loss: 0.1718 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.1722 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.1733 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.1729 [Training] 53/106 [==============>...............] - ETA: 13s  batch_loss: 0.1732 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.1725 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.1724 [Training] 56/106 [==============>...............] - ETA: 12s  batch_loss: 0.1727 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.1719 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.1718 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.1715 [Training] 60/106 [===============>..............] - ETA: 11s  batch_loss: 0.1714 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.1708 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.1711 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.1712 [Training] 64/106 [=================>............] - ETA: 10s  batch_loss: 0.1716 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.1711 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.1709 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.1716 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.1727 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.1729 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.1729 [Training] 71/106 [===================>..........] - ETA: 8s  batch_loss: 0.1728 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.1723 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.1719 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.1716 [Training] 75/106 [====================>.........] - ETA: 7s  batch_loss: 0.1718 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.1721 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.1716 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.1718 [Training] 79/106 [=====================>........] - ETA: 6s  batch_loss: 0.1722 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.1723 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.1723 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.1717 [Training] 83/106 [======================>.......] - ETA: 5s  batch_loss: 0.1714 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.1721 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.1716 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.1716 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.1721 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.1727 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.1722 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.1722 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.1717 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.1711 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.1717 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.1717 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.1717 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.1720 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.1725 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.1725 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.1724 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.1726 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.1724 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.1729 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.1731 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.1732 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.1731 [Training] 106/106 [==============================] 256.1ms/step  batch_loss: 0.1734 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:33:07 - INFO - root -   The F1-score is 0.6326586061676402
09/04/2022 06:33:07 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:26  batch_loss: 0.1942 [Training] 2/106 [..............................] - ETA: 56s  batch_loss: 0.1696 [Training] 3/106 [..............................] - ETA: 46s  batch_loss: 0.1558 [Training] 4/106 [>.............................] - ETA: 40s  batch_loss: 0.1585 [Training] 5/106 [>.............................] - ETA: 37s  batch_loss: 0.1497 [Training] 6/106 [>.............................] - ETA: 34s  batch_loss: 0.1539 [Training] 7/106 [>.............................] - ETA: 33s  batch_loss: 0.1651 [Training] 8/106 [=>............................] - ETA: 31s  batch_loss: 0.1635 [Training] 9/106 [=>............................] - ETA: 30s  batch_loss: 0.1621 [Training] 10/106 [=>............................] - ETA: 29s  batch_loss: 0.1591 [Training] 11/106 [==>...........................] - ETA: 28s  batch_loss: 0.1585 [Training] 12/106 [==>...........................] - ETA: 28s  batch_loss: 0.1607 [Training] 13/106 [==>...........................] - ETA: 27s  batch_loss: 0.1630 [Training] 14/106 [==>...........................] - ETA: 26s  batch_loss: 0.1629 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 0.1604 [Training] 16/106 [===>..........................] - ETA: 25s  batch_loss: 0.1609 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.1620 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 0.1596 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 0.1578 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.1561 [Training] 21/106 [====>.........................] - ETA: 23s  batch_loss: 0.1560 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.1557 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 0.1581 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.1591 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.1590 [Training] 26/106 [======>.......................] - ETA: 21s  batch_loss: 0.1598 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.1608 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.1603 [Training] 29/106 [=======>......................] - ETA: 20s  batch_loss: 0.1593 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.1603 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.1606 [Training] 32/106 [========>.....................] - ETA: 20s  batch_loss: 0.1609 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.1600 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.1599 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.1608 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.1615 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.1620 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.1614 [Training] 39/106 [==========>...................] - ETA: 17s  batch_loss: 0.1619 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.1617 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.1606 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 0.1598 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.1594 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.1603 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.1606 [Training] 46/106 [============>.................] - ETA: 15s  batch_loss: 0.1605 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.1599 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.1602 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.1596 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.1591 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.1590 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.1594 [Training] 53/106 [==============>...............] - ETA: 13s  batch_loss: 0.1594 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.1594 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.1592 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.1588 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.1594 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.1604 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.1607 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.1609 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.1612 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.1613 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.1615 [Training] 64/106 [=================>............] - ETA: 10s  batch_loss: 0.1619 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.1620 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.1625 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.1626 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.1631 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.1632 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.1630 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.1626 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.1632 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.1634 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.1643 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.1644 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.1648 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.1652 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.1654 [Training] 79/106 [=====================>........] - ETA: 6s  batch_loss: 0.1658 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.1660 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.1664 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.1667 [Training] 83/106 [======================>.......] - ETA: 5s  batch_loss: 0.1667 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.1669 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.1664 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.1665 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.1666 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.1665 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.1662 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.1660 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.1667 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.1667 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.1671 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.1666 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.1671 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.1672 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.1671 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.1670 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.1671 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.1672 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.1670 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.1669 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.1668 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.1666 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.1664 [Training] 106/106 [==============================] 257.8ms/step  batch_loss: 0.1664 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:33:41 - INFO - root -   The F1-score is 0.6414704667492771
09/04/2022 06:33:41 - INFO - root -   the best eval f1 is 0.6415, saving model !!
09/04/2022 06:33:43 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:38  batch_loss: 0.1491 [Training] 2/106 [..............................] - ETA: 1:01  batch_loss: 0.1530 [Training] 3/106 [..............................] - ETA: 49s  batch_loss: 0.1589 [Training] 4/106 [>.............................] - ETA: 43s  batch_loss: 0.1631 [Training] 5/106 [>.............................] - ETA: 39s  batch_loss: 0.1661 [Training] 6/106 [>.............................] - ETA: 36s  batch_loss: 0.1654 [Training] 7/106 [>.............................] - ETA: 34s  batch_loss: 0.1660 [Training] 8/106 [=>............................] - ETA: 33s  batch_loss: 0.1596 [Training] 9/106 [=>............................] - ETA: 32s  batch_loss: 0.1623 [Training] 10/106 [=>............................] - ETA: 30s  batch_loss: 0.1576 [Training] 11/106 [==>...........................] - ETA: 29s  batch_loss: 0.1611 [Training] 12/106 [==>...........................] - ETA: 29s  batch_loss: 0.1618 [Training] 13/106 [==>...........................] - ETA: 28s  batch_loss: 0.1636 [Training] 14/106 [==>...........................] - ETA: 27s  batch_loss: 0.1636 [Training] 15/106 [===>..........................] - ETA: 27s  batch_loss: 0.1612 [Training] 16/106 [===>..........................] - ETA: 26s  batch_loss: 0.1595 [Training] 17/106 [===>..........................] - ETA: 26s  batch_loss: 0.1588 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 0.1568 [Training] 19/106 [====>.........................] - ETA: 25s  batch_loss: 0.1560 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.1550 [Training] 21/106 [====>.........................] - ETA: 24s  batch_loss: 0.1552 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.1566 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 0.1585 [Training] 24/106 [=====>........................] - ETA: 23s  batch_loss: 0.1570 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.1565 [Training] 26/106 [======>.......................] - ETA: 22s  batch_loss: 0.1565 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.1557 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.1573 [Training] 29/106 [=======>......................] - ETA: 21s  batch_loss: 0.1577 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.1573 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.1559 [Training] 32/106 [========>.....................] - ETA: 20s  batch_loss: 0.1556 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.1556 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.1575 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.1586 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.1584 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.1590 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.1594 [Training] 39/106 [==========>...................] - ETA: 18s  batch_loss: 0.1592 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.1601 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.1595 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 0.1595 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.1587 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.1577 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.1572 [Training] 46/106 [============>.................] - ETA: 16s  batch_loss: 0.1577 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.1583 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.1589 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.1592 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.1592 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.1590 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.1587 [Training] 53/106 [==============>...............] - ETA: 14s  batch_loss: 0.1584 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.1580 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.1587 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.1590 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.1593 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.1588 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.1582 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.1583 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.1585 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.1590 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.1586 [Training] 64/106 [=================>............] - ETA: 11s  batch_loss: 0.1582 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.1586 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.1580 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.1579 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.1578 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.1573 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.1572 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.1576 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.1574 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.1565 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.1566 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.1565 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.1569 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.1567 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.1570 [Training] 79/106 [=====================>........] - ETA: 7s  batch_loss: 0.1568 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.1567 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.1564 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.1570 [Training] 83/106 [======================>.......] - ETA: 5s  batch_loss: 0.1569 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.1575 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.1571 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.1571 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.1564 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.1562 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.1570 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.1566 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.1575 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.1570 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.1569 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.1566 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.1566 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.1566 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.1566 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.1564 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.1565 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.1564 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.1568 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.1562 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.1565 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.1569 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.1574 [Training] 106/106 [==============================] 259.1ms/step  batch_loss: 0.1579 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:34:18 - INFO - root -   The F1-score is 0.6409795701071405
09/04/2022 06:34:18 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:25  batch_loss: 0.1144 [Training] 2/106 [..............................] - ETA: 55s  batch_loss: 0.1127 [Training] 3/106 [..............................] - ETA: 45s  batch_loss: 0.1262 [Training] 4/106 [>.............................] - ETA: 40s  batch_loss: 0.1307 [Training] 5/106 [>.............................] - ETA: 37s  batch_loss: 0.1288 [Training] 6/106 [>.............................] - ETA: 34s  batch_loss: 0.1330 [Training] 7/106 [>.............................] - ETA: 33s  batch_loss: 0.1438 [Training] 8/106 [=>............................] - ETA: 31s  batch_loss: 0.1428 [Training] 9/106 [=>............................] - ETA: 30s  batch_loss: 0.1402 [Training] 10/106 [=>............................] - ETA: 29s  batch_loss: 0.1416 [Training] 11/106 [==>...........................] - ETA: 29s  batch_loss: 0.1388 [Training] 12/106 [==>...........................] - ETA: 28s  batch_loss: 0.1374 [Training] 13/106 [==>...........................] - ETA: 27s  batch_loss: 0.1386 [Training] 14/106 [==>...........................] - ETA: 27s  batch_loss: 0.1370 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 0.1389 [Training] 16/106 [===>..........................] - ETA: 26s  batch_loss: 0.1405 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.1397 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 0.1425 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 0.1442 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.1453 [Training] 21/106 [====>.........................] - ETA: 23s  batch_loss: 0.1452 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.1451 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 0.1450 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.1447 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.1452 [Training] 26/106 [======>.......................] - ETA: 22s  batch_loss: 0.1454 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.1461 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.1462 [Training] 29/106 [=======>......................] - ETA: 21s  batch_loss: 0.1471 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.1465 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.1456 [Training] 32/106 [========>.....................] - ETA: 20s  batch_loss: 0.1460 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.1460 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.1466 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.1469 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.1470 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.1465 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.1455 [Training] 39/106 [==========>...................] - ETA: 18s  batch_loss: 0.1452 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.1460 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.1473 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 0.1481 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.1481 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.1477 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.1480 [Training] 46/106 [============>.................] - ETA: 16s  batch_loss: 0.1476 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.1478 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.1477 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.1482 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.1490 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.1482 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.1495 [Training] 53/106 [==============>...............] - ETA: 14s  batch_loss: 0.1501 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.1501 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.1497 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.1495 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.1489 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.1488 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.1482 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.1486 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.1484 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.1484 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.1487 [Training] 64/106 [=================>............] - ETA: 11s  batch_loss: 0.1496 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.1496 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.1495 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.1496 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.1492 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.1494 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.1493 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.1484 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.1485 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.1488 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.1491 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.1501 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.1501 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.1501 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.1497 [Training] 79/106 [=====================>........] - ETA: 7s  batch_loss: 0.1497 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.1499 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.1497 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.1500 [Training] 83/106 [======================>.......] - ETA: 6s  batch_loss: 0.1501 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.1503 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.1501 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.1498 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.1498 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.1496 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.1495 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.1494 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.1492 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.1495 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.1495 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.1495 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.1498 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.1502 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.1504 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.1504 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.1505 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.1508 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.1507 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.1507 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.1506 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.1513 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.1515 [Training] 106/106 [==============================] 259.6ms/step  batch_loss: 0.1511 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:34:52 - INFO - root -   The F1-score is 0.6341729067392784
09/04/2022 06:34:52 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:31  batch_loss: 0.1230 [Training] 2/106 [..............................] - ETA: 58s  batch_loss: 0.1124 [Training] 3/106 [..............................] - ETA: 46s  batch_loss: 0.1149 [Training] 4/106 [>.............................] - ETA: 41s  batch_loss: 0.1169 [Training] 5/106 [>.............................] - ETA: 37s  batch_loss: 0.1240 [Training] 6/106 [>.............................] - ETA: 35s  batch_loss: 0.1209 [Training] 7/106 [>.............................] - ETA: 33s  batch_loss: 0.1211 [Training] 8/106 [=>............................] - ETA: 31s  batch_loss: 0.1250 [Training] 9/106 [=>............................] - ETA: 30s  batch_loss: 0.1226 [Training] 10/106 [=>............................] - ETA: 29s  batch_loss: 0.1227 [Training] 11/106 [==>...........................] - ETA: 28s  batch_loss: 0.1225 [Training] 12/106 [==>...........................] - ETA: 28s  batch_loss: 0.1251 [Training] 13/106 [==>...........................] - ETA: 27s  batch_loss: 0.1265 [Training] 14/106 [==>...........................] - ETA: 27s  batch_loss: 0.1269 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 0.1273 [Training] 16/106 [===>..........................] - ETA: 26s  batch_loss: 0.1288 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.1280 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 0.1270 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 0.1270 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.1277 [Training] 21/106 [====>.........................] - ETA: 23s  batch_loss: 0.1302 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.1282 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 0.1269 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.1270 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.1277 [Training] 26/106 [======>.......................] - ETA: 22s  batch_loss: 0.1285 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.1297 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.1313 [Training] 29/106 [=======>......................] - ETA: 21s  batch_loss: 0.1306 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.1321 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.1336 [Training] 32/106 [========>.....................] - ETA: 20s  batch_loss: 0.1353 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.1363 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.1366 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.1368 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.1375 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.1378 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.1384 [Training] 39/106 [==========>...................] - ETA: 18s  batch_loss: 0.1379 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.1372 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.1385 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 0.1386 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.1405 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.1407 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.1399 [Training] 46/106 [============>.................] - ETA: 16s  batch_loss: 0.1401 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.1410 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.1411 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.1405 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.1402 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.1400 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.1402 [Training] 53/106 [==============>...............] - ETA: 14s  batch_loss: 0.1405 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.1403 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.1401 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.1402 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.1406 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.1410 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.1410 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.1411 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.1415 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.1416 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.1423 [Training] 64/106 [=================>............] - ETA: 10s  batch_loss: 0.1422 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.1422 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.1424 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.1419 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.1415 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.1423 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.1425 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.1430 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.1427 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.1425 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.1432 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.1428 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.1425 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.1427 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.1438 [Training] 79/106 [=====================>........] - ETA: 7s  batch_loss: 0.1440 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.1443 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.1443 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.1443 [Training] 83/106 [======================>.......] - ETA: 5s  batch_loss: 0.1443 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.1443 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.1444 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.1440 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.1439 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.1442 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.1441 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.1441 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.1448 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.1454 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.1455 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.1456 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.1459 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.1457 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.1462 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.1460 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.1456 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.1457 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.1455 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.1455 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.1456 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.1456 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.1455 [Training] 106/106 [==============================] 257.6ms/step  batch_loss: 0.1454 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:35:26 - INFO - root -   The F1-score is 0.6418185541675199
09/04/2022 06:35:26 - INFO - root -   the best eval f1 is 0.6418, saving model !!
09/04/2022 06:35:28 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:39  batch_loss: 0.1229 [Training] 2/106 [..............................] - ETA: 1:02  batch_loss: 0.1333 [Training] 3/106 [..............................] - ETA: 50s  batch_loss: 0.1396 [Training] 4/106 [>.............................] - ETA: 43s  batch_loss: 0.1369 [Training] 5/106 [>.............................] - ETA: 39s  batch_loss: 0.1374 [Training] 6/106 [>.............................] - ETA: 37s  batch_loss: 0.1364 [Training] 7/106 [>.............................] - ETA: 35s  batch_loss: 0.1420 [Training] 8/106 [=>............................] - ETA: 33s  batch_loss: 0.1429 [Training] 9/106 [=>............................] - ETA: 32s  batch_loss: 0.1419 [Training] 10/106 [=>............................] - ETA: 31s  batch_loss: 0.1479 [Training] 11/106 [==>...........................] - ETA: 30s  batch_loss: 0.1453 [Training] 12/106 [==>...........................] - ETA: 29s  batch_loss: 0.1486 [Training] 13/106 [==>...........................] - ETA: 28s  batch_loss: 0.1458 [Training] 14/106 [==>...........................] - ETA: 27s  batch_loss: 0.1445 [Training] 15/106 [===>..........................] - ETA: 27s  batch_loss: 0.1435 [Training] 16/106 [===>..........................] - ETA: 26s  batch_loss: 0.1441 [Training] 17/106 [===>..........................] - ETA: 26s  batch_loss: 0.1433 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 0.1422 [Training] 19/106 [====>.........................] - ETA: 25s  batch_loss: 0.1426 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.1421 [Training] 21/106 [====>.........................] - ETA: 24s  batch_loss: 0.1438 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.1413 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 0.1423 [Training] 24/106 [=====>........................] - ETA: 23s  batch_loss: 0.1408 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.1398 [Training] 26/106 [======>.......................] - ETA: 22s  batch_loss: 0.1400 [Training] 27/106 [======>.......................] - ETA: 22s  batch_loss: 0.1405 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.1425 [Training] 29/106 [=======>......................] - ETA: 21s  batch_loss: 0.1421 [Training] 30/106 [=======>......................] - ETA: 21s  batch_loss: 0.1426 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.1421 [Training] 32/106 [========>.....................] - ETA: 20s  batch_loss: 0.1411 [Training] 33/106 [========>.....................] - ETA: 20s  batch_loss: 0.1410 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.1404 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.1409 [Training] 36/106 [=========>....................] - ETA: 19s  batch_loss: 0.1411 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.1410 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.1400 [Training] 39/106 [==========>...................] - ETA: 18s  batch_loss: 0.1397 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.1395 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.1398 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 0.1395 [Training] 43/106 [===========>..................] - ETA: 17s  batch_loss: 0.1391 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.1397 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.1393 [Training] 46/106 [============>.................] - ETA: 16s  batch_loss: 0.1385 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.1379 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.1385 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.1389 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.1397 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.1393 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.1391 [Training] 53/106 [==============>...............] - ETA: 14s  batch_loss: 0.1388 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.1388 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.1384 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.1386 [Training] 57/106 [===============>..............] - ETA: 13s  batch_loss: 0.1397 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.1395 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.1397 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.1396 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.1392 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.1395 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.1393 [Training] 64/106 [=================>............] - ETA: 11s  batch_loss: 0.1386 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.1387 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.1387 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.1385 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.1383 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.1381 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.1379 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.1388 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.1383 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.1384 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.1387 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.1388 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.1386 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.1387 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.1382 [Training] 79/106 [=====================>........] - ETA: 7s  batch_loss: 0.1380 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.1383 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.1382 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.1381 [Training] 83/106 [======================>.......] - ETA: 6s  batch_loss: 0.1385 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.1385 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.1383 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.1384 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.1388 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.1387 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.1384 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.1383 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.1389 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.1393 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.1393 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.1395 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.1393 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.1392 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.1390 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.1388 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.1388 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.1394 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.1396 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.1394 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.1401 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.1398 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.1401 [Training] 106/106 [==============================] 259.6ms/step  batch_loss: 0.1403 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:36:02 - INFO - root -   The F1-score is 0.6439674315321984
09/04/2022 06:36:02 - INFO - root -   the best eval f1 is 0.6440, saving model !!
09/04/2022 06:36:05 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:32  batch_loss: 0.1409 [Training] 2/106 [..............................] - ETA: 57s  batch_loss: 0.1198 [Training] 3/106 [..............................] - ETA: 46s  batch_loss: 0.1175 [Training] 4/106 [>.............................] - ETA: 40s  batch_loss: 0.1246 [Training] 5/106 [>.............................] - ETA: 37s  batch_loss: 0.1177 [Training] 6/106 [>.............................] - ETA: 35s  batch_loss: 0.1183 [Training] 7/106 [>.............................] - ETA: 33s  batch_loss: 0.1212 [Training] 8/106 [=>............................] - ETA: 31s  batch_loss: 0.1193 [Training] 9/106 [=>............................] - ETA: 30s  batch_loss: 0.1145 [Training] 10/106 [=>............................] - ETA: 29s  batch_loss: 0.1150 [Training] 11/106 [==>...........................] - ETA: 29s  batch_loss: 0.1143 [Training] 12/106 [==>...........................] - ETA: 28s  batch_loss: 0.1140 [Training] 13/106 [==>...........................] - ETA: 27s  batch_loss: 0.1139 [Training] 14/106 [==>...........................] - ETA: 27s  batch_loss: 0.1132 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 0.1155 [Training] 16/106 [===>..........................] - ETA: 25s  batch_loss: 0.1170 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.1176 [Training] 18/106 [====>.........................] - ETA: 25s  batch_loss: 0.1173 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 0.1168 [Training] 20/106 [====>.........................] - ETA: 24s  batch_loss: 0.1199 [Training] 21/106 [====>.........................] - ETA: 23s  batch_loss: 0.1201 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.1219 [Training] 23/106 [=====>........................] - ETA: 23s  batch_loss: 0.1211 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.1237 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.1257 [Training] 26/106 [======>.......................] - ETA: 22s  batch_loss: 0.1249 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.1254 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.1265 [Training] 29/106 [=======>......................] - ETA: 21s  batch_loss: 0.1280 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.1285 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.1283 [Training] 32/106 [========>.....................] - ETA: 20s  batch_loss: 0.1285 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.1280 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.1277 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.1274 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.1266 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.1281 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.1274 [Training] 39/106 [==========>...................] - ETA: 17s  batch_loss: 0.1263 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.1272 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.1270 [Training] 42/106 [==========>...................] - ETA: 17s  batch_loss: 0.1279 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.1277 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.1279 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.1280 [Training] 46/106 [============>.................] - ETA: 15s  batch_loss: 0.1286 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.1286 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.1284 [Training] 49/106 [============>.................] - ETA: 14s  batch_loss: 0.1286 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.1284 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.1292 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.1298 [Training] 53/106 [==============>...............] - ETA: 13s  batch_loss: 0.1305 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.1307 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.1304 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.1304 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.1318 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.1315 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.1311 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.1314 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.1316 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.1315 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.1329 [Training] 64/106 [=================>............] - ETA: 11s  batch_loss: 0.1332 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.1329 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.1328 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.1328 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.1325 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.1322 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.1323 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.1322 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.1322 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.1318 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.1315 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.1315 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.1315 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.1319 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.1323 [Training] 79/106 [=====================>........] - ETA: 7s  batch_loss: 0.1323 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.1324 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.1324 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.1324 [Training] 83/106 [======================>.......] - ETA: 6s  batch_loss: 0.1329 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.1335 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.1335 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.1337 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.1336 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.1333 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.1331 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.1332 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.1332 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.1334 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.1335 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.1339 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.1339 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.1338 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.1337 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.1342 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.1340 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.1340 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.1337 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.1336 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.1339 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.1339 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.1340 [Training] 106/106 [==============================] 259.5ms/step  batch_loss: 0.1343 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:36:39 - INFO - root -   The F1-score is 0.6495400340715503
09/04/2022 06:36:39 - INFO - root -   the best eval f1 is 0.6495, saving model !!
09/04/2022 06:36:42 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/106 [..............................] - ETA: 1:26  batch_loss: 0.1811 [Training] 2/106 [..............................] - ETA: 55s  batch_loss: 0.1455 [Training] 3/106 [..............................] - ETA: 45s  batch_loss: 0.1355 [Training] 4/106 [>.............................] - ETA: 40s  batch_loss: 0.1270 [Training] 5/106 [>.............................] - ETA: 37s  batch_loss: 0.1246 [Training] 6/106 [>.............................] - ETA: 34s  batch_loss: 0.1219 [Training] 7/106 [>.............................] - ETA: 33s  batch_loss: 0.1189 [Training] 8/106 [=>............................] - ETA: 31s  batch_loss: 0.1155 [Training] 9/106 [=>............................] - ETA: 30s  batch_loss: 0.1167 [Training] 10/106 [=>............................] - ETA: 29s  batch_loss: 0.1172 [Training] 11/106 [==>...........................] - ETA: 28s  batch_loss: 0.1173 [Training] 12/106 [==>...........................] - ETA: 27s  batch_loss: 0.1188 [Training] 13/106 [==>...........................] - ETA: 27s  batch_loss: 0.1192 [Training] 14/106 [==>...........................] - ETA: 26s  batch_loss: 0.1192 [Training] 15/106 [===>..........................] - ETA: 26s  batch_loss: 0.1204 [Training] 16/106 [===>..........................] - ETA: 25s  batch_loss: 0.1206 [Training] 17/106 [===>..........................] - ETA: 25s  batch_loss: 0.1233 [Training] 18/106 [====>.........................] - ETA: 24s  batch_loss: 0.1218 [Training] 19/106 [====>.........................] - ETA: 24s  batch_loss: 0.1207 [Training] 20/106 [====>.........................] - ETA: 23s  batch_loss: 0.1223 [Training] 21/106 [====>.........................] - ETA: 23s  batch_loss: 0.1216 [Training] 22/106 [=====>........................] - ETA: 23s  batch_loss: 0.1227 [Training] 23/106 [=====>........................] - ETA: 22s  batch_loss: 0.1218 [Training] 24/106 [=====>........................] - ETA: 22s  batch_loss: 0.1223 [Training] 25/106 [======>.......................] - ETA: 22s  batch_loss: 0.1230 [Training] 26/106 [======>.......................] - ETA: 21s  batch_loss: 0.1232 [Training] 27/106 [======>.......................] - ETA: 21s  batch_loss: 0.1223 [Training] 28/106 [======>.......................] - ETA: 21s  batch_loss: 0.1227 [Training] 29/106 [=======>......................] - ETA: 20s  batch_loss: 0.1218 [Training] 30/106 [=======>......................] - ETA: 20s  batch_loss: 0.1223 [Training] 31/106 [=======>......................] - ETA: 20s  batch_loss: 0.1235 [Training] 32/106 [========>.....................] - ETA: 19s  batch_loss: 0.1238 [Training] 33/106 [========>.....................] - ETA: 19s  batch_loss: 0.1254 [Training] 34/106 [========>.....................] - ETA: 19s  batch_loss: 0.1260 [Training] 35/106 [========>.....................] - ETA: 19s  batch_loss: 0.1256 [Training] 36/106 [=========>....................] - ETA: 18s  batch_loss: 0.1253 [Training] 37/106 [=========>....................] - ETA: 18s  batch_loss: 0.1258 [Training] 38/106 [=========>....................] - ETA: 18s  batch_loss: 0.1260 [Training] 39/106 [==========>...................] - ETA: 17s  batch_loss: 0.1257 [Training] 40/106 [==========>...................] - ETA: 17s  batch_loss: 0.1262 [Training] 41/106 [==========>...................] - ETA: 17s  batch_loss: 0.1272 [Training] 42/106 [==========>...................] - ETA: 16s  batch_loss: 0.1265 [Training] 43/106 [===========>..................] - ETA: 16s  batch_loss: 0.1264 [Training] 44/106 [===========>..................] - ETA: 16s  batch_loss: 0.1260 [Training] 45/106 [===========>..................] - ETA: 16s  batch_loss: 0.1257 [Training] 46/106 [============>.................] - ETA: 15s  batch_loss: 0.1255 [Training] 47/106 [============>.................] - ETA: 15s  batch_loss: 0.1248 [Training] 48/106 [============>.................] - ETA: 15s  batch_loss: 0.1244 [Training] 49/106 [============>.................] - ETA: 15s  batch_loss: 0.1242 [Training] 50/106 [=============>................] - ETA: 14s  batch_loss: 0.1254 [Training] 51/106 [=============>................] - ETA: 14s  batch_loss: 0.1255 [Training] 52/106 [=============>................] - ETA: 14s  batch_loss: 0.1261 [Training] 53/106 [==============>...............] - ETA: 13s  batch_loss: 0.1264 [Training] 54/106 [==============>...............] - ETA: 13s  batch_loss: 0.1256 [Training] 55/106 [==============>...............] - ETA: 13s  batch_loss: 0.1254 [Training] 56/106 [==============>...............] - ETA: 13s  batch_loss: 0.1256 [Training] 57/106 [===============>..............] - ETA: 12s  batch_loss: 0.1256 [Training] 58/106 [===============>..............] - ETA: 12s  batch_loss: 0.1258 [Training] 59/106 [===============>..............] - ETA: 12s  batch_loss: 0.1261 [Training] 60/106 [===============>..............] - ETA: 12s  batch_loss: 0.1258 [Training] 61/106 [================>.............] - ETA: 11s  batch_loss: 0.1256 [Training] 62/106 [================>.............] - ETA: 11s  batch_loss: 0.1259 [Training] 63/106 [================>.............] - ETA: 11s  batch_loss: 0.1252 [Training] 64/106 [=================>............] - ETA: 10s  batch_loss: 0.1260 [Training] 65/106 [=================>............] - ETA: 10s  batch_loss: 0.1263 [Training] 66/106 [=================>............] - ETA: 10s  batch_loss: 0.1256 [Training] 67/106 [=================>............] - ETA: 10s  batch_loss: 0.1255 [Training] 68/106 [==================>...........] - ETA: 9s  batch_loss: 0.1253 [Training] 69/106 [==================>...........] - ETA: 9s  batch_loss: 0.1253 [Training] 70/106 [==================>...........] - ETA: 9s  batch_loss: 0.1255 [Training] 71/106 [===================>..........] - ETA: 9s  batch_loss: 0.1257 [Training] 72/106 [===================>..........] - ETA: 8s  batch_loss: 0.1259 [Training] 73/106 [===================>..........] - ETA: 8s  batch_loss: 0.1268 [Training] 74/106 [===================>..........] - ETA: 8s  batch_loss: 0.1267 [Training] 75/106 [====================>.........] - ETA: 8s  batch_loss: 0.1265 [Training] 76/106 [====================>.........] - ETA: 7s  batch_loss: 0.1270 [Training] 77/106 [====================>.........] - ETA: 7s  batch_loss: 0.1269 [Training] 78/106 [=====================>........] - ETA: 7s  batch_loss: 0.1271 [Training] 79/106 [=====================>........] - ETA: 7s  batch_loss: 0.1273 [Training] 80/106 [=====================>........] - ETA: 6s  batch_loss: 0.1272 [Training] 81/106 [=====================>........] - ETA: 6s  batch_loss: 0.1278 [Training] 82/106 [======================>.......] - ETA: 6s  batch_loss: 0.1280 [Training] 83/106 [======================>.......] - ETA: 5s  batch_loss: 0.1283 [Training] 84/106 [======================>.......] - ETA: 5s  batch_loss: 0.1282 [Training] 85/106 [=======================>......] - ETA: 5s  batch_loss: 0.1278 [Training] 86/106 [=======================>......] - ETA: 5s  batch_loss: 0.1286 [Training] 87/106 [=======================>......] - ETA: 4s  batch_loss: 0.1287 [Training] 88/106 [=======================>......] - ETA: 4s  batch_loss: 0.1286 [Training] 89/106 [========================>.....] - ETA: 4s  batch_loss: 0.1287 [Training] 90/106 [========================>.....] - ETA: 4s  batch_loss: 0.1289 [Training] 91/106 [========================>.....] - ETA: 3s  batch_loss: 0.1289 [Training] 92/106 [=========================>....] - ETA: 3s  batch_loss: 0.1287 [Training] 93/106 [=========================>....] - ETA: 3s  batch_loss: 0.1293 [Training] 94/106 [=========================>....] - ETA: 3s  batch_loss: 0.1290 [Training] 95/106 [=========================>....] - ETA: 2s  batch_loss: 0.1290 [Training] 96/106 [==========================>...] - ETA: 2s  batch_loss: 0.1288 [Training] 97/106 [==========================>...] - ETA: 2s  batch_loss: 0.1289 [Training] 98/106 [==========================>...] - ETA: 2s  batch_loss: 0.1288 [Training] 99/106 [===========================>..] - ETA: 1s  batch_loss: 0.1288 [Training] 100/106 [===========================>..] - ETA: 1s  batch_loss: 0.1287 [Training] 101/106 [===========================>..] - ETA: 1s  batch_loss: 0.1285 [Training] 102/106 [===========================>..] - ETA: 1s  batch_loss: 0.1289 [Training] 103/106 [============================>.] - ETA: 0s  batch_loss: 0.1288 [Training] 104/106 [============================>.] - ETA: 0s  batch_loss: 0.1289 [Training] 105/106 [============================>.] - ETA: 0s  batch_loss: 0.1290 [Training] 106/106 [==============================] 260.9ms/step  batch_loss: 0.1295 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/04/2022 06:37:16 - INFO - root -   The F1-score is 0.6451698513800425
********** predict start ***********
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 8, 
    "event_type": "role", 
    "fine_tunning_model_path": "./output/DuEE1.0/role/best_model.pkl", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 0.001, 
    "max_len": 250, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 50.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 128, 
    "per_gpu_train_batch_size": 8, 
    "predict_save_path": "./output/DuEE1.0/role/test_result.json", 
    "seed": 66, 
    "stride": 100, 
    "test_json": "./data/DuEE1.0/subor_news.json", 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/147540 [00:00<?, ?it/s]tokenizing...:   0%|          | 198/147540 [00:00<01:14, 1967.35it/s]tokenizing...:   0%|          | 408/147540 [00:00<01:11, 2044.55it/s]tokenizing...:   0%|          | 615/147540 [00:00<01:11, 2052.45it/s]tokenizing...:   1%|          | 821/147540 [00:00<01:12, 2028.02it/s]tokenizing...:   1%|          | 1024/147540 [00:00<01:24, 1741.59it/s]tokenizing...:   1%|          | 1204/147540 [00:00<01:29, 1636.85it/s]tokenizing...:   1%|          | 1405/147540 [00:00<01:23, 1743.04it/s]tokenizing...:   1%|          | 1608/147540 [00:00<01:20, 1823.85it/s]tokenizing...:   1%|          | 1815/147540 [00:00<01:17, 1869.65it/s]tokenizing...:   1%|▏         | 2005/147540 [00:01<03:11, 759.52it/s] tokenizing...:   1%|▏         | 2202/147540 [00:01<02:35, 934.09it/s]tokenizing...:   2%|▏         | 2406/147540 [00:01<02:09, 1121.46it/s]tokenizing...:   2%|▏         | 2593/147540 [00:01<01:54, 1265.94it/s]tokenizing...:   2%|▏         | 2777/147540 [00:01<01:44, 1390.23it/s]tokenizing...:   2%|▏         | 2955/147540 [00:02<01:40, 1445.44it/s]tokenizing...:   2%|▏         | 3128/147540 [00:02<01:35, 1509.71it/s]tokenizing...:   2%|▏         | 3300/147540 [00:02<01:38, 1467.91it/s]tokenizing...:   2%|▏         | 3517/147540 [00:02<01:27, 1648.57it/s]tokenizing...:   3%|▎         | 3695/147540 [00:02<01:35, 1506.24it/s]tokenizing...:   3%|▎         | 3905/147540 [00:02<01:26, 1657.25it/s]tokenizing...:   3%|▎         | 4091/147540 [00:02<01:23, 1710.55it/s]tokenizing...:   3%|▎         | 4286/147540 [00:02<01:20, 1773.40it/s]tokenizing...:   3%|▎         | 4480/147540 [00:02<01:18, 1820.52it/s]tokenizing...:   3%|▎         | 4667/147540 [00:03<01:20, 1767.65it/s]tokenizing...:   3%|▎         | 4848/147540 [00:03<01:20, 1765.93it/s]tokenizing...:   3%|▎         | 5027/147540 [00:03<01:24, 1682.76it/s]tokenizing...:   4%|▎         | 5198/147540 [00:03<01:28, 1606.28it/s]tokenizing...:   4%|▎         | 5361/147540 [00:03<01:28, 1604.49it/s]tokenizing...:   4%|▎         | 5523/147540 [00:03<01:29, 1591.42it/s]tokenizing...:   4%|▍         | 5684/147540 [00:03<01:29, 1583.43it/s]tokenizing...:   4%|▍         | 5843/147540 [00:03<01:29, 1579.81it/s]tokenizing...:   4%|▍         | 6002/147540 [00:03<01:30, 1563.45it/s]tokenizing...:   4%|▍         | 6159/147540 [00:04<01:32, 1532.35it/s]tokenizing...:   4%|▍         | 6319/147540 [00:04<01:31, 1551.52it/s]tokenizing...:   4%|▍         | 6475/147540 [00:04<01:30, 1550.30it/s]tokenizing...:   4%|▍         | 6631/147540 [00:04<01:31, 1543.08it/s]tokenizing...:   5%|▍         | 6786/147540 [00:04<01:32, 1527.05it/s]tokenizing...:   5%|▍         | 6939/147540 [00:04<01:32, 1525.15it/s]tokenizing...:   5%|▍         | 7092/147540 [00:04<01:35, 1471.66it/s]tokenizing...:   5%|▍         | 7240/147540 [00:04<01:35, 1469.87it/s]tokenizing...:   5%|▌         | 7396/147540 [00:04<01:33, 1495.87it/s]tokenizing...:   5%|▌         | 7546/147540 [00:04<01:33, 1496.62it/s]tokenizing...:   5%|▌         | 7696/147540 [00:05<01:34, 1483.08it/s]tokenizing...:   5%|▌         | 7856/147540 [00:05<01:32, 1516.49it/s]tokenizing...:   5%|▌         | 8008/147540 [00:05<01:32, 1516.35it/s]tokenizing...:   6%|▌         | 8160/147540 [00:05<01:39, 1398.90it/s]tokenizing...:   6%|▌         | 8302/147540 [00:05<01:59, 1164.11it/s]tokenizing...:   6%|▌         | 8426/147540 [00:05<02:00, 1152.03it/s]tokenizing...:   6%|▌         | 8583/147540 [00:05<01:50, 1256.62it/s]tokenizing...:   6%|▌         | 8742/147540 [00:05<01:43, 1344.35it/s]tokenizing...:   6%|▌         | 8942/147540 [00:05<01:30, 1525.55it/s]tokenizing...:   6%|▌         | 9132/147540 [00:06<01:24, 1629.89it/s]tokenizing...:   6%|▋         | 9299/147540 [00:06<01:28, 1563.10it/s]tokenizing...:   6%|▋         | 9459/147540 [00:06<01:30, 1522.60it/s]tokenizing...:   7%|▋         | 9680/147540 [00:06<01:20, 1714.91it/s]tokenizing...:   7%|▋         | 9855/147540 [00:06<01:20, 1700.51it/s]tokenizing...:   7%|▋         | 10048/147540 [00:06<01:17, 1762.74it/s]tokenizing...:   7%|▋         | 10226/147540 [00:06<01:18, 1754.71it/s]tokenizing...:   7%|▋         | 10410/147540 [00:06<01:17, 1776.31it/s]tokenizing...:   7%|▋         | 10589/147540 [00:06<01:19, 1721.26it/s]tokenizing...:   7%|▋         | 10763/147540 [00:07<01:19, 1722.70it/s]tokenizing...:   7%|▋         | 10957/147540 [00:07<01:16, 1785.62it/s]tokenizing...:   8%|▊         | 11152/147540 [00:07<01:14, 1833.49it/s]tokenizing...:   8%|▊         | 11339/147540 [00:07<01:13, 1843.12it/s]tokenizing...:   8%|▊         | 11545/147540 [00:07<01:11, 1906.63it/s]tokenizing...:   8%|▊         | 11743/147540 [00:07<01:10, 1923.21it/s]tokenizing...:   8%|▊         | 11936/147540 [00:07<01:10, 1911.95it/s]tokenizing...:   8%|▊         | 12134/147540 [00:07<01:10, 1929.43it/s]tokenizing...:   8%|▊         | 12328/147540 [00:07<01:20, 1683.05it/s]tokenizing...:   8%|▊         | 12503/147540 [00:07<01:20, 1676.55it/s]tokenizing...:   9%|▊         | 12675/147540 [00:08<01:22, 1626.81it/s]tokenizing...:   9%|▊         | 12841/147540 [00:08<01:32, 1448.40it/s]tokenizing...:   9%|▉         | 12991/147540 [00:08<01:38, 1360.65it/s]tokenizing...:   9%|▉         | 13131/147540 [00:08<01:42, 1312.73it/s]tokenizing...:   9%|▉         | 13301/147540 [00:08<01:35, 1412.19it/s]tokenizing...:   9%|▉         | 13480/147540 [00:08<01:28, 1513.92it/s]tokenizing...:   9%|▉         | 13682/147540 [00:08<01:21, 1652.40it/s]tokenizing...:   9%|▉         | 13851/147540 [00:08<01:21, 1634.22it/s]tokenizing...:  10%|▉         | 14017/147540 [00:09<01:32, 1438.42it/s]tokenizing...:  10%|▉         | 14167/147540 [00:09<01:58, 1125.22it/s]tokenizing...:  10%|▉         | 14360/147540 [00:09<01:42, 1303.00it/s]tokenizing...:  10%|▉         | 14538/147540 [00:09<01:34, 1412.85it/s]tokenizing...:  10%|▉         | 14693/147540 [00:09<01:32, 1437.56it/s]tokenizing...:  10%|█         | 14867/147540 [00:09<01:27, 1517.87it/s]tokenizing...:  10%|█         | 15078/147540 [00:09<01:18, 1680.56it/s]tokenizing...:  10%|█         | 15273/147540 [00:09<01:15, 1754.02it/s]tokenizing...:  10%|█         | 15454/147540 [00:10<01:20, 1634.42it/s]tokenizing...:  11%|█         | 15623/147540 [00:10<01:20, 1634.15it/s]tokenizing...:  11%|█         | 15790/147540 [00:10<01:20, 1642.16it/s]tokenizing...:  11%|█         | 15968/147540 [00:10<01:18, 1680.64it/s]tokenizing...:  11%|█         | 16141/147540 [00:10<01:17, 1694.51it/s]tokenizing...:  11%|█         | 16342/147540 [00:10<01:13, 1783.11it/s]tokenizing...:  11%|█         | 16522/147540 [00:10<01:17, 1696.87it/s]tokenizing...:  11%|█▏        | 16694/147540 [00:10<01:33, 1396.56it/s]tokenizing...:  11%|█▏        | 16861/147540 [00:10<01:29, 1461.48it/s]tokenizing...:  12%|█▏        | 17017/147540 [00:11<01:27, 1485.50it/s]tokenizing...:  12%|█▏        | 17172/147540 [00:11<01:29, 1458.92it/s]tokenizing...:  12%|█▏        | 17355/147540 [00:11<01:23, 1560.20it/s]tokenizing...:  12%|█▏        | 17520/147540 [00:11<01:22, 1582.75it/s]tokenizing...:  12%|█▏        | 17682/147540 [00:11<01:28, 1462.16it/s]tokenizing...:  12%|█▏        | 17832/147540 [00:11<01:28, 1468.29it/s]tokenizing...:  12%|█▏        | 17996/147540 [00:11<01:25, 1512.46it/s]tokenizing...:  12%|█▏        | 18151/147540 [00:11<01:24, 1522.34it/s]tokenizing...:  12%|█▏        | 18309/147540 [00:11<01:24, 1538.07it/s]tokenizing...:  13%|█▎        | 18464/147540 [00:11<01:32, 1396.68it/s]tokenizing...:  13%|█▎        | 18630/147540 [00:12<01:27, 1466.32it/s]tokenizing...:  13%|█▎        | 18780/147540 [00:12<01:29, 1441.08it/s]tokenizing...:  13%|█▎        | 18927/147540 [00:12<01:35, 1341.21it/s]tokenizing...:  13%|█▎        | 19064/147540 [00:12<01:42, 1257.77it/s]tokenizing...:  13%|█▎        | 19192/147540 [00:12<01:49, 1170.68it/s]tokenizing...:  13%|█▎        | 19312/147540 [00:12<01:56, 1104.11it/s]tokenizing...:  13%|█▎        | 19463/147540 [00:12<01:46, 1207.70it/s]tokenizing...:  13%|█▎        | 19617/147540 [00:12<01:38, 1294.18it/s]tokenizing...:  13%|█▎        | 19750/147540 [00:13<01:40, 1270.81it/s]tokenizing...:  13%|█▎        | 19905/147540 [00:13<01:34, 1344.85it/s]tokenizing...:  14%|█▎        | 20089/147540 [00:13<01:25, 1484.79it/s]tokenizing...:  14%|█▎        | 20281/147540 [00:13<01:19, 1610.04it/s]tokenizing...:  14%|█▍        | 20467/147540 [00:13<01:15, 1678.53it/s]tokenizing...:  14%|█▍        | 20643/147540 [00:13<01:14, 1700.07it/s]tokenizing...:  14%|█▍        | 20815/147540 [00:14<05:08, 410.62it/s] tokenizing...:  14%|█▍        | 20975/147540 [00:14<04:03, 520.17it/s]tokenizing...:  14%|█▍        | 21129/147540 [00:14<03:17, 638.66it/s]tokenizing...:  14%|█▍        | 21286/147540 [00:14<02:43, 770.50it/s]tokenizing...:  15%|█▍        | 21439/147540 [00:15<02:20, 897.93it/s]tokenizing...:  15%|█▍        | 21596/147540 [00:15<02:02, 1027.13it/s]tokenizing...:  15%|█▍        | 21751/147540 [00:15<01:50, 1140.34it/s]tokenizing...:  15%|█▍        | 21903/147540 [00:15<01:45, 1188.66it/s]tokenizing...:  15%|█▍        | 22049/147540 [00:15<01:41, 1237.98it/s]tokenizing...:  15%|█▌        | 22195/147540 [00:15<01:36, 1294.62it/s]tokenizing...:  15%|█▌        | 22356/147540 [00:15<01:30, 1378.66it/s]tokenizing...:  15%|█▌        | 22506/147540 [00:15<01:47, 1162.00it/s]tokenizing...:  15%|█▌        | 22636/147540 [00:16<01:47, 1161.98it/s]tokenizing...:  15%|█▌        | 22792/147540 [00:16<01:38, 1261.42it/s]tokenizing...:  16%|█▌        | 22933/147540 [00:16<01:36, 1297.84it/s]tokenizing...:  16%|█▌        | 23088/147540 [00:16<01:31, 1366.65it/s]tokenizing...:  16%|█▌        | 23243/147540 [00:16<01:27, 1417.62it/s]tokenizing...:  16%|█▌        | 23389/147540 [00:16<01:30, 1371.54it/s]tokenizing...:  16%|█▌        | 23539/147540 [00:16<01:28, 1406.73it/s]tokenizing...:  16%|█▌        | 23695/147540 [00:16<01:25, 1447.43it/s]tokenizing...:  16%|█▌        | 23851/147540 [00:16<01:23, 1479.40it/s]tokenizing...:  16%|█▋        | 24001/147540 [00:16<01:24, 1462.66it/s]tokenizing...:  16%|█▋        | 24153/147540 [00:17<01:23, 1477.52it/s]tokenizing...:  16%|█▋        | 24309/147540 [00:17<01:22, 1500.57it/s]tokenizing...:  17%|█▋        | 24471/147540 [00:17<01:20, 1534.13it/s]tokenizing...:  17%|█▋        | 24674/147540 [00:17<01:13, 1679.54it/s]tokenizing...:  17%|█▋        | 24874/147540 [00:17<01:09, 1774.79it/s]tokenizing...:  17%|█▋        | 25052/147540 [00:17<01:19, 1543.76it/s]tokenizing...:  17%|█▋        | 25212/147540 [00:17<01:20, 1512.84it/s]tokenizing...:  17%|█▋        | 25375/147540 [00:17<01:19, 1539.31it/s]tokenizing...:  17%|█▋        | 25566/147540 [00:17<01:14, 1642.89it/s]tokenizing...:  17%|█▋        | 25750/147540 [00:17<01:11, 1698.24it/s]tokenizing...:  18%|█▊        | 25937/147540 [00:18<01:09, 1742.78it/s]tokenizing...:  18%|█▊        | 26113/147540 [00:18<01:10, 1712.19it/s]tokenizing...:  18%|█▊        | 26311/147540 [00:18<01:07, 1789.21it/s]tokenizing...:  18%|█▊        | 26520/147540 [00:18<01:04, 1876.18it/s]tokenizing...:  18%|█▊        | 26721/147540 [00:18<01:03, 1915.28it/s]tokenizing...:  18%|█▊        | 26943/147540 [00:18<01:00, 2004.54it/s]tokenizing...:  18%|█▊        | 27145/147540 [00:18<01:02, 1929.91it/s]tokenizing...:  19%|█▊        | 27340/147540 [00:18<01:04, 1871.25it/s]tokenizing...:  19%|█▊        | 27529/147540 [00:18<01:07, 1788.36it/s]tokenizing...:  19%|█▉        | 27710/147540 [00:19<01:16, 1570.27it/s]tokenizing...:  19%|█▉        | 27930/147540 [00:19<01:09, 1732.90it/s]tokenizing...:  19%|█▉        | 28110/147540 [00:19<01:13, 1631.01it/s]tokenizing...:  19%|█▉        | 28283/147540 [00:19<01:12, 1656.24it/s]tokenizing...:  19%|█▉        | 28492/147540 [00:19<01:07, 1775.12it/s]tokenizing...:  19%|█▉        | 28696/147540 [00:19<01:04, 1849.32it/s]tokenizing...:  20%|█▉        | 28904/147540 [00:19<01:01, 1913.89it/s]tokenizing...:  20%|█▉        | 29098/147540 [00:19<01:02, 1900.61it/s]tokenizing...:  20%|█▉        | 29297/147540 [00:19<01:01, 1924.79it/s]tokenizing...:  20%|█▉        | 29500/147540 [00:20<01:00, 1952.80it/s]tokenizing...:  20%|██        | 29698/147540 [00:20<01:00, 1960.02it/s]tokenizing...:  20%|██        | 29905/147540 [00:20<00:59, 1992.30it/s]tokenizing...:  20%|██        | 30120/147540 [00:20<00:57, 2037.49it/s]tokenizing...:  21%|██        | 30325/147540 [00:20<00:58, 2010.51it/s]tokenizing...:  21%|██        | 30527/147540 [00:20<00:59, 1962.58it/s]tokenizing...:  21%|██        | 30736/147540 [00:20<00:58, 1999.38it/s]tokenizing...:  21%|██        | 30960/147540 [00:20<00:56, 2066.79it/s]tokenizing...:  21%|██        | 31168/147540 [00:20<00:56, 2060.07it/s]tokenizing...:  21%|██▏       | 31381/147540 [00:20<00:55, 2079.50it/s]tokenizing...:  21%|██▏       | 31600/147540 [00:21<00:54, 2111.71it/s]tokenizing...:  22%|██▏       | 31812/147540 [00:21<00:58, 1973.54it/s]tokenizing...:  22%|██▏       | 32018/147540 [00:21<00:57, 1996.03it/s]tokenizing...:  22%|██▏       | 32220/147540 [00:21<00:58, 1959.35it/s]tokenizing...:  22%|██▏       | 32418/147540 [00:21<01:01, 1863.14it/s]tokenizing...:  22%|██▏       | 32606/147540 [00:21<01:07, 1700.20it/s]tokenizing...:  22%|██▏       | 32780/147540 [00:21<01:09, 1652.66it/s]tokenizing...:  22%|██▏       | 32948/147540 [00:21<01:15, 1526.43it/s]tokenizing...:  22%|██▏       | 33104/147540 [00:22<01:22, 1385.02it/s]tokenizing...:  23%|██▎       | 33278/147540 [00:22<01:17, 1471.26it/s]tokenizing...:  23%|██▎       | 33429/147540 [00:22<01:17, 1463.20it/s]tokenizing...:  23%|██▎       | 33595/147540 [00:22<01:15, 1516.01it/s]tokenizing...:  23%|██▎       | 33791/147540 [00:22<01:09, 1635.71it/s]tokenizing...:  23%|██▎       | 33969/147540 [00:22<01:07, 1674.61it/s]tokenizing...:  23%|██▎       | 34140/147540 [00:22<01:07, 1684.29it/s]tokenizing...:  23%|██▎       | 34310/147540 [00:22<01:07, 1688.47it/s]tokenizing...:  23%|██▎       | 34480/147540 [00:22<01:08, 1650.41it/s]tokenizing...:  23%|██▎       | 34650/147540 [00:22<01:07, 1663.40it/s]tokenizing...:  24%|██▎       | 34837/147540 [00:23<01:05, 1720.46it/s]tokenizing...:  24%|██▎       | 35010/147540 [00:23<01:05, 1710.21it/s]tokenizing...:  24%|██▍       | 35182/147540 [00:23<01:06, 1683.11it/s]tokenizing...:  24%|██▍       | 35357/147540 [00:23<01:05, 1702.27it/s]tokenizing...:  24%|██▍       | 35528/147540 [00:23<01:07, 1650.90it/s]tokenizing...:  24%|██▍       | 35694/147540 [00:23<01:07, 1652.18it/s]tokenizing...:  24%|██▍       | 35863/147540 [00:23<01:07, 1658.77it/s]tokenizing...:  24%|██▍       | 36031/147540 [00:23<01:07, 1660.97it/s]tokenizing...:  25%|██▍       | 36198/147540 [00:23<01:07, 1660.88it/s]tokenizing...:  25%|██▍       | 36377/147540 [00:23<01:05, 1698.23it/s]tokenizing...:  25%|██▍       | 36550/147540 [00:24<01:05, 1706.14it/s]tokenizing...:  25%|██▍       | 36750/147540 [00:24<01:01, 1792.72it/s]tokenizing...:  25%|██▌       | 36945/147540 [00:24<01:00, 1838.56it/s]tokenizing...:  25%|██▌       | 37129/147540 [00:24<01:03, 1728.64it/s]tokenizing...:  25%|██▌       | 37304/147540 [00:24<01:05, 1687.90it/s]tokenizing...:  25%|██▌       | 37474/147540 [00:24<01:06, 1653.11it/s]tokenizing...:  26%|██▌       | 37641/147540 [00:24<01:11, 1542.93it/s]tokenizing...:  26%|██▌       | 37797/147540 [00:24<01:15, 1462.74it/s]tokenizing...:  26%|██▌       | 37945/147540 [00:24<01:17, 1407.01it/s]tokenizing...:  26%|██▌       | 38087/147540 [00:25<01:19, 1371.90it/s]tokenizing...:  26%|██▌       | 38239/147540 [00:25<01:17, 1410.50it/s]tokenizing...:  26%|██▌       | 38401/147540 [00:25<01:14, 1466.30it/s]tokenizing...:  26%|██▌       | 38560/147540 [00:25<01:13, 1480.21it/s]tokenizing...:  26%|██▌       | 38725/147540 [00:25<01:11, 1525.91it/s]tokenizing...:  26%|██▋       | 38889/147540 [00:25<01:09, 1557.68it/s]tokenizing...:  26%|██▋       | 39052/147540 [00:25<01:08, 1577.92it/s]tokenizing...:  27%|██▋       | 39214/147540 [00:25<01:08, 1590.29it/s]tokenizing...:  27%|██▋       | 39379/147540 [00:25<01:07, 1607.64it/s]tokenizing...:  27%|██▋       | 39543/147540 [00:25<01:06, 1615.68it/s]tokenizing...:  27%|██▋       | 39705/147540 [00:26<01:08, 1579.98it/s]tokenizing...:  27%|██▋       | 39864/147540 [00:26<01:08, 1581.18it/s]tokenizing...:  27%|██▋       | 40030/147540 [00:26<01:07, 1601.79it/s]tokenizing...:  27%|██▋       | 40200/147540 [00:26<01:05, 1628.20it/s]tokenizing...:  27%|██▋       | 40375/147540 [00:26<01:04, 1663.01it/s]tokenizing...:  27%|██▋       | 40543/147540 [00:26<01:04, 1666.82it/s]tokenizing...:  28%|██▊       | 40710/147540 [00:26<01:05, 1637.91it/s]tokenizing...:  28%|██▊       | 40874/147540 [00:26<01:06, 1609.50it/s]tokenizing...:  28%|██▊       | 41036/147540 [00:26<01:06, 1594.42it/s]tokenizing...:  28%|██▊       | 41217/147540 [00:27<01:04, 1656.52it/s]tokenizing...:  28%|██▊       | 41383/147540 [00:27<01:05, 1626.69it/s]tokenizing...:  28%|██▊       | 41579/147540 [00:27<01:02, 1701.07it/s]tokenizing...:  28%|██▊       | 41750/147540 [00:27<01:03, 1674.00it/s]tokenizing...:  28%|██▊       | 41950/147540 [00:27<00:59, 1766.03it/s]tokenizing...:  29%|██▊       | 42143/147540 [00:27<00:58, 1812.71it/s]tokenizing...:  29%|██▊       | 42329/147540 [00:27<00:57, 1823.27it/s]tokenizing...:  29%|██▉       | 42512/147540 [00:27<01:00, 1732.82it/s]tokenizing...:  29%|██▉       | 42687/147540 [00:27<01:04, 1630.27it/s]tokenizing...:  29%|██▉       | 42852/147540 [00:27<01:06, 1577.12it/s]tokenizing...:  29%|██▉       | 43011/147540 [00:28<01:06, 1571.58it/s]tokenizing...:  29%|██▉       | 43235/147540 [00:28<00:59, 1757.69it/s]tokenizing...:  29%|██▉       | 43449/147540 [00:28<00:55, 1865.81it/s]tokenizing...:  30%|██▉       | 43639/147540 [00:28<00:55, 1872.71it/s]tokenizing...:  30%|██▉       | 43828/147540 [00:28<01:30, 1150.48it/s]tokenizing...:  30%|██▉       | 43978/147540 [00:30<06:24, 269.03it/s] tokenizing...:  30%|██▉       | 44230/147540 [00:30<04:15, 404.68it/s]tokenizing...:  30%|███       | 44433/147540 [00:30<03:13, 533.48it/s]tokenizing...:  30%|███       | 44605/147540 [00:30<02:37, 654.18it/s]tokenizing...:  30%|███       | 44788/147540 [00:30<02:08, 802.63it/s]tokenizing...:  30%|███       | 44960/147540 [00:31<01:49, 936.06it/s]tokenizing...:  31%|███       | 45185/147540 [00:31<01:27, 1170.41it/s]tokenizing...:  31%|███       | 45417/147540 [00:31<01:12, 1402.97it/s]tokenizing...:  31%|███       | 45654/147540 [00:31<01:03, 1607.86it/s]tokenizing...:  31%|███       | 45863/147540 [00:31<00:59, 1719.20it/s]tokenizing...:  31%|███       | 46095/147540 [00:31<00:54, 1871.80it/s]tokenizing...:  31%|███▏      | 46310/147540 [00:31<00:53, 1882.39it/s]tokenizing...:  32%|███▏      | 46518/147540 [00:31<00:53, 1881.08it/s]tokenizing...:  32%|███▏      | 46768/147540 [00:31<00:49, 2049.00it/s]tokenizing...:  32%|███▏      | 46995/147540 [00:31<00:49, 2048.12it/s]tokenizing...:  32%|███▏      | 47276/147540 [00:32<00:44, 2257.89it/s]tokenizing...:  32%|███▏      | 47509/147540 [00:32<00:45, 2213.72it/s]tokenizing...:  32%|███▏      | 47736/147540 [00:32<00:44, 2219.67it/s]tokenizing...:  33%|███▎      | 47962/147540 [00:32<00:51, 1921.39it/s]tokenizing...:  33%|███▎      | 48164/147540 [00:32<00:51, 1930.72it/s]tokenizing...:  33%|███▎      | 48364/147540 [00:32<00:50, 1946.41it/s]tokenizing...:  33%|███▎      | 48564/147540 [00:32<00:53, 1836.87it/s]tokenizing...:  33%|███▎      | 48752/147540 [00:32<00:55, 1786.55it/s]tokenizing...:  33%|███▎      | 48973/147540 [00:32<00:51, 1900.88it/s]tokenizing...:  33%|███▎      | 49167/147540 [00:33<00:51, 1908.25it/s]tokenizing...:  33%|███▎      | 49371/147540 [00:33<00:50, 1944.37it/s]tokenizing...:  34%|███▎      | 49568/147540 [00:33<00:59, 1659.27it/s]tokenizing...:  34%|███▎      | 49773/147540 [00:33<00:55, 1757.44it/s]tokenizing...:  34%|███▍      | 49990/147540 [00:33<00:52, 1868.68it/s]tokenizing...:  34%|███▍      | 50189/147540 [00:33<00:51, 1902.25it/s]tokenizing...:  34%|███▍      | 50384/147540 [00:33<01:01, 1587.54it/s]tokenizing...:  34%|███▍      | 50562/147540 [00:33<00:59, 1630.82it/s]tokenizing...:  34%|███▍      | 50734/147540 [00:34<01:00, 1607.41it/s]tokenizing...:  35%|███▍      | 50924/147540 [00:34<00:57, 1685.01it/s]tokenizing...:  35%|███▍      | 51109/147540 [00:34<00:55, 1730.65it/s]tokenizing...:  35%|███▍      | 51287/147540 [00:34<00:57, 1683.08it/s]tokenizing...:  35%|███▍      | 51459/147540 [00:34<00:59, 1620.20it/s]tokenizing...:  35%|███▍      | 51636/147540 [00:34<00:57, 1659.85it/s]tokenizing...:  35%|███▌      | 51804/147540 [00:34<00:57, 1657.04it/s]tokenizing...:  35%|███▌      | 52011/147540 [00:34<00:53, 1774.92it/s]tokenizing...:  35%|███▌      | 52190/147540 [00:34<00:56, 1694.87it/s]tokenizing...:  35%|███▌      | 52362/147540 [00:34<00:57, 1641.44it/s]tokenizing...:  36%|███▌      | 52528/147540 [00:35<00:57, 1642.54it/s]tokenizing...:  36%|███▌      | 52735/147540 [00:35<00:53, 1762.12it/s]tokenizing...:  36%|███▌      | 52913/147540 [00:35<00:54, 1737.23it/s]tokenizing...:  36%|███▌      | 53088/147540 [00:35<00:54, 1730.11it/s]tokenizing...:  36%|███▌      | 53262/147540 [00:35<00:57, 1641.90it/s]tokenizing...:  36%|███▌      | 53434/147540 [00:35<00:56, 1662.64it/s]tokenizing...:  36%|███▋      | 53629/147540 [00:35<00:53, 1742.12it/s]tokenizing...:  36%|███▋      | 53805/147540 [00:35<00:54, 1729.16it/s]tokenizing...:  37%|███▋      | 53979/147540 [00:35<00:54, 1709.90it/s]tokenizing...:  37%|███▋      | 54151/147540 [00:36<00:59, 1557.16it/s]tokenizing...:  37%|███▋      | 54310/147540 [00:36<01:04, 1438.11it/s]tokenizing...:  37%|███▋      | 54457/147540 [00:36<01:06, 1409.37it/s]tokenizing...:  37%|███▋      | 54627/147540 [00:36<01:02, 1487.20it/s]tokenizing...:  37%|███▋      | 54799/147540 [00:36<00:59, 1550.24it/s]tokenizing...:  37%|███▋      | 54969/147540 [00:36<00:58, 1591.73it/s]tokenizing...:  37%|███▋      | 55157/147540 [00:36<00:55, 1674.62it/s]tokenizing...:  38%|███▊      | 55347/147540 [00:36<00:54, 1701.59it/s]tokenizing...:  38%|███▊      | 55535/147540 [00:36<00:52, 1752.87it/s]tokenizing...:  38%|███▊      | 55712/147540 [00:37<00:55, 1669.02it/s]tokenizing...:  38%|███▊      | 55881/147540 [00:37<01:01, 1480.43it/s]tokenizing...:  38%|███▊      | 56034/147540 [00:37<01:02, 1474.70it/s]tokenizing...:  38%|███▊      | 56196/147540 [00:37<01:00, 1513.70it/s]tokenizing...:  38%|███▊      | 56367/147540 [00:37<00:58, 1567.12it/s]tokenizing...:  38%|███▊      | 56526/147540 [00:37<00:59, 1539.65it/s]tokenizing...:  38%|███▊      | 56694/147540 [00:37<00:57, 1576.13it/s]tokenizing...:  39%|███▊      | 56868/147540 [00:37<00:55, 1619.47it/s]tokenizing...:  39%|███▊      | 57037/147540 [00:37<00:55, 1639.92it/s]tokenizing...:  39%|███▉      | 57207/147540 [00:38<00:54, 1654.52it/s]tokenizing...:  39%|███▉      | 57374/147540 [00:38<00:54, 1653.77it/s]tokenizing...:  39%|███▉      | 57540/147540 [00:38<00:55, 1624.27it/s]tokenizing...:  39%|███▉      | 57713/147540 [00:38<00:54, 1653.26it/s]tokenizing...:  39%|███▉      | 57884/147540 [00:38<00:53, 1669.66it/s]tokenizing...:  39%|███▉      | 58055/147540 [00:38<00:53, 1680.80it/s]tokenizing...:  39%|███▉      | 58224/147540 [00:38<00:55, 1612.64it/s]tokenizing...:  40%|███▉      | 58386/147540 [00:38<00:59, 1509.63it/s]tokenizing...:  40%|███▉      | 58539/147540 [00:38<00:59, 1490.60it/s]tokenizing...:  40%|███▉      | 58698/147540 [00:38<00:58, 1517.99it/s]tokenizing...:  40%|███▉      | 58857/147540 [00:39<00:57, 1535.37it/s]tokenizing...:  40%|████      | 59016/147540 [00:39<00:57, 1547.29it/s]tokenizing...:  40%|████      | 59175/147540 [00:39<00:56, 1554.40it/s]tokenizing...:  40%|████      | 59340/147540 [00:39<00:55, 1580.31it/s]tokenizing...:  40%|████      | 59516/147540 [00:39<00:53, 1630.86it/s]tokenizing...:  40%|████      | 59697/147540 [00:39<00:52, 1680.44it/s]tokenizing...:  41%|████      | 59866/147540 [00:39<00:53, 1635.21it/s]tokenizing...:  41%|████      | 60067/147540 [00:39<00:50, 1744.59it/s]tokenizing...:  41%|████      | 60243/147540 [00:39<00:50, 1744.07it/s]tokenizing...:  41%|████      | 60454/147540 [00:39<00:47, 1851.62it/s]tokenizing...:  41%|████      | 60640/147540 [00:40<00:47, 1846.21it/s]tokenizing...:  41%|████      | 60840/147540 [00:40<00:45, 1891.75it/s]tokenizing...:  41%|████▏     | 61030/147540 [00:40<00:49, 1736.47it/s]tokenizing...:  41%|████▏     | 61207/147540 [00:40<00:58, 1463.92it/s]tokenizing...:  42%|████▏     | 61362/147540 [00:40<01:03, 1366.83it/s]tokenizing...:  42%|████▏     | 61582/147540 [00:40<00:54, 1573.01it/s]tokenizing...:  42%|████▏     | 61753/147540 [00:40<00:53, 1604.19it/s]tokenizing...:  42%|████▏     | 61955/147540 [00:40<00:49, 1716.70it/s]tokenizing...:  42%|████▏     | 62182/147540 [00:41<00:45, 1869.93it/s]tokenizing...:  42%|████▏     | 62404/147540 [00:41<00:43, 1969.24it/s]tokenizing...:  42%|████▏     | 62625/147540 [00:41<00:41, 2035.55it/s]tokenizing...:  43%|████▎     | 62832/147540 [00:41<00:49, 1707.62it/s]tokenizing...:  43%|████▎     | 63034/147540 [00:41<00:47, 1785.29it/s]tokenizing...:  43%|████▎     | 63222/147540 [00:41<00:47, 1781.78it/s]tokenizing...:  43%|████▎     | 63407/147540 [00:41<00:50, 1676.87it/s]tokenizing...:  43%|████▎     | 63580/147540 [00:41<00:52, 1608.94it/s]tokenizing...:  43%|████▎     | 63745/147540 [00:41<00:53, 1568.08it/s]tokenizing...:  43%|████▎     | 63914/147540 [00:42<00:52, 1599.31it/s]tokenizing...:  43%|████▎     | 64076/147540 [00:42<00:52, 1582.34it/s]tokenizing...:  44%|████▎     | 64249/147540 [00:42<00:51, 1622.75it/s]tokenizing...:  44%|████▎     | 64490/147540 [00:42<00:45, 1844.58it/s]tokenizing...:  44%|████▍     | 64677/147540 [00:42<00:46, 1790.76it/s]tokenizing...:  44%|████▍     | 64865/147540 [00:42<00:45, 1814.46it/s]tokenizing...:  44%|████▍     | 65117/147540 [00:42<00:40, 2018.32it/s]tokenizing...:  44%|████▍     | 65344/147540 [00:42<00:39, 2091.19it/s]tokenizing...:  44%|████▍     | 65638/147540 [00:42<00:34, 2340.31it/s]tokenizing...:  45%|████▍     | 65874/147540 [00:42<00:35, 2318.56it/s]tokenizing...:  45%|████▍     | 66107/147540 [00:43<00:36, 2215.54it/s]tokenizing...:  45%|████▍     | 66331/147540 [00:43<00:37, 2193.92it/s]tokenizing...:  45%|████▌     | 66552/147540 [00:43<00:38, 2119.61it/s]tokenizing...:  45%|████▌     | 66765/147540 [00:43<00:38, 2120.70it/s]tokenizing...:  45%|████▌     | 66978/147540 [00:43<00:41, 1939.37it/s]tokenizing...:  46%|████▌     | 67176/147540 [00:43<00:44, 1796.87it/s]tokenizing...:  46%|████▌     | 67360/147540 [00:43<00:44, 1785.92it/s]tokenizing...:  46%|████▌     | 67570/147540 [00:43<00:42, 1870.50it/s]tokenizing...:  46%|████▌     | 67760/147540 [00:43<00:42, 1874.61it/s]tokenizing...:  46%|████▌     | 67950/147540 [00:44<00:43, 1843.38it/s]tokenizing...:  46%|████▌     | 68136/147540 [00:44<00:49, 1591.58it/s]tokenizing...:  46%|████▋     | 68302/147540 [00:44<00:53, 1477.22it/s]tokenizing...:  46%|████▋     | 68455/147540 [00:44<00:55, 1413.11it/s]tokenizing...:  46%|████▋     | 68600/147540 [00:44<01:00, 1298.93it/s]tokenizing...:  47%|████▋     | 68734/147540 [00:44<01:01, 1283.94it/s]tokenizing...:  47%|████▋     | 68905/147540 [00:44<00:56, 1393.99it/s]tokenizing...:  47%|████▋     | 69060/147540 [00:44<00:54, 1436.17it/s]tokenizing...:  47%|████▋     | 69244/147540 [00:45<00:51, 1526.66it/s]tokenizing...:  47%|████▋     | 69399/147540 [00:45<00:58, 1329.10it/s]tokenizing...:  47%|████▋     | 69538/147540 [00:45<01:00, 1293.22it/s]tokenizing...:  47%|████▋     | 69672/147540 [00:45<01:01, 1270.15it/s]tokenizing...:  47%|████▋     | 69802/147540 [00:45<01:01, 1264.60it/s]tokenizing...:  47%|████▋     | 69962/147540 [00:45<00:58, 1327.71it/s]tokenizing...:  48%|████▊     | 70124/147540 [00:45<00:54, 1408.56it/s]tokenizing...:  48%|████▊     | 70286/147540 [00:45<00:52, 1465.74it/s]tokenizing...:  48%|████▊     | 70486/147540 [00:45<00:47, 1619.30it/s]tokenizing...:  48%|████▊     | 70650/147540 [00:46<00:48, 1575.26it/s]tokenizing...:  48%|████▊     | 70817/147540 [00:46<00:47, 1600.06it/s]tokenizing...:  48%|████▊     | 70979/147540 [00:46<00:47, 1595.06it/s]tokenizing...:  48%|████▊     | 71140/147540 [00:46<00:48, 1574.57it/s]tokenizing...:  48%|████▊     | 71306/147540 [00:46<00:47, 1597.15it/s]tokenizing...:  48%|████▊     | 71474/147540 [00:46<00:46, 1620.10it/s]tokenizing...:  49%|████▊     | 71639/147540 [00:46<00:46, 1628.90it/s]tokenizing...:  49%|████▊     | 71803/147540 [00:46<00:46, 1623.93it/s]tokenizing...:  49%|████▉     | 71969/147540 [00:46<00:46, 1633.50it/s]tokenizing...:  49%|████▉     | 72133/147540 [00:46<00:47, 1585.14it/s]tokenizing...:  49%|████▉     | 72304/147540 [00:47<00:46, 1618.18it/s]tokenizing...:  49%|████▉     | 72467/147540 [00:47<00:47, 1587.90it/s]tokenizing...:  49%|████▉     | 72629/147540 [00:47<00:46, 1594.64it/s]tokenizing...:  49%|████▉     | 72791/147540 [00:47<00:46, 1598.61it/s]tokenizing...:  49%|████▉     | 72956/147540 [00:47<00:46, 1611.39it/s]tokenizing...:  50%|████▉     | 73118/147540 [00:47<00:48, 1535.38it/s]tokenizing...:  50%|████▉     | 73273/147540 [00:49<04:46, 259.32it/s] tokenizing...:  50%|████▉     | 73440/147540 [00:49<03:31, 350.43it/s]tokenizing...:  50%|████▉     | 73604/147540 [00:49<02:40, 459.35it/s]tokenizing...:  50%|████▉     | 73759/147540 [00:49<02:07, 576.61it/s]tokenizing...:  50%|█████     | 73925/147540 [00:49<01:42, 720.61it/s]tokenizing...:  50%|█████     | 74092/147540 [00:49<01:24, 871.92it/s]tokenizing...:  50%|█████     | 74247/147540 [00:50<01:13, 992.50it/s]tokenizing...:  50%|█████     | 74414/147540 [00:50<01:04, 1133.44it/s]tokenizing...:  51%|█████     | 74572/147540 [00:50<00:59, 1230.09it/s]tokenizing...:  51%|█████     | 74740/147540 [00:50<00:54, 1340.82it/s]tokenizing...:  51%|█████     | 74909/147540 [00:50<00:50, 1431.20it/s]tokenizing...:  51%|█████     | 75072/147540 [00:50<00:49, 1449.63it/s]tokenizing...:  51%|█████     | 75236/147540 [00:50<00:48, 1500.35it/s]tokenizing...:  51%|█████     | 75418/147540 [00:50<00:45, 1589.84it/s]tokenizing...:  51%|█████     | 75585/147540 [00:50<00:44, 1607.46it/s]tokenizing...:  51%|█████▏    | 75752/147540 [00:50<00:45, 1580.18it/s]tokenizing...:  51%|█████▏    | 75914/147540 [00:51<00:46, 1546.11it/s]tokenizing...:  52%|█████▏    | 76077/147540 [00:51<00:45, 1568.97it/s]tokenizing...:  52%|█████▏    | 76250/147540 [00:51<00:44, 1614.59it/s]tokenizing...:  52%|█████▏    | 76414/147540 [00:51<00:45, 1550.76it/s]tokenizing...:  52%|█████▏    | 76622/147540 [00:51<00:41, 1697.31it/s]tokenizing...:  52%|█████▏    | 76825/147540 [00:51<00:39, 1783.70it/s]tokenizing...:  52%|█████▏    | 77023/147540 [00:51<00:38, 1839.89it/s]tokenizing...:  52%|█████▏    | 77235/147540 [00:51<00:36, 1918.89it/s]tokenizing...:  52%|█████▏    | 77428/147540 [00:51<00:37, 1860.01it/s]tokenizing...:  53%|█████▎    | 77616/147540 [00:52<00:42, 1626.75it/s]tokenizing...:  53%|█████▎    | 77846/147540 [00:52<00:38, 1803.88it/s]tokenizing...:  53%|█████▎    | 78061/147540 [00:52<00:36, 1896.36it/s]tokenizing...:  53%|█████▎    | 78259/147540 [00:52<00:36, 1916.93it/s]tokenizing...:  53%|█████▎    | 78455/147540 [00:52<00:39, 1745.53it/s]tokenizing...:  53%|█████▎    | 78635/147540 [00:52<00:41, 1662.57it/s]tokenizing...:  53%|█████▎    | 78806/147540 [00:52<00:42, 1603.26it/s]tokenizing...:  54%|█████▎    | 78996/147540 [00:52<00:40, 1672.52it/s]tokenizing...:  54%|█████▎    | 79192/147540 [00:52<00:39, 1751.35it/s]tokenizing...:  54%|█████▍    | 79370/147540 [00:53<00:41, 1659.65it/s]tokenizing...:  54%|█████▍    | 79539/147540 [00:53<00:42, 1594.95it/s]tokenizing...:  54%|█████▍    | 79701/147540 [00:53<00:44, 1524.34it/s]tokenizing...:  54%|█████▍    | 79855/147540 [00:53<00:45, 1502.71it/s]tokenizing...:  54%|█████▍    | 80019/147540 [00:53<00:43, 1538.05it/s]tokenizing...:  54%|█████▍    | 80174/147540 [00:53<00:45, 1466.71it/s]tokenizing...:  54%|█████▍    | 80323/147540 [00:53<00:45, 1472.36it/s]tokenizing...:  55%|█████▍    | 80472/147540 [00:53<00:48, 1375.22it/s]tokenizing...:  55%|█████▍    | 80638/147540 [00:53<00:46, 1452.47it/s]tokenizing...:  55%|█████▍    | 80815/147540 [00:54<00:43, 1541.64it/s]tokenizing...:  55%|█████▍    | 81047/147540 [00:54<00:37, 1763.89it/s]tokenizing...:  55%|█████▌    | 81266/147540 [00:54<00:35, 1886.50it/s]tokenizing...:  55%|█████▌    | 81457/147540 [00:54<00:37, 1758.07it/s]tokenizing...:  55%|█████▌    | 81636/147540 [00:54<00:40, 1623.86it/s]tokenizing...:  55%|█████▌    | 81803/147540 [00:54<00:42, 1558.29it/s]tokenizing...:  56%|█████▌    | 81962/147540 [00:54<00:42, 1559.49it/s]tokenizing...:  56%|█████▌    | 82120/147540 [00:54<00:44, 1486.01it/s]tokenizing...:  56%|█████▌    | 82293/147540 [00:54<00:42, 1551.62it/s]tokenizing...:  56%|█████▌    | 82450/147540 [00:55<00:43, 1480.10it/s]tokenizing...:  56%|█████▌    | 82600/147540 [00:55<00:43, 1480.72it/s]tokenizing...:  56%|█████▌    | 82900/147540 [00:55<00:33, 1906.57it/s]tokenizing...:  56%|█████▋    | 83109/147540 [00:55<00:32, 1958.49it/s]tokenizing...:  56%|█████▋    | 83322/147540 [00:55<00:32, 2006.40it/s]tokenizing...:  57%|█████▋    | 83575/147540 [00:55<00:29, 2159.04it/s]tokenizing...:  57%|█████▋    | 83818/147540 [00:55<00:28, 2237.42it/s]tokenizing...:  57%|█████▋    | 84089/147540 [00:55<00:26, 2356.39it/s]tokenizing...:  57%|█████▋    | 84326/147540 [00:55<00:27, 2281.34it/s]tokenizing...:  57%|█████▋    | 84557/147540 [00:55<00:27, 2287.03it/s]tokenizing...:  57%|█████▋    | 84787/147540 [00:56<00:28, 2167.92it/s]tokenizing...:  58%|█████▊    | 85006/147540 [00:56<00:29, 2123.84it/s]tokenizing...:  58%|█████▊    | 85220/147540 [00:56<00:36, 1723.36it/s]tokenizing...:  58%|█████▊    | 85405/147540 [00:56<00:37, 1657.25it/s]tokenizing...:  58%|█████▊    | 85580/147540 [00:56<00:40, 1547.92it/s]tokenizing...:  58%|█████▊    | 85756/147540 [00:56<00:38, 1597.80it/s]tokenizing...:  58%|█████▊    | 85962/147540 [00:56<00:35, 1711.47it/s]tokenizing...:  58%|█████▊    | 86166/147540 [00:56<00:34, 1799.48it/s]tokenizing...:  59%|█████▊    | 86351/147540 [00:57<00:34, 1796.56it/s]tokenizing...:  59%|█████▊    | 86554/147540 [00:57<00:32, 1858.46it/s]tokenizing...:  59%|█████▉    | 86743/147540 [00:57<00:32, 1864.84it/s]tokenizing...:  59%|█████▉    | 86932/147540 [00:57<00:33, 1809.21it/s]tokenizing...:  59%|█████▉    | 87118/147540 [00:57<00:33, 1820.07it/s]tokenizing...:  59%|█████▉    | 87320/147540 [00:57<00:32, 1876.75it/s]tokenizing...:  59%|█████▉    | 87509/147540 [00:57<00:33, 1788.75it/s]tokenizing...:  59%|█████▉    | 87690/147540 [00:57<00:33, 1778.13it/s]tokenizing...:  60%|█████▉    | 87869/147540 [00:57<00:34, 1711.42it/s]tokenizing...:  60%|█████▉    | 88042/147540 [00:57<00:34, 1706.53it/s]tokenizing...:  60%|█████▉    | 88214/147540 [00:58<00:35, 1674.01it/s]tokenizing...:  60%|█████▉    | 88397/147540 [00:58<00:34, 1717.45it/s]tokenizing...:  60%|██████    | 88570/147540 [00:58<00:34, 1704.87it/s]tokenizing...:  60%|██████    | 88741/147540 [00:58<00:34, 1691.91it/s]tokenizing...:  60%|██████    | 88913/147540 [00:58<00:34, 1698.96it/s]tokenizing...:  60%|██████    | 89084/147540 [00:58<00:34, 1673.61it/s]tokenizing...:  60%|██████    | 89252/147540 [00:58<00:38, 1517.93it/s]tokenizing...:  61%|██████    | 89407/147540 [00:58<00:38, 1525.94it/s]tokenizing...:  61%|██████    | 89564/147540 [00:58<00:37, 1538.10it/s]tokenizing...:  61%|██████    | 89747/147540 [00:59<00:35, 1621.37it/s]tokenizing...:  61%|██████    | 89923/147540 [00:59<00:34, 1661.34it/s]tokenizing...:  61%|██████    | 90092/147540 [00:59<00:34, 1668.99it/s]tokenizing...:  61%|██████    | 90260/147540 [00:59<00:36, 1554.37it/s]tokenizing...:  61%|██████▏   | 90418/147540 [00:59<00:39, 1457.78it/s]tokenizing...:  61%|██████▏   | 90567/147540 [00:59<00:40, 1393.38it/s]tokenizing...:  61%|██████▏   | 90709/147540 [00:59<00:46, 1233.24it/s]tokenizing...:  62%|██████▏   | 90837/147540 [00:59<00:49, 1151.39it/s]tokenizing...:  62%|██████▏   | 90956/147540 [00:59<00:49, 1136.75it/s]tokenizing...:  62%|██████▏   | 91110/147540 [01:00<00:45, 1240.13it/s]tokenizing...:  62%|██████▏   | 91326/147540 [01:00<00:37, 1486.39it/s]tokenizing...:  62%|██████▏   | 91530/147540 [01:00<00:34, 1637.82it/s]tokenizing...:  62%|██████▏   | 91699/147540 [01:00<00:37, 1501.01it/s]tokenizing...:  62%|██████▏   | 91855/147540 [01:00<00:37, 1483.08it/s]tokenizing...:  62%|██████▏   | 92012/147540 [01:00<00:36, 1506.81it/s]tokenizing...:  62%|██████▏   | 92166/147540 [01:00<00:36, 1500.24it/s]tokenizing...:  63%|██████▎   | 92363/147540 [01:00<00:33, 1633.33it/s]tokenizing...:  63%|██████▎   | 92529/147540 [01:00<00:35, 1530.22it/s]tokenizing...:  63%|██████▎   | 92685/147540 [01:01<00:41, 1321.99it/s]tokenizing...:  63%|██████▎   | 92858/147540 [01:01<00:38, 1423.48it/s]tokenizing...:  63%|██████▎   | 93023/147540 [01:01<00:36, 1481.44it/s]tokenizing...:  63%|██████▎   | 93211/147540 [01:01<00:34, 1590.96it/s]tokenizing...:  63%|██████▎   | 93391/147540 [01:01<00:32, 1649.31it/s]tokenizing...:  63%|██████▎   | 93588/147540 [01:01<00:30, 1740.58it/s]tokenizing...:  64%|██████▎   | 93765/147540 [01:01<00:31, 1693.25it/s]tokenizing...:  64%|██████▎   | 93944/147540 [01:01<00:31, 1719.49it/s]tokenizing...:  64%|██████▍   | 94118/147540 [01:01<00:30, 1723.40it/s]tokenizing...:  64%|██████▍   | 94292/147540 [01:02<00:30, 1723.94it/s]tokenizing...:  64%|██████▍   | 94466/147540 [01:02<00:33, 1566.53it/s]tokenizing...:  64%|██████▍   | 94626/147540 [01:02<00:33, 1564.10it/s]tokenizing...:  64%|██████▍   | 94810/147540 [01:02<00:32, 1640.72it/s]tokenizing...:  64%|██████▍   | 94977/147540 [01:02<00:35, 1492.09it/s]tokenizing...:  64%|██████▍   | 95130/147540 [01:02<00:38, 1374.54it/s]tokenizing...:  65%|██████▍   | 95272/147540 [01:02<00:38, 1365.36it/s]tokenizing...:  65%|██████▍   | 95412/147540 [01:02<00:41, 1269.77it/s]tokenizing...:  65%|██████▍   | 95602/147540 [01:02<00:36, 1427.64it/s]tokenizing...:  65%|██████▍   | 95756/147540 [01:03<00:35, 1457.98it/s]tokenizing...:  65%|██████▌   | 95947/147540 [01:03<00:32, 1583.85it/s]tokenizing...:  65%|██████▌   | 96145/147540 [01:03<00:31, 1647.35it/s]tokenizing...:  65%|██████▌   | 96346/147540 [01:03<00:29, 1746.64it/s]tokenizing...:  65%|██████▌   | 96523/147540 [01:03<00:29, 1749.86it/s]tokenizing...:  66%|██████▌   | 96700/147540 [01:03<00:29, 1716.55it/s]tokenizing...:  66%|██████▌   | 96873/147540 [01:03<00:31, 1631.27it/s]tokenizing...:  66%|██████▌   | 97038/147540 [01:03<00:31, 1598.93it/s]tokenizing...:  66%|██████▌   | 97199/147540 [01:03<00:32, 1531.07it/s]tokenizing...:  66%|██████▌   | 97358/147540 [01:04<00:32, 1545.70it/s]tokenizing...:  66%|██████▌   | 97518/147540 [01:04<00:32, 1559.28it/s]tokenizing...:  66%|██████▌   | 97675/147540 [01:04<00:32, 1554.08it/s]tokenizing...:  66%|██████▋   | 97831/147540 [01:04<00:32, 1541.34it/s]tokenizing...:  66%|██████▋   | 97986/147540 [01:04<00:32, 1539.14it/s]tokenizing...:  67%|██████▋   | 98141/147540 [01:04<00:32, 1525.63it/s]tokenizing...:  67%|██████▋   | 98294/147540 [01:04<00:34, 1422.54it/s]tokenizing...:  67%|██████▋   | 98438/147540 [01:04<00:35, 1372.53it/s]tokenizing...:  67%|██████▋   | 98585/147540 [01:04<00:35, 1398.00it/s]tokenizing...:  67%|██████▋   | 98726/147540 [01:05<00:35, 1381.02it/s]tokenizing...:  67%|██████▋   | 98865/147540 [01:05<00:35, 1360.04it/s]tokenizing...:  67%|██████▋   | 99017/147540 [01:05<00:34, 1403.50it/s]tokenizing...:  67%|██████▋   | 99162/147540 [01:05<00:34, 1416.24it/s]tokenizing...:  67%|██████▋   | 99305/147540 [01:05<00:39, 1222.37it/s]tokenizing...:  67%|██████▋   | 99447/147540 [01:05<00:37, 1272.79it/s]tokenizing...:  68%|██████▊   | 99596/147540 [01:05<00:36, 1331.74it/s]tokenizing...:  68%|██████▊   | 99738/147540 [01:05<00:35, 1351.52it/s]tokenizing...:  68%|██████▊   | 99876/147540 [01:05<00:35, 1337.61it/s]tokenizing...:  68%|██████▊   | 100012/147540 [01:06<00:37, 1253.24it/s]tokenizing...:  68%|██████▊   | 100158/147540 [01:06<00:36, 1310.28it/s]tokenizing...:  68%|██████▊   | 100309/147540 [01:06<00:34, 1364.83it/s]tokenizing...:  68%|██████▊   | 100448/147540 [01:06<00:35, 1337.40it/s]tokenizing...:  68%|██████▊   | 100635/147540 [01:06<00:31, 1488.88it/s]tokenizing...:  68%|██████▊   | 100796/147540 [01:06<00:33, 1383.92it/s]tokenizing...:  68%|██████▊   | 100938/147540 [01:06<00:36, 1269.98it/s]tokenizing...:  69%|██████▊   | 101111/147540 [01:06<00:33, 1390.37it/s]tokenizing...:  69%|██████▊   | 101330/147540 [01:06<00:28, 1607.17it/s]tokenizing...:  69%|██████▉   | 101502/147540 [01:06<00:28, 1632.05it/s]tokenizing...:  69%|██████▉   | 101669/147540 [01:07<00:28, 1595.43it/s]tokenizing...:  69%|██████▉   | 101832/147540 [01:07<00:28, 1596.73it/s]tokenizing...:  69%|██████▉   | 101994/147540 [01:07<00:29, 1552.66it/s]tokenizing...:  69%|██████▉   | 102189/147540 [01:07<00:27, 1663.19it/s]tokenizing...:  69%|██████▉   | 102386/147540 [01:07<00:25, 1751.45it/s]tokenizing...:  70%|██████▉   | 102563/147540 [01:07<00:26, 1716.80it/s]tokenizing...:  70%|██████▉   | 102759/147540 [01:07<00:25, 1785.50it/s]tokenizing...:  70%|██████▉   | 102962/147540 [01:07<00:24, 1846.72it/s]tokenizing...:  70%|██████▉   | 103148/147540 [01:07<00:26, 1667.19it/s]tokenizing...:  70%|███████   | 103332/147540 [01:08<00:25, 1704.34it/s]tokenizing...:  70%|███████   | 103519/147540 [01:08<00:25, 1750.04it/s]tokenizing...:  70%|███████   | 103697/147540 [01:08<00:25, 1721.45it/s]tokenizing...:  70%|███████   | 103893/147540 [01:08<00:24, 1788.50it/s]tokenizing...:  71%|███████   | 104086/147540 [01:08<00:23, 1827.91it/s]tokenizing...:  71%|███████   | 104291/147540 [01:08<00:22, 1891.34it/s]tokenizing...:  71%|███████   | 104521/147540 [01:08<00:21, 2011.46it/s]tokenizing...:  71%|███████   | 104724/147540 [01:08<00:21, 1992.50it/s]tokenizing...:  71%|███████   | 104924/147540 [01:08<00:23, 1832.61it/s]tokenizing...:  71%|███████   | 105111/147540 [01:09<00:26, 1611.55it/s]tokenizing...:  71%|███████▏  | 105321/147540 [01:09<00:24, 1737.17it/s]tokenizing...:  72%|███████▏  | 105541/147540 [01:09<00:23, 1810.08it/s]tokenizing...:  72%|███████▏  | 105809/147540 [01:09<00:20, 2045.71it/s]tokenizing...:  72%|███████▏  | 106050/147540 [01:09<00:19, 2146.27it/s]tokenizing...:  72%|███████▏  | 106270/147540 [01:09<00:19, 2136.00it/s]tokenizing...:  72%|███████▏  | 106487/147540 [01:09<00:22, 1814.28it/s]tokenizing...:  72%|███████▏  | 106679/147540 [01:09<00:22, 1782.70it/s]tokenizing...:  72%|███████▏  | 106877/147540 [01:09<00:22, 1833.30it/s]tokenizing...:  73%|███████▎  | 107066/147540 [01:10<00:23, 1759.51it/s]tokenizing...:  73%|███████▎  | 107247/147540 [01:10<00:23, 1733.80it/s]tokenizing...:  73%|███████▎  | 107455/147540 [01:10<00:22, 1816.36it/s]tokenizing...:  73%|███████▎  | 107640/147540 [01:10<00:24, 1661.71it/s]tokenizing...:  73%|███████▎  | 107810/147540 [01:10<00:25, 1547.12it/s]tokenizing...:  73%|███████▎  | 107998/147540 [01:10<00:24, 1633.17it/s]tokenizing...:  73%|███████▎  | 108165/147540 [01:10<00:24, 1598.21it/s]tokenizing...:  73%|███████▎  | 108369/147540 [01:10<00:22, 1719.14it/s]tokenizing...:  74%|███████▎  | 108558/147540 [01:10<00:22, 1764.44it/s]tokenizing...:  74%|███████▎  | 108737/147540 [01:11<00:24, 1611.86it/s]tokenizing...:  74%|███████▍  | 108903/147540 [01:11<00:28, 1373.86it/s]tokenizing...:  74%|███████▍  | 109049/147540 [01:11<00:30, 1257.89it/s]tokenizing...:  74%|███████▍  | 109182/147540 [01:11<00:33, 1134.91it/s]tokenizing...:  74%|███████▍  | 109301/147540 [01:11<00:38, 986.90it/s] tokenizing...:  74%|███████▍  | 109406/147540 [01:11<00:40, 931.73it/s]tokenizing...:  74%|███████▍  | 109503/147540 [01:11<00:44, 856.01it/s]tokenizing...:  74%|███████▍  | 109591/147540 [01:12<00:44, 852.62it/s]tokenizing...:  74%|███████▍  | 109694/147540 [01:12<00:42, 895.76it/s]tokenizing...:  74%|███████▍  | 109786/147540 [01:12<00:44, 857.23it/s]tokenizing...:  74%|███████▍  | 109874/147540 [01:15<07:24, 84.74it/s] tokenizing...:  75%|███████▍  | 110022/147540 [01:16<04:40, 133.85it/s]tokenizing...:  75%|███████▍  | 110166/147540 [01:16<03:10, 196.39it/s]tokenizing...:  75%|███████▍  | 110344/147540 [01:16<02:04, 297.77it/s]tokenizing...:  75%|███████▍  | 110523/147540 [01:16<01:27, 422.75it/s]tokenizing...:  75%|███████▌  | 110667/147540 [01:16<01:09, 531.37it/s]tokenizing...:  75%|███████▌  | 110814/147540 [01:16<00:56, 655.13it/s]tokenizing...:  75%|███████▌  | 110958/147540 [01:16<00:47, 764.49it/s]tokenizing...:  75%|███████▌  | 111105/147540 [01:16<00:40, 891.59it/s]tokenizing...:  75%|███████▌  | 111246/147540 [01:16<00:39, 928.05it/s]tokenizing...:  76%|███████▌  | 111420/147540 [01:17<00:32, 1101.34it/s]tokenizing...:  76%|███████▌  | 111570/147540 [01:17<00:30, 1193.96it/s]tokenizing...:  76%|███████▌  | 111715/147540 [01:17<00:28, 1248.89it/s]tokenizing...:  76%|███████▌  | 111871/147540 [01:17<00:26, 1329.77it/s]tokenizing...:  76%|███████▌  | 112019/147540 [01:17<00:26, 1320.56it/s]tokenizing...:  76%|███████▌  | 112171/147540 [01:17<00:25, 1373.53it/s]tokenizing...:  76%|███████▌  | 112322/147540 [01:17<00:25, 1407.96it/s]tokenizing...:  76%|███████▌  | 112474/147540 [01:17<00:24, 1434.92it/s]tokenizing...:  76%|███████▋  | 112622/147540 [01:17<00:26, 1314.96it/s]tokenizing...:  76%|███████▋  | 112759/147540 [01:17<00:26, 1292.79it/s]tokenizing...:  77%|███████▋  | 112892/147540 [01:18<00:27, 1265.91it/s]tokenizing...:  77%|███████▋  | 113021/147540 [01:18<00:28, 1211.85it/s]tokenizing...:  77%|███████▋  | 113175/147540 [01:18<00:26, 1299.89it/s]tokenizing...:  77%|███████▋  | 113331/147540 [01:18<00:24, 1369.45it/s]tokenizing...:  77%|███████▋  | 113470/147540 [01:18<00:25, 1349.47it/s]tokenizing...:  77%|███████▋  | 113623/147540 [01:18<00:24, 1400.40it/s]tokenizing...:  77%|███████▋  | 113791/147540 [01:18<00:22, 1480.69it/s]tokenizing...:  77%|███████▋  | 113955/147540 [01:18<00:22, 1525.72it/s]tokenizing...:  77%|███████▋  | 114109/147540 [01:18<00:22, 1507.02it/s]tokenizing...:  77%|███████▋  | 114263/147540 [01:19<00:21, 1516.23it/s]tokenizing...:  78%|███████▊  | 114418/147540 [01:19<00:21, 1525.68it/s]tokenizing...:  78%|███████▊  | 114573/147540 [01:19<00:21, 1531.91it/s]tokenizing...:  78%|███████▊  | 114727/147540 [01:19<00:21, 1523.26it/s]tokenizing...:  78%|███████▊  | 114880/147540 [01:19<00:28, 1145.09it/s]tokenizing...:  78%|███████▊  | 115009/147540 [01:19<00:27, 1171.08it/s]tokenizing...:  78%|███████▊  | 115157/147540 [01:19<00:25, 1249.49it/s]tokenizing...:  78%|███████▊  | 115299/147540 [01:19<00:24, 1294.89it/s]tokenizing...:  78%|███████▊  | 115449/147540 [01:19<00:23, 1349.99it/s]tokenizing...:  78%|███████▊  | 115590/147540 [01:20<00:23, 1339.10it/s]tokenizing...:  78%|███████▊  | 115747/147540 [01:20<00:22, 1403.71it/s]tokenizing...:  79%|███████▊  | 115907/147540 [01:20<00:21, 1459.59it/s]tokenizing...:  79%|███████▊  | 116061/147540 [01:20<00:21, 1481.87it/s]tokenizing...:  79%|███████▉  | 116211/147540 [01:20<00:21, 1452.79it/s]tokenizing...:  79%|███████▉  | 116362/147540 [01:20<00:21, 1467.14it/s]tokenizing...:  79%|███████▉  | 116521/147540 [01:20<00:20, 1501.52it/s]tokenizing...:  79%|███████▉  | 116678/147540 [01:20<00:20, 1520.69it/s]tokenizing...:  79%|███████▉  | 116835/147540 [01:20<00:20, 1533.38it/s]tokenizing...:  79%|███████▉  | 116995/147540 [01:20<00:19, 1549.34it/s]tokenizing...:  79%|███████▉  | 117151/147540 [01:21<00:20, 1498.15it/s]tokenizing...:  80%|███████▉  | 117304/147540 [01:21<00:20, 1505.89it/s]tokenizing...:  80%|███████▉  | 117460/147540 [01:21<00:19, 1518.67it/s]tokenizing...:  80%|███████▉  | 117627/147540 [01:21<00:19, 1550.52it/s]tokenizing...:  80%|███████▉  | 117783/147540 [01:21<00:21, 1390.30it/s]tokenizing...:  80%|███████▉  | 117963/147540 [01:21<00:19, 1500.66it/s]tokenizing...:  80%|████████  | 118117/147540 [01:21<00:21, 1362.39it/s]tokenizing...:  80%|████████  | 118258/147540 [01:21<00:27, 1058.31it/s]tokenizing...:  80%|████████  | 118440/147540 [01:22<00:23, 1230.22it/s]tokenizing...:  80%|████████  | 118633/147540 [01:22<00:20, 1399.87it/s]tokenizing...:  81%|████████  | 118838/147540 [01:22<00:18, 1566.93it/s]tokenizing...:  81%|████████  | 119025/147540 [01:22<00:17, 1647.94it/s]tokenizing...:  81%|████████  | 119200/147540 [01:22<00:17, 1612.80it/s]tokenizing...:  81%|████████  | 119369/147540 [01:22<00:22, 1261.02it/s]tokenizing...:  81%|████████  | 119542/147540 [01:22<00:20, 1370.51it/s]tokenizing...:  81%|████████  | 119694/147540 [01:22<00:19, 1394.97it/s]tokenizing...:  81%|████████▏ | 119895/147540 [01:22<00:17, 1555.44it/s]tokenizing...:  81%|████████▏ | 120061/147540 [01:23<00:17, 1542.30it/s]tokenizing...:  81%|████████▏ | 120241/147540 [01:23<00:17, 1559.34it/s]tokenizing...:  82%|████████▏ | 120402/147540 [01:23<00:17, 1560.12it/s]tokenizing...:  82%|████████▏ | 120566/147540 [01:23<00:17, 1580.41it/s]tokenizing...:  82%|████████▏ | 120755/147540 [01:23<00:16, 1667.42it/s]tokenizing...:  82%|████████▏ | 120959/147540 [01:23<00:14, 1774.25it/s]tokenizing...:  82%|████████▏ | 121144/147540 [01:23<00:14, 1793.22it/s]tokenizing...:  82%|████████▏ | 121325/147540 [01:23<00:15, 1746.01it/s]tokenizing...:  82%|████████▏ | 121533/147540 [01:23<00:14, 1842.32it/s]tokenizing...:  83%|████████▎ | 121747/147540 [01:24<00:13, 1928.49it/s]tokenizing...:  83%|████████▎ | 121941/147540 [01:24<00:13, 1896.29it/s]tokenizing...:  83%|████████▎ | 122145/147540 [01:24<00:13, 1937.09it/s]tokenizing...:  83%|████████▎ | 122340/147540 [01:24<00:13, 1855.39it/s]tokenizing...:  83%|████████▎ | 122527/147540 [01:24<00:14, 1736.79it/s]tokenizing...:  83%|████████▎ | 122703/147540 [01:24<00:14, 1712.68it/s]tokenizing...:  83%|████████▎ | 122891/147540 [01:24<00:14, 1758.20it/s]tokenizing...:  83%|████████▎ | 123090/147540 [01:24<00:13, 1823.36it/s]tokenizing...:  84%|████████▎ | 123274/147540 [01:24<00:13, 1769.92it/s]tokenizing...:  84%|████████▎ | 123453/147540 [01:25<00:14, 1668.20it/s]tokenizing...:  84%|████████▍ | 123663/147540 [01:25<00:13, 1786.40it/s]tokenizing...:  84%|████████▍ | 123883/147540 [01:25<00:12, 1899.65it/s]tokenizing...:  84%|████████▍ | 124111/147540 [01:25<00:11, 2006.26it/s]tokenizing...:  84%|████████▍ | 124379/147540 [01:25<00:10, 2200.96it/s]tokenizing...:  84%|████████▍ | 124603/147540 [01:25<00:10, 2209.16it/s]tokenizing...:  85%|████████▍ | 124826/147540 [01:25<00:10, 2108.23it/s]tokenizing...:  85%|████████▍ | 125039/147540 [01:25<00:11, 2042.69it/s]tokenizing...:  85%|████████▍ | 125245/147540 [01:25<00:11, 1884.54it/s]tokenizing...:  85%|████████▌ | 125437/147540 [01:25<00:12, 1839.88it/s]tokenizing...:  85%|████████▌ | 125623/147540 [01:26<00:12, 1814.15it/s]tokenizing...:  85%|████████▌ | 125819/147540 [01:26<00:11, 1854.60it/s]tokenizing...:  85%|████████▌ | 126016/147540 [01:26<00:11, 1885.95it/s]tokenizing...:  86%|████████▌ | 126221/147540 [01:26<00:11, 1931.84it/s]tokenizing...:  86%|████████▌ | 126423/147540 [01:26<00:10, 1956.65it/s]tokenizing...:  86%|████████▌ | 126620/147540 [01:26<00:11, 1874.20it/s]tokenizing...:  86%|████████▌ | 126819/147540 [01:26<00:10, 1906.14it/s]tokenizing...:  86%|████████▌ | 127011/147540 [01:26<00:11, 1863.47it/s]tokenizing...:  86%|████████▌ | 127199/147540 [01:26<00:11, 1842.38it/s]tokenizing...:  86%|████████▋ | 127384/147540 [01:27<00:10, 1832.42it/s]tokenizing...:  86%|████████▋ | 127568/147540 [01:27<00:11, 1735.49it/s]tokenizing...:  87%|████████▋ | 127744/147540 [01:27<00:11, 1741.71it/s]tokenizing...:  87%|████████▋ | 127919/147540 [01:27<00:12, 1602.86it/s]tokenizing...:  87%|████████▋ | 128107/147540 [01:27<00:11, 1677.88it/s]tokenizing...:  87%|████████▋ | 128278/147540 [01:27<00:11, 1643.93it/s]tokenizing...:  87%|████████▋ | 128445/147540 [01:27<00:11, 1620.83it/s]tokenizing...:  87%|████████▋ | 128611/147540 [01:27<00:11, 1630.36it/s]tokenizing...:  87%|████████▋ | 128794/147540 [01:27<00:11, 1685.25it/s]tokenizing...:  87%|████████▋ | 128964/147540 [01:27<00:11, 1642.82it/s]tokenizing...:  88%|████████▊ | 129129/147540 [01:28<00:12, 1427.24it/s]tokenizing...:  88%|████████▊ | 129277/147540 [01:28<00:13, 1334.04it/s]tokenizing...:  88%|████████▊ | 129415/147540 [01:28<00:13, 1308.96it/s]tokenizing...:  88%|████████▊ | 129549/147540 [01:28<00:15, 1161.07it/s]tokenizing...:  88%|████████▊ | 129686/147540 [01:28<00:14, 1212.76it/s]tokenizing...:  88%|████████▊ | 129847/147540 [01:28<00:13, 1315.39it/s]tokenizing...:  88%|████████▊ | 130004/147540 [01:28<00:12, 1384.55it/s]tokenizing...:  88%|████████▊ | 130161/147540 [01:28<00:12, 1436.11it/s]tokenizing...:  88%|████████▊ | 130308/147540 [01:29<00:12, 1433.30it/s]tokenizing...:  88%|████████▊ | 130454/147540 [01:29<00:12, 1399.21it/s]tokenizing...:  89%|████████▊ | 130596/147540 [01:29<00:12, 1318.27it/s]tokenizing...:  89%|████████▊ | 130734/147540 [01:29<00:12, 1333.67it/s]tokenizing...:  89%|████████▊ | 130891/147540 [01:29<00:11, 1397.56it/s]tokenizing...:  89%|████████▉ | 131047/147540 [01:29<00:11, 1443.68it/s]tokenizing...:  89%|████████▉ | 131218/147540 [01:29<00:10, 1519.52it/s]tokenizing...:  89%|████████▉ | 131407/147540 [01:29<00:09, 1623.08it/s]tokenizing...:  89%|████████▉ | 131571/147540 [01:29<00:09, 1606.96it/s]tokenizing...:  89%|████████▉ | 131747/147540 [01:29<00:09, 1649.46it/s]tokenizing...:  89%|████████▉ | 131918/147540 [01:30<00:09, 1666.97it/s]tokenizing...:  90%|████████▉ | 132086/147540 [01:30<00:09, 1596.94it/s]tokenizing...:  90%|████████▉ | 132247/147540 [01:30<00:10, 1485.79it/s]tokenizing...:  90%|████████▉ | 132398/147540 [01:30<00:10, 1464.89it/s]tokenizing...:  90%|████████▉ | 132553/147540 [01:30<00:10, 1486.96it/s]tokenizing...:  90%|████████▉ | 132712/147540 [01:30<00:09, 1514.93it/s]tokenizing...:  90%|█████████ | 132865/147540 [01:30<00:09, 1484.34it/s]tokenizing...:  90%|█████████ | 133016/147540 [01:30<00:09, 1490.77it/s]tokenizing...:  90%|█████████ | 133175/147540 [01:30<00:09, 1515.55it/s]tokenizing...:  90%|█████████ | 133327/147540 [01:31<00:09, 1510.99it/s]tokenizing...:  90%|█████████ | 133481/147540 [01:31<00:09, 1515.14it/s]tokenizing...:  91%|█████████ | 133636/147540 [01:31<00:09, 1523.23it/s]tokenizing...:  91%|█████████ | 133791/147540 [01:31<00:08, 1530.64it/s]tokenizing...:  91%|█████████ | 133945/147540 [01:31<00:09, 1473.07it/s]tokenizing...:  91%|█████████ | 134097/147540 [01:31<00:09, 1485.20it/s]tokenizing...:  91%|█████████ | 134249/147540 [01:31<00:08, 1491.27it/s]tokenizing...:  91%|█████████ | 134399/147540 [01:31<00:09, 1459.25it/s]tokenizing...:  91%|█████████ | 134551/147540 [01:31<00:08, 1475.70it/s]tokenizing...:  91%|█████████▏| 134706/147540 [01:31<00:08, 1496.29it/s]tokenizing...:  91%|█████████▏| 134857/147540 [01:32<00:08, 1462.21it/s]tokenizing...:  92%|█████████▏| 135008/147540 [01:32<00:08, 1472.70it/s]tokenizing...:  92%|█████████▏| 135159/147540 [01:32<00:08, 1483.27it/s]tokenizing...:  92%|█████████▏| 135326/147540 [01:32<00:07, 1537.72it/s]tokenizing...:  92%|█████████▏| 135480/147540 [01:32<00:07, 1524.15it/s]tokenizing...:  92%|█████████▏| 135635/147540 [01:32<00:07, 1528.87it/s]tokenizing...:  92%|█████████▏| 135789/147540 [01:32<00:07, 1527.40it/s]tokenizing...:  92%|█████████▏| 135959/147540 [01:32<00:07, 1578.13it/s]tokenizing...:  92%|█████████▏| 136148/147540 [01:32<00:06, 1669.92it/s]tokenizing...:  92%|█████████▏| 136336/147540 [01:32<00:06, 1729.54it/s]tokenizing...:  93%|█████████▎| 136510/147540 [01:33<00:06, 1666.31it/s]tokenizing...:  93%|█████████▎| 136678/147540 [01:33<00:07, 1358.64it/s]tokenizing...:  93%|█████████▎| 136824/147540 [01:33<00:07, 1352.16it/s]tokenizing...:  93%|█████████▎| 136966/147540 [01:33<00:08, 1310.00it/s]tokenizing...:  93%|█████████▎| 137102/147540 [01:33<00:07, 1307.71it/s]tokenizing...:  93%|█████████▎| 137256/147540 [01:33<00:07, 1370.49it/s]tokenizing...:  93%|█████████▎| 137396/147540 [01:33<00:09, 1070.34it/s]tokenizing...:  93%|█████████▎| 137529/147540 [01:34<00:08, 1129.03it/s]tokenizing...:  93%|█████████▎| 137652/147540 [01:34<00:10, 971.51it/s] tokenizing...:  93%|█████████▎| 137846/147540 [01:34<00:08, 1195.71it/s]tokenizing...:  94%|█████████▎| 138017/147540 [01:34<00:07, 1322.93it/s]tokenizing...:  94%|█████████▎| 138206/147540 [01:34<00:06, 1469.83it/s]tokenizing...:  94%|█████████▍| 138397/147540 [01:34<00:05, 1586.39it/s]tokenizing...:  94%|█████████▍| 138564/147540 [01:34<00:05, 1512.57it/s]tokenizing...:  94%|█████████▍| 138728/147540 [01:34<00:05, 1546.91it/s]tokenizing...:  94%|█████████▍| 138949/147540 [01:34<00:04, 1726.20it/s]tokenizing...:  94%|█████████▍| 139132/147540 [01:35<00:04, 1755.05it/s]tokenizing...:  94%|█████████▍| 139311/147540 [01:35<00:04, 1713.80it/s]tokenizing...:  95%|█████████▍| 139485/147540 [01:35<00:05, 1509.78it/s]tokenizing...:  95%|█████████▍| 139642/147540 [01:35<00:05, 1520.41it/s]tokenizing...:  95%|█████████▍| 139816/147540 [01:35<00:04, 1577.27it/s]tokenizing...:  95%|█████████▍| 140000/147540 [01:35<00:04, 1650.02it/s]tokenizing...:  95%|█████████▌| 140215/147540 [01:35<00:04, 1790.04it/s]tokenizing...:  95%|█████████▌| 140414/147540 [01:35<00:03, 1843.96it/s]tokenizing...:  95%|█████████▌| 140601/147540 [01:35<00:03, 1777.41it/s]tokenizing...:  95%|█████████▌| 140822/147540 [01:35<00:03, 1898.06it/s]tokenizing...:  96%|█████████▌| 141026/147540 [01:36<00:03, 1939.00it/s]tokenizing...:  96%|█████████▌| 141222/147540 [01:36<00:03, 1850.05it/s]tokenizing...:  96%|█████████▌| 141450/147540 [01:36<00:03, 1972.13it/s]tokenizing...:  96%|█████████▌| 141688/147540 [01:36<00:02, 2089.60it/s]tokenizing...:  96%|█████████▌| 141943/147540 [01:36<00:02, 2223.32it/s]tokenizing...:  96%|█████████▋| 142194/147540 [01:36<00:02, 2306.37it/s]tokenizing...:  97%|█████████▋| 142426/147540 [01:36<00:02, 2217.71it/s]tokenizing...:  97%|█████████▋| 142650/147540 [01:36<00:02, 2205.61it/s]tokenizing...:  97%|█████████▋| 142872/147540 [01:36<00:02, 2007.13it/s]tokenizing...:  97%|█████████▋| 143077/147540 [01:37<00:02, 1909.44it/s]tokenizing...:  97%|█████████▋| 143272/147540 [01:37<00:02, 1897.57it/s]tokenizing...:  97%|█████████▋| 143478/147540 [01:37<00:02, 1941.01it/s]tokenizing...:  97%|█████████▋| 143674/147540 [01:37<00:02, 1829.73it/s]tokenizing...:  98%|█████████▊| 143860/147540 [01:37<00:02, 1761.66it/s]tokenizing...:  98%|█████████▊| 144038/147540 [01:37<00:02, 1685.56it/s]tokenizing...:  98%|█████████▊| 144208/147540 [01:37<00:02, 1552.47it/s]tokenizing...:  98%|█████████▊| 144366/147540 [01:37<00:02, 1545.30it/s]tokenizing...:  98%|█████████▊| 144556/147540 [01:37<00:01, 1640.84it/s]tokenizing...:  98%|█████████▊| 144742/147540 [01:38<00:01, 1700.94it/s]tokenizing...:  98%|█████████▊| 144919/147540 [01:38<00:01, 1711.78it/s]tokenizing...:  98%|█████████▊| 145092/147540 [01:38<00:01, 1564.40it/s]tokenizing...:  98%|█████████▊| 145297/147540 [01:38<00:01, 1666.48it/s]tokenizing...:  99%|█████████▊| 145492/147540 [01:38<00:01, 1742.16it/s]tokenizing...:  99%|█████████▊| 145669/147540 [01:38<00:01, 1674.34it/s]tokenizing...:  99%|█████████▉| 145844/147540 [01:38<00:01, 1687.26it/s]tokenizing...:  99%|█████████▉| 146015/147540 [01:38<00:01, 1455.29it/s]tokenizing...:  99%|█████████▉| 146167/147540 [01:39<00:00, 1393.23it/s]tokenizing...:  99%|█████████▉| 146311/147540 [01:39<00:00, 1357.69it/s]tokenizing...:  99%|█████████▉| 146450/147540 [01:39<00:00, 1343.17it/s]tokenizing...:  99%|█████████▉| 146633/147540 [01:39<00:00, 1470.77it/s]tokenizing...:  99%|█████████▉| 146783/147540 [01:39<00:00, 1472.73it/s]tokenizing...: 100%|█████████▉| 146936/147540 [01:39<00:00, 1488.62it/s]tokenizing...: 100%|█████████▉| 147087/147540 [01:39<00:00, 1492.46it/s]tokenizing...: 100%|█████████▉| 147238/147540 [01:39<00:00, 1450.86it/s]tokenizing...: 100%|█████████▉| 147384/147540 [01:39<00:00, 1414.60it/s]tokenizing...: 100%|██████████| 147540/147540 [01:39<00:00, 1475.89it/s]
09/04/2022 06:39:02 - INFO - root -   The nums of the test_dataset features is 147540
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 0/1153 [00:00<?, ?it/s]  0%|          | 1/1153 [00:02<40:22,  2.10s/it]  0%|          | 2/1153 [00:08<1:25:47,  4.47s/it]  0%|          | 3/1153 [00:09<1:00:16,  3.14s/it]  0%|          | 4/1153 [00:11<47:34,  2.48s/it]    0%|          | 5/1153 [00:12<40:00,  2.09s/it]  1%|          | 6/1153 [00:13<34:12,  1.79s/it]  1%|          | 7/1153 [00:15<32:13,  1.69s/it]  1%|          | 8/1153 [00:16<30:44,  1.61s/it]  1%|          | 9/1153 [00:18<28:39,  1.50s/it]  1%|          | 10/1153 [00:19<27:16,  1.43s/it]  1%|          | 11/1153 [00:20<26:38,  1.40s/it]  1%|          | 12/1153 [00:21<24:25,  1.28s/it]  1%|          | 13/1153 [00:22<24:36,  1.30s/it]  1%|          | 14/1153 [00:24<25:24,  1.34s/it]  1%|▏         | 15/1153 [00:25<25:39,  1.35s/it]  1%|▏         | 16/1153 [00:27<25:15,  1.33s/it]  1%|▏         | 17/1153 [00:28<25:19,  1.34s/it]  2%|▏         | 18/1153 [00:33<46:03,  2.43s/it]  2%|▏         | 19/1153 [00:34<39:43,  2.10s/it]  2%|▏         | 20/1153 [00:36<36:04,  1.91s/it]  2%|▏         | 21/1153 [00:37<32:34,  1.73s/it]  2%|▏         | 22/1153 [00:38<30:04,  1.60s/it]  2%|▏         | 23/1153 [00:40<28:23,  1.51s/it]  2%|▏         | 24/1153 [00:41<27:24,  1.46s/it]  2%|▏         | 25/1153 [00:42<26:41,  1.42s/it]  2%|▏         | 26/1153 [00:44<26:29,  1.41s/it]  2%|▏         | 27/1153 [00:45<24:09,  1.29s/it]  2%|▏         | 28/1153 [00:46<24:20,  1.30s/it]  3%|▎         | 29/1153 [00:47<24:19,  1.30s/it]  3%|▎         | 30/1153 [00:49<24:40,  1.32s/it]  3%|▎         | 31/1153 [00:50<25:41,  1.37s/it]  3%|▎         | 32/1153 [00:52<25:36,  1.37s/it]  3%|▎         | 33/1153 [00:53<25:42,  1.38s/it]  3%|▎         | 34/1153 [00:58<44:22,  2.38s/it]  3%|▎         | 35/1153 [00:59<38:34,  2.07s/it]  3%|▎         | 36/1153 [01:00<34:14,  1.84s/it]  3%|▎         | 37/1153 [01:02<31:10,  1.68s/it]  3%|▎         | 38/1153 [01:03<29:07,  1.57s/it]  3%|▎         | 39/1153 [01:04<27:43,  1.49s/it]  3%|▎         | 40/1153 [01:05<25:51,  1.39s/it]  4%|▎         | 41/1153 [01:07<24:31,  1.32s/it]  4%|▎         | 42/1153 [01:08<23:49,  1.29s/it]  4%|▎         | 43/1153 [01:09<22:50,  1.23s/it]  4%|▍         | 44/1153 [01:10<22:03,  1.19s/it]  4%|▍         | 45/1153 [01:11<22:19,  1.21s/it]  4%|▍         | 46/1153 [01:12<22:05,  1.20s/it]  4%|▍         | 47/1153 [01:14<22:08,  1.20s/it]  4%|▍         | 48/1153 [01:15<22:16,  1.21s/it]  4%|▍         | 49/1153 [01:16<22:17,  1.21s/it]  4%|▍         | 50/1153 [01:21<43:51,  2.39s/it]  4%|▍         | 51/1153 [01:22<37:08,  2.02s/it]  5%|▍         | 52/1153 [01:24<32:29,  1.77s/it]  5%|▍         | 53/1153 [01:25<29:04,  1.59s/it]  5%|▍         | 54/1153 [01:26<26:49,  1.46s/it]  5%|▍         | 55/1153 [01:27<25:18,  1.38s/it]  5%|▍         | 56/1153 [01:28<23:49,  1.30s/it]  5%|▍         | 57/1153 [01:29<23:36,  1.29s/it]  5%|▌         | 58/1153 [01:31<22:57,  1.26s/it]  5%|▌         | 59/1153 [01:32<22:23,  1.23s/it]  5%|▌         | 60/1153 [01:33<22:29,  1.23s/it]  5%|▌         | 61/1153 [01:34<21:48,  1.20s/it]  5%|▌         | 62/1153 [01:35<22:06,  1.22s/it]  5%|▌         | 63/1153 [01:37<21:32,  1.19s/it]  6%|▌         | 64/1153 [01:38<21:19,  1.17s/it]  6%|▌         | 65/1153 [01:39<21:48,  1.20s/it]  6%|▌         | 66/1153 [01:40<21:32,  1.19s/it]  6%|▌         | 67/1153 [01:41<21:33,  1.19s/it]  6%|▌         | 68/1153 [01:46<40:43,  2.25s/it]  6%|▌         | 69/1153 [01:47<36:03,  2.00s/it]  6%|▌         | 70/1153 [01:48<29:42,  1.65s/it]  6%|▌         | 71/1153 [01:50<28:27,  1.58s/it]  6%|▌         | 72/1153 [01:51<26:57,  1.50s/it]  6%|▋         | 73/1153 [01:52<26:12,  1.46s/it]  6%|▋         | 74/1153 [01:53<24:37,  1.37s/it]  7%|▋         | 75/1153 [01:55<24:29,  1.36s/it]  7%|▋         | 76/1153 [01:56<24:08,  1.35s/it]  7%|▋         | 77/1153 [01:57<23:54,  1.33s/it]  7%|▋         | 78/1153 [01:59<23:50,  1.33s/it]  7%|▋         | 79/1153 [02:00<23:36,  1.32s/it]  7%|▋         | 80/1153 [02:01<24:01,  1.34s/it]  7%|▋         | 81/1153 [02:03<23:31,  1.32s/it]  7%|▋         | 82/1153 [02:04<23:19,  1.31s/it]  7%|▋         | 83/1153 [02:05<23:12,  1.30s/it]  7%|▋         | 84/1153 [02:07<23:33,  1.32s/it]  7%|▋         | 85/1153 [02:12<42:47,  2.40s/it]  7%|▋         | 86/1153 [02:13<37:04,  2.08s/it]  8%|▊         | 87/1153 [02:14<33:02,  1.86s/it]  8%|▊         | 88/1153 [02:16<30:09,  1.70s/it]  8%|▊         | 89/1153 [02:17<27:34,  1.56s/it]  8%|▊         | 90/1153 [02:18<26:26,  1.49s/it]  8%|▊         | 91/1153 [02:19<25:32,  1.44s/it]  8%|▊         | 92/1153 [02:21<24:46,  1.40s/it]  8%|▊         | 93/1153 [02:22<24:24,  1.38s/it]  8%|▊         | 94/1153 [02:23<23:57,  1.36s/it]  8%|▊         | 95/1153 [02:25<23:32,  1.34s/it]  8%|▊         | 96/1153 [02:26<23:50,  1.35s/it]  8%|▊         | 97/1153 [02:27<23:26,  1.33s/it]  8%|▊         | 98/1153 [02:29<23:12,  1.32s/it]  9%|▊         | 99/1153 [02:30<23:10,  1.32s/it]  9%|▊         | 100/1153 [02:35<40:23,  2.30s/it]  9%|▉         | 101/1153 [02:36<35:07,  2.00s/it]  9%|▉         | 102/1153 [02:37<31:23,  1.79s/it]  9%|▉         | 103/1153 [02:39<29:01,  1.66s/it]  9%|▉         | 104/1153 [02:40<27:08,  1.55s/it]  9%|▉         | 105/1153 [02:41<25:37,  1.47s/it]  9%|▉         | 106/1153 [02:42<24:47,  1.42s/it]  9%|▉         | 107/1153 [02:44<24:06,  1.38s/it]  9%|▉         | 108/1153 [02:45<23:34,  1.35s/it]  9%|▉         | 109/1153 [02:46<22:16,  1.28s/it] 10%|▉         | 110/1153 [02:47<22:15,  1.28s/it] 10%|▉         | 111/1153 [02:49<22:24,  1.29s/it] 10%|▉         | 112/1153 [02:50<22:27,  1.29s/it] 10%|▉         | 113/1153 [02:51<22:26,  1.29s/it] 10%|▉         | 114/1153 [02:53<22:27,  1.30s/it] 10%|▉         | 115/1153 [02:54<22:18,  1.29s/it] 10%|█         | 116/1153 [02:59<41:11,  2.38s/it] 10%|█         | 117/1153 [03:00<35:09,  2.04s/it] 10%|█         | 118/1153 [03:01<31:29,  1.83s/it] 10%|█         | 119/1153 [03:03<28:37,  1.66s/it] 10%|█         | 120/1153 [03:04<26:44,  1.55s/it] 10%|█         | 121/1153 [03:05<25:25,  1.48s/it] 11%|█         | 122/1153 [03:06<23:32,  1.37s/it] 11%|█         | 123/1153 [03:08<23:14,  1.35s/it] 11%|█         | 124/1153 [03:09<23:03,  1.34s/it] 11%|█         | 125/1153 [03:10<22:42,  1.33s/it] 11%|█         | 126/1153 [03:11<22:00,  1.29s/it] 11%|█         | 127/1153 [03:13<21:03,  1.23s/it] 11%|█         | 128/1153 [03:14<20:36,  1.21s/it] 11%|█         | 129/1153 [03:15<21:09,  1.24s/it] 11%|█▏        | 130/1153 [03:16<21:24,  1.26s/it] 11%|█▏        | 131/1153 [03:18<21:40,  1.27s/it] 11%|█▏        | 132/1153 [03:22<39:15,  2.31s/it] 12%|█▏        | 133/1153 [03:23<33:05,  1.95s/it] 12%|█▏        | 134/1153 [03:25<28:54,  1.70s/it] 12%|█▏        | 135/1153 [03:26<26:01,  1.53s/it] 12%|█▏        | 136/1153 [03:27<23:49,  1.41s/it] 12%|█▏        | 137/1153 [03:28<22:32,  1.33s/it] 12%|█▏        | 138/1153 [03:29<21:54,  1.30s/it] 12%|█▏        | 139/1153 [03:30<21:01,  1.24s/it] 12%|█▏        | 140/1153 [03:32<20:34,  1.22s/it] 12%|█▏        | 141/1153 [03:33<20:14,  1.20s/it] 12%|█▏        | 142/1153 [03:34<19:41,  1.17s/it] 12%|█▏        | 143/1153 [03:35<20:16,  1.20s/it] 12%|█▏        | 144/1153 [03:36<20:38,  1.23s/it] 13%|█▎        | 145/1153 [03:38<20:52,  1.24s/it] 13%|█▎        | 146/1153 [03:39<20:11,  1.20s/it] 13%|█▎        | 147/1153 [03:40<19:57,  1.19s/it] 13%|█▎        | 148/1153 [03:41<19:36,  1.17s/it] 13%|█▎        | 149/1153 [03:42<19:19,  1.15s/it] 13%|█▎        | 150/1153 [03:43<19:14,  1.15s/it] 13%|█▎        | 151/1153 [03:47<34:13,  2.05s/it] 13%|█▎        | 152/1153 [03:49<30:14,  1.81s/it] 13%|█▎        | 153/1153 [03:50<26:46,  1.61s/it] 13%|█▎        | 154/1153 [03:51<24:19,  1.46s/it] 13%|█▎        | 155/1153 [03:52<22:40,  1.36s/it] 14%|█▎        | 156/1153 [03:53<22:11,  1.34s/it] 14%|█▎        | 157/1153 [03:55<21:55,  1.32s/it] 14%|█▎        | 158/1153 [03:56<21:43,  1.31s/it] 14%|█▍        | 159/1153 [03:57<21:45,  1.31s/it] 14%|█▍        | 160/1153 [03:59<21:45,  1.31s/it] 14%|█▍        | 161/1153 [04:00<21:44,  1.31s/it] 14%|█▍        | 162/1153 [04:01<20:48,  1.26s/it] 14%|█▍        | 163/1153 [04:02<21:13,  1.29s/it] 14%|█▍        | 164/1153 [04:03<20:25,  1.24s/it] 14%|█▍        | 165/1153 [04:05<19:48,  1.20s/it] 14%|█▍        | 166/1153 [04:06<19:24,  1.18s/it] 14%|█▍        | 167/1153 [04:07<19:18,  1.18s/it] 15%|█▍        | 168/1153 [04:11<33:11,  2.02s/it] 15%|█▍        | 169/1153 [04:12<28:57,  1.77s/it] 15%|█▍        | 170/1153 [04:13<25:49,  1.58s/it] 15%|█▍        | 171/1153 [04:14<23:44,  1.45s/it] 15%|█▍        | 172/1153 [04:15<22:05,  1.35s/it] 15%|█▌        | 173/1153 [04:17<21:01,  1.29s/it] 15%|█▌        | 174/1153 [04:18<20:09,  1.24s/it] 15%|█▌        | 175/1153 [04:19<19:31,  1.20s/it] 15%|█▌        | 176/1153 [04:20<19:08,  1.18s/it] 15%|█▌        | 177/1153 [04:21<18:58,  1.17s/it] 15%|█▌        | 178/1153 [04:22<18:55,  1.16s/it] 16%|█▌        | 179/1153 [04:23<18:59,  1.17s/it] 16%|█▌        | 180/1153 [04:25<18:39,  1.15s/it] 16%|█▌        | 181/1153 [04:26<18:40,  1.15s/it] 16%|█▌        | 182/1153 [04:27<18:31,  1.14s/it] 16%|█▌        | 183/1153 [04:28<18:37,  1.15s/it] 16%|█▌        | 184/1153 [04:29<18:34,  1.15s/it] 16%|█▌        | 185/1153 [04:30<18:21,  1.14s/it] 16%|█▌        | 186/1153 [04:34<31:26,  1.95s/it] 16%|█▌        | 187/1153 [04:35<27:59,  1.74s/it] 16%|█▋        | 188/1153 [04:36<25:12,  1.57s/it] 16%|█▋        | 189/1153 [04:38<23:09,  1.44s/it] 16%|█▋        | 190/1153 [04:39<21:32,  1.34s/it] 17%|█▋        | 191/1153 [04:40<21:23,  1.33s/it] 17%|█▋        | 192/1153 [04:41<21:17,  1.33s/it] 17%|█▋        | 193/1153 [04:43<21:12,  1.33s/it] 17%|█▋        | 194/1153 [04:44<21:07,  1.32s/it] 17%|█▋        | 195/1153 [04:45<20:58,  1.31s/it] 17%|█▋        | 196/1153 [04:47<20:55,  1.31s/it] 17%|█▋        | 197/1153 [04:48<20:51,  1.31s/it] 17%|█▋        | 198/1153 [04:49<20:49,  1.31s/it] 17%|█▋        | 199/1153 [04:51<20:54,  1.31s/it] 17%|█▋        | 200/1153 [04:52<20:54,  1.32s/it] 17%|█▋        | 201/1153 [04:53<20:48,  1.31s/it] 18%|█▊        | 202/1153 [04:55<21:10,  1.34s/it] 18%|█▊        | 203/1153 [04:58<32:40,  2.06s/it] 18%|█▊        | 204/1153 [05:00<29:13,  1.85s/it] 18%|█▊        | 205/1153 [05:01<26:49,  1.70s/it] 18%|█▊        | 206/1153 [05:02<25:01,  1.59s/it] 18%|█▊        | 207/1153 [05:04<24:04,  1.53s/it] 18%|█▊        | 208/1153 [05:05<22:58,  1.46s/it] 18%|█▊        | 209/1153 [05:06<22:16,  1.42s/it] 18%|█▊        | 210/1153 [05:07<20:27,  1.30s/it] 18%|█▊        | 211/1153 [05:09<20:32,  1.31s/it] 18%|█▊        | 212/1153 [05:10<18:16,  1.17s/it] 18%|█▊        | 213/1153 [05:11<18:56,  1.21s/it] 19%|█▊        | 214/1153 [05:12<19:48,  1.27s/it] 19%|█▊        | 215/1153 [05:14<20:00,  1.28s/it] 19%|█▊        | 216/1153 [05:15<20:21,  1.30s/it] 19%|█▉        | 217/1153 [05:16<20:16,  1.30s/it] 19%|█▉        | 218/1153 [05:17<19:51,  1.27s/it] 19%|█▉        | 219/1153 [05:21<32:19,  2.08s/it] 19%|█▉        | 220/1153 [05:23<28:54,  1.86s/it] 19%|█▉        | 221/1153 [05:24<26:12,  1.69s/it] 19%|█▉        | 222/1153 [05:25<24:23,  1.57s/it] 19%|█▉        | 223/1153 [05:27<23:05,  1.49s/it] 19%|█▉        | 224/1153 [05:28<22:11,  1.43s/it] 20%|█▉        | 225/1153 [05:29<21:37,  1.40s/it] 20%|█▉        | 226/1153 [05:31<21:14,  1.37s/it] 20%|█▉        | 227/1153 [05:32<20:48,  1.35s/it] 20%|█▉        | 228/1153 [05:33<20:29,  1.33s/it] 20%|█▉        | 229/1153 [05:34<19:35,  1.27s/it] 20%|█▉        | 230/1153 [05:36<19:41,  1.28s/it] 20%|██        | 231/1153 [05:37<19:01,  1.24s/it] 20%|██        | 232/1153 [05:38<19:16,  1.26s/it] 20%|██        | 233/1153 [05:39<19:19,  1.26s/it] 20%|██        | 234/1153 [05:41<19:35,  1.28s/it] 20%|██        | 235/1153 [05:42<19:40,  1.29s/it] 20%|██        | 236/1153 [05:46<32:21,  2.12s/it] 21%|██        | 237/1153 [05:47<29:08,  1.91s/it] 21%|██        | 238/1153 [05:49<26:58,  1.77s/it] 21%|██        | 239/1153 [05:50<24:50,  1.63s/it] 21%|██        | 240/1153 [05:52<23:42,  1.56s/it] 21%|██        | 241/1153 [05:53<22:01,  1.45s/it] 21%|██        | 242/1153 [05:54<22:12,  1.46s/it] 21%|██        | 243/1153 [05:56<21:37,  1.43s/it] 21%|██        | 244/1153 [05:57<21:11,  1.40s/it] 21%|██        | 245/1153 [05:58<19:18,  1.28s/it] 21%|██▏       | 246/1153 [05:59<18:28,  1.22s/it] 21%|██▏       | 247/1153 [06:00<18:54,  1.25s/it] 22%|██▏       | 248/1153 [06:02<19:27,  1.29s/it] 22%|██▏       | 249/1153 [06:03<19:47,  1.31s/it] 22%|██▏       | 250/1153 [06:04<19:54,  1.32s/it] 22%|██▏       | 251/1153 [06:06<19:46,  1.32s/it] 22%|██▏       | 252/1153 [06:09<30:40,  2.04s/it] 22%|██▏       | 253/1153 [06:11<27:56,  1.86s/it] 22%|██▏       | 254/1153 [06:12<25:27,  1.70s/it] 22%|██▏       | 255/1153 [06:13<23:06,  1.54s/it] 22%|██▏       | 256/1153 [06:15<22:06,  1.48s/it] 22%|██▏       | 257/1153 [06:16<21:25,  1.43s/it] 22%|██▏       | 258/1153 [06:17<20:19,  1.36s/it] 22%|██▏       | 259/1153 [06:18<19:51,  1.33s/it] 23%|██▎       | 260/1153 [06:20<19:51,  1.33s/it] 23%|██▎       | 261/1153 [06:21<19:55,  1.34s/it] 23%|██▎       | 262/1153 [06:22<19:00,  1.28s/it] 23%|██▎       | 263/1153 [06:24<18:40,  1.26s/it] 23%|██▎       | 264/1153 [06:25<18:47,  1.27s/it] 23%|██▎       | 265/1153 [06:26<19:11,  1.30s/it] 23%|██▎       | 266/1153 [06:27<18:35,  1.26s/it] 23%|██▎       | 267/1153 [06:29<18:32,  1.26s/it] 23%|██▎       | 268/1153 [06:30<18:47,  1.27s/it] 23%|██▎       | 269/1153 [06:31<18:15,  1.24s/it] 23%|██▎       | 270/1153 [06:36<33:26,  2.27s/it] 24%|██▎       | 271/1153 [06:37<28:34,  1.94s/it] 24%|██▎       | 272/1153 [06:38<25:47,  1.76s/it] 24%|██▎       | 273/1153 [06:39<23:22,  1.59s/it] 24%|██▍       | 274/1153 [06:41<21:37,  1.48s/it] 24%|██▍       | 275/1153 [06:42<20:27,  1.40s/it] 24%|██▍       | 276/1153 [06:43<19:29,  1.33s/it] 24%|██▍       | 277/1153 [06:44<18:59,  1.30s/it] 24%|██▍       | 278/1153 [06:45<18:20,  1.26s/it] 24%|██▍       | 279/1153 [06:47<17:41,  1.21s/it] 24%|██▍       | 280/1153 [06:48<17:22,  1.19s/it] 24%|██▍       | 281/1153 [06:49<17:02,  1.17s/it] 24%|██▍       | 282/1153 [06:50<16:59,  1.17s/it] 25%|██▍       | 283/1153 [06:51<16:58,  1.17s/it] 25%|██▍       | 284/1153 [06:53<17:46,  1.23s/it] 25%|██▍       | 285/1153 [06:54<18:08,  1.25s/it] 25%|██▍       | 286/1153 [06:55<18:22,  1.27s/it] 25%|██▍       | 287/1153 [06:56<18:36,  1.29s/it] 25%|██▍       | 288/1153 [07:01<32:38,  2.26s/it] 25%|██▌       | 289/1153 [07:02<28:47,  2.00s/it] 25%|██▌       | 290/1153 [07:04<25:48,  1.79s/it] 25%|██▌       | 291/1153 [07:05<22:56,  1.60s/it] 25%|██▌       | 292/1153 [07:06<21:00,  1.46s/it] 25%|██▌       | 293/1153 [07:07<19:40,  1.37s/it] 25%|██▌       | 294/1153 [07:08<18:36,  1.30s/it] 26%|██▌       | 295/1153 [07:09<18:01,  1.26s/it] 26%|██▌       | 296/1153 [07:11<17:54,  1.25s/it] 26%|██▌       | 297/1153 [07:12<17:47,  1.25s/it] 26%|██▌       | 298/1153 [07:13<17:38,  1.24s/it] 26%|██▌       | 299/1153 [07:14<17:13,  1.21s/it] 26%|██▌       | 300/1153 [07:15<16:55,  1.19s/it] 26%|██▌       | 301/1153 [07:17<16:49,  1.18s/it] 26%|██▌       | 302/1153 [07:18<16:36,  1.17s/it] 26%|██▋       | 303/1153 [07:19<16:32,  1.17s/it] 26%|██▋       | 304/1153 [07:20<16:31,  1.17s/it] 26%|██▋       | 305/1153 [07:21<16:45,  1.19s/it] 27%|██▋       | 306/1153 [07:23<16:47,  1.19s/it] 27%|██▋       | 307/1153 [07:26<28:06,  1.99s/it] 27%|██▋       | 308/1153 [07:28<24:40,  1.75s/it] 27%|██▋       | 309/1153 [07:29<22:10,  1.58s/it] 27%|██▋       | 310/1153 [07:30<20:41,  1.47s/it] 27%|██▋       | 311/1153 [07:31<20:02,  1.43s/it] 27%|██▋       | 312/1153 [07:33<19:14,  1.37s/it] 27%|██▋       | 313/1153 [07:34<18:17,  1.31s/it] 27%|██▋       | 314/1153 [07:35<17:45,  1.27s/it] 27%|██▋       | 315/1153 [07:36<17:19,  1.24s/it] 27%|██▋       | 316/1153 [07:37<17:09,  1.23s/it] 27%|██▋       | 317/1153 [07:38<16:56,  1.22s/it] 28%|██▊       | 318/1153 [07:40<16:38,  1.20s/it] 28%|██▊       | 319/1153 [07:41<17:10,  1.24s/it] 28%|██▊       | 320/1153 [07:42<17:36,  1.27s/it] 28%|██▊       | 321/1153 [07:44<17:32,  1.27s/it] 28%|██▊       | 322/1153 [07:45<17:53,  1.29s/it] 28%|██▊       | 323/1153 [07:46<18:07,  1.31s/it] 28%|██▊       | 324/1153 [07:48<18:23,  1.33s/it] 28%|██▊       | 325/1153 [07:52<29:50,  2.16s/it] 28%|██▊       | 326/1153 [07:53<26:34,  1.93s/it] 28%|██▊       | 327/1153 [07:54<23:59,  1.74s/it] 28%|██▊       | 328/1153 [07:56<22:29,  1.64s/it] 29%|██▊       | 329/1153 [07:57<21:16,  1.55s/it] 29%|██▊       | 330/1153 [07:58<20:27,  1.49s/it] 29%|██▊       | 331/1153 [08:00<19:53,  1.45s/it] 29%|██▉       | 332/1153 [08:01<19:37,  1.43s/it] 29%|██▉       | 333/1153 [08:03<19:11,  1.40s/it] 29%|██▉       | 334/1153 [08:04<18:54,  1.39s/it] 29%|██▉       | 335/1153 [08:05<18:46,  1.38s/it] 29%|██▉       | 336/1153 [08:07<18:47,  1.38s/it] 29%|██▉       | 337/1153 [08:08<18:59,  1.40s/it] 29%|██▉       | 338/1153 [08:09<18:55,  1.39s/it] 29%|██▉       | 339/1153 [08:11<18:42,  1.38s/it] 29%|██▉       | 340/1153 [08:12<18:32,  1.37s/it] 30%|██▉       | 341/1153 [08:14<18:32,  1.37s/it] 30%|██▉       | 342/1153 [08:17<28:34,  2.11s/it] 30%|██▉       | 343/1153 [08:19<25:38,  1.90s/it] 30%|██▉       | 344/1153 [08:20<23:20,  1.73s/it] 30%|██▉       | 345/1153 [08:21<21:44,  1.62s/it] 30%|███       | 346/1153 [08:23<20:41,  1.54s/it] 30%|███       | 347/1153 [08:24<20:06,  1.50s/it] 30%|███       | 348/1153 [08:26<19:31,  1.46s/it] 30%|███       | 349/1153 [08:27<19:19,  1.44s/it] 30%|███       | 350/1153 [08:28<18:57,  1.42s/it] 30%|███       | 351/1153 [08:30<18:40,  1.40s/it] 31%|███       | 352/1153 [08:31<18:30,  1.39s/it] 31%|███       | 353/1153 [08:32<18:38,  1.40s/it] 31%|███       | 354/1153 [08:34<18:02,  1.35s/it] 31%|███       | 355/1153 [08:35<18:47,  1.41s/it] 31%|███       | 356/1153 [08:37<18:33,  1.40s/it] 31%|███       | 357/1153 [08:38<18:21,  1.38s/it] 31%|███       | 358/1153 [08:42<29:42,  2.24s/it] 31%|███       | 359/1153 [08:44<26:11,  1.98s/it] 31%|███       | 360/1153 [08:45<23:56,  1.81s/it] 31%|███▏      | 361/1153 [08:46<22:14,  1.68s/it] 31%|███▏      | 362/1153 [08:48<21:02,  1.60s/it] 31%|███▏      | 363/1153 [08:49<20:05,  1.53s/it] 32%|███▏      | 364/1153 [08:51<19:26,  1.48s/it] 32%|███▏      | 365/1153 [08:52<18:32,  1.41s/it] 32%|███▏      | 366/1153 [08:53<18:27,  1.41s/it] 32%|███▏      | 367/1153 [08:55<18:12,  1.39s/it] 32%|███▏      | 368/1153 [08:56<18:04,  1.38s/it] 32%|███▏      | 369/1153 [08:57<15:23,  1.18s/it] 32%|███▏      | 370/1153 [08:58<16:01,  1.23s/it] 32%|███▏      | 371/1153 [08:59<16:23,  1.26s/it] 32%|███▏      | 372/1153 [09:01<16:35,  1.27s/it] 32%|███▏      | 373/1153 [09:02<16:45,  1.29s/it] 32%|███▏      | 374/1153 [09:03<17:20,  1.34s/it] 33%|███▎      | 375/1153 [09:08<29:23,  2.27s/it] 33%|███▎      | 376/1153 [09:09<25:54,  2.00s/it] 33%|███▎      | 377/1153 [09:11<23:18,  1.80s/it] 33%|███▎      | 378/1153 [09:12<21:45,  1.69s/it] 33%|███▎      | 379/1153 [09:13<20:24,  1.58s/it] 33%|███▎      | 380/1153 [09:15<19:45,  1.53s/it] 33%|███▎      | 381/1153 [09:16<19:08,  1.49s/it] 33%|███▎      | 382/1153 [09:18<19:02,  1.48s/it] 33%|███▎      | 383/1153 [09:19<18:28,  1.44s/it] 33%|███▎      | 384/1153 [09:20<18:05,  1.41s/it] 33%|███▎      | 385/1153 [09:22<17:53,  1.40s/it] 33%|███▎      | 386/1153 [09:23<17:42,  1.38s/it] 34%|███▎      | 387/1153 [09:24<17:38,  1.38s/it] 34%|███▎      | 388/1153 [09:26<17:27,  1.37s/it] 34%|███▎      | 389/1153 [09:27<17:23,  1.37s/it] 34%|███▍      | 390/1153 [09:28<17:33,  1.38s/it] 34%|███▍      | 391/1153 [09:30<17:27,  1.38s/it] 34%|███▍      | 392/1153 [09:34<28:11,  2.22s/it] 34%|███▍      | 393/1153 [09:35<25:03,  1.98s/it] 34%|███▍      | 394/1153 [09:37<22:53,  1.81s/it] 34%|███▍      | 395/1153 [09:38<20:16,  1.60s/it] 34%|███▍      | 396/1153 [09:39<19:32,  1.55s/it] 34%|███▍      | 397/1153 [09:41<18:19,  1.45s/it] 35%|███▍      | 398/1153 [09:42<17:58,  1.43s/it] 35%|███▍      | 399/1153 [09:43<17:46,  1.41s/it] 35%|███▍      | 400/1153 [09:45<17:28,  1.39s/it] 35%|███▍      | 401/1153 [09:46<17:17,  1.38s/it] 35%|███▍      | 402/1153 [09:47<17:11,  1.37s/it] 35%|███▍      | 403/1153 [09:49<16:26,  1.32s/it] 35%|███▌      | 404/1153 [09:50<16:08,  1.29s/it] 35%|███▌      | 405/1153 [09:51<15:42,  1.26s/it] 35%|███▌      | 406/1153 [09:52<16:16,  1.31s/it] 35%|███▌      | 407/1153 [09:54<16:28,  1.33s/it] 35%|███▌      | 408/1153 [09:55<15:55,  1.28s/it] 35%|███▌      | 409/1153 [09:56<15:36,  1.26s/it] 36%|███▌      | 410/1153 [10:00<26:17,  2.12s/it] 36%|███▌      | 411/1153 [10:02<23:37,  1.91s/it] 36%|███▌      | 412/1153 [10:03<20:53,  1.69s/it] 36%|███▌      | 413/1153 [10:04<18:49,  1.53s/it] 36%|███▌      | 414/1153 [10:05<17:19,  1.41s/it] 36%|███▌      | 415/1153 [10:06<16:18,  1.33s/it] 36%|███▌      | 416/1153 [10:07<15:31,  1.26s/it] 36%|███▌      | 417/1153 [10:09<15:16,  1.25s/it] 36%|███▋      | 418/1153 [10:10<14:52,  1.21s/it] 36%|███▋      | 419/1153 [10:11<15:09,  1.24s/it] 36%|███▋      | 420/1153 [10:12<14:35,  1.19s/it] 37%|███▋      | 421/1153 [10:13<14:27,  1.18s/it] 37%|███▋      | 422/1153 [10:14<14:18,  1.17s/it] 37%|███▋      | 423/1153 [10:16<14:51,  1.22s/it] 37%|███▋      | 424/1153 [10:17<14:38,  1.20s/it] 37%|███▋      | 425/1153 [10:18<14:41,  1.21s/it] 37%|███▋      | 426/1153 [10:19<14:37,  1.21s/it] 37%|███▋      | 427/1153 [10:21<15:12,  1.26s/it] 37%|███▋      | 428/1153 [10:24<23:31,  1.95s/it] 37%|███▋      | 429/1153 [10:25<20:32,  1.70s/it] 37%|███▋      | 430/1153 [10:27<18:25,  1.53s/it] 37%|███▋      | 431/1153 [10:28<17:31,  1.46s/it] 37%|███▋      | 432/1153 [10:29<16:53,  1.41s/it] 38%|███▊      | 433/1153 [10:30<16:33,  1.38s/it] 38%|███▊      | 434/1153 [10:32<15:58,  1.33s/it] 38%|███▊      | 435/1153 [10:33<15:51,  1.33s/it] 38%|███▊      | 436/1153 [10:34<15:07,  1.27s/it] 38%|███▊      | 437/1153 [10:35<14:37,  1.23s/it] 38%|███▊      | 438/1153 [10:36<14:17,  1.20s/it] 38%|███▊      | 439/1153 [10:38<14:39,  1.23s/it] 38%|███▊      | 440/1153 [10:39<14:20,  1.21s/it] 38%|███▊      | 441/1153 [10:40<14:52,  1.25s/it] 38%|███▊      | 442/1153 [10:41<14:25,  1.22s/it] 38%|███▊      | 443/1153 [10:42<14:00,  1.18s/it] 39%|███▊      | 444/1153 [10:44<13:53,  1.18s/it] 39%|███▊      | 445/1153 [10:45<14:22,  1.22s/it] 39%|███▊      | 446/1153 [10:49<23:28,  1.99s/it] 39%|███▉      | 447/1153 [10:50<20:24,  1.73s/it] 39%|███▉      | 448/1153 [10:51<18:16,  1.55s/it] 39%|███▉      | 449/1153 [10:52<16:47,  1.43s/it] 39%|███▉      | 450/1153 [10:53<15:49,  1.35s/it] 39%|███▉      | 451/1153 [10:54<15:04,  1.29s/it] 39%|███▉      | 452/1153 [10:56<14:28,  1.24s/it] 39%|███▉      | 453/1153 [10:57<14:40,  1.26s/it] 39%|███▉      | 454/1153 [10:58<14:40,  1.26s/it] 39%|███▉      | 455/1153 [10:59<14:16,  1.23s/it] 40%|███▉      | 456/1153 [11:00<13:54,  1.20s/it] 40%|███▉      | 457/1153 [11:02<13:39,  1.18s/it] 40%|███▉      | 458/1153 [11:03<13:28,  1.16s/it] 40%|███▉      | 459/1153 [11:04<13:23,  1.16s/it] 40%|███▉      | 460/1153 [11:05<13:25,  1.16s/it] 40%|███▉      | 461/1153 [11:06<13:14,  1.15s/it] 40%|████      | 462/1153 [11:07<13:12,  1.15s/it] 40%|████      | 463/1153 [11:08<13:06,  1.14s/it] 40%|████      | 464/1153 [11:10<13:16,  1.16s/it] 40%|████      | 465/1153 [11:14<22:47,  1.99s/it] 40%|████      | 466/1153 [11:15<20:20,  1.78s/it] 41%|████      | 467/1153 [11:16<17:41,  1.55s/it] 41%|████      | 468/1153 [11:17<16:46,  1.47s/it] 41%|████      | 469/1153 [11:18<16:06,  1.41s/it] 41%|████      | 470/1153 [11:20<15:40,  1.38s/it] 41%|████      | 471/1153 [11:21<15:29,  1.36s/it] 41%|████      | 472/1153 [11:22<14:41,  1.29s/it] 41%|████      | 473/1153 [11:23<14:17,  1.26s/it] 41%|████      | 474/1153 [11:25<14:17,  1.26s/it] 41%|████      | 475/1153 [11:26<14:20,  1.27s/it] 41%|████▏     | 476/1153 [11:27<14:21,  1.27s/it] 41%|████▏     | 477/1153 [11:28<14:21,  1.28s/it] 41%|████▏     | 478/1153 [11:30<14:23,  1.28s/it] 42%|████▏     | 479/1153 [11:31<14:25,  1.28s/it] 42%|████▏     | 480/1153 [11:32<14:23,  1.28s/it] 42%|████▏     | 481/1153 [11:33<14:02,  1.25s/it] 42%|████▏     | 482/1153 [11:35<14:13,  1.27s/it] 42%|████▏     | 483/1153 [11:39<22:49,  2.04s/it] 42%|████▏     | 484/1153 [11:40<20:17,  1.82s/it] 42%|████▏     | 485/1153 [11:41<17:18,  1.55s/it] 42%|████▏     | 486/1153 [11:42<15:41,  1.41s/it] 42%|████▏     | 487/1153 [11:43<15:11,  1.37s/it] 42%|████▏     | 488/1153 [11:44<14:48,  1.34s/it] 42%|████▏     | 489/1153 [11:46<14:36,  1.32s/it] 42%|████▏     | 490/1153 [11:47<14:30,  1.31s/it] 43%|████▎     | 491/1153 [11:48<14:34,  1.32s/it] 43%|████▎     | 492/1153 [11:50<14:34,  1.32s/it] 43%|████▎     | 493/1153 [11:51<14:32,  1.32s/it] 43%|████▎     | 494/1153 [11:52<14:19,  1.30s/it] 43%|████▎     | 495/1153 [11:54<14:24,  1.31s/it] 43%|████▎     | 496/1153 [11:55<14:32,  1.33s/it] 43%|████▎     | 497/1153 [11:56<14:37,  1.34s/it] 43%|████▎     | 498/1153 [11:58<14:36,  1.34s/it] 43%|████▎     | 499/1153 [11:59<14:35,  1.34s/it] 43%|████▎     | 500/1153 [12:03<23:50,  2.19s/it] 43%|████▎     | 501/1153 [12:05<21:57,  2.02s/it] 44%|████▎     | 502/1153 [12:06<19:48,  1.83s/it] 44%|████▎     | 503/1153 [12:08<18:27,  1.70s/it] 44%|████▎     | 504/1153 [12:08<15:28,  1.43s/it] 44%|████▍     | 505/1153 [12:10<15:07,  1.40s/it] 44%|████▍     | 506/1153 [12:11<15:16,  1.42s/it] 44%|████▍     | 507/1153 [12:13<15:10,  1.41s/it] 44%|████▍     | 508/1153 [12:14<14:08,  1.32s/it] 44%|████▍     | 509/1153 [12:14<11:54,  1.11s/it] 44%|████▍     | 510/1153 [12:16<13:08,  1.23s/it] 44%|████▍     | 511/1153 [12:17<11:32,  1.08s/it] 44%|████▍     | 512/1153 [12:18<12:28,  1.17s/it] 44%|████▍     | 513/1153 [12:19<11:17,  1.06s/it] 45%|████▍     | 514/1153 [12:20<10:26,  1.02it/s] 45%|████▍     | 515/1153 [12:21<11:30,  1.08s/it] 45%|████▍     | 516/1153 [12:22<12:23,  1.17s/it] 45%|████▍     | 517/1153 [12:24<12:46,  1.20s/it] 45%|████▍     | 518/1153 [12:25<13:21,  1.26s/it] 45%|████▌     | 519/1153 [12:30<24:45,  2.34s/it] 45%|████▌     | 520/1153 [12:31<21:46,  2.06s/it] 45%|████▌     | 521/1153 [12:33<19:33,  1.86s/it] 45%|████▌     | 522/1153 [12:34<17:47,  1.69s/it] 45%|████▌     | 523/1153 [12:35<16:39,  1.59s/it] 45%|████▌     | 524/1153 [12:37<15:46,  1.50s/it] 46%|████▌     | 525/1153 [12:38<15:09,  1.45s/it] 46%|████▌     | 526/1153 [12:39<14:45,  1.41s/it] 46%|████▌     | 527/1153 [12:41<14:39,  1.40s/it] 46%|████▌     | 528/1153 [12:42<14:24,  1.38s/it] 46%|████▌     | 529/1153 [12:43<14:15,  1.37s/it] 46%|████▌     | 530/1153 [12:45<14:31,  1.40s/it] 46%|████▌     | 531/1153 [12:46<14:22,  1.39s/it] 46%|████▌     | 532/1153 [12:47<14:22,  1.39s/it] 46%|████▌     | 533/1153 [12:49<13:46,  1.33s/it] 46%|████▋     | 534/1153 [12:50<13:16,  1.29s/it] 46%|████▋     | 535/1153 [12:51<13:25,  1.30s/it] 46%|████▋     | 536/1153 [12:52<12:44,  1.24s/it] 47%|████▋     | 537/1153 [12:57<23:08,  2.25s/it] 47%|████▋     | 538/1153 [12:58<20:04,  1.96s/it] 47%|████▋     | 539/1153 [12:59<17:36,  1.72s/it] 47%|████▋     | 540/1153 [13:01<16:22,  1.60s/it] 47%|████▋     | 541/1153 [13:02<15:31,  1.52s/it] 47%|████▋     | 542/1153 [13:03<14:46,  1.45s/it] 47%|████▋     | 543/1153 [13:04<13:56,  1.37s/it] 47%|████▋     | 544/1153 [13:06<13:21,  1.32s/it] 47%|████▋     | 545/1153 [13:07<12:58,  1.28s/it] 47%|████▋     | 546/1153 [13:08<12:38,  1.25s/it] 47%|████▋     | 547/1153 [13:09<12:45,  1.26s/it] 48%|████▊     | 548/1153 [13:11<12:35,  1.25s/it] 48%|████▊     | 549/1153 [13:12<12:33,  1.25s/it] 48%|████▊     | 550/1153 [13:13<12:47,  1.27s/it] 48%|████▊     | 551/1153 [13:14<12:32,  1.25s/it] 48%|████▊     | 552/1153 [13:16<12:25,  1.24s/it] 48%|████▊     | 553/1153 [13:17<12:20,  1.23s/it] 48%|████▊     | 554/1153 [13:18<12:15,  1.23s/it] 48%|████▊     | 555/1153 [13:19<12:11,  1.22s/it] 48%|████▊     | 556/1153 [13:23<20:51,  2.10s/it] 48%|████▊     | 557/1153 [13:24<18:07,  1.82s/it] 48%|████▊     | 558/1153 [13:26<16:11,  1.63s/it] 48%|████▊     | 559/1153 [13:27<15:02,  1.52s/it] 49%|████▊     | 560/1153 [13:28<14:09,  1.43s/it] 49%|████▊     | 561/1153 [13:29<13:28,  1.37s/it] 49%|████▊     | 562/1153 [13:31<13:35,  1.38s/it] 49%|████▉     | 563/1153 [13:32<13:02,  1.33s/it] 49%|████▉     | 564/1153 [13:33<12:57,  1.32s/it] 49%|████▉     | 565/1153 [13:34<12:33,  1.28s/it] 49%|████▉     | 566/1153 [13:36<12:40,  1.30s/it] 49%|████▉     | 567/1153 [13:37<12:16,  1.26s/it] 49%|████▉     | 568/1153 [13:38<11:54,  1.22s/it] 49%|████▉     | 569/1153 [13:39<11:38,  1.20s/it] 49%|████▉     | 570/1153 [13:40<11:32,  1.19s/it] 50%|████▉     | 571/1153 [13:42<11:22,  1.17s/it] 50%|████▉     | 572/1153 [13:43<11:09,  1.15s/it] 50%|████▉     | 573/1153 [13:44<11:07,  1.15s/it] 50%|████▉     | 574/1153 [13:45<11:03,  1.15s/it] 50%|████▉     | 575/1153 [13:46<10:59,  1.14s/it] 50%|████▉     | 576/1153 [13:50<19:05,  1.98s/it] 50%|█████     | 577/1153 [13:51<16:40,  1.74s/it] 50%|█████     | 578/1153 [13:52<14:48,  1.54s/it] 50%|█████     | 579/1153 [13:53<13:39,  1.43s/it] 50%|█████     | 580/1153 [13:55<12:49,  1.34s/it] 50%|█████     | 581/1153 [13:56<12:17,  1.29s/it] 50%|█████     | 582/1153 [13:57<11:50,  1.25s/it] 51%|█████     | 583/1153 [13:58<11:29,  1.21s/it] 51%|█████     | 584/1153 [13:59<11:18,  1.19s/it] 51%|█████     | 585/1153 [14:00<11:10,  1.18s/it] 51%|█████     | 586/1153 [14:01<11:00,  1.16s/it] 51%|█████     | 587/1153 [14:03<10:57,  1.16s/it] 51%|█████     | 588/1153 [14:04<10:52,  1.15s/it] 51%|█████     | 589/1153 [14:05<11:17,  1.20s/it] 51%|█████     | 590/1153 [14:06<10:59,  1.17s/it] 51%|█████▏    | 591/1153 [14:07<11:23,  1.22s/it] 51%|█████▏    | 592/1153 [14:09<11:13,  1.20s/it] 51%|█████▏    | 593/1153 [14:10<11:29,  1.23s/it] 52%|█████▏    | 594/1153 [14:11<11:41,  1.26s/it] 52%|█████▏    | 595/1153 [14:13<12:00,  1.29s/it] 52%|█████▏    | 596/1153 [14:16<18:54,  2.04s/it] 52%|█████▏    | 597/1153 [14:18<16:49,  1.82s/it] 52%|█████▏    | 598/1153 [14:19<15:18,  1.66s/it] 52%|█████▏    | 599/1153 [14:20<13:42,  1.49s/it] 52%|█████▏    | 600/1153 [14:21<13:11,  1.43s/it] 52%|█████▏    | 601/1153 [14:23<12:55,  1.40s/it] 52%|█████▏    | 602/1153 [14:24<12:40,  1.38s/it] 52%|█████▏    | 603/1153 [14:25<12:28,  1.36s/it] 52%|█████▏    | 604/1153 [14:27<12:13,  1.34s/it] 52%|█████▏    | 605/1153 [14:28<12:03,  1.32s/it] 53%|█████▎    | 606/1153 [14:29<11:57,  1.31s/it] 53%|█████▎    | 607/1153 [14:31<12:03,  1.33s/it] 53%|█████▎    | 608/1153 [14:32<11:01,  1.21s/it] 53%|█████▎    | 609/1153 [14:33<10:46,  1.19s/it] 53%|█████▎    | 610/1153 [14:34<11:04,  1.22s/it] 53%|█████▎    | 611/1153 [14:35<11:16,  1.25s/it] 53%|█████▎    | 612/1153 [14:37<11:24,  1.27s/it] 53%|█████▎    | 613/1153 [14:38<11:26,  1.27s/it] 53%|█████▎    | 614/1153 [14:42<18:49,  2.10s/it] 53%|█████▎    | 615/1153 [14:43<16:40,  1.86s/it] 53%|█████▎    | 616/1153 [14:44<15:10,  1.69s/it] 54%|█████▎    | 617/1153 [14:46<14:05,  1.58s/it] 54%|█████▎    | 618/1153 [14:47<13:22,  1.50s/it] 54%|█████▎    | 619/1153 [14:48<12:46,  1.44s/it] 54%|█████▍    | 620/1153 [14:50<12:21,  1.39s/it] 54%|█████▍    | 621/1153 [14:51<12:04,  1.36s/it] 54%|█████▍    | 622/1153 [14:52<11:21,  1.28s/it] 54%|█████▍    | 623/1153 [14:53<11:22,  1.29s/it] 54%|█████▍    | 624/1153 [14:55<11:24,  1.29s/it] 54%|█████▍    | 625/1153 [14:56<11:26,  1.30s/it] 54%|█████▍    | 626/1153 [14:57<11:22,  1.30s/it] 54%|█████▍    | 627/1153 [14:58<10:58,  1.25s/it] 54%|█████▍    | 628/1153 [15:00<11:08,  1.27s/it] 55%|█████▍    | 629/1153 [15:01<11:09,  1.28s/it] 55%|█████▍    | 630/1153 [15:02<11:10,  1.28s/it] 55%|█████▍    | 631/1153 [15:04<11:14,  1.29s/it] 55%|█████▍    | 632/1153 [15:08<18:08,  2.09s/it] 55%|█████▍    | 633/1153 [15:09<15:53,  1.83s/it] 55%|█████▍    | 634/1153 [15:10<14:06,  1.63s/it] 55%|█████▌    | 635/1153 [15:11<13:33,  1.57s/it] 55%|█████▌    | 636/1153 [15:13<13:01,  1.51s/it] 55%|█████▌    | 637/1153 [15:14<12:42,  1.48s/it] 55%|█████▌    | 638/1153 [15:16<12:26,  1.45s/it] 55%|█████▌    | 639/1153 [15:17<12:12,  1.43s/it] 56%|█████▌    | 640/1153 [15:18<11:58,  1.40s/it] 56%|█████▌    | 641/1153 [15:20<11:47,  1.38s/it] 56%|█████▌    | 642/1153 [15:21<11:41,  1.37s/it] 56%|█████▌    | 643/1153 [15:22<11:54,  1.40s/it] 56%|█████▌    | 644/1153 [15:24<12:01,  1.42s/it] 56%|█████▌    | 645/1153 [15:25<11:58,  1.41s/it] 56%|█████▌    | 646/1153 [15:27<11:55,  1.41s/it] 56%|█████▌    | 647/1153 [15:27<09:48,  1.16s/it] 56%|█████▌    | 648/1153 [15:29<10:13,  1.22s/it] 56%|█████▋    | 649/1153 [15:30<10:33,  1.26s/it] 56%|█████▋    | 650/1153 [15:34<18:31,  2.21s/it] 56%|█████▋    | 651/1153 [15:36<16:26,  1.97s/it] 57%|█████▋    | 652/1153 [15:36<13:03,  1.56s/it] 57%|█████▋    | 653/1153 [15:38<12:08,  1.46s/it] 57%|█████▋    | 654/1153 [15:39<11:47,  1.42s/it] 57%|█████▋    | 655/1153 [15:40<09:49,  1.18s/it] 57%|█████▋    | 656/1153 [15:40<08:28,  1.02s/it] 57%|█████▋    | 657/1153 [15:42<09:16,  1.12s/it] 57%|█████▋    | 658/1153 [15:43<09:49,  1.19s/it] 57%|█████▋    | 659/1153 [15:44<10:18,  1.25s/it] 57%|█████▋    | 660/1153 [15:46<10:35,  1.29s/it] 57%|█████▋    | 661/1153 [15:47<10:40,  1.30s/it] 57%|█████▋    | 662/1153 [15:48<10:46,  1.32s/it] 58%|█████▊    | 663/1153 [15:50<10:51,  1.33s/it] 58%|█████▊    | 664/1153 [15:51<10:54,  1.34s/it] 58%|█████▊    | 665/1153 [15:53<11:06,  1.37s/it] 58%|█████▊    | 666/1153 [15:54<11:04,  1.36s/it] 58%|█████▊    | 667/1153 [15:55<10:57,  1.35s/it] 58%|█████▊    | 668/1153 [15:57<10:51,  1.34s/it] 58%|█████▊    | 669/1153 [16:01<18:11,  2.25s/it] 58%|█████▊    | 670/1153 [16:02<16:03,  1.99s/it] 58%|█████▊    | 671/1153 [16:04<14:29,  1.80s/it] 58%|█████▊    | 672/1153 [16:05<13:27,  1.68s/it] 58%|█████▊    | 673/1153 [16:06<12:37,  1.58s/it] 58%|█████▊    | 674/1153 [16:08<12:06,  1.52s/it] 59%|█████▊    | 675/1153 [16:09<11:50,  1.49s/it] 59%|█████▊    | 676/1153 [16:11<11:27,  1.44s/it] 59%|█████▊    | 677/1153 [16:12<11:20,  1.43s/it] 59%|█████▉    | 678/1153 [16:13<10:43,  1.36s/it] 59%|█████▉    | 679/1153 [16:15<10:54,  1.38s/it] 59%|█████▉    | 680/1153 [16:16<10:32,  1.34s/it] 59%|█████▉    | 681/1153 [16:17<10:08,  1.29s/it] 59%|█████▉    | 682/1153 [16:18<10:25,  1.33s/it] 59%|█████▉    | 683/1153 [16:20<10:39,  1.36s/it] 59%|█████▉    | 684/1153 [16:21<10:19,  1.32s/it] 59%|█████▉    | 685/1153 [16:22<10:26,  1.34s/it] 59%|█████▉    | 686/1153 [16:24<10:45,  1.38s/it] 60%|█████▉    | 687/1153 [16:25<10:20,  1.33s/it] 60%|█████▉    | 688/1153 [16:29<16:36,  2.14s/it] 60%|█████▉    | 689/1153 [16:31<15:07,  1.96s/it] 60%|█████▉    | 690/1153 [16:32<13:14,  1.71s/it] 60%|█████▉    | 691/1153 [16:33<12:02,  1.56s/it] 60%|██████    | 692/1153 [16:34<11:30,  1.50s/it] 60%|██████    | 693/1153 [16:36<10:46,  1.41s/it] 60%|██████    | 694/1153 [16:37<10:49,  1.42s/it] 60%|██████    | 695/1153 [16:38<10:43,  1.41s/it] 60%|██████    | 696/1153 [16:40<10:37,  1.39s/it] 60%|██████    | 697/1153 [16:41<10:30,  1.38s/it] 61%|██████    | 698/1153 [16:43<10:31,  1.39s/it] 61%|██████    | 699/1153 [16:44<10:28,  1.38s/it] 61%|██████    | 700/1153 [16:45<10:24,  1.38s/it] 61%|██████    | 701/1153 [16:47<10:25,  1.38s/it] 61%|██████    | 702/1153 [16:48<10:22,  1.38s/it] 61%|██████    | 703/1153 [16:49<10:24,  1.39s/it] 61%|██████    | 704/1153 [16:51<10:33,  1.41s/it] 61%|██████    | 705/1153 [16:52<10:33,  1.41s/it] 61%|██████    | 706/1153 [16:57<17:34,  2.36s/it] 61%|██████▏   | 707/1153 [16:58<15:17,  2.06s/it] 61%|██████▏   | 708/1153 [17:00<13:31,  1.82s/it] 61%|██████▏   | 709/1153 [17:01<12:23,  1.67s/it] 62%|██████▏   | 710/1153 [17:02<11:37,  1.57s/it] 62%|██████▏   | 711/1153 [17:04<11:05,  1.51s/it] 62%|██████▏   | 712/1153 [17:05<10:42,  1.46s/it] 62%|██████▏   | 713/1153 [17:07<11:02,  1.51s/it] 62%|██████▏   | 714/1153 [17:08<10:39,  1.46s/it] 62%|██████▏   | 715/1153 [17:09<10:23,  1.42s/it] 62%|██████▏   | 716/1153 [17:11<10:32,  1.45s/it] 62%|██████▏   | 717/1153 [17:12<10:32,  1.45s/it] 62%|██████▏   | 718/1153 [17:14<10:26,  1.44s/it] 62%|██████▏   | 719/1153 [17:15<10:21,  1.43s/it] 62%|██████▏   | 720/1153 [17:16<10:12,  1.42s/it] 63%|██████▎   | 721/1153 [17:18<10:09,  1.41s/it] 63%|██████▎   | 722/1153 [17:19<10:08,  1.41s/it] 63%|██████▎   | 723/1153 [17:24<18:22,  2.56s/it] 63%|██████▎   | 724/1153 [17:26<15:48,  2.21s/it] 63%|██████▎   | 725/1153 [17:27<14:01,  1.97s/it] 63%|██████▎   | 726/1153 [17:29<13:19,  1.87s/it] 63%|██████▎   | 727/1153 [17:30<12:13,  1.72s/it] 63%|██████▎   | 728/1153 [17:32<11:24,  1.61s/it] 63%|██████▎   | 729/1153 [17:33<11:00,  1.56s/it] 63%|██████▎   | 730/1153 [17:34<10:44,  1.52s/it] 63%|██████▎   | 731/1153 [17:36<10:21,  1.47s/it] 63%|██████▎   | 732/1153 [17:37<10:17,  1.47s/it] 64%|██████▎   | 733/1153 [17:39<10:16,  1.47s/it] 64%|██████▎   | 734/1153 [17:40<10:19,  1.48s/it] 64%|██████▎   | 735/1153 [17:42<10:09,  1.46s/it] 64%|██████▍   | 736/1153 [17:43<09:58,  1.44s/it] 64%|██████▍   | 737/1153 [17:45<09:57,  1.44s/it] 64%|██████▍   | 738/1153 [17:46<09:48,  1.42s/it] 64%|██████▍   | 739/1153 [17:47<09:45,  1.41s/it] 64%|██████▍   | 740/1153 [17:49<09:42,  1.41s/it] 64%|██████▍   | 741/1153 [17:54<17:06,  2.49s/it] 64%|██████▍   | 742/1153 [17:55<15:02,  2.20s/it] 64%|██████▍   | 743/1153 [17:57<13:25,  1.97s/it] 65%|██████▍   | 744/1153 [17:58<11:38,  1.71s/it] 65%|██████▍   | 745/1153 [17:59<10:56,  1.61s/it] 65%|██████▍   | 746/1153 [18:00<10:17,  1.52s/it] 65%|██████▍   | 747/1153 [18:02<10:07,  1.50s/it] 65%|██████▍   | 748/1153 [18:03<10:02,  1.49s/it] 65%|██████▍   | 749/1153 [18:05<09:52,  1.47s/it] 65%|██████▌   | 750/1153 [18:06<09:43,  1.45s/it] 65%|██████▌   | 751/1153 [18:08<09:33,  1.43s/it] 65%|██████▌   | 752/1153 [18:09<09:31,  1.43s/it] 65%|██████▌   | 753/1153 [18:10<09:20,  1.40s/it] 65%|██████▌   | 754/1153 [18:12<09:25,  1.42s/it] 65%|██████▌   | 755/1153 [18:13<09:19,  1.41s/it] 66%|██████▌   | 756/1153 [18:14<08:58,  1.36s/it] 66%|██████▌   | 757/1153 [18:16<08:49,  1.34s/it] 66%|██████▌   | 758/1153 [18:17<08:41,  1.32s/it] 66%|██████▌   | 759/1153 [18:18<08:29,  1.29s/it] 66%|██████▌   | 760/1153 [18:23<16:08,  2.46s/it] 66%|██████▌   | 761/1153 [18:25<14:01,  2.15s/it] 66%|██████▌   | 762/1153 [18:26<12:22,  1.90s/it] 66%|██████▌   | 763/1153 [18:27<10:56,  1.68s/it] 66%|██████▋   | 764/1153 [18:29<10:02,  1.55s/it] 66%|██████▋   | 765/1153 [18:30<09:31,  1.47s/it] 66%|██████▋   | 766/1153 [18:31<09:05,  1.41s/it] 67%|██████▋   | 767/1153 [18:32<08:51,  1.38s/it] 67%|██████▋   | 768/1153 [18:34<08:32,  1.33s/it] 67%|██████▋   | 769/1153 [18:35<08:14,  1.29s/it] 67%|██████▋   | 770/1153 [18:36<08:17,  1.30s/it] 67%|██████▋   | 771/1153 [18:37<08:07,  1.28s/it] 67%|██████▋   | 772/1153 [18:39<08:10,  1.29s/it] 67%|██████▋   | 773/1153 [18:40<08:09,  1.29s/it] 67%|██████▋   | 774/1153 [18:41<08:02,  1.27s/it] 67%|██████▋   | 775/1153 [18:42<07:57,  1.26s/it] 67%|██████▋   | 776/1153 [18:44<07:51,  1.25s/it] 67%|██████▋   | 777/1153 [18:45<07:48,  1.24s/it] 67%|██████▋   | 778/1153 [18:46<07:45,  1.24s/it] 68%|██████▊   | 779/1153 [18:47<07:44,  1.24s/it] 68%|██████▊   | 780/1153 [18:49<07:52,  1.27s/it] 68%|██████▊   | 781/1153 [18:53<14:14,  2.30s/it] 68%|██████▊   | 782/1153 [18:55<12:17,  1.99s/it] 68%|██████▊   | 783/1153 [18:56<11:02,  1.79s/it] 68%|██████▊   | 784/1153 [18:57<09:58,  1.62s/it] 68%|██████▊   | 785/1153 [18:59<09:26,  1.54s/it] 68%|██████▊   | 786/1153 [19:00<09:03,  1.48s/it] 68%|██████▊   | 787/1153 [19:01<08:47,  1.44s/it] 68%|██████▊   | 788/1153 [19:03<08:31,  1.40s/it] 68%|██████▊   | 789/1153 [19:04<08:21,  1.38s/it] 69%|██████▊   | 790/1153 [19:05<08:13,  1.36s/it] 69%|██████▊   | 791/1153 [19:06<07:42,  1.28s/it] 69%|██████▊   | 792/1153 [19:07<07:21,  1.22s/it] 69%|██████▉   | 793/1153 [19:09<08:00,  1.33s/it] 69%|██████▉   | 794/1153 [19:10<08:00,  1.34s/it] 69%|██████▉   | 795/1153 [19:12<08:10,  1.37s/it] 69%|██████▉   | 796/1153 [19:13<08:06,  1.36s/it] 69%|██████▉   | 797/1153 [19:15<08:09,  1.37s/it] 69%|██████▉   | 798/1153 [19:16<08:00,  1.35s/it] 69%|██████▉   | 799/1153 [19:17<07:55,  1.34s/it] 69%|██████▉   | 800/1153 [19:22<14:08,  2.40s/it] 69%|██████▉   | 801/1153 [19:23<12:18,  2.10s/it] 70%|██████▉   | 802/1153 [19:25<10:58,  1.88s/it] 70%|██████▉   | 803/1153 [19:26<10:12,  1.75s/it] 70%|██████▉   | 804/1153 [19:28<09:30,  1.63s/it] 70%|██████▉   | 805/1153 [19:29<08:59,  1.55s/it] 70%|██████▉   | 806/1153 [19:30<08:34,  1.48s/it] 70%|██████▉   | 807/1153 [19:32<08:18,  1.44s/it] 70%|███████   | 808/1153 [19:33<08:07,  1.41s/it] 70%|███████   | 809/1153 [19:34<07:59,  1.39s/it] 70%|███████   | 810/1153 [19:36<07:50,  1.37s/it] 70%|███████   | 811/1153 [19:37<07:44,  1.36s/it] 70%|███████   | 812/1153 [19:38<07:41,  1.35s/it] 71%|███████   | 813/1153 [19:40<07:39,  1.35s/it] 71%|███████   | 814/1153 [19:41<07:37,  1.35s/it] 71%|███████   | 815/1153 [19:42<07:36,  1.35s/it] 71%|███████   | 816/1153 [19:43<06:16,  1.12s/it] 71%|███████   | 817/1153 [19:44<06:36,  1.18s/it] 71%|███████   | 818/1153 [19:49<12:19,  2.21s/it] 71%|███████   | 819/1153 [19:50<10:56,  1.97s/it] 71%|███████   | 820/1153 [19:52<09:57,  1.79s/it] 71%|███████   | 821/1153 [19:53<08:56,  1.61s/it] 71%|███████▏  | 822/1153 [19:54<07:28,  1.35s/it] 71%|███████▏  | 823/1153 [19:55<07:19,  1.33s/it] 71%|███████▏  | 824/1153 [19:56<07:18,  1.33s/it] 72%|███████▏  | 825/1153 [19:57<06:48,  1.24s/it] 72%|███████▏  | 826/1153 [19:59<06:56,  1.27s/it] 72%|███████▏  | 827/1153 [20:00<06:44,  1.24s/it] 72%|███████▏  | 828/1153 [20:01<06:53,  1.27s/it] 72%|███████▏  | 829/1153 [20:02<06:22,  1.18s/it] 72%|███████▏  | 830/1153 [20:03<06:41,  1.24s/it] 72%|███████▏  | 831/1153 [20:05<06:51,  1.28s/it] 72%|███████▏  | 832/1153 [20:06<06:58,  1.31s/it] 72%|███████▏  | 833/1153 [20:08<07:10,  1.35s/it] 72%|███████▏  | 834/1153 [20:09<07:14,  1.36s/it] 72%|███████▏  | 835/1153 [20:10<07:17,  1.38s/it] 73%|███████▎  | 836/1153 [20:12<07:19,  1.39s/it] 73%|███████▎  | 837/1153 [20:13<07:18,  1.39s/it] 73%|███████▎  | 838/1153 [20:18<12:32,  2.39s/it] 73%|███████▎  | 839/1153 [20:19<11:00,  2.10s/it] 73%|███████▎  | 840/1153 [20:21<09:57,  1.91s/it] 73%|███████▎  | 841/1153 [20:22<09:18,  1.79s/it] 73%|███████▎  | 842/1153 [20:24<08:45,  1.69s/it] 73%|███████▎  | 843/1153 [20:25<08:19,  1.61s/it] 73%|███████▎  | 844/1153 [20:27<07:59,  1.55s/it] 73%|███████▎  | 845/1153 [20:28<07:49,  1.52s/it] 73%|███████▎  | 846/1153 [20:29<07:24,  1.45s/it] 73%|███████▎  | 847/1153 [20:31<07:19,  1.44s/it] 74%|███████▎  | 848/1153 [20:32<06:58,  1.37s/it] 74%|███████▎  | 849/1153 [20:33<06:50,  1.35s/it] 74%|███████▎  | 850/1153 [20:35<06:55,  1.37s/it] 74%|███████▍  | 851/1153 [20:36<06:55,  1.37s/it] 74%|███████▍  | 852/1153 [20:38<06:56,  1.38s/it] 74%|███████▍  | 853/1153 [20:39<06:36,  1.32s/it] 74%|███████▍  | 854/1153 [20:40<06:39,  1.34s/it] 74%|███████▍  | 855/1153 [20:41<06:44,  1.36s/it] 74%|███████▍  | 856/1153 [20:43<06:35,  1.33s/it] 74%|███████▍  | 857/1153 [20:47<11:19,  2.30s/it] 74%|███████▍  | 858/1153 [20:49<09:59,  2.03s/it] 75%|███████▍  | 859/1153 [20:50<08:47,  1.79s/it] 75%|███████▍  | 860/1153 [20:51<07:58,  1.63s/it] 75%|███████▍  | 861/1153 [20:52<07:23,  1.52s/it] 75%|███████▍  | 862/1153 [20:54<07:14,  1.49s/it] 75%|███████▍  | 863/1153 [20:55<07:04,  1.47s/it] 75%|███████▍  | 864/1153 [20:57<06:47,  1.41s/it] 75%|███████▌  | 865/1153 [20:58<06:31,  1.36s/it] 75%|███████▌  | 866/1153 [20:59<06:18,  1.32s/it] 75%|███████▌  | 867/1153 [21:00<06:23,  1.34s/it] 75%|███████▌  | 868/1153 [21:02<06:12,  1.31s/it] 75%|███████▌  | 869/1153 [21:03<06:08,  1.30s/it] 75%|███████▌  | 870/1153 [21:04<06:06,  1.29s/it] 76%|███████▌  | 871/1153 [21:06<06:13,  1.33s/it] 76%|███████▌  | 872/1153 [21:07<06:18,  1.35s/it] 76%|███████▌  | 873/1153 [21:08<06:05,  1.31s/it] 76%|███████▌  | 874/1153 [21:09<05:58,  1.29s/it] 76%|███████▌  | 875/1153 [21:11<05:56,  1.28s/it] 76%|███████▌  | 876/1153 [21:12<05:53,  1.28s/it] 76%|███████▌  | 877/1153 [21:16<10:14,  2.23s/it] 76%|███████▌  | 878/1153 [21:18<08:54,  1.95s/it] 76%|███████▌  | 879/1153 [21:19<07:54,  1.73s/it] 76%|███████▋  | 880/1153 [21:20<07:13,  1.59s/it] 76%|███████▋  | 881/1153 [21:22<06:51,  1.51s/it] 76%|███████▋  | 882/1153 [21:23<06:33,  1.45s/it] 77%|███████▋  | 883/1153 [21:24<06:19,  1.40s/it] 77%|███████▋  | 884/1153 [21:26<06:22,  1.42s/it] 77%|███████▋  | 885/1153 [21:27<06:19,  1.42s/it] 77%|███████▋  | 886/1153 [21:28<06:18,  1.42s/it] 77%|███████▋  | 887/1153 [21:30<06:20,  1.43s/it] 77%|███████▋  | 888/1153 [21:31<06:22,  1.44s/it] 77%|███████▋  | 889/1153 [21:33<06:08,  1.39s/it] 77%|███████▋  | 890/1153 [21:34<06:03,  1.38s/it] 77%|███████▋  | 891/1153 [21:35<06:05,  1.40s/it] 77%|███████▋  | 892/1153 [21:37<05:53,  1.35s/it] 77%|███████▋  | 893/1153 [21:38<05:39,  1.31s/it] 78%|███████▊  | 894/1153 [21:39<05:36,  1.30s/it] 78%|███████▊  | 895/1153 [21:41<05:40,  1.32s/it] 78%|███████▊  | 896/1153 [21:42<05:31,  1.29s/it] 78%|███████▊  | 897/1153 [21:43<05:28,  1.28s/it] 78%|███████▊  | 898/1153 [21:47<09:18,  2.19s/it] 78%|███████▊  | 899/1153 [21:49<08:05,  1.91s/it] 78%|███████▊  | 900/1153 [21:50<07:09,  1.70s/it] 78%|███████▊  | 901/1153 [21:51<06:35,  1.57s/it] 78%|███████▊  | 902/1153 [21:52<06:09,  1.47s/it] 78%|███████▊  | 903/1153 [21:54<05:54,  1.42s/it] 78%|███████▊  | 904/1153 [21:55<05:42,  1.38s/it] 78%|███████▊  | 905/1153 [21:56<05:34,  1.35s/it] 79%|███████▊  | 906/1153 [21:57<05:30,  1.34s/it] 79%|███████▊  | 907/1153 [21:59<05:20,  1.30s/it] 79%|███████▉  | 908/1153 [22:00<05:13,  1.28s/it] 79%|███████▉  | 909/1153 [22:01<05:10,  1.27s/it] 79%|███████▉  | 910/1153 [22:03<05:14,  1.29s/it] 79%|███████▉  | 911/1153 [22:04<05:10,  1.28s/it] 79%|███████▉  | 912/1153 [22:05<05:05,  1.27s/it] 79%|███████▉  | 913/1153 [22:06<05:05,  1.27s/it] 79%|███████▉  | 914/1153 [22:08<05:06,  1.28s/it] 79%|███████▉  | 915/1153 [22:09<05:05,  1.28s/it] 79%|███████▉  | 916/1153 [22:10<05:03,  1.28s/it] 80%|███████▉  | 917/1153 [22:11<05:05,  1.29s/it] 80%|███████▉  | 918/1153 [22:13<05:00,  1.28s/it] 80%|███████▉  | 919/1153 [22:17<08:40,  2.22s/it] 80%|███████▉  | 920/1153 [22:19<07:43,  1.99s/it] 80%|███████▉  | 921/1153 [22:20<07:02,  1.82s/it] 80%|███████▉  | 922/1153 [22:21<06:35,  1.71s/it] 80%|████████  | 923/1153 [22:23<06:18,  1.64s/it] 80%|████████  | 924/1153 [22:24<06:03,  1.59s/it] 80%|████████  | 925/1153 [22:26<05:51,  1.54s/it] 80%|████████  | 926/1153 [22:27<05:52,  1.55s/it] 80%|████████  | 927/1153 [22:29<05:27,  1.45s/it] 80%|████████  | 928/1153 [22:30<05:25,  1.45s/it] 81%|████████  | 929/1153 [22:32<05:21,  1.44s/it] 81%|████████  | 930/1153 [22:33<05:12,  1.40s/it] 81%|████████  | 931/1153 [22:34<05:11,  1.40s/it] 81%|████████  | 932/1153 [22:36<05:12,  1.42s/it] 81%|████████  | 933/1153 [22:37<05:14,  1.43s/it] 81%|████████  | 934/1153 [22:39<05:17,  1.45s/it] 81%|████████  | 935/1153 [22:40<05:16,  1.45s/it] 81%|████████  | 936/1153 [22:42<05:11,  1.44s/it] 81%|████████▏ | 937/1153 [22:46<08:29,  2.36s/it] 81%|████████▏ | 938/1153 [22:48<07:34,  2.11s/it] 81%|████████▏ | 939/1153 [22:49<06:51,  1.92s/it] 82%|████████▏ | 940/1153 [22:51<06:20,  1.79s/it] 82%|████████▏ | 941/1153 [22:52<05:58,  1.69s/it] 82%|████████▏ | 942/1153 [22:53<05:39,  1.61s/it] 82%|████████▏ | 943/1153 [22:55<05:25,  1.55s/it] 82%|████████▏ | 944/1153 [22:56<05:15,  1.51s/it] 82%|████████▏ | 945/1153 [22:58<05:01,  1.45s/it] 82%|████████▏ | 946/1153 [22:59<04:51,  1.41s/it] 82%|████████▏ | 947/1153 [23:00<04:48,  1.40s/it] 82%|████████▏ | 948/1153 [23:02<04:50,  1.42s/it] 82%|████████▏ | 949/1153 [23:03<04:47,  1.41s/it] 82%|████████▏ | 950/1153 [23:04<04:44,  1.40s/it] 82%|████████▏ | 951/1153 [23:06<04:45,  1.41s/it] 83%|████████▎ | 952/1153 [23:07<04:46,  1.42s/it] 83%|████████▎ | 953/1153 [23:09<04:47,  1.44s/it] 83%|████████▎ | 954/1153 [23:10<04:40,  1.41s/it] 83%|████████▎ | 955/1153 [23:12<04:40,  1.42s/it] 83%|████████▎ | 956/1153 [23:16<07:58,  2.43s/it] 83%|████████▎ | 957/1153 [23:18<06:59,  2.14s/it] 83%|████████▎ | 958/1153 [23:19<06:19,  1.95s/it] 83%|████████▎ | 959/1153 [23:21<05:46,  1.79s/it] 83%|████████▎ | 960/1153 [23:22<05:23,  1.67s/it] 83%|████████▎ | 961/1153 [23:23<04:23,  1.37s/it] 83%|████████▎ | 962/1153 [23:24<04:13,  1.32s/it] 84%|████████▎ | 963/1153 [23:25<04:18,  1.36s/it] 84%|████████▎ | 964/1153 [23:27<04:20,  1.38s/it] 84%|████████▎ | 965/1153 [23:28<04:14,  1.36s/it] 84%|████████▍ | 966/1153 [23:30<04:20,  1.39s/it] 84%|████████▍ | 967/1153 [23:31<04:17,  1.38s/it] 84%|████████▍ | 968/1153 [23:33<04:20,  1.41s/it] 84%|████████▍ | 969/1153 [23:34<04:21,  1.42s/it] 84%|████████▍ | 970/1153 [23:35<03:56,  1.29s/it] 84%|████████▍ | 971/1153 [23:35<03:08,  1.03s/it] 84%|████████▍ | 972/1153 [23:36<02:38,  1.14it/s] 84%|████████▍ | 973/1153 [23:37<02:39,  1.13it/s] 84%|████████▍ | 974/1153 [23:38<03:07,  1.05s/it] 85%|████████▍ | 975/1153 [23:40<03:25,  1.16s/it] 85%|████████▍ | 976/1153 [23:41<03:37,  1.23s/it] 85%|████████▍ | 977/1153 [23:42<03:44,  1.28s/it] 85%|████████▍ | 978/1153 [23:47<06:30,  2.23s/it] 85%|████████▍ | 979/1153 [23:48<05:44,  1.98s/it] 85%|████████▍ | 980/1153 [23:50<05:11,  1.80s/it] 85%|████████▌ | 981/1153 [23:51<04:47,  1.67s/it] 85%|████████▌ | 982/1153 [23:52<04:29,  1.58s/it] 85%|████████▌ | 983/1153 [23:54<04:20,  1.53s/it] 85%|████████▌ | 984/1153 [23:55<04:06,  1.46s/it] 85%|████████▌ | 985/1153 [23:57<04:06,  1.47s/it] 86%|████████▌ | 986/1153 [23:58<04:07,  1.49s/it] 86%|████████▌ | 987/1153 [24:00<04:04,  1.47s/it] 86%|████████▌ | 988/1153 [24:01<04:03,  1.48s/it] 86%|████████▌ | 989/1153 [24:02<03:59,  1.46s/it] 86%|████████▌ | 990/1153 [24:04<03:59,  1.47s/it] 86%|████████▌ | 991/1153 [24:05<03:46,  1.40s/it] 86%|████████▌ | 992/1153 [24:07<03:46,  1.41s/it] 86%|████████▌ | 993/1153 [24:08<03:47,  1.42s/it] 86%|████████▌ | 994/1153 [24:10<03:46,  1.43s/it] 86%|████████▋ | 995/1153 [24:11<03:46,  1.43s/it] 86%|████████▋ | 996/1153 [24:12<03:36,  1.38s/it] 86%|████████▋ | 997/1153 [24:17<06:09,  2.37s/it] 87%|████████▋ | 998/1153 [24:18<05:16,  2.04s/it] 87%|████████▋ | 999/1153 [24:19<04:37,  1.80s/it] 87%|████████▋ | 1000/1153 [24:21<04:19,  1.69s/it] 87%|████████▋ | 1001/1153 [24:22<04:06,  1.62s/it] 87%|████████▋ | 1002/1153 [24:24<03:47,  1.51s/it] 87%|████████▋ | 1003/1153 [24:25<03:35,  1.44s/it] 87%|████████▋ | 1004/1153 [24:26<03:27,  1.39s/it] 87%|████████▋ | 1005/1153 [24:28<03:27,  1.41s/it] 87%|████████▋ | 1006/1153 [24:29<03:26,  1.40s/it] 87%|████████▋ | 1007/1153 [24:30<03:19,  1.37s/it] 87%|████████▋ | 1008/1153 [24:31<03:13,  1.33s/it] 88%|████████▊ | 1009/1153 [24:33<03:10,  1.32s/it] 88%|████████▊ | 1010/1153 [24:34<03:06,  1.30s/it] 88%|████████▊ | 1011/1153 [24:35<03:03,  1.29s/it] 88%|████████▊ | 1012/1153 [24:37<03:03,  1.30s/it] 88%|████████▊ | 1013/1153 [24:38<03:11,  1.37s/it] 88%|████████▊ | 1014/1153 [24:39<03:06,  1.34s/it] 88%|████████▊ | 1015/1153 [24:41<03:02,  1.32s/it] 88%|████████▊ | 1016/1153 [24:42<03:01,  1.33s/it] 88%|████████▊ | 1017/1153 [24:43<03:00,  1.33s/it] 88%|████████▊ | 1018/1153 [24:45<02:56,  1.31s/it] 88%|████████▊ | 1019/1153 [24:49<05:11,  2.32s/it] 88%|████████▊ | 1020/1153 [24:51<04:27,  2.01s/it] 89%|████████▊ | 1021/1153 [24:52<03:57,  1.80s/it] 89%|████████▊ | 1022/1153 [24:53<03:38,  1.67s/it] 89%|████████▊ | 1023/1153 [24:55<03:20,  1.54s/it] 89%|████████▉ | 1024/1153 [24:56<03:09,  1.47s/it] 89%|████████▉ | 1025/1153 [24:57<02:59,  1.40s/it] 89%|████████▉ | 1026/1153 [24:59<03:01,  1.43s/it] 89%|████████▉ | 1027/1153 [25:00<03:01,  1.44s/it] 89%|████████▉ | 1028/1153 [25:02<03:03,  1.46s/it] 89%|████████▉ | 1029/1153 [25:03<03:05,  1.49s/it] 89%|████████▉ | 1030/1153 [25:05<03:08,  1.53s/it] 89%|████████▉ | 1031/1153 [25:06<03:05,  1.52s/it] 90%|████████▉ | 1032/1153 [25:08<03:02,  1.51s/it] 90%|████████▉ | 1033/1153 [25:09<02:55,  1.46s/it] 90%|████████▉ | 1034/1153 [25:11<02:54,  1.47s/it] 90%|████████▉ | 1035/1153 [25:12<02:48,  1.42s/it] 90%|████████▉ | 1036/1153 [25:13<02:44,  1.41s/it] 90%|████████▉ | 1037/1153 [25:15<02:40,  1.38s/it] 90%|█████████ | 1038/1153 [25:16<02:34,  1.34s/it] 90%|█████████ | 1039/1153 [25:21<04:43,  2.48s/it] 90%|█████████ | 1040/1153 [25:22<04:02,  2.15s/it] 90%|█████████ | 1041/1153 [25:24<03:31,  1.89s/it] 90%|█████████ | 1042/1153 [25:25<03:11,  1.72s/it] 90%|█████████ | 1043/1153 [25:26<03:00,  1.64s/it] 91%|█████████ | 1044/1153 [25:28<02:46,  1.53s/it] 91%|█████████ | 1045/1153 [25:29<02:39,  1.48s/it] 91%|█████████ | 1046/1153 [25:30<02:31,  1.42s/it] 91%|█████████ | 1047/1153 [25:32<02:29,  1.41s/it] 91%|█████████ | 1048/1153 [25:33<02:24,  1.37s/it] 91%|█████████ | 1049/1153 [25:34<02:21,  1.36s/it] 91%|█████████ | 1050/1153 [25:36<02:19,  1.36s/it] 91%|█████████ | 1051/1153 [25:37<02:16,  1.34s/it] 91%|█████████ | 1052/1153 [25:38<02:13,  1.32s/it] 91%|█████████▏| 1053/1153 [25:40<02:18,  1.38s/it] 91%|█████████▏| 1054/1153 [25:41<02:17,  1.38s/it] 92%|█████████▏| 1055/1153 [25:42<02:12,  1.35s/it] 92%|█████████▏| 1056/1153 [25:44<02:09,  1.34s/it] 92%|█████████▏| 1057/1153 [25:45<02:08,  1.34s/it] 92%|█████████▏| 1058/1153 [25:46<02:07,  1.34s/it] 92%|█████████▏| 1059/1153 [25:48<02:04,  1.33s/it] 92%|█████████▏| 1060/1153 [25:49<02:02,  1.31s/it] 92%|█████████▏| 1061/1153 [25:55<04:00,  2.61s/it] 92%|█████████▏| 1062/1153 [25:56<03:26,  2.27s/it] 92%|█████████▏| 1063/1153 [25:58<03:03,  2.04s/it] 92%|█████████▏| 1064/1153 [25:59<02:48,  1.89s/it] 92%|█████████▏| 1065/1153 [26:01<02:35,  1.76s/it] 92%|█████████▏| 1066/1153 [26:02<02:24,  1.66s/it] 93%|█████████▎| 1067/1153 [26:04<02:23,  1.66s/it] 93%|█████████▎| 1068/1153 [26:05<02:15,  1.60s/it] 93%|█████████▎| 1069/1153 [26:07<02:09,  1.55s/it] 93%|█████████▎| 1070/1153 [26:08<02:06,  1.53s/it] 93%|█████████▎| 1071/1153 [26:10<02:05,  1.53s/it] 93%|█████████▎| 1072/1153 [26:11<02:01,  1.50s/it] 93%|█████████▎| 1073/1153 [26:12<01:58,  1.48s/it] 93%|█████████▎| 1074/1153 [26:14<01:55,  1.47s/it] 93%|█████████▎| 1075/1153 [26:15<01:55,  1.48s/it] 93%|█████████▎| 1076/1153 [26:17<01:53,  1.48s/it] 93%|█████████▎| 1077/1153 [26:18<01:51,  1.47s/it] 93%|█████████▎| 1078/1153 [26:20<01:49,  1.46s/it] 94%|█████████▎| 1079/1153 [26:25<03:06,  2.52s/it] 94%|█████████▎| 1080/1153 [26:26<02:40,  2.20s/it] 94%|█████████▍| 1081/1153 [26:28<02:28,  2.06s/it] 94%|█████████▍| 1082/1153 [26:29<02:15,  1.91s/it] 94%|█████████▍| 1083/1153 [26:31<02:05,  1.79s/it] 94%|█████████▍| 1084/1153 [26:32<01:56,  1.68s/it] 94%|█████████▍| 1085/1153 [26:34<01:52,  1.65s/it] 94%|█████████▍| 1086/1153 [26:35<01:47,  1.60s/it] 94%|█████████▍| 1087/1153 [26:37<01:38,  1.49s/it] 94%|█████████▍| 1088/1153 [26:38<01:35,  1.47s/it] 94%|█████████▍| 1089/1153 [26:40<01:33,  1.47s/it] 95%|█████████▍| 1090/1153 [26:41<01:32,  1.47s/it] 95%|█████████▍| 1091/1153 [26:43<01:32,  1.49s/it] 95%|█████████▍| 1092/1153 [26:44<01:31,  1.49s/it] 95%|█████████▍| 1093/1153 [26:46<01:30,  1.50s/it] 95%|█████████▍| 1094/1153 [26:47<01:34,  1.60s/it] 95%|█████████▍| 1095/1153 [26:49<01:28,  1.53s/it] 95%|█████████▌| 1096/1153 [26:50<01:26,  1.51s/it] 95%|█████████▌| 1097/1153 [26:52<01:24,  1.50s/it] 95%|█████████▌| 1098/1153 [26:54<01:29,  1.62s/it] 95%|█████████▌| 1099/1153 [26:59<02:30,  2.80s/it] 95%|█████████▌| 1100/1153 [27:00<01:56,  2.20s/it] 95%|█████████▌| 1101/1153 [27:02<01:45,  2.04s/it] 96%|█████████▌| 1102/1153 [27:03<01:37,  1.90s/it] 96%|█████████▌| 1103/1153 [27:05<01:29,  1.79s/it] 96%|█████████▌| 1104/1153 [27:06<01:23,  1.71s/it] 96%|█████████▌| 1105/1153 [27:08<01:16,  1.59s/it] 96%|█████████▌| 1106/1153 [27:09<01:13,  1.56s/it] 96%|█████████▌| 1107/1153 [27:10<01:02,  1.35s/it] 96%|█████████▌| 1108/1153 [27:11<01:02,  1.39s/it] 96%|█████████▌| 1109/1153 [27:12<00:50,  1.15s/it] 96%|█████████▋| 1110/1153 [27:13<00:50,  1.18s/it] 96%|█████████▋| 1111/1153 [27:15<00:55,  1.33s/it] 96%|█████████▋| 1112/1153 [27:16<00:56,  1.38s/it] 97%|█████████▋| 1113/1153 [27:18<00:58,  1.45s/it] 97%|█████████▋| 1114/1153 [27:20<00:57,  1.47s/it] 97%|█████████▋| 1115/1153 [27:21<00:55,  1.47s/it] 97%|█████████▋| 1116/1153 [27:23<00:54,  1.47s/it] 97%|█████████▋| 1117/1153 [27:24<00:53,  1.48s/it] 97%|█████████▋| 1118/1153 [27:26<00:52,  1.49s/it] 97%|█████████▋| 1119/1153 [27:27<00:50,  1.49s/it] 97%|█████████▋| 1120/1153 [27:33<01:29,  2.70s/it] 97%|█████████▋| 1121/1153 [27:34<01:13,  2.31s/it] 97%|█████████▋| 1122/1153 [27:35<01:03,  2.04s/it] 97%|█████████▋| 1123/1153 [27:37<00:56,  1.90s/it] 97%|█████████▋| 1124/1153 [27:39<00:51,  1.79s/it] 98%|█████████▊| 1125/1153 [27:40<00:47,  1.71s/it] 98%|█████████▊| 1126/1153 [27:42<00:44,  1.64s/it] 98%|█████████▊| 1127/1153 [27:43<00:42,  1.64s/it] 98%|█████████▊| 1128/1153 [27:45<00:43,  1.72s/it] 98%|█████████▊| 1129/1153 [27:47<00:40,  1.67s/it] 98%|█████████▊| 1130/1153 [27:48<00:38,  1.66s/it] 98%|█████████▊| 1131/1153 [27:50<00:34,  1.58s/it] 98%|█████████▊| 1132/1153 [27:51<00:33,  1.58s/it] 98%|█████████▊| 1133/1153 [27:53<00:31,  1.55s/it] 98%|█████████▊| 1134/1153 [27:54<00:29,  1.55s/it] 98%|█████████▊| 1135/1153 [27:56<00:27,  1.54s/it] 99%|█████████▊| 1136/1153 [27:57<00:25,  1.52s/it] 99%|█████████▊| 1137/1153 [27:59<00:26,  1.64s/it] 99%|█████████▊| 1138/1153 [28:01<00:23,  1.60s/it] 99%|█████████▉| 1139/1153 [28:06<00:38,  2.73s/it] 99%|█████████▉| 1140/1153 [28:08<00:30,  2.37s/it] 99%|█████████▉| 1141/1153 [28:09<00:25,  2.10s/it] 99%|█████████▉| 1142/1153 [28:10<00:20,  1.88s/it] 99%|█████████▉| 1143/1153 [28:12<00:17,  1.77s/it] 99%|█████████▉| 1144/1153 [28:13<00:15,  1.71s/it] 99%|█████████▉| 1145/1153 [28:15<00:13,  1.66s/it] 99%|█████████▉| 1146/1153 [28:16<00:10,  1.57s/it] 99%|█████████▉| 1147/1153 [28:18<00:09,  1.59s/it]100%|█████████▉| 1148/1153 [28:19<00:07,  1.51s/it]100%|█████████▉| 1149/1153 [28:21<00:05,  1.45s/it]100%|█████████▉| 1150/1153 [28:22<00:04,  1.44s/it]100%|█████████▉| 1151/1153 [28:24<00:02,  1.45s/it]100%|█████████▉| 1152/1153 [28:25<00:01,  1.47s/it]100%|██████████| 1153/1153 [28:26<00:00,  1.33s/it]100%|██████████| 1153/1153 [28:26<00:00,  1.48s/it]
147540
147540
saving data 147540 to ./output/DuEE1.0/role/test_result.json
trigger predict 147540 load from ./output/DuEE1.0/role/test_result.json
role predict 147540 load from ./output/DuEE1.0/role/test_result.json
schema 66 load from ./conf/DuEE1.0/event_schema.json
submit data 147540 save to ./output/DuEE1.0/surbot_news_together.json
********** Done ***************
