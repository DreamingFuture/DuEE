nohup: ignoring input
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 45, 
    "event_type": "trigger", 
    "gradient_accumulation_steps": 8, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 1e-05, 
    "max_len": 150, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 30.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 48, 
    "per_gpu_train_batch_size": 56, 
    "seed": 66, 
    "stride": 100, 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/11958 [00:00<?, ?it/s]tokenizing...:   2%|▏         | 297/11958 [00:00<00:03, 2956.73it/s]tokenizing...:   5%|▍         | 593/11958 [00:00<00:03, 2875.58it/s]tokenizing...:   7%|▋         | 881/11958 [00:00<00:03, 2811.14it/s]tokenizing...:  10%|█         | 1200/11958 [00:00<00:03, 2951.98it/s]tokenizing...:  13%|█▎        | 1496/11958 [00:00<00:03, 2947.99it/s]tokenizing...:  15%|█▍        | 1792/11958 [00:00<00:03, 2927.68it/s]tokenizing...:  17%|█▋        | 2085/11958 [00:00<00:03, 2887.29it/s]tokenizing...:  20%|█▉        | 2377/11958 [00:00<00:03, 2893.36it/s]tokenizing...:  22%|██▏       | 2670/11958 [00:00<00:03, 2903.60it/s]tokenizing...:  25%|██▍       | 2961/11958 [00:01<00:03, 2747.22it/s]tokenizing...:  27%|██▋       | 3282/11958 [00:01<00:03, 2881.38it/s]tokenizing...:  30%|███       | 3590/11958 [00:01<00:02, 2936.90it/s]tokenizing...:  33%|███▎      | 3893/11958 [00:01<00:02, 2962.11it/s]tokenizing...:  35%|███▌      | 4216/11958 [00:01<00:02, 3037.54it/s]tokenizing...:  38%|███▊      | 4521/11958 [00:01<00:02, 2979.95it/s]tokenizing...:  41%|████      | 4843/11958 [00:01<00:02, 3046.55it/s]tokenizing...:  43%|████▎     | 5149/11958 [00:01<00:03, 2219.00it/s]tokenizing...:  46%|████▌     | 5441/11958 [00:01<00:02, 2381.92it/s]tokenizing...:  48%|████▊     | 5768/11958 [00:02<00:02, 2604.83it/s]tokenizing...:  51%|█████     | 6078/11958 [00:02<00:02, 2734.10it/s]tokenizing...:  54%|█████▎    | 6421/11958 [00:02<00:01, 2921.99it/s]tokenizing...:  56%|█████▋    | 6728/11958 [00:02<00:01, 2935.17it/s]tokenizing...:  59%|█████▉    | 7042/11958 [00:02<00:01, 2968.32it/s]tokenizing...:  62%|██████▏   | 7392/11958 [00:02<00:01, 3120.19it/s]tokenizing...:  65%|██████▍   | 7729/11958 [00:02<00:01, 3187.47it/s]tokenizing...:  67%|██████▋   | 8052/11958 [00:02<00:01, 3157.75it/s]tokenizing...:  70%|███████   | 8371/11958 [00:02<00:01, 3043.64it/s]tokenizing...:  73%|███████▎  | 8683/11958 [00:03<00:01, 3064.02it/s]tokenizing...:  75%|███████▌  | 8992/11958 [00:03<00:00, 3008.93it/s]tokenizing...:  78%|███████▊  | 9310/11958 [00:03<00:00, 3055.80it/s]tokenizing...:  81%|████████  | 9638/11958 [00:03<00:00, 3120.67it/s]tokenizing...:  83%|████████▎ | 9952/11958 [00:03<00:00, 2984.55it/s]tokenizing...:  86%|████████▌ | 10253/11958 [00:03<00:00, 2791.00it/s]tokenizing...:  88%|████████▊ | 10536/11958 [00:03<00:00, 2733.00it/s]tokenizing...:  90%|█████████ | 10812/11958 [00:03<00:00, 2676.60it/s]tokenizing...:  93%|█████████▎| 11082/11958 [00:03<00:00, 2650.89it/s]tokenizing...:  95%|█████████▍| 11348/11958 [00:03<00:00, 2540.97it/s]tokenizing...:  97%|█████████▋| 11659/11958 [00:04<00:00, 2699.26it/s]tokenizing...: 100%|██████████| 11958/11958 [00:04<00:00, 2863.60it/s]
tokenizing...:   0%|          | 0/1498 [00:00<?, ?it/s]tokenizing...:  20%|██        | 304/1498 [00:00<00:00, 3039.87it/s]tokenizing...:  41%|████      | 611/1498 [00:00<00:00, 3057.31it/s]tokenizing...:  61%|██████    | 917/1498 [00:00<00:00, 2954.56it/s]tokenizing...:  81%|████████  | 1214/1498 [00:00<00:00, 2959.24it/s]tokenizing...: 100%|██████████| 1498/1498 [00:00<00:00, 2918.76it/s]
08/11/2022 14:58:18 - INFO - root -   The nums of the train_dataset features is 11958
08/11/2022 14:58:18 - INFO - root -   The nums of the eval_dataset features is 1498
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
08/11/2022 14:58:22 - INFO - root -   ***** Running train *****
********************************************************************************
/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/214 [..............................] - ETA: 3:54  batch_loss: 4.9811 [Training] 2/214 [..............................] - ETA: 2:43  batch_loss: 4.9680 [Training] 3/214 [..............................] - ETA: 2:19  batch_loss: 4.9595 [Training] 4/214 [..............................] - ETA: 2:07  batch_loss: 4.9691 [Training] 5/214 [..............................] - ETA: 1:59  batch_loss: 4.9711 [Training] 6/214 [..............................] - ETA: 1:54  batch_loss: 4.9727 [Training] 7/214 [..............................] - ETA: 1:50  batch_loss: 4.9717 [Training] 8/214 [>.............................] - ETA: 1:48  batch_loss: 4.9714 [Training] 9/214 [>.............................] - ETA: 1:45  batch_loss: 4.9422 [Training] 10/214 [>.............................] - ETA: 1:43  batch_loss: 4.9217 [Training] 11/214 [>.............................] - ETA: 1:41  batch_loss: 4.9057 [Training] 12/214 [>.............................] - ETA: 1:40  batch_loss: 4.8956 [Training] 13/214 [>.............................] - ETA: 1:38  batch_loss: 4.8854 [Training] 14/214 [>.............................] - ETA: 1:37  batch_loss: 4.8731 [Training] 15/214 [=>............................] - ETA: 1:36  batch_loss: 4.8628 [Training] 16/214 [=>............................] - ETA: 1:35  batch_loss: 4.8569 [Training] 17/214 [=>............................] - ETA: 1:34  batch_loss: 4.8354 [Training] 18/214 [=>............................] - ETA: 1:33  batch_loss: 4.8178 [Training] 19/214 [=>............................] - ETA: 1:32  batch_loss: 4.8017 [Training] 20/214 [=>............................] - ETA: 1:32  batch_loss: 4.7866 [Training] 21/214 [=>............................] - ETA: 1:31  batch_loss: 4.7728 [Training] 22/214 [==>...........................] - ETA: 1:30  batch_loss: 4.7606 [Training] 23/214 [==>...........................] - ETA: 1:29  batch_loss: 4.7486 [Training] 24/214 [==>...........................] - ETA: 1:29  batch_loss: 4.7386 [Training] 25/214 [==>...........................] - ETA: 1:28  batch_loss: 4.7208 [Training] 26/214 [==>...........................] - ETA: 1:28  batch_loss: 4.7040 [Training] 27/214 [==>...........................] - ETA: 1:27  batch_loss: 4.6881 [Training] 28/214 [==>...........................] - ETA: 1:26  batch_loss: 4.6739 [Training] 29/214 [===>..........................] - ETA: 1:26  batch_loss: 4.6607 [Training] 30/214 [===>..........................] - ETA: 1:25  batch_loss: 4.6490 [Training] 31/214 [===>..........................] - ETA: 1:24  batch_loss: 4.6372 [Training] 32/214 [===>..........................] - ETA: 1:24  batch_loss: 4.6269 [Training] 33/214 [===>..........................] - ETA: 1:23  batch_loss: 4.6096 [Training] 34/214 [===>..........................] - ETA: 1:23  batch_loss: 4.5945 [Training] 35/214 [===>..........................] - ETA: 1:22  batch_loss: 4.5782 [Training] 36/214 [====>.........................] - ETA: 1:22  batch_loss: 4.5653 [Training] 37/214 [====>.........................] - ETA: 1:21  batch_loss: 4.5525 [Training] 38/214 [====>.........................] - ETA: 1:21  batch_loss: 4.5396 [Training] 39/214 [====>.........................] - ETA: 1:20  batch_loss: 4.5261 [Training] 40/214 [====>.........................] - ETA: 1:20  batch_loss: 4.5134 [Training] 41/214 [====>.........................] - ETA: 1:19  batch_loss: 4.4955 [Training] 42/214 [====>.........................] - ETA: 1:19  batch_loss: 4.4799 [Training] 43/214 [=====>........................] - ETA: 1:18  batch_loss: 4.4657 [Training] 44/214 [=====>........................] - ETA: 1:18  batch_loss: 4.4506 [Training] 45/214 [=====>........................] - ETA: 1:17  batch_loss: 4.4354 [Training] 46/214 [=====>........................] - ETA: 1:17  batch_loss: 4.4227 [Training] 47/214 [=====>........................] - ETA: 1:16  batch_loss: 4.4086 [Training] 48/214 [=====>........................] - ETA: 1:16  batch_loss: 4.3968 [Training] 49/214 [=====>........................] - ETA: 1:15  batch_loss: 4.3795 [Training] 50/214 [======>.......................] - ETA: 1:15  batch_loss: 4.3645 [Training] 51/214 [======>.......................] - ETA: 1:14  batch_loss: 4.3498 [Training] 52/214 [======>.......................] - ETA: 1:14  batch_loss: 4.3345 [Training] 53/214 [======>.......................] - ETA: 1:13  batch_loss: 4.3199 [Training] 54/214 [======>.......................] - ETA: 1:13  batch_loss: 4.3065 [Training] 55/214 [======>.......................] - ETA: 1:12  batch_loss: 4.2941 [Training] 56/214 [======>.......................] - ETA: 1:12  batch_loss: 4.2816 [Training] 57/214 [======>.......................] - ETA: 1:11  batch_loss: 4.2652 [Training] 58/214 [=======>......................] - ETA: 1:11  batch_loss: 4.2491 [Training] 59/214 [=======>......................] - ETA: 1:10  batch_loss: 4.2344 [Training] 60/214 [=======>......................] - ETA: 1:10  batch_loss: 4.2198 [Training] 61/214 [=======>......................] - ETA: 1:09  batch_loss: 4.2058 [Training] 62/214 [=======>......................] - ETA: 1:09  batch_loss: 4.1917 [Training] 63/214 [=======>......................] - ETA: 1:08  batch_loss: 4.1780 [Training] 64/214 [=======>......................] - ETA: 1:08  batch_loss: 4.1643 [Training] 65/214 [========>.....................] - ETA: 1:07  batch_loss: 4.1472 [Training] 66/214 [========>.....................] - ETA: 1:07  batch_loss: 4.1320 [Training] 67/214 [========>.....................] - ETA: 1:06  batch_loss: 4.1172 [Training] 68/214 [========>.....................] - ETA: 1:06  batch_loss: 4.1011 [Training] 69/214 [========>.....................] - ETA: 1:06  batch_loss: 4.0862 [Training] 70/214 [========>.....................] - ETA: 1:05  batch_loss: 4.0719 [Training] 71/214 [========>.....................] - ETA: 1:05  batch_loss: 4.0576 [Training] 72/214 [=========>....................] - ETA: 1:04  batch_loss: 4.0444 [Training] 73/214 [=========>....................] - ETA: 1:04  batch_loss: 4.0281 [Training] 74/214 [=========>....................] - ETA: 1:03  batch_loss: 4.0111 [Training] 75/214 [=========>....................] - ETA: 1:03  batch_loss: 3.9950 [Training] 76/214 [=========>....................] - ETA: 1:02  batch_loss: 3.9803 [Training] 77/214 [=========>....................] - ETA: 1:02  batch_loss: 3.9647 [Training] 78/214 [=========>....................] - ETA: 1:01  batch_loss: 3.9503 [Training] 79/214 [==========>...................] - ETA: 1:01  batch_loss: 3.9361 [Training] 80/214 [==========>...................] - ETA: 1:00  batch_loss: 3.9223 [Training] 81/214 [==========>...................] - ETA: 1:00  batch_loss: 3.9053 [Training] 82/214 [==========>...................] - ETA: 1:00  batch_loss: 3.8896 [Training] 83/214 [==========>...................] - ETA: 59s  batch_loss: 3.8742 [Training] 84/214 [==========>...................] - ETA: 59s  batch_loss: 3.8587 [Training] 85/214 [==========>...................] - ETA: 58s  batch_loss: 3.8444 [Training] 86/214 [===========>..................] - ETA: 58s  batch_loss: 3.8292 [Training] 87/214 [===========>..................] - ETA: 57s  batch_loss: 3.8150 [Training] 88/214 [===========>..................] - ETA: 57s  batch_loss: 3.8012 [Training] 89/214 [===========>..................] - ETA: 56s  batch_loss: 3.7843 [Training] 90/214 [===========>..................] - ETA: 56s  batch_loss: 3.7676 [Training] 91/214 [===========>..................] - ETA: 55s  batch_loss: 3.7510 [Training] 92/214 [===========>..................] - ETA: 55s  batch_loss: 3.7352 [Training] 93/214 [============>.................] - ETA: 55s  batch_loss: 3.7204 [Training] 94/214 [============>.................] - ETA: 54s  batch_loss: 3.7051 [Training] 95/214 [============>.................] - ETA: 54s  batch_loss: 3.6907 [Training] 96/214 [============>.................] - ETA: 53s  batch_loss: 3.6765 [Training] 97/214 [============>.................] - ETA: 53s  batch_loss: 3.6595 [Training] 98/214 [============>.................] - ETA: 52s  batch_loss: 3.6432 [Training] 99/214 [============>.................] - ETA: 52s  batch_loss: 3.6263 [Training] 100/214 [=============>................] - ETA: 51s  batch_loss: 3.6106 [Training] 101/214 [=============>................] - ETA: 51s  batch_loss: 3.5946 [Training] 102/214 [=============>................] - ETA: 50s  batch_loss: 3.5792 [Training] 103/214 [=============>................] - ETA: 50s  batch_loss: 3.5638 [Training] 104/214 [=============>................] - ETA: 50s  batch_loss: 3.5485 [Training] 105/214 [=============>................] - ETA: 49s  batch_loss: 3.5315 [Training] 106/214 [=============>................] - ETA: 49s  batch_loss: 3.5153 [Training] 107/214 [==============>...............] - ETA: 48s  batch_loss: 3.4993 [Training] 108/214 [==============>...............] - ETA: 48s  batch_loss: 3.4838 [Training] 109/214 [==============>...............] - ETA: 47s  batch_loss: 3.4679 [Training] 110/214 [==============>...............] - ETA: 47s  batch_loss: 3.4527 [Training] 111/214 [==============>...............] - ETA: 46s  batch_loss: 3.4377 [Training] 112/214 [==============>...............] - ETA: 46s  batch_loss: 3.4234 [Training] 113/214 [==============>...............] - ETA: 45s  batch_loss: 3.4069 [Training] 114/214 [==============>...............] - ETA: 45s  batch_loss: 3.3905 [Training] 115/214 [===============>..............] - ETA: 44s  batch_loss: 3.3745 [Training] 116/214 [===============>..............] - ETA: 44s  batch_loss: 3.3591 [Training] 117/214 [===============>..............] - ETA: 44s  batch_loss: 3.3441 [Training] 118/214 [===============>..............] - ETA: 43s  batch_loss: 3.3289 [Training] 119/214 [===============>..............] - ETA: 43s  batch_loss: 3.3146 [Training] 120/214 [===============>..............] - ETA: 42s  batch_loss: 3.3005 [Training] 121/214 [===============>..............] - ETA: 42s  batch_loss: 3.2842 [Training] 122/214 [================>.............] - ETA: 41s  batch_loss: 3.2686 [Training] 123/214 [================>.............] - ETA: 41s  batch_loss: 3.2527 [Training] 124/214 [================>.............] - ETA: 40s  batch_loss: 3.2376 [Training] 125/214 [================>.............] - ETA: 40s  batch_loss: 3.2228 [Training] 126/214 [================>.............] - ETA: 39s  batch_loss: 3.2079 [Training] 127/214 [================>.............] - ETA: 39s  batch_loss: 3.1929 [Training] 128/214 [================>.............] - ETA: 39s  batch_loss: 3.1777 [Training] 129/214 [=================>............] - ETA: 38s  batch_loss: 3.1615 [Training] 130/214 [=================>............] - ETA: 38s  batch_loss: 3.1454 [Training] 131/214 [=================>............] - ETA: 37s  batch_loss: 3.1301 [Training] 132/214 [=================>............] - ETA: 37s  batch_loss: 3.1150 [Training] 133/214 [=================>............] - ETA: 36s  batch_loss: 3.0997 [Training] 134/214 [=================>............] - ETA: 36s  batch_loss: 3.0846 [Training] 135/214 [=================>............] - ETA: 35s  batch_loss: 3.0710 [Training] 136/214 [==================>...........] - ETA: 35s  batch_loss: 3.0564 [Training] 137/214 [==================>...........] - ETA: 34s  batch_loss: 3.0412 [Training] 138/214 [==================>...........] - ETA: 34s  batch_loss: 3.0259 [Training] 139/214 [==================>...........] - ETA: 34s  batch_loss: 3.0113 [Training] 140/214 [==================>...........] - ETA: 33s  batch_loss: 2.9968 [Training] 141/214 [==================>...........] - ETA: 33s  batch_loss: 2.9821 [Training] 142/214 [==================>...........] - ETA: 32s  batch_loss: 2.9677 [Training] 143/214 [===================>..........] - ETA: 32s  batch_loss: 2.9535 [Training] 144/214 [===================>..........] - ETA: 31s  batch_loss: 2.9396 [Training] 145/214 [===================>..........] - ETA: 31s  batch_loss: 2.9250 [Training] 146/214 [===================>..........] - ETA: 30s  batch_loss: 2.9104 [Training] 147/214 [===================>..........] - ETA: 30s  batch_loss: 2.8961 [Training] 148/214 [===================>..........] - ETA: 29s  batch_loss: 2.8820 [Training] 149/214 [===================>..........] - ETA: 29s  batch_loss: 2.8685 [Training] 150/214 [====================>.........] - ETA: 29s  batch_loss: 2.8546 [Training] 151/214 [====================>.........] - ETA: 28s  batch_loss: 2.8409 [Training] 152/214 [====================>.........] - ETA: 28s  batch_loss: 2.8275 [Training] 153/214 [====================>.........] - ETA: 27s  batch_loss: 2.8135 [Training] 154/214 [====================>.........] - ETA: 27s  batch_loss: 2.7994 [Training] 155/214 [====================>.........] - ETA: 26s  batch_loss: 2.7855 [Training] 156/214 [====================>.........] - ETA: 26s  batch_loss: 2.7720 [Training] 157/214 [=====================>........] - ETA: 25s  batch_loss: 2.7583 [Training] 158/214 [=====================>........] - ETA: 25s  batch_loss: 2.7453 [Training] 159/214 [=====================>........] - ETA: 24s  batch_loss: 2.7322 [Training] 160/214 [=====================>........] - ETA: 24s  batch_loss: 2.7193 [Training] 161/214 [=====================>........] - ETA: 24s  batch_loss: 2.7065 [Training] 162/214 [=====================>........] - ETA: 23s  batch_loss: 2.6932 [Training] 163/214 [=====================>........] - ETA: 23s  batch_loss: 2.6804 [Training] 164/214 [=====================>........] - ETA: 22s  batch_loss: 2.6678 [Training] 165/214 [======================>.......] - ETA: 22s  batch_loss: 2.6552 [Training] 166/214 [======================>.......] - ETA: 21s  batch_loss: 2.6430 [Training] 167/214 [======================>.......] - ETA: 21s  batch_loss: 2.6305 [Training] 168/214 [======================>.......] - ETA: 20s  batch_loss: 2.6182 [Training] 169/214 [======================>.......] - ETA: 20s  batch_loss: 2.6057 [Training] 170/214 [======================>.......] - ETA: 19s  batch_loss: 2.5935 [Training] 171/214 [======================>.......] - ETA: 19s  batch_loss: 2.5814 [Training] 172/214 [=======================>......] - ETA: 19s  batch_loss: 2.5693 [Training] 173/214 [=======================>......] - ETA: 18s  batch_loss: 2.5574 [Training] 174/214 [=======================>......] - ETA: 18s  batch_loss: 2.5458 [Training] 175/214 [=======================>......] - ETA: 17s  batch_loss: 2.5342 [Training] 176/214 [=======================>......] - ETA: 17s  batch_loss: 2.5230 [Training] 177/214 [=======================>......] - ETA: 16s  batch_loss: 2.5117 [Training] 178/214 [=======================>......] - ETA: 16s  batch_loss: 2.5007 [Training] 179/214 [========================>.....] - ETA: 15s  batch_loss: 2.4893 [Training] 180/214 [========================>.....] - ETA: 15s  batch_loss: 2.4780 [Training] 181/214 [========================>.....] - ETA: 14s  batch_loss: 2.4671 [Training] 182/214 [========================>.....] - ETA: 14s  batch_loss: 2.4561 [Training] 183/214 [========================>.....] - ETA: 14s  batch_loss: 2.4450 [Training] 184/214 [========================>.....] - ETA: 13s  batch_loss: 2.4344 [Training] 185/214 [========================>.....] - ETA: 13s  batch_loss: 2.4235 [Training] 186/214 [=========================>....] - ETA: 12s  batch_loss: 2.4129 [Training] 187/214 [=========================>....] - ETA: 12s  batch_loss: 2.4023 [Training] 188/214 [=========================>....] - ETA: 11s  batch_loss: 2.3919 [Training] 189/214 [=========================>....] - ETA: 11s  batch_loss: 2.3820 [Training] 190/214 [=========================>....] - ETA: 10s  batch_loss: 2.3717 [Training] 191/214 [=========================>....] - ETA: 10s  batch_loss: 2.3615 [Training] 192/214 [=========================>....] - ETA: 9s  batch_loss: 2.3517 [Training] 193/214 [==========================>...] - ETA: 9s  batch_loss: 2.3419 [Training] 194/214 [==========================>...] - ETA: 9s  batch_loss: 2.3320 [Training] 195/214 [==========================>...] - ETA: 8s  batch_loss: 2.3220 [Training] 196/214 [==========================>...] - ETA: 8s  batch_loss: 2.3125 [Training] 197/214 [==========================>...] - ETA: 7s  batch_loss: 2.3030 [Training] 198/214 [==========================>...] - ETA: 7s  batch_loss: 2.2936 [Training] 199/214 [==========================>...] - ETA: 6s  batch_loss: 2.2844 [Training] 200/214 [===========================>..] - ETA: 6s  batch_loss: 2.2752 [Training] 201/214 [===========================>..] - ETA: 5s  batch_loss: 2.2658 [Training] 202/214 [===========================>..] - ETA: 5s  batch_loss: 2.2567 [Training] 203/214 [===========================>..] - ETA: 4s  batch_loss: 2.2478 [Training] 204/214 [===========================>..] - ETA: 4s  batch_loss: 2.2389 [Training] 205/214 [===========================>..] - ETA: 4s  batch_loss: 2.2300 [Training] 206/214 [===========================>..] - ETA: 3s  batch_loss: 2.2208 [Training] 207/214 [============================>.] - ETA: 3s  batch_loss: 2.2118 [Training] 208/214 [============================>.] - ETA: 2s  batch_loss: 2.2033 [Training] 209/214 [============================>.] - ETA: 2s  batch_loss: 2.1946 [Training] 210/214 [============================>.] - ETA: 1s  batch_loss: 2.1861 [Training] 211/214 [============================>.] - ETA: 1s  batch_loss: 2.1774 [Training] 212/214 [============================>.] - ETA: 0s  batch_loss: 2.1691 [Training] 213/214 [============================>.] - ETA: 0s  batch_loss: 2.1607 [Training] 214/214 [==============================] 453.0ms/step  batch_loss: 2.1524 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/11/2022 15:00:04 - INFO - root -   The F1-score is 0.0
08/11/2022 15:00:04 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/214 [..............................] - ETA: 3:05  batch_loss: 0.3688 [Training] 2/214 [..............................] - ETA: 2:19  batch_loss: 0.3744 [Training] 3/214 [..............................] - ETA: 2:04  batch_loss: 0.3910 [Training] 4/214 [..............................] - ETA: 1:56  batch_loss: 0.3924 [Training] 5/214 [..............................] - ETA: 1:51  batch_loss: 0.4117 [Training] 6/214 [..............................] - ETA: 1:46  batch_loss: 0.4093 [Training] 7/214 [..............................] - ETA: 1:44  batch_loss: 0.4053 [Training] 8/214 [>.............................] - ETA: 1:43  batch_loss: 0.4045 [Training] 9/214 [>.............................] - ETA: 1:41  batch_loss: 0.4033 [Training] 10/214 [>.............................] - ETA: 1:40  batch_loss: 0.4061 [Training] 11/214 [>.............................] - ETA: 1:39  batch_loss: 0.4013 [Training] 12/214 [>.............................] - ETA: 1:38  batch_loss: 0.4000 [Training] 13/214 [>.............................] - ETA: 1:37  batch_loss: 0.3988 [Training] 14/214 [>.............................] - ETA: 1:36  batch_loss: 0.4097 [Training] 15/214 [=>............................] - ETA: 1:35  batch_loss: 0.4086 [Training] 16/214 [=>............................] - ETA: 1:34  batch_loss: 0.4099 [Training] 17/214 [=>............................] - ETA: 1:33  batch_loss: 0.4091 [Training] 18/214 [=>............................] - ETA: 1:33  batch_loss: 0.4093 [Training] 19/214 [=>............................] - ETA: 1:32  batch_loss: 0.4062 [Training] 20/214 [=>............................] - ETA: 1:31  batch_loss: 0.4055 [Training] 21/214 [=>............................] - ETA: 1:31  batch_loss: 0.4035 [Training] 22/214 [==>...........................] - ETA: 1:30  batch_loss: 0.4026 [Training] 23/214 [==>...........................] - ETA: 1:30  batch_loss: 0.4025 [Training] 24/214 [==>...........................] - ETA: 1:29  batch_loss: 0.4012 [Training] 25/214 [==>...........................] - ETA: 1:29  batch_loss: 0.3982 [Training] 26/214 [==>...........................] - ETA: 1:28  batch_loss: 0.3968 [Training] 27/214 [==>...........................] - ETA: 1:27  batch_loss: 0.3958 [Training] 28/214 [==>...........................] - ETA: 1:27  batch_loss: 0.3943 [Training] 29/214 [===>..........................] - ETA: 1:26  batch_loss: 0.3928 [Training] 30/214 [===>..........................] - ETA: 1:26  batch_loss: 0.3933 [Training] 31/214 [===>..........................] - ETA: 1:25  batch_loss: 0.3917 [Training] 32/214 [===>..........................] - ETA: 1:25  batch_loss: 0.3914 [Training] 33/214 [===>..........................] - ETA: 1:24  batch_loss: 0.3917 [Training] 34/214 [===>..........................] - ETA: 1:24  batch_loss: 0.3914 [Training] 35/214 [===>..........................] - ETA: 1:23  batch_loss: 0.3901 [Training] 36/214 [====>.........................] - ETA: 1:23  batch_loss: 0.3894 [Training] 37/214 [====>.........................] - ETA: 1:22  batch_loss: 0.3880 [Training] 38/214 [====>.........................] - ETA: 1:22  batch_loss: 0.3877 [Training] 39/214 [====>.........................] - ETA: 1:21  batch_loss: 0.3877 [Training] 40/214 [====>.........................] - ETA: 1:21  batch_loss: 0.3861 [Training] 41/214 [====>.........................] - ETA: 1:20  batch_loss: 0.3845 [Training] 42/214 [====>.........................] - ETA: 1:20  batch_loss: 0.3841 [Training] 43/214 [=====>........................] - ETA: 1:19  batch_loss: 0.3852 [Training] 44/214 [=====>........................] - ETA: 1:19  batch_loss: 0.3865 [Training] 45/214 [=====>........................] - ETA: 1:18  batch_loss: 0.3860 [Training] 46/214 [=====>........................] - ETA: 1:17  batch_loss: 0.3853 [Training] 47/214 [=====>........................] - ETA: 1:17  batch_loss: 0.3853 [Training] 48/214 [=====>........................] - ETA: 1:17  batch_loss: 0.3840 [Training] 49/214 [=====>........................] - ETA: 1:16  batch_loss: 0.3830 [Training] 50/214 [======>.......................] - ETA: 1:16  batch_loss: 0.3819 [Training] 51/214 [======>.......................] - ETA: 1:15  batch_loss: 0.3819 [Training] 52/214 [======>.......................] - ETA: 1:15  batch_loss: 0.3821 [Training] 53/214 [======>.......................] - ETA: 1:14  batch_loss: 0.3815 [Training] 54/214 [======>.......................] - ETA: 1:14  batch_loss: 0.3809 [Training] 55/214 [======>.......................] - ETA: 1:13  batch_loss: 0.3799 [Training] 56/214 [======>.......................] - ETA: 1:13  batch_loss: 0.3809 [Training] 57/214 [======>.......................] - ETA: 1:12  batch_loss: 0.3804 [Training] 58/214 [=======>......................] - ETA: 1:12  batch_loss: 0.3801 [Training] 59/214 [=======>......................] - ETA: 1:11  batch_loss: 0.3794 [Training] 60/214 [=======>......................] - ETA: 1:11  batch_loss: 0.3786 [Training] 61/214 [=======>......................] - ETA: 1:10  batch_loss: 0.3786 [Training] 62/214 [=======>......................] - ETA: 1:10  batch_loss: 0.3778 [Training] 63/214 [=======>......................] - ETA: 1:09  batch_loss: 0.3774 [Training] 64/214 [=======>......................] - ETA: 1:09  batch_loss: 0.3769 [Training] 65/214 [========>.....................] - ETA: 1:08  batch_loss: 0.3766 [Training] 66/214 [========>.....................] - ETA: 1:08  batch_loss: 0.3759 [Training] 67/214 [========>.....................] - ETA: 1:08  batch_loss: 0.3764 [Training] 68/214 [========>.....................] - ETA: 1:07  batch_loss: 0.3766 [Training] 69/214 [========>.....................] - ETA: 1:07  batch_loss: 0.3768 [Training] 70/214 [========>.....................] - ETA: 1:06  batch_loss: 0.3767 [Training] 71/214 [========>.....................] - ETA: 1:06  batch_loss: 0.3762 [Training] 72/214 [=========>....................] - ETA: 1:05  batch_loss: 0.3757 [Training] 73/214 [=========>....................] - ETA: 1:05  batch_loss: 0.3761 [Training] 74/214 [=========>....................] - ETA: 1:04  batch_loss: 0.3755 [Training] 75/214 [=========>....................] - ETA: 1:04  batch_loss: 0.3747 [Training] 76/214 [=========>....................] - ETA: 1:03  batch_loss: 0.3745 [Training] 77/214 [=========>....................] - ETA: 1:03  batch_loss: 0.3735 [Training] 78/214 [=========>....................] - ETA: 1:02  batch_loss: 0.3727 [Training] 79/214 [==========>...................] - ETA: 1:02  batch_loss: 0.3724 [Training] 80/214 [==========>...................] - ETA: 1:01  batch_loss: 0.3717 [Training] 81/214 [==========>...................] - ETA: 1:01  batch_loss: 0.3714 [Training] 82/214 [==========>...................] - ETA: 1:00  batch_loss: 0.3708 [Training] 83/214 [==========>...................] - ETA: 1:00  batch_loss: 0.3704 [Training] 84/214 [==========>...................] - ETA: 1:00  batch_loss: 0.3701 [Training] 85/214 [==========>...................] - ETA: 59s  batch_loss: 0.3705 [Training] 86/214 [===========>..................] - ETA: 59s  batch_loss: 0.3702 [Training] 87/214 [===========>..................] - ETA: 58s  batch_loss: 0.3698 [Training] 88/214 [===========>..................] - ETA: 58s  batch_loss: 0.3691 [Training] 89/214 [===========>..................] - ETA: 57s  batch_loss: 0.3689 [Training] 90/214 [===========>..................] - ETA: 57s  batch_loss: 0.3687 [Training] 91/214 [===========>..................] - ETA: 56s  batch_loss: 0.3686 [Training] 92/214 [===========>..................] - ETA: 56s  batch_loss: 0.3681 [Training] 93/214 [============>.................] - ETA: 55s  batch_loss: 0.3680 [Training] 94/214 [============>.................] - ETA: 55s  batch_loss: 0.3678 [Training] 95/214 [============>.................] - ETA: 54s  batch_loss: 0.3674 [Training] 96/214 [============>.................] - ETA: 54s  batch_loss: 0.3668 [Training] 97/214 [============>.................] - ETA: 54s  batch_loss: 0.3668 [Training] 98/214 [============>.................] - ETA: 53s  batch_loss: 0.3664 [Training] 99/214 [============>.................] - ETA: 53s  batch_loss: 0.3659 [Training] 100/214 [=============>................] - ETA: 52s  batch_loss: 0.3660 [Training] 101/214 [=============>................] - ETA: 52s  batch_loss: 0.3661 [Training] 102/214 [=============>................] - ETA: 51s  batch_loss: 0.3664 [Training] 103/214 [=============>................] - ETA: 51s  batch_loss: 0.3660 [Training] 104/214 [=============>................] - ETA: 50s  batch_loss: 0.3661 [Training] 105/214 [=============>................] - ETA: 50s  batch_loss: 0.3663 [Training] 106/214 [=============>................] - ETA: 49s  batch_loss: 0.3659 [Training] 107/214 [==============>...............] - ETA: 49s  batch_loss: 0.3656 [Training] 108/214 [==============>...............] - ETA: 48s  batch_loss: 0.3654 [Training] 109/214 [==============>...............] - ETA: 48s  batch_loss: 0.3654 [Training] 110/214 [==============>...............] - ETA: 47s  batch_loss: 0.3655 [Training] 111/214 [==============>...............] - ETA: 47s  batch_loss: 0.3653 [Training] 112/214 [==============>...............] - ETA: 47s  batch_loss: 0.3649 [Training] 113/214 [==============>...............] - ETA: 46s  batch_loss: 0.3647 [Training] 114/214 [==============>...............] - ETA: 46s  batch_loss: 0.3645 [Training] 115/214 [===============>..............] - ETA: 45s  batch_loss: 0.3638 [Training] 116/214 [===============>..............] - ETA: 45s  batch_loss: 0.3641 [Training] 117/214 [===============>..............] - ETA: 44s  batch_loss: 0.3637 [Training] 118/214 [===============>..............] - ETA: 44s  batch_loss: 0.3634 [Training] 119/214 [===============>..............] - ETA: 43s  batch_loss: 0.3630 [Training] 120/214 [===============>..............] - ETA: 43s  batch_loss: 0.3626 [Training] 121/214 [===============>..............] - ETA: 42s  batch_loss: 0.3627 [Training] 122/214 [================>.............] - ETA: 42s  batch_loss: 0.3621 [Training] 123/214 [================>.............] - ETA: 41s  batch_loss: 0.3620 [Training] 124/214 [================>.............] - ETA: 41s  batch_loss: 0.3619 [Training] 125/214 [================>.............] - ETA: 40s  batch_loss: 0.3617 [Training] 126/214 [================>.............] - ETA: 40s  batch_loss: 0.3621 [Training] 127/214 [================>.............] - ETA: 40s  batch_loss: 0.3623 [Training] 128/214 [================>.............] - ETA: 39s  batch_loss: 0.3622 [Training] 129/214 [=================>............] - ETA: 39s  batch_loss: 0.3617 [Training] 130/214 [=================>............] - ETA: 38s  batch_loss: 0.3613 [Training] 131/214 [=================>............] - ETA: 38s  batch_loss: 0.3608 [Training] 132/214 [=================>............] - ETA: 37s  batch_loss: 0.3605 [Training] 133/214 [=================>............] - ETA: 37s  batch_loss: 0.3604 [Training] 134/214 [=================>............] - ETA: 36s  batch_loss: 0.3605 [Training] 135/214 [=================>............] - ETA: 36s  batch_loss: 0.3607 [Training] 136/214 [==================>...........] - ETA: 35s  batch_loss: 0.3607 [Training] 137/214 [==================>...........] - ETA: 35s  batch_loss: 0.3605 [Training] 138/214 [==================>...........] - ETA: 35s  batch_loss: 0.3605 [Training] 139/214 [==================>...........] - ETA: 34s  batch_loss: 0.3603 [Training] 140/214 [==================>...........] - ETA: 34s  batch_loss: 0.3601 [Training] 141/214 [==================>...........] - ETA: 33s  batch_loss: 0.3601 [Training] 142/214 [==================>...........] - ETA: 33s  batch_loss: 0.3599 [Training] 143/214 [===================>..........] - ETA: 32s  batch_loss: 0.3594 [Training] 144/214 [===================>..........] - ETA: 32s  batch_loss: 0.3591 [Training] 145/214 [===================>..........] - ETA: 31s  batch_loss: 0.3587 [Training] 146/214 [===================>..........] - ETA: 31s  batch_loss: 0.3581 [Training] 147/214 [===================>..........] - ETA: 30s  batch_loss: 0.3578 [Training] 148/214 [===================>..........] - ETA: 30s  batch_loss: 0.3575 [Training] 149/214 [===================>..........] - ETA: 29s  batch_loss: 0.3571 [Training] 150/214 [====================>.........] - ETA: 29s  batch_loss: 0.3571 [Training] 151/214 [====================>.........] - ETA: 28s  batch_loss: 0.3569 [Training] 152/214 [====================>.........] - ETA: 28s  batch_loss: 0.3565 [Training] 153/214 [====================>.........] - ETA: 28s  batch_loss: 0.3561 [Training] 154/214 [====================>.........] - ETA: 27s  batch_loss: 0.3557 [Training] 155/214 [====================>.........] - ETA: 27s  batch_loss: 0.3552 [Training] 156/214 [====================>.........] - ETA: 26s  batch_loss: 0.3551 [Training] 157/214 [=====================>........] - ETA: 26s  batch_loss: 0.3551 [Training] 158/214 [=====================>........] - ETA: 25s  batch_loss: 0.3549 [Training] 159/214 [=====================>........] - ETA: 25s  batch_loss: 0.3548 [Training] 160/214 [=====================>........] - ETA: 24s  batch_loss: 0.3543 [Training] 161/214 [=====================>........] - ETA: 24s  batch_loss: 0.3542 [Training] 162/214 [=====================>........] - ETA: 23s  batch_loss: 0.3542 [Training] 163/214 [=====================>........] - ETA: 23s  batch_loss: 0.3543 [Training] 164/214 [=====================>........] - ETA: 22s  batch_loss: 0.3538 [Training] 165/214 [======================>.......] - ETA: 22s  batch_loss: 0.3535 [Training] 166/214 [======================>.......] - ETA: 22s  batch_loss: 0.3532 [Training] 167/214 [======================>.......] - ETA: 21s  batch_loss: 0.3529 [Training] 168/214 [======================>.......] - ETA: 21s  batch_loss: 0.3523 [Training] 169/214 [======================>.......] - ETA: 20s  batch_loss: 0.3519 [Training] 170/214 [======================>.......] - ETA: 20s  batch_loss: 0.3515 [Training] 171/214 [======================>.......] - ETA: 19s  batch_loss: 0.3510 [Training] 172/214 [=======================>......] - ETA: 19s  batch_loss: 0.3507 [Training] 173/214 [=======================>......] - ETA: 18s  batch_loss: 0.3509 [Training] 174/214 [=======================>......] - ETA: 18s  batch_loss: 0.3507 [Training] 175/214 [=======================>......] - ETA: 17s  batch_loss: 0.3500 [Training] 176/214 [=======================>......] - ETA: 17s  batch_loss: 0.3497 [Training] 177/214 [=======================>......] - ETA: 17s  batch_loss: 0.3491 [Training] 178/214 [=======================>......] - ETA: 16s  batch_loss: 0.3485 [Training] 179/214 [========================>.....] - ETA: 16s  batch_loss: 0.3483 [Training] 180/214 [========================>.....] - ETA: 15s  batch_loss: 0.3482 [Training] 181/214 [========================>.....] - ETA: 15s  batch_loss: 0.3478 [Training] 182/214 [========================>.....] - ETA: 14s  batch_loss: 0.3477 [Training] 183/214 [========================>.....] - ETA: 14s  batch_loss: 0.3475 [Training] 184/214 [========================>.....] - ETA: 13s  batch_loss: 0.3471 [Training] 185/214 [========================>.....] - ETA: 13s  batch_loss: 0.3469 [Training] 186/214 [=========================>....] - ETA: 12s  batch_loss: 0.3463 [Training] 187/214 [=========================>....] - ETA: 12s  batch_loss: 0.3458 [Training] 188/214 [=========================>....] - ETA: 11s  batch_loss: 0.3457 [Training] 189/214 [=========================>....] - ETA: 11s  batch_loss: 0.3455 [Training] 190/214 [=========================>....] - ETA: 11s  batch_loss: 0.3451 [Training] 191/214 [=========================>....] - ETA: 10s  batch_loss: 0.3446 [Training] 192/214 [=========================>....] - ETA: 10s  batch_loss: 0.3442 [Training] 193/214 [==========================>...] - ETA: 9s  batch_loss: 0.3441 [Training] 194/214 [==========================>...] - ETA: 9s  batch_loss: 0.3439 [Training] 195/214 [==========================>...] - ETA: 8s  batch_loss: 0.3434 [Training] 196/214 [==========================>...] - ETA: 8s  batch_loss: 0.3431 [Training] 197/214 [==========================>...] - ETA: 7s  batch_loss: 0.3428 [Training] 198/214 [==========================>...] - ETA: 7s  batch_loss: 0.3426 [Training] 199/214 [==========================>...] - ETA: 6s  batch_loss: 0.3422 [Training] 200/214 [===========================>..] - ETA: 6s  batch_loss: 0.3422 [Training] 201/214 [===========================>..] - ETA: 5s  batch_loss: 0.3418 [Training] 202/214 [===========================>..] - ETA: 5s  batch_loss: 0.3414 [Training] 203/214 [===========================>..] - ETA: 5s  batch_loss: 0.3413 [Training] 204/214 [===========================>..] - ETA: 4s  batch_loss: 0.3412 [Training] 205/214 [===========================>..] - ETA: 4s  batch_loss: 0.3411 [Training] 206/214 [===========================>..] - ETA: 3s  batch_loss: 0.3405 [Training] 207/214 [============================>.] - ETA: 3s  batch_loss: 0.3401 [Training] 208/214 [============================>.] - ETA: 2s  batch_loss: 0.3399 [Training] 209/214 [============================>.] - ETA: 2s  batch_loss: 0.3393 [Training] 210/214 [============================>.] - ETA: 1s  batch_loss: 0.3389 [Training] 211/214 [============================>.] - ETA: 1s  batch_loss: 0.3386 [Training] 212/214 [============================>.] - ETA: 0s  batch_loss: 0.3383 [Training] 213/214 [============================>.] - ETA: 0s  batch_loss: 0.3378 [Training] 214/214 [==============================] 458.1ms/step  batch_loss: 0.3376 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/11/2022 15:01:48 - INFO - root -   The F1-score is 0.0
08/11/2022 15:01:48 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/214 [..............................] - ETA: 3:16  batch_loss: 0.2784 [Training] 2/214 [..............................] - ETA: 2:26  batch_loss: 0.2826 [Training] 3/214 [..............................] - ETA: 2:10  batch_loss: 0.2808 [Training] 4/214 [..............................] - ETA: 2:01  batch_loss: 0.2898 [Training] 5/214 [..............................] - ETA: 1:55  batch_loss: 0.2788 [Training] 6/214 [..............................] - ETA: 1:51  batch_loss: 0.2751 [Training] 7/214 [..............................] - ETA: 1:48  batch_loss: 0.2696 [Training] 8/214 [>.............................] - ETA: 1:47  batch_loss: 0.2659 [Training] 9/214 [>.............................] - ETA: 1:45  batch_loss: 0.2616 [Training] 10/214 [>.............................] - ETA: 1:43  batch_loss: 0.2602 [Training] 11/214 [>.............................] - ETA: 1:42  batch_loss: 0.2593 [Training] 12/214 [>.............................] - ETA: 1:40  batch_loss: 0.2617 [Training] 13/214 [>.............................] - ETA: 1:38  batch_loss: 0.2624 [Training] 14/214 [>.............................] - ETA: 1:37  batch_loss: 0.2608 [Training] 15/214 [=>............................] - ETA: 1:36  batch_loss: 0.2613 [Training] 16/214 [=>............................] - ETA: 1:36  batch_loss: 0.2646 [Training] 17/214 [=>............................] - ETA: 1:35  batch_loss: 0.2637 [Training] 18/214 [=>............................] - ETA: 1:34  batch_loss: 0.2660 [Training] 19/214 [=>............................] - ETA: 1:33  batch_loss: 0.2665 [Training] 20/214 [=>............................] - ETA: 1:33  batch_loss: 0.2639 [Training] 21/214 [=>............................] - ETA: 1:32  batch_loss: 0.2616 [Training] 22/214 [==>...........................] - ETA: 1:31  batch_loss: 0.2620 [Training] 23/214 [==>...........................] - ETA: 1:31  batch_loss: 0.2633 [Training] 24/214 [==>...........................] - ETA: 1:30  batch_loss: 0.2615 [Training] 25/214 [==>...........................] - ETA: 1:30  batch_loss: 0.2614 [Training] 26/214 [==>...........................] - ETA: 1:29  batch_loss: 0.2591 [Training] 27/214 [==>...........................] - ETA: 1:28  batch_loss: 0.2604 [Training] 28/214 [==>...........................] - ETA: 1:28  batch_loss: 0.2595 [Training] 29/214 [===>..........................] - ETA: 1:27  batch_loss: 0.2595 [Training] 30/214 [===>..........................] - ETA: 1:26  batch_loss: 0.2596 [Training] 31/214 [===>..........................] - ETA: 1:26  batch_loss: 0.2594 [Training] 32/214 [===>..........................] - ETA: 1:25  batch_loss: 0.2594 [Training] 33/214 [===>..........................] - ETA: 1:25  batch_loss: 0.2579 [Training] 34/214 [===>..........................] - ETA: 1:24  batch_loss: 0.2596 [Training] 35/214 [===>..........................] - ETA: 1:24  batch_loss: 0.2590 [Training] 36/214 [====>.........................] - ETA: 1:23  batch_loss: 0.2588 [Training] 37/214 [====>.........................] - ETA: 1:23  batch_loss: 0.2586 [Training] 38/214 [====>.........................] - ETA: 1:22  batch_loss: 0.2586 [Training] 39/214 [====>.........................] - ETA: 1:22  batch_loss: 0.2587 [Training] 40/214 [====>.........................] - ETA: 1:21  batch_loss: 0.2581 [Training] 41/214 [====>.........................] - ETA: 1:21  batch_loss: 0.2581 [Training] 42/214 [====>.........................] - ETA: 1:20  batch_loss: 0.2585 [Training] 43/214 [=====>........................] - ETA: 1:20  batch_loss: 0.2580 [Training] 44/214 [=====>........................] - ETA: 1:19  batch_loss: 0.2568 [Training] 45/214 [=====>........................] - ETA: 1:19  batch_loss: 0.2564 [Training] 46/214 [=====>........................] - ETA: 1:18  batch_loss: 0.2565 [Training] 47/214 [=====>........................] - ETA: 1:18  batch_loss: 0.2566 [Training] 48/214 [=====>........................] - ETA: 1:17  batch_loss: 0.2561 [Training] 49/214 [=====>........................] - ETA: 1:17  batch_loss: 0.2556 [Training] 50/214 [======>.......................] - ETA: 1:16  batch_loss: 0.2553 [Training] 51/214 [======>.......................] - ETA: 1:16  batch_loss: 0.2553 [Training] 52/214 [======>.......................] - ETA: 1:15  batch_loss: 0.2550 [Training] 53/214 [======>.......................] - ETA: 1:15  batch_loss: 0.2542 [Training] 54/214 [======>.......................] - ETA: 1:14  batch_loss: 0.2538 [Training] 55/214 [======>.......................] - ETA: 1:14  batch_loss: 0.2550 [Training] 56/214 [======>.......................] - ETA: 1:13  batch_loss: 0.2553 [Training] 57/214 [======>.......................] - ETA: 1:13  batch_loss: 0.2549 [Training] 58/214 [=======>......................] - ETA: 1:12  batch_loss: 0.2545 [Training] 59/214 [=======>......................] - ETA: 1:12  batch_loss: 0.2546 [Training] 60/214 [=======>......................] - ETA: 1:11  batch_loss: 0.2543 [Training] 61/214 [=======>......................] - ETA: 1:11  batch_loss: 0.2544 [Training] 62/214 [=======>......................] - ETA: 1:10  batch_loss: 0.2541 [Training] 63/214 [=======>......................] - ETA: 1:10  batch_loss: 0.2540 [Training] 64/214 [=======>......................] - ETA: 1:09  batch_loss: 0.2536 [Training] 65/214 [========>.....................] - ETA: 1:09  batch_loss: 0.2533 [Training] 66/214 [========>.....................] - ETA: 1:08  batch_loss: 0.2526 [Training] 67/214 [========>.....................] - ETA: 1:08  batch_loss: 0.2520 [Training] 68/214 [========>.....................] - ETA: 1:08  batch_loss: 0.2519 [Training] 69/214 [========>.....................] - ETA: 1:07  batch_loss: 0.2517 [Training] 70/214 [========>.....................] - ETA: 1:07  batch_loss: 0.2517 [Training] 71/214 [========>.....................] - ETA: 1:06  batch_loss: 0.2513 [Training] 72/214 [=========>....................] - ETA: 1:06  batch_loss: 0.2509 [Training] 73/214 [=========>....................] - ETA: 1:05  batch_loss: 0.2506 [Training] 74/214 [=========>....................] - ETA: 1:05  batch_loss: 0.2510 [Training] 75/214 [=========>....................] - ETA: 1:04  batch_loss: 0.2504 [Training] 76/214 [=========>....................] - ETA: 1:04  batch_loss: 0.2502 [Training] 77/214 [=========>....................] - ETA: 1:03  batch_loss: 0.2499 [Training] 78/214 [=========>....................] - ETA: 1:03  batch_loss: 0.2497 [Training] 79/214 [==========>...................] - ETA: 1:02  batch_loss: 0.2492 [Training] 80/214 [==========>...................] - ETA: 1:02  batch_loss: 0.2488 [Training] 81/214 [==========>...................] - ETA: 1:01  batch_loss: 0.2493 [Training] 82/214 [==========>...................] - ETA: 1:01  batch_loss: 0.2493 [Training] 83/214 [==========>...................] - ETA: 1:00  batch_loss: 0.2489 [Training] 84/214 [==========>...................] - ETA: 1:00  batch_loss: 0.2487 [Training] 85/214 [==========>...................] - ETA: 59s  batch_loss: 0.2481 [Training] 86/214 [===========>..................] - ETA: 59s  batch_loss: 0.2479 [Training] 87/214 [===========>..................] - ETA: 59s  batch_loss: 0.2477 [Training] 88/214 [===========>..................] - ETA: 58s  batch_loss: 0.2472 [Training] 89/214 [===========>..................] - ETA: 58s  batch_loss: 0.2469 [Training] 90/214 [===========>..................] - ETA: 57s  batch_loss: 0.2464 [Training] 91/214 [===========>..................] - ETA: 57s  batch_loss: 0.2458 [Training] 92/214 [===========>..................] - ETA: 56s  batch_loss: 0.2455 [Training] 93/214 [============>.................] - ETA: 56s  batch_loss: 0.2453 [Training] 94/214 [============>.................] - ETA: 55s  batch_loss: 0.2453 [Training] 95/214 [============>.................] - ETA: 55s  batch_loss: 0.2450 [Training] 96/214 [============>.................] - ETA: 54s  batch_loss: 0.2454 [Training] 97/214 [============>.................] - ETA: 54s  batch_loss: 0.2450 [Training] 98/214 [============>.................] - ETA: 53s  batch_loss: 0.2449 [Training] 99/214 [============>.................] - ETA: 53s  batch_loss: 0.2444 [Training] 100/214 [=============>................] - ETA: 52s  batch_loss: 0.2443 [Training] 101/214 [=============>................] - ETA: 52s  batch_loss: 0.2437 [Training] 102/214 [=============>................] - ETA: 52s  batch_loss: 0.2434 [Training] 103/214 [=============>................] - ETA: 51s  batch_loss: 0.2430 [Training] 104/214 [=============>................] - ETA: 51s  batch_loss: 0.2430 [Training] 105/214 [=============>................] - ETA: 50s  batch_loss: 0.2428 [Training] 106/214 [=============>................] - ETA: 50s  batch_loss: 0.2425 [Training] 107/214 [==============>...............] - ETA: 49s  batch_loss: 0.2422 [Training] 108/214 [==============>...............] - ETA: 49s  batch_loss: 0.2426 [Training] 109/214 [==============>...............] - ETA: 48s  batch_loss: 0.2426 [Training] 110/214 [==============>...............] - ETA: 48s  batch_loss: 0.2425 [Training] 111/214 [==============>...............] - ETA: 47s  batch_loss: 0.2426 [Training] 112/214 [==============>...............] - ETA: 47s  batch_loss: 0.2422 [Training] 113/214 [==============>...............] - ETA: 46s  batch_loss: 0.2426 [Training] 114/214 [==============>...............] - ETA: 46s  batch_loss: 0.2426 [Training] 115/214 [===============>..............] - ETA: 45s  batch_loss: 0.2423 [Training] 116/214 [===============>..............] - ETA: 45s  batch_loss: 0.2424 [Training] 117/214 [===============>..............] - ETA: 44s  batch_loss: 0.2423 [Training] 118/214 [===============>..............] - ETA: 44s  batch_loss: 0.2419 [Training] 119/214 [===============>..............] - ETA: 44s  batch_loss: 0.2416 [Training] 120/214 [===============>..............] - ETA: 43s  batch_loss: 0.2414 [Training] 121/214 [===============>..............] - ETA: 43s  batch_loss: 0.2411 [Training] 122/214 [================>.............] - ETA: 42s  batch_loss: 0.2408 [Training] 123/214 [================>.............] - ETA: 42s  batch_loss: 0.2406 [Training] 124/214 [================>.............] - ETA: 41s  batch_loss: 0.2403 [Training] 125/214 [================>.............] - ETA: 41s  batch_loss: 0.2401 [Training] 126/214 [================>.............] - ETA: 40s  batch_loss: 0.2400 [Training] 127/214 [================>.............] - ETA: 40s  batch_loss: 0.2398 [Training] 128/214 [================>.............] - ETA: 39s  batch_loss: 0.2393 [Training] 129/214 [=================>............] - ETA: 39s  batch_loss: 0.2390 [Training] 130/214 [=================>............] - ETA: 38s  batch_loss: 0.2388 [Training] 131/214 [=================>............] - ETA: 38s  batch_loss: 0.2385 [Training] 132/214 [=================>............] - ETA: 38s  batch_loss: 0.2382 [Training] 133/214 [=================>............] - ETA: 37s  batch_loss: 0.2378 [Training] 134/214 [=================>............] - ETA: 37s  batch_loss: 0.2380 [Training] 135/214 [=================>............] - ETA: 36s  batch_loss: 0.2377 [Training] 136/214 [==================>...........] - ETA: 36s  batch_loss: 0.2372 [Training] 137/214 [==================>...........] - ETA: 35s  batch_loss: 0.2370 [Training] 138/214 [==================>...........] - ETA: 35s  batch_loss: 0.2369 [Training] 139/214 [==================>...........] - ETA: 34s  batch_loss: 0.2369 [Training] 140/214 [==================>...........] - ETA: 34s  batch_loss: 0.2365 [Training] 141/214 [==================>...........] - ETA: 33s  batch_loss: 0.2364 [Training] 142/214 [==================>...........] - ETA: 33s  batch_loss: 0.2365 [Training] 143/214 [===================>..........] - ETA: 32s  batch_loss: 0.2366 [Training] 144/214 [===================>..........] - ETA: 32s  batch_loss: 0.2364 [Training] 145/214 [===================>..........] - ETA: 31s  batch_loss: 0.2360 [Training] 146/214 [===================>..........] - ETA: 31s  batch_loss: 0.2359 [Training] 147/214 [===================>..........] - ETA: 31s  batch_loss: 0.2359 [Training] 148/214 [===================>..........] - ETA: 30s  batch_loss: 0.2355 [Training] 149/214 [===================>..........] - ETA: 30s  batch_loss: 0.2354 [Training] 150/214 [====================>.........] - ETA: 29s  batch_loss: 0.2351 [Training] 151/214 [====================>.........] - ETA: 29s  batch_loss: 0.2349 [Training] 152/214 [====================>.........] - ETA: 28s  batch_loss: 0.2349 [Training] 153/214 [====================>.........] - ETA: 28s  batch_loss: 0.2350 [Training] 154/214 [====================>.........] - ETA: 27s  batch_loss: 0.2349 [Training] 155/214 [====================>.........] - ETA: 27s  batch_loss: 0.2347 [Training] 156/214 [====================>.........] - ETA: 26s  batch_loss: 0.2344 [Training] 157/214 [=====================>........] - ETA: 26s  batch_loss: 0.2341 [Training] 158/214 [=====================>........] - ETA: 25s  batch_loss: 0.2338 [Training] 159/214 [=====================>........] - ETA: 25s  batch_loss: 0.2339 [Training] 160/214 [=====================>........] - ETA: 25s  batch_loss: 0.2338 [Training] 161/214 [=====================>........] - ETA: 24s  batch_loss: 0.2334 [Training] 162/214 [=====================>........] - ETA: 24s  batch_loss: 0.2332 [Training] 163/214 [=====================>........] - ETA: 23s  batch_loss: 0.2330 [Training] 164/214 [=====================>........] - ETA: 23s  batch_loss: 0.2330 [Training] 165/214 [======================>.......] - ETA: 22s  batch_loss: 0.2335 [Training] 166/214 [======================>.......] - ETA: 22s  batch_loss: 0.2333 [Training] 167/214 [======================>.......] - ETA: 21s  batch_loss: 0.2333 [Training] 168/214 [======================>.......] - ETA: 21s  batch_loss: 0.2329 [Training] 169/214 [======================>.......] - ETA: 20s  batch_loss: 0.2329 [Training] 170/214 [======================>.......] - ETA: 20s  batch_loss: 0.2333 [Training] 171/214 [======================>.......] - ETA: 19s  batch_loss: 0.2330 [Training] 172/214 [=======================>......] - ETA: 19s  batch_loss: 0.2329 [Training] 173/214 [=======================>......] - ETA: 18s  batch_loss: 0.2329 [Training] 174/214 [=======================>......] - ETA: 18s  batch_loss: 0.2327 [Training] 175/214 [=======================>......] - ETA: 18s  batch_loss: 0.2325 [Training] 176/214 [=======================>......] - ETA: 17s  batch_loss: 0.2324 [Training] 177/214 [=======================>......] - ETA: 17s  batch_loss: 0.2323 [Training] 178/214 [=======================>......] - ETA: 16s  batch_loss: 0.2323 [Training] 179/214 [========================>.....] - ETA: 16s  batch_loss: 0.2321 [Training] 180/214 [========================>.....] - ETA: 15s  batch_loss: 0.2319 [Training] 181/214 [========================>.....] - ETA: 15s  batch_loss: 0.2317 [Training] 182/214 [========================>.....] - ETA: 14s  batch_loss: 0.2316 [Training] 183/214 [========================>.....] - ETA: 14s  batch_loss: 0.2314 [Training] 184/214 [========================>.....] - ETA: 13s  batch_loss: 0.2313 [Training] 185/214 [========================>.....] - ETA: 13s  batch_loss: 0.2310 [Training] 186/214 [=========================>....] - ETA: 12s  batch_loss: 0.2309 [Training] 187/214 [=========================>....] - ETA: 12s  batch_loss: 0.2309 [Training] 188/214 [=========================>....] - ETA: 12s  batch_loss: 0.2308 [Training] 189/214 [=========================>....] - ETA: 11s  batch_loss: 0.2307 [Training] 190/214 [=========================>....] - ETA: 11s  batch_loss: 0.2305 [Training] 191/214 [=========================>....] - ETA: 10s  batch_loss: 0.2302 [Training] 192/214 [=========================>....] - ETA: 10s  batch_loss: 0.2299 [Training] 193/214 [==========================>...] - ETA: 9s  batch_loss: 0.2295 [Training] 194/214 [==========================>...] - ETA: 9s  batch_loss: 0.2294 [Training] 195/214 [==========================>...] - ETA: 8s  batch_loss: 0.2292 [Training] 196/214 [==========================>...] - ETA: 8s  batch_loss: 0.2292 [Training] 197/214 [==========================>...] - ETA: 7s  batch_loss: 0.2291 [Training] 198/214 [==========================>...] - ETA: 7s  batch_loss: 0.2290 [Training] 199/214 [==========================>...] - ETA: 6s  batch_loss: 0.2291 [Training] 200/214 [===========================>..] - ETA: 6s  batch_loss: 0.2290 [Training] 201/214 [===========================>..] - ETA: 6s  batch_loss: 0.2286 [Training] 202/214 [===========================>..] - ETA: 5s  batch_loss: 0.2284 [Training] 203/214 [===========================>..] - ETA: 5s  batch_loss: 0.2284 [Training] 204/214 [===========================>..] - ETA: 4s  batch_loss: 0.2282 [Training] 205/214 [===========================>..] - ETA: 4s  batch_loss: 0.2281 [Training] 206/214 [===========================>..] - ETA: 3s  batch_loss: 0.2279 [Training] 207/214 [============================>.] - ETA: 3s  batch_loss: 0.2280 [Training] 208/214 [============================>.] - ETA: 2s  batch_loss: 0.2279 [Training] 209/214 [============================>.] - ETA: 2s  batch_loss: 0.2278 [Training] 210/214 [============================>.] - ETA: 1s  batch_loss: 0.2275 [Training] 211/214 [============================>.] - ETA: 1s  batch_loss: 0.2274 [Training] 212/214 [============================>.] - ETA: 0s  batch_loss: 0.2273 [Training] 213/214 [============================>.] - ETA: 0s  batch_loss: 0.2274 [Training] 214/214 [==============================] 461.7ms/step  batch_loss: 0.2271 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
08/11/2022 15:03:32 - INFO - root -   The F1-score is 0.33437744714173845
08/11/2022 15:03:32 - INFO - root -   the best eval f1 is 0.3344, saving model !!
08/11/2022 15:03:34 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/214 [..............................] - ETA: 3:08  batch_loss: 0.2169 [Training] 2/214 [..............................] - ETA: 2:23  batch_loss: 0.2144 [Training] 3/214 [..............................] - ETA: 2:07  batch_loss: 0.2162 [Training] 4/214 [..............................] - ETA: 1:58  batch_loss: 0.2206 [Training] 5/214 [..............................] - ETA: 1:53  batch_loss: 0.2126 [Training] 6/214 [..............................] - ETA: 1:49  batch_loss: 0.2159 [Training] 7/214 [..............................] - ETA: 1:46  batch_loss: 0.2112 [Training] 8/214 [>.............................] - ETA: 1:45  batch_loss: 0.2099 [Training] 9/214 [>.............................] - ETA: 1:43  batch_loss: 0.2040 [Training] 10/214 [>.............................] - ETA: 1:42  batch_loss: 0.2037 [Training] 11/214 [>.............................] - ETA: 1:40  batch_loss: 0.2027 [Training] 12/214 [>.............................] - ETA: 1:39  batch_loss: 0.2021 [Training] 13/214 [>.............................] - ETA: 1:38  batch_loss: 0.1977 [Training] 14/214 [>.............................] - ETA: 1:37  batch_loss: 0.1982 [Training] 15/214 [=>............................] - ETA: 1:36  batch_loss: 0.1997 [Training] 16/214 [=>............................] - ETA: 1:35  batch_loss: 0.2006 [Training] 17/214 [=>............................] - ETA: 1:34  batch_loss: 0.1988 [Training] 18/214 [=>............................] - ETA: 1:34  batch_loss: 0.1959 [Training] 19/214 [=>............................] - ETA: 1:33  batch_loss: 0.1945 [Training] 20/214 [=>............................] - ETA: 1:32  batch_loss: 0.1937 [Training] 21/214 [=>............................] - ETA: 1:32  batch_loss: 0.1951 [Training] 22/214 [==>...........................] - ETA: 1:31  batch_loss: 0.1937 [Training] 23/214 [==>...........................] - ETA: 1:30  batch_loss: 0.1943 [Training] 24/214 [==>...........................] - ETA: 1:30  batch_loss: 0.1941 [Training] 25/214 [==>...........................] - ETA: 1:29  batch_loss: 0.1931 [Training] 26/214 [==>...........................] - ETA: 1:28  batch_loss: 0.1938 [Training] 27/214 [==>...........................] - ETA: 1:28  batch_loss: 0.1947 [Training] 28/214 [==>...........................] - ETA: 1:27  batch_loss: 0.1944 [Training] 29/214 [===>..........................] - ETA: 1:27  batch_loss: 0.1940 [Training] 30/214 [===>..........................] - ETA: 1:26  batch_loss: 0.1940 [Training] 31/214 [===>..........................] - ETA: 1:26  batch_loss: 0.1931 [Training] 32/214 [===>..........................] - ETA: 1:25  batch_loss: 0.1922 [Training] 33/214 [===>..........................] - ETA: 1:25  batch_loss: 0.1923 [Training] 34/214 [===>..........................] - ETA: 1:24  batch_loss: 0.1915 [Training] 35/214 [===>..........................] - ETA: 1:24  batch_loss: 0.1915 [Training] 36/214 [====>.........................] - ETA: 1:23  batch_loss: 0.1914 [Training] 37/214 [====>.........................] - ETA: 1:22  batch_loss: 0.1917 [Training] 38/214 [====>.........................] - ETA: 1:22  batch_loss: 0.1912 [Training] 39/214 [====>.........................] - ETA: 1:21  batch_loss: 0.1913 [Training] 40/214 [====>.........................] - ETA: 1:21  batch_loss: 0.1910 [Training] 41/214 [====>.........................] - ETA: 1:21  batch_loss: 0.1912 [Training] 42/214 [====>.........................] - ETA: 1:20  batch_loss: 0.1903 [Training] 43/214 [=====>........................] - ETA: 1:20  batch_loss: 0.1903 [Training] 44/214 [=====>........................] - ETA: 1:19  batch_loss: 0.1902 [Training] 45/214 [=====>........................] - ETA: 1:19  batch_loss: 0.1894 [Training] 46/214 [=====>........................] - ETA: 1:18  batch_loss: 0.1892 [Training] 47/214 [=====>........................] - ETA: 1:18  batch_loss: 0.1891 [Training] 48/214 [=====>........................] - ETA: 1:17  batch_loss: 0.1889 [Training] 49/214 [=====>........................] - ETA: 1:17  batch_loss: 0.1888 [Training] 50/214 [======>.......................] - ETA: 1:16  batch_loss: 0.1880 [Training] 51/214 [======>.......................] - ETA: 1:16  batch_loss: 0.1877 [Training] 52/214 [======>.......................] - ETA: 1:15  batch_loss: 0.1873 [Training] 53/214 [======>.......................] - ETA: 1:15  batch_loss: 0.1871 [Training] 54/214 [======>.......................] - ETA: 1:14  batch_loss: 0.1869 [Training] 55/214 [======>.......................] - ETA: 1:14  batch_loss: 0.1866 [Training] 56/214 [======>.......................] - ETA: 1:13  batch_loss: 0.1865 [Training] 57/214 [======>.......................] - ETA: 1:13  batch_loss: 0.1862 [Training] 58/214 [=======>......................] - ETA: 1:12  batch_loss: 0.1861 [Training] 59/214 [=======>......................] - ETA: 1:12  batch_loss: 0.1868 [Training] 60/214 [=======>......................] - ETA: 1:11  batch_loss: 0.1865 [Training] 61/214 [=======>......................] - ETA: 1:11  batch_loss: 0.1860 [Training] 62/214 [=======>......................] - ETA: 1:10  batch_loss: 0.1858 [Training] 63/214 [=======>......................] - ETA: 1:10  batch_loss: 0.1853 [Training] 64/214 [=======>......................] - ETA: 1:09  batch_loss: 0.1850 [Training] 65/214 [========>.....................] - ETA: 1:09  batch_loss: 0.1849 [Training] 66/214 [========>.....................] - ETA: 1:08  batch_loss: 0.1851 [Training] 67/214 [========>.....................] - ETA: 1:08  batch_loss: 0.1852 [Training] 68/214 [========>.....................] - ETA: 1:07  batch_loss: 0.1846 [Training] 69/214 [========>.....................] - ETA: 1:07  batch_loss: 0.1841 [Training] 70/214 [========>.....................] - ETA: 1:07  batch_loss: 0.1840 [Training] 71/214 [========>.....................] - ETA: 1:06  batch_loss: 0.1839 [Training] 72/214 [=========>....................] - ETA: 1:06  batch_loss: 0.1836 [Training] 73/214 [=========>....................] - ETA: 1:05  batch_loss: 0.1839 [Training] 74/214 [=========>....................] - ETA: 1:05  batch_loss: 0.1832 [Training] 75/214 [=========>....................] - ETA: 1:04  batch_loss: 0.1833 [Training] 76/214 [=========>....................] - ETA: 1:04  batch_loss: 0.1837 [Training] 77/214 [=========>....................] - ETA: 1:03  batch_loss: 0.1832 [Training] 78/214 [=========>....................] - ETA: 1:03  batch_loss: 0.1829 [Training] 79/214 [==========>...................] - ETA: 1:02  batch_loss: 0.1824 [Training] 80/214 [==========>...................] - ETA: 1:02  batch_loss: 0.1823 [Training] 81/214 [==========>...................] - ETA: 1:01  batch_loss: 0.1823 [Training] 82/214 [==========>...................] - ETA: 1:01  batch_loss: 0.1821 [Training] 83/214 [==========>...................] - ETA: 1:00  batch_loss: 0.1817 [Training] 84/214 [==========>...................] - ETA: 1:00  batch_loss: 0.1815 [Training] 85/214 [==========>...................] - ETA: 59s  batch_loss: 0.1824 [Training] 86/214 [===========>..................] - ETA: 59s  batch_loss: 0.1827 [Training] 87/214 [===========>..................] - ETA: 59s  batch_loss: 0.1827 [Training] 88/214 [===========>..................] - ETA: 58s  batch_loss: 0.1822 [Training] 89/214 [===========>..................] - ETA: 58s  batch_loss: 0.1823 [Training] 90/214 [===========>..................] - ETA: 57s  batch_loss: 0.1825 [Training] 91/214 [===========>..................] - ETA: 57s  batch_loss: 0.1826 [Training] 92/214 [===========>..................] - ETA: 56s  batch_loss: 0.1822 [Training] 93/214 [============>.................] - ETA: 56s  batch_loss: 0.1818 [Training] 94/214 [============>.................] - ETA: 55s  batch_loss: 0.1818 [Training] 95/214 [============>.................] - ETA: 55s  batch_loss: 0.1814 [Training] 96/214 [============>.................] - ETA: 54s  batch_loss: 0.1815 [Training] 97/214 [============>.................] - ETA: 54s  batch_loss: 0.1811 ./1-run_trigger_for_sentences.sh: line 12: 38749 Killed                  CUDA_VISIBLE_DEVICES=3 python run_ner.py --dataset DuEE1.0 --event_type trigger --max_len 150 --per_gpu_train_batch_size 56 --num_train_epochs 30 --per_gpu_eval_batch_size 48 --gradient_accumulation_steps 8 --model_name_or_path /data/qingyang/data/chinese-roberta-wwm-ext --learning_rate 1e-5 --linear_learning_rate 1e-5 --early_stop 45
