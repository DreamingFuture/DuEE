nohup: ignoring input
*********** data_prepare *************
开始数据预处理
开始处理tag.dict 和 tsv文件

=================DUEE 1.0 DATASET==============

=================start schema process==============
input path ./conf/DuEE1.0/event_schema.json
save trigger tag 393 at ./conf/DuEE1.0/role_tag.dict
=================end schema process===============

=================start sentence process==============

----trigger------for dir ./data/DuEE1.0 to ./data/DuEE1.0/trigger
train 6379 dev 1633 test 147541

----role------for dir ./data/DuEE1.0 to ./data/DuEE1.0/role
train 7502 dev 1925 test 147541
=================end schema process==============
********** train start *************
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 5, 
    "event_type": "role", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 1e-05, 
    "max_len": 150, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 30.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 48, 
    "per_gpu_train_batch_size": 64, 
    "seed": 66, 
    "stride": 100, 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/7501 [00:00<?, ?it/s]tokenizing...:   5%|▍         | 348/7501 [00:00<00:02, 3475.49it/s]tokenizing...:  10%|█         | 763/7501 [00:00<00:01, 3869.67it/s]tokenizing...:  15%|█▌        | 1150/7501 [00:00<00:01, 3202.48it/s]tokenizing...:  20%|█▉        | 1482/7501 [00:00<00:01, 3034.92it/s]tokenizing...:  24%|██▍       | 1793/7501 [00:00<00:01, 2970.41it/s]tokenizing...:  28%|██▊       | 2094/7501 [00:00<00:01, 2865.69it/s]tokenizing...:  32%|███▏      | 2383/7501 [00:00<00:01, 2737.19it/s]tokenizing...:  35%|███▌      | 2659/7501 [00:00<00:01, 2741.29it/s]tokenizing...:  39%|███▉      | 2935/7501 [00:01<00:01, 2686.75it/s]tokenizing...:  43%|████▎     | 3213/7501 [00:01<00:01, 2712.67it/s]tokenizing...:  46%|████▋     | 3485/7501 [00:01<00:01, 2629.63it/s]tokenizing...:  50%|█████     | 3755/7501 [00:01<00:01, 2647.25it/s]tokenizing...:  54%|█████▎    | 4027/7501 [00:01<00:01, 2659.67it/s]tokenizing...:  57%|█████▋    | 4294/7501 [00:01<00:01, 2565.34it/s]tokenizing...:  61%|██████    | 4554/7501 [00:01<00:01, 2572.78it/s]tokenizing...:  64%|██████▍   | 4815/7501 [00:01<00:01, 2581.50it/s]tokenizing...:  68%|██████▊   | 5075/7501 [00:01<00:00, 2580.69it/s]tokenizing...:  72%|███████▏  | 5364/7501 [00:01<00:00, 2668.36it/s]tokenizing...:  75%|███████▌  | 5632/7501 [00:02<00:00, 1906.39it/s]tokenizing...:  78%|███████▊  | 5854/7501 [00:02<00:00, 1972.88it/s]tokenizing...:  81%|████████▏ | 6097/7501 [00:02<00:00, 2086.04it/s]tokenizing...:  85%|████████▌ | 6385/7501 [00:02<00:00, 2290.78it/s]tokenizing...:  89%|████████▊ | 6656/7501 [00:02<00:00, 2403.78it/s]tokenizing...:  92%|█████████▏| 6909/7501 [00:02<00:00, 2342.93it/s]tokenizing...:  95%|█████████▌| 7153/7501 [00:02<00:00, 2294.49it/s]tokenizing...:  99%|█████████▉| 7457/7501 [00:02<00:00, 2500.67it/s]tokenizing...: 100%|██████████| 7501/7501 [00:02<00:00, 2564.43it/s]
tokenizing...:   0%|          | 0/1924 [00:00<?, ?it/s]tokenizing...:  16%|█▌        | 311/1924 [00:00<00:00, 3104.06it/s]tokenizing...:  32%|███▏      | 622/1924 [00:00<00:00, 2797.00it/s]tokenizing...:  47%|████▋     | 904/1924 [00:00<00:00, 2679.54it/s]tokenizing...:  61%|██████    | 1174/1924 [00:00<00:00, 2636.94it/s]tokenizing...:  75%|███████▍  | 1439/1924 [00:00<00:00, 2565.91it/s]tokenizing...:  88%|████████▊ | 1696/1924 [00:00<00:00, 2443.18it/s]tokenizing...: 100%|██████████| 1924/1924 [00:00<00:00, 2560.21it/s]
09/25/2022 12:37:58 - INFO - root -   The nums of the train_dataset features is 7501
09/25/2022 12:37:58 - INFO - root -   The nums of the eval_dataset features is 1924
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
09/25/2022 12:38:03 - INFO - root -   ***** Running train *****
********************************************************************************
/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 2:02  batch_loss: 6.3595 [Training] 2/118 [..............................] - ETA: 1:14  batch_loss: 6.3496 [Training] 3/118 [..............................] - ETA: 57s  batch_loss: 6.3427 [Training] 4/118 [>.............................] - ETA: 50s  batch_loss: 6.3387 [Training] 5/118 [>.............................] - ETA: 45s  batch_loss: 6.3086 [Training] 6/118 [>.............................] - ETA: 42s  batch_loss: 6.2622 [Training] 7/118 [>.............................] - ETA: 39s  batch_loss: 6.2080 [Training] 8/118 [=>............................] - ETA: 37s  batch_loss: 6.1466 [Training] 9/118 [=>............................] - ETA: 36s  batch_loss: 6.0844 [Training] 10/118 [=>............................] - ETA: 35s  batch_loss: 6.0195 [Training] 11/118 [=>............................] - ETA: 33s  batch_loss: 5.9613 [Training] 12/118 [==>...........................] - ETA: 32s  batch_loss: 5.8884 [Training] 13/118 [==>...........................] - ETA: 32s  batch_loss: 5.8213 [Training] 14/118 [==>...........................] - ETA: 31s  batch_loss: 5.7505 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 5.6768 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 5.5993 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 5.5196 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 5.4566 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 5.3849 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 5.2905 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 5.2091 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 5.1184 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 5.0338 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 4.9491 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 4.8649 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 4.7728 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 4.6899 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 4.5983 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 4.5237 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 4.4407 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 4.3816 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 4.3260 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 4.2589 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 4.2070 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 4.1559 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 4.1051 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 4.0593 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 4.0110 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 3.9649 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 3.9187 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 3.8823 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 3.8438 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 3.8106 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 3.7737 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 3.7418 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 3.7176 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 3.6872 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 3.6571 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 3.6306 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 3.6028 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 3.5753 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 3.5457 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 3.5202 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 3.4932 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 3.4758 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 3.4592 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 3.4383 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 3.4176 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 3.3960 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 3.3822 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 3.3641 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 3.3400 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 3.3225 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 3.3063 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 3.2924 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 3.2775 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 3.2603 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 3.2468 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 3.2354 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 3.2188 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 3.2067 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 3.1909 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 3.1794 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 3.1657 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 3.1541 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 3.1423 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 3.1291 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 3.1155 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 3.1030 [Training] 80/118 [===================>..........] - ETA: 10s  batch_loss: 3.0887 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 3.0738 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 3.0637 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 3.0521 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 3.0406 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 3.0285 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 3.0174 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 3.0040 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 2.9989 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 2.9844 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 2.9722 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 2.9654 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 2.9566 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 2.9451 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 2.9363 [Training] 95/118 [=======================>......] - ETA: 6s  batch_loss: 2.9270 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 2.9183 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 2.9091 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 2.9009 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 2.8925 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 2.8824 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 2.8734 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 2.8650 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 2.8567 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 2.8509 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 2.8431 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 2.8346 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 2.8281 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 2.8198 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 2.8137 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 2.8058 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 2.7989 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 2.7928 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 2.7870 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 2.7797 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 2.7702 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 2.7627 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 2.7546 [Training] 118/118 [==============================] 258.3ms/step  batch_loss: 2.7498 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:38:40 - INFO - root -   The F1-score is 0.03605408112168252
09/25/2022 12:38:40 - INFO - root -   the best eval f1 is 0.0361, saving model !!
09/25/2022 12:38:43 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:33  batch_loss: 1.7934 [Training] 2/118 [..............................] - ETA: 1:00  batch_loss: 1.9376 [Training] 3/118 [..............................] - ETA: 49s  batch_loss: 1.9127 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 1.8895 [Training] 5/118 [>.............................] - ETA: 41s  batch_loss: 1.9074 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 1.9107 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 1.8735 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 1.8610 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 1.8464 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 1.8572 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 1.8459 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 1.8789 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 1.8732 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 1.8747 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 1.8517 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 1.8534 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 1.8530 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 1.8464 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 1.8360 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 1.8410 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 1.8537 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 1.8471 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 1.8615 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 1.8582 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 1.8588 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 1.8487 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 1.8516 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 1.8613 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 1.8527 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 1.8554 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 1.8454 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 1.8408 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 1.8339 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 1.8369 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 1.8307 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 1.8232 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 1.8207 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 1.8215 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 1.8253 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 1.8262 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 1.8243 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 1.8242 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 1.8183 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 1.8181 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 1.8211 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 1.8198 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 1.8226 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 1.8210 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 1.8192 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 1.8163 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 1.8147 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 1.8150 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 1.8133 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 1.8052 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 1.8014 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 1.8030 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 1.8019 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 1.7968 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 1.7916 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 1.7881 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 1.7862 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 1.7855 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 1.7859 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 1.7869 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 1.7881 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 1.7811 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 1.7766 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 1.7727 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 1.7695 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 1.7689 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 1.7668 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 1.7645 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 1.7615 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 1.7615 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 1.7608 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 1.7594 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 1.7560 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 1.7577 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 1.7548 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 1.7478 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 1.7459 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 1.7450 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 1.7404 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 1.7377 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 1.7364 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 1.7355 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 1.7340 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 1.7337 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 1.7322 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 1.7302 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 1.7277 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 1.7248 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 1.7245 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 1.7205 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 1.7184 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 1.7154 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 1.7145 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 1.7114 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 1.7090 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 1.7052 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 1.7016 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 1.7000 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 1.6979 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 1.6956 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 1.6934 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 1.6928 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 1.6904 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 1.6883 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 1.6853 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 1.6822 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 1.6805 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 1.6772 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 1.6748 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 1.6732 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 1.6710 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 1.6697 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 1.6711 [Training] 118/118 [==============================] 256.0ms/step  batch_loss: 1.6678 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:39:20 - INFO - root -   The F1-score is 0.13478303457733373
09/25/2022 12:39:20 - INFO - root -   the best eval f1 is 0.1348, saving model !!
09/25/2022 12:39:23 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:36  batch_loss: 1.3971 [Training] 2/118 [..............................] - ETA: 1:03  batch_loss: 1.4175 [Training] 3/118 [..............................] - ETA: 51s  batch_loss: 1.4002 [Training] 4/118 [>.............................] - ETA: 45s  batch_loss: 1.4051 [Training] 5/118 [>.............................] - ETA: 41s  batch_loss: 1.4442 [Training] 6/118 [>.............................] - ETA: 39s  batch_loss: 1.4370 [Training] 7/118 [>.............................] - ETA: 37s  batch_loss: 1.4066 [Training] 8/118 [=>............................] - ETA: 36s  batch_loss: 1.4039 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 1.3913 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 1.4000 [Training] 11/118 [=>............................] - ETA: 33s  batch_loss: 1.3963 [Training] 12/118 [==>...........................] - ETA: 32s  batch_loss: 1.3858 [Training] 13/118 [==>...........................] - ETA: 31s  batch_loss: 1.3830 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 1.3751 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 1.3752 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 1.3837 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 1.3869 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 1.3788 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 1.3824 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 1.3916 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 1.3931 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 1.3864 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 1.3836 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 1.3745 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 1.3734 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 1.3730 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 1.3683 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 1.3774 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 1.3789 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 1.3795 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 1.3842 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 1.3847 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 1.3824 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 1.3805 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 1.3837 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 1.3816 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 1.3767 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 1.3750 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 1.3744 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 1.3755 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 1.3729 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 1.3672 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 1.3677 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 1.3677 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 1.3637 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 1.3624 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 1.3568 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 1.3562 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 1.3572 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 1.3566 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 1.3553 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 1.3550 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 1.3532 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 1.3520 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 1.3499 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 1.3542 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 1.3520 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 1.3493 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 1.3475 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 1.3438 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 1.3423 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 1.3408 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 1.3402 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 1.3421 [Training] 65/118 [===============>..............] - ETA: 14s  batch_loss: 1.3388 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 1.3374 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 1.3359 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 1.3341 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 1.3355 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 1.3344 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 1.3338 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 1.3310 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 1.3286 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 1.3274 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 1.3257 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 1.3259 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 1.3239 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 1.3244 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 1.3224 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 1.3233 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 1.3231 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 1.3209 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 1.3209 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 1.3196 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 1.3194 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 1.3194 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 1.3189 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 1.3192 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 1.3197 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 1.3189 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 1.3168 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 1.3168 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 1.3168 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 1.3164 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 1.3157 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 1.3140 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 1.3125 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 1.3115 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 1.3099 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 1.3070 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 1.3040 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 1.3035 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 1.3035 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 1.3031 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 1.3011 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 1.3001 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 1.2993 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 1.2998 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 1.2994 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 1.2973 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 1.2963 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 1.2958 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 1.2954 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 1.2940 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 1.2916 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 1.2907 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 1.2904 [Training] 118/118 [==============================] 258.1ms/step  batch_loss: 1.2886 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:40:00 - INFO - root -   The F1-score is 0.21512032809719103
09/25/2022 12:40:00 - INFO - root -   the best eval f1 is 0.2151, saving model !!
09/25/2022 12:40:03 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:30  batch_loss: 1.2597 [Training] 2/118 [..............................] - ETA: 1:00  batch_loss: 1.2566 [Training] 3/118 [..............................] - ETA: 49s  batch_loss: 1.2213 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 1.2093 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 1.1822 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 1.1451 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 1.1299 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 1.1261 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 1.1374 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 1.1547 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 1.1451 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 1.1464 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 1.1424 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 1.1459 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 1.1396 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 1.1315 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 1.1400 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 1.1416 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 1.1312 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 1.1332 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 1.1258 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 1.1274 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 1.1273 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 1.1252 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 1.1259 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 1.1293 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 1.1244 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 1.1234 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 1.1209 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 1.1214 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 1.1157 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 1.1136 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 1.1116 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 1.1131 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 1.1114 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 1.1112 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 1.1140 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 1.1151 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 1.1128 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 1.1122 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 1.1091 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 1.1077 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 1.1083 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 1.1077 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 1.1069 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 1.1082 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 1.1081 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 1.1106 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 1.1080 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 1.1076 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 1.1084 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 1.1110 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 1.1115 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 1.1085 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 1.1069 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 1.1076 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 1.1076 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 1.1040 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 1.1041 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 1.1048 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 1.1027 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 1.1035 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 1.1030 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 1.1031 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 1.1025 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 1.1018 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 1.1019 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 1.1011 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 1.0992 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 1.0986 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 1.0971 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 1.0956 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 1.0947 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 1.0957 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 1.0970 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 1.0960 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 1.0959 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 1.0969 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 1.0954 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 1.0943 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 1.0911 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 1.0892 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 1.0878 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 1.0879 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 1.0860 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 1.0846 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 1.0855 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 1.0846 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 1.0838 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 1.0834 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 1.0813 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 1.0804 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 1.0781 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 1.0754 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 1.0742 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 1.0724 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 1.0715 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 1.0707 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 1.0698 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 1.0695 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 1.0689 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 1.0689 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 1.0678 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 1.0669 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 1.0653 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 1.0654 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 1.0637 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 1.0629 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 1.0609 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 1.0606 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 1.0594 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 1.0583 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 1.0564 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 1.0544 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 1.0532 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 1.0519 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 1.0509 [Training] 118/118 [==============================] 254.5ms/step  batch_loss: 1.0504 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:40:39 - INFO - root -   The F1-score is 0.3354762432572231
09/25/2022 12:40:39 - INFO - root -   the best eval f1 is 0.3355, saving model !!
09/25/2022 12:40:42 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:42  batch_loss: 1.0136 [Training] 2/118 [..............................] - ETA: 1:06  batch_loss: 0.9570 [Training] 3/118 [..............................] - ETA: 54s  batch_loss: 0.9463 [Training] 4/118 [>.............................] - ETA: 47s  batch_loss: 0.9658 [Training] 5/118 [>.............................] - ETA: 43s  batch_loss: 0.9763 [Training] 6/118 [>.............................] - ETA: 41s  batch_loss: 0.9307 [Training] 7/118 [>.............................] - ETA: 38s  batch_loss: 0.9496 [Training] 8/118 [=>............................] - ETA: 37s  batch_loss: 0.9389 [Training] 9/118 [=>............................] - ETA: 35s  batch_loss: 0.9345 [Training] 10/118 [=>............................] - ETA: 34s  batch_loss: 0.9376 [Training] 11/118 [=>............................] - ETA: 33s  batch_loss: 0.9424 [Training] 12/118 [==>...........................] - ETA: 32s  batch_loss: 0.9443 [Training] 13/118 [==>...........................] - ETA: 32s  batch_loss: 0.9422 [Training] 14/118 [==>...........................] - ETA: 31s  batch_loss: 0.9374 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 0.9364 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 0.9410 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.9384 [Training] 18/118 [===>..........................] - ETA: 29s  batch_loss: 0.9350 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.9271 [Training] 20/118 [====>.........................] - ETA: 28s  batch_loss: 0.9193 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.9261 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 0.9311 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.9314 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.9303 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.9300 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.9322 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.9352 [Training] 28/118 [======>.......................] - ETA: 25s  batch_loss: 0.9300 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.9311 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.9283 [Training] 31/118 [======>.......................] - ETA: 24s  batch_loss: 0.9231 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.9251 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.9248 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 0.9236 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.9232 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.9183 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.9219 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.9190 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.9189 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.9216 [Training] 41/118 [=========>....................] - ETA: 21s  batch_loss: 0.9242 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.9197 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.9177 [Training] 44/118 [==========>...................] - ETA: 20s  batch_loss: 0.9153 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.9177 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.9184 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.9183 [Training] 48/118 [===========>..................] - ETA: 19s  batch_loss: 0.9208 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.9183 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.9150 [Training] 51/118 [===========>..................] - ETA: 18s  batch_loss: 0.9132 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.9120 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.9122 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.9094 [Training] 55/118 [============>.................] - ETA: 17s  batch_loss: 0.9069 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.9052 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.9047 [Training] 58/118 [=============>................] - ETA: 16s  batch_loss: 0.9037 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.9023 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.9029 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.9012 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.9021 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.9035 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.9027 [Training] 65/118 [===============>..............] - ETA: 14s  batch_loss: 0.9019 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.9028 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.8997 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.8984 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.8974 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.8965 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.8954 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.8949 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.8945 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.8936 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.8931 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.8916 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.8901 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.8907 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.8898 [Training] 80/118 [===================>..........] - ETA: 10s  batch_loss: 0.8889 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.8897 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.8890 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.8872 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.8861 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.8865 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.8871 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.8877 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.8862 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.8863 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.8863 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.8877 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.8877 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.8877 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.8857 [Training] 95/118 [=======================>......] - ETA: 6s  batch_loss: 0.8859 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.8845 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.8833 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.8824 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.8806 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.8800 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.8792 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.8797 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.8793 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.8781 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.8776 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.8769 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.8771 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.8766 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.8767 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.8760 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.8763 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.8784 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.8775 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.8775 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.8770 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.8772 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.8767 [Training] 118/118 [==============================] 258.4ms/step  batch_loss: 0.8766 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:41:19 - INFO - root -   The F1-score is 0.4321738536185334
09/25/2022 12:41:19 - INFO - root -   the best eval f1 is 0.4322, saving model !!
09/25/2022 12:41:22 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:36  batch_loss: 0.9120 [Training] 2/118 [..............................] - ETA: 1:03  batch_loss: 0.8391 [Training] 3/118 [..............................] - ETA: 51s  batch_loss: 0.8618 [Training] 4/118 [>.............................] - ETA: 45s  batch_loss: 0.8452 [Training] 5/118 [>.............................] - ETA: 41s  batch_loss: 0.8192 [Training] 6/118 [>.............................] - ETA: 39s  batch_loss: 0.8352 [Training] 7/118 [>.............................] - ETA: 37s  batch_loss: 0.8353 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.8269 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.8220 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.8166 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.8108 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.8063 [Training] 13/118 [==>...........................] - ETA: 31s  batch_loss: 0.8113 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.8097 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.8081 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.8058 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.8082 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.8002 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.8000 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.8034 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.8052 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.8086 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.8098 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.8018 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.7963 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.7947 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.7920 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.7902 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.7881 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.7859 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.7849 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.7827 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.7811 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.7797 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.7774 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.7754 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.7700 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.7706 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.7707 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.7686 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.7698 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.7706 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.7713 [Training] 44/118 [==========>...................] - ETA: 20s  batch_loss: 0.7711 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.7733 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.7724 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.7736 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.7764 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.7757 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.7737 [Training] 51/118 [===========>..................] - ETA: 18s  batch_loss: 0.7696 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.7675 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.7667 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.7655 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.7645 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.7637 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.7608 [Training] 58/118 [=============>................] - ETA: 16s  batch_loss: 0.7598 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.7590 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.7628 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.7627 [Training] 62/118 [==============>...............] - ETA: 15s  batch_loss: 0.7631 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.7629 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.7614 [Training] 65/118 [===============>..............] - ETA: 14s  batch_loss: 0.7607 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.7621 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.7622 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.7613 [Training] 69/118 [================>.............] - ETA: 13s  batch_loss: 0.7613 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.7616 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.7612 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.7599 [Training] 73/118 [=================>............] - ETA: 12s  batch_loss: 0.7596 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.7581 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.7571 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.7559 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.7559 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.7557 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.7568 [Training] 80/118 [===================>..........] - ETA: 10s  batch_loss: 0.7551 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.7553 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.7544 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.7532 [Training] 84/118 [====================>.........] - ETA: 9s  batch_loss: 0.7521 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.7509 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.7500 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.7499 [Training] 88/118 [=====================>........] - ETA: 8s  batch_loss: 0.7495 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.7488 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.7478 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.7469 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.7462 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.7467 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.7448 [Training] 95/118 [=======================>......] - ETA: 6s  batch_loss: 0.7449 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.7455 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.7441 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.7458 [Training] 99/118 [========================>.....] - ETA: 5s  batch_loss: 0.7453 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.7441 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.7451 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.7453 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.7442 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.7441 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.7450 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.7448 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.7444 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.7443 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.7446 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.7446 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.7448 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.7448 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.7445 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.7434 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.7431 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.7426 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.7434 [Training] 118/118 [==============================] 264.0ms/step  batch_loss: 0.7466 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:42:00 - INFO - root -   The F1-score is 0.4891156462585034
09/25/2022 12:42:00 - INFO - root -   the best eval f1 is 0.4891, saving model !!
09/25/2022 12:42:03 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:35  batch_loss: 0.6651 [Training] 2/118 [..............................] - ETA: 1:02  batch_loss: 0.6619 [Training] 3/118 [..............................] - ETA: 50s  batch_loss: 0.6277 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 0.6340 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.6451 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.6543 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.6673 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.6828 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.6738 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.6738 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.6821 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.6798 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.6856 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.6797 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.6816 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.6850 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.6819 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.6850 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.6844 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.6827 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.6811 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.6783 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.6772 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.6754 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.6720 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.6732 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.6724 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.6720 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.6677 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.6682 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.6702 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.6694 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.6695 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.6718 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.6725 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.6732 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.6727 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.6729 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.6725 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.6698 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.6698 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.6669 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.6654 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.6658 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.6649 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.6634 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.6625 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.6636 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.6614 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.6585 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.6610 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.6623 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.6603 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.6592 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.6595 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.6571 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.6576 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.6564 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.6543 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.6553 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.6552 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.6517 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.6506 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.6514 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.6496 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.6495 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.6505 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.6518 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.6517 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.6510 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.6495 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.6487 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.6486 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.6486 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.6481 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.6476 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.6467 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.6449 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.6449 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.6445 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.6448 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.6442 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.6438 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.6452 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.6444 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.6455 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.6448 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.6443 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.6443 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.6443 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.6432 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.6424 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.6445 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.6446 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.6449 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.6449 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.6450 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.6438 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.6444 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.6448 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.6444 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.6447 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.6441 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.6433 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.6430 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.6434 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.6432 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.6423 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.6419 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.6409 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.6405 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.6407 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.6403 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.6399 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.6393 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.6391 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.6386 [Training] 118/118 [==============================] 254.6ms/step  batch_loss: 0.6440 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:42:40 - INFO - root -   The F1-score is 0.5094945172506018
09/25/2022 12:42:40 - INFO - root -   the best eval f1 is 0.5095, saving model !!
09/25/2022 12:42:42 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:37  batch_loss: 0.5909 [Training] 2/118 [..............................] - ETA: 1:03  batch_loss: 0.6605 [Training] 3/118 [..............................] - ETA: 51s  batch_loss: 0.6085 [Training] 4/118 [>.............................] - ETA: 45s  batch_loss: 0.5937 [Training] 5/118 [>.............................] - ETA: 41s  batch_loss: 0.5844 [Training] 6/118 [>.............................] - ETA: 39s  batch_loss: 0.6215 [Training] 7/118 [>.............................] - ETA: 37s  batch_loss: 0.6167 [Training] 8/118 [=>............................] - ETA: 36s  batch_loss: 0.6117 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.6086 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.6094 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.6030 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.5960 [Training] 13/118 [==>...........................] - ETA: 31s  batch_loss: 0.5990 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.5995 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.5991 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.6005 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.5952 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.5894 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.5890 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.5894 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.5872 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.5844 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.5842 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.5810 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.5785 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.5762 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.5739 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.5778 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.5772 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.5767 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.5763 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.5805 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.5777 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.5774 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.5763 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.5765 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.5762 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.5778 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.5784 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.5800 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.5797 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.5787 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.5790 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.5789 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.5785 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.5794 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.5783 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.5768 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.5763 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.5767 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.5756 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.5765 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.5757 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.5764 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.5751 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.5740 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.5727 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.5724 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.5719 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.5716 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.5708 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.5709 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.5720 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.5729 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.5723 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.5705 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.5696 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.5701 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.5695 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.5686 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.5685 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.5673 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.5667 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.5662 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.5658 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.5656 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.5650 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.5644 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.5638 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.5631 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.5633 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.5626 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.5616 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.5616 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.5614 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.5604 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.5601 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.5591 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.5589 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.5590 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.5595 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.5592 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.5584 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.5585 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.5584 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.5582 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.5578 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.5574 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.5575 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.5569 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.5565 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.5560 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.5566 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.5572 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.5568 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.5556 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.5558 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.5562 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.5561 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.5561 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.5576 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.5570 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.5573 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.5564 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.5560 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.5561 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.5556 [Training] 118/118 [==============================] 258.4ms/step  batch_loss: 0.5561 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:43:20 - INFO - root -   The F1-score is 0.5393185645809043
09/25/2022 12:43:20 - INFO - root -   the best eval f1 is 0.5393, saving model !!
09/25/2022 12:43:23 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:33  batch_loss: 0.5551 [Training] 2/118 [..............................] - ETA: 1:01  batch_loss: 0.5459 [Training] 3/118 [..............................] - ETA: 49s  batch_loss: 0.5383 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 0.5129 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.5164 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.5246 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.5308 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.5205 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.5233 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.5156 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.5243 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.5358 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.5271 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.5213 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.5218 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.5191 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.5203 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.5237 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.5196 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.5213 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.5168 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.5215 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.5197 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.5206 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.5172 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.5170 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.5145 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.5147 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.5140 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.5153 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.5142 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.5123 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.5122 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 0.5130 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.5124 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.5124 [Training] 37/118 [========>.....................] - ETA: 20s  batch_loss: 0.5133 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.5141 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.5120 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.5109 [Training] 41/118 [=========>....................] - ETA: 19s  batch_loss: 0.5107 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.5117 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.5104 [Training] 44/118 [==========>...................] - ETA: 18s  batch_loss: 0.5105 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.5094 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.5096 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.5072 [Training] 48/118 [===========>..................] - ETA: 17s  batch_loss: 0.5047 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.5041 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.5030 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.5022 [Training] 52/118 [============>.................] - ETA: 16s  batch_loss: 0.5022 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.5033 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.5035 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.5050 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.5048 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.5042 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.5032 [Training] 59/118 [==============>...............] - ETA: 14s  batch_loss: 0.5021 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.5020 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.4998 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.4991 [Training] 63/118 [===============>..............] - ETA: 13s  batch_loss: 0.4986 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.4983 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.4976 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.4971 [Training] 67/118 [================>.............] - ETA: 12s  batch_loss: 0.4974 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.4967 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.4971 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.4964 [Training] 71/118 [=================>............] - ETA: 11s  batch_loss: 0.4975 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.4969 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.4966 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.4960 [Training] 75/118 [==================>...........] - ETA: 10s  batch_loss: 0.4966 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.4964 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.4960 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.4956 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.4952 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.4942 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.4944 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.4938 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.4933 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.4923 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.4921 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.4908 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.4906 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.4897 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.4899 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.4893 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.4892 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.4899 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.4889 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.4890 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.4888 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.4883 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.4886 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.4881 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.4871 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.4868 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.4867 [Training] 102/118 [========================>.....] - ETA: 3s  batch_loss: 0.4867 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.4869 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.4872 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.4877 [Training] 106/118 [=========================>....] - ETA: 2s  batch_loss: 0.4873 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.4880 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.4885 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.4878 [Training] 110/118 [==========================>...] - ETA: 1s  batch_loss: 0.4871 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.4861 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.4856 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.4858 [Training] 114/118 [===========================>..] - ETA: 0s  batch_loss: 0.4854 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.4858 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.4856 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.4860 [Training] 118/118 [==============================] 247.8ms/step  batch_loss: 0.4852 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:43:59 - INFO - root -   The F1-score is 0.572678594749902
09/25/2022 12:43:59 - INFO - root -   the best eval f1 is 0.5727, saving model !!
09/25/2022 12:44:02 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:35  batch_loss: 0.3489 [Training] 2/118 [..............................] - ETA: 1:02  batch_loss: 0.4006 [Training] 3/118 [..............................] - ETA: 51s  batch_loss: 0.3899 [Training] 4/118 [>.............................] - ETA: 45s  batch_loss: 0.3861 [Training] 5/118 [>.............................] - ETA: 41s  batch_loss: 0.3926 [Training] 6/118 [>.............................] - ETA: 39s  batch_loss: 0.4074 [Training] 7/118 [>.............................] - ETA: 37s  batch_loss: 0.4075 [Training] 8/118 [=>............................] - ETA: 36s  batch_loss: 0.4057 [Training] 9/118 [=>............................] - ETA: 35s  batch_loss: 0.4016 [Training] 10/118 [=>............................] - ETA: 34s  batch_loss: 0.4091 [Training] 11/118 [=>............................] - ETA: 33s  batch_loss: 0.4128 [Training] 12/118 [==>...........................] - ETA: 32s  batch_loss: 0.4093 [Training] 13/118 [==>...........................] - ETA: 31s  batch_loss: 0.4103 [Training] 14/118 [==>...........................] - ETA: 31s  batch_loss: 0.4108 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 0.4147 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 0.4202 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.4217 [Training] 18/118 [===>..........................] - ETA: 29s  batch_loss: 0.4254 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.4275 [Training] 20/118 [====>.........................] - ETA: 28s  batch_loss: 0.4250 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.4243 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 0.4231 [Training] 23/118 [====>.........................] - ETA: 27s  batch_loss: 0.4221 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.4254 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.4280 [Training] 26/118 [=====>........................] - ETA: 26s  batch_loss: 0.4297 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.4298 [Training] 28/118 [======>.......................] - ETA: 25s  batch_loss: 0.4284 [Training] 29/118 [======>.......................] - ETA: 25s  batch_loss: 0.4323 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.4356 [Training] 31/118 [======>.......................] - ETA: 24s  batch_loss: 0.4388 [Training] 32/118 [=======>......................] - ETA: 24s  batch_loss: 0.4379 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.4380 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 0.4357 [Training] 35/118 [=======>......................] - ETA: 23s  batch_loss: 0.4363 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.4364 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.4382 [Training] 38/118 [========>.....................] - ETA: 22s  batch_loss: 0.4368 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.4368 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.4367 [Training] 41/118 [=========>....................] - ETA: 21s  batch_loss: 0.4346 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.4341 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.4355 [Training] 44/118 [==========>...................] - ETA: 20s  batch_loss: 0.4369 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.4350 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.4358 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.4359 [Training] 48/118 [===========>..................] - ETA: 19s  batch_loss: 0.4353 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.4368 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.4371 [Training] 51/118 [===========>..................] - ETA: 18s  batch_loss: 0.4366 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.4386 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.4376 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.4359 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.4363 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.4360 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.4356 [Training] 58/118 [=============>................] - ETA: 16s  batch_loss: 0.4338 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.4351 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.4348 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.4351 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.4361 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.4361 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.4359 [Training] 65/118 [===============>..............] - ETA: 14s  batch_loss: 0.4360 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.4364 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.4381 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.4379 [Training] 69/118 [================>.............] - ETA: 13s  batch_loss: 0.4369 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.4376 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.4373 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.4372 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.4371 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.4365 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.4366 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.4363 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.4364 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.4362 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.4358 [Training] 80/118 [===================>..........] - ETA: 10s  batch_loss: 0.4351 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.4363 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.4352 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.4353 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.4364 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.4368 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.4361 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.4357 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.4355 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.4353 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.4358 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.4354 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.4356 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.4350 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.4346 [Training] 95/118 [=======================>......] - ETA: 6s  batch_loss: 0.4345 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.4341 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.4336 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.4337 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.4332 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.4327 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.4326 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.4319 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.4323 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.4321 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.4321 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.4311 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.4310 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.4305 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.4304 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.4297 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.4294 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.4302 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.4304 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.4310 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.4317 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.4310 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.4305 [Training] 118/118 [==============================] 259.9ms/step  batch_loss: 0.4309 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:44:39 - INFO - root -   The F1-score is 0.5792698826597132
09/25/2022 12:44:39 - INFO - root -   the best eval f1 is 0.5793, saving model !!
09/25/2022 12:44:42 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:34  batch_loss: 0.4384 [Training] 2/118 [..............................] - ETA: 1:01  batch_loss: 0.4018 [Training] 3/118 [..............................] - ETA: 50s  batch_loss: 0.4076 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 0.3995 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.4073 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.4234 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.4258 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.4109 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.4075 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.4024 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.4057 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.4017 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.4047 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.4046 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.4013 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.4084 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.4095 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.4079 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.4054 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.4031 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.4023 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.4034 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.4012 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.3997 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.4005 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.4003 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.3983 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.3972 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.3976 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.3959 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.3950 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.3955 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.3939 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.3921 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.3914 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.3926 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.3934 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.3928 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.3926 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.3919 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.3922 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.3933 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.3927 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.3918 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.3912 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.3917 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.3910 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.3913 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.3917 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.3936 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.3940 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.3920 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.3903 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.3906 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.3902 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.3888 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.3899 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.3895 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.3922 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.3934 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.3932 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.3929 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.3937 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.3928 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.3932 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.3931 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.3921 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.3913 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.3908 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.3907 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.3911 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.3910 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.3900 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.3907 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.3910 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.3918 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.3915 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.3923 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.3924 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.3930 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.3929 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.3926 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.3922 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.3920 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.3912 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.3917 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.3919 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.3909 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.3909 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.3911 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.3905 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.3894 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.3895 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.3893 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.3890 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.3883 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.3880 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.3879 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.3878 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.3877 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.3870 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.3867 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.3863 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.3857 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.3860 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.3856 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.3852 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.3856 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.3853 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.3849 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.3850 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.3855 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.3854 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.3847 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.3848 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.3852 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.3848 [Training] 118/118 [==============================] 255.9ms/step  batch_loss: 0.3844 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:45:19 - INFO - root -   The F1-score is 0.5915992900808519
09/25/2022 12:45:19 - INFO - root -   the best eval f1 is 0.5916, saving model !!
09/25/2022 12:45:22 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:34  batch_loss: 0.3521 [Training] 2/118 [..............................] - ETA: 1:01  batch_loss: 0.3648 [Training] 3/118 [..............................] - ETA: 50s  batch_loss: 0.3523 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 0.3401 [Training] 5/118 [>.............................] - ETA: 41s  batch_loss: 0.3386 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.3458 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.3440 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.3403 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.3410 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.3427 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.3409 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.3406 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.3425 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.3401 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.3396 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.3431 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.3439 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.3436 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.3420 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.3429 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.3415 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.3440 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.3441 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.3421 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.3435 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.3439 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.3442 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.3448 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.3454 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.3432 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.3435 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.3428 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.3453 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.3475 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.3471 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.3475 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.3481 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.3498 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.3493 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.3481 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.3471 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.3478 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.3484 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.3473 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.3476 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.3484 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.3485 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.3488 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.3486 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.3481 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.3482 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.3485 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.3471 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.3456 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.3475 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.3484 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.3471 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.3471 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.3470 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.3471 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.3467 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.3466 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.3465 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.3459 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.3467 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.3467 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.3471 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.3465 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.3454 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.3459 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.3453 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.3449 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.3445 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.3442 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.3437 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.3436 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.3436 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.3437 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.3434 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.3434 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.3427 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.3433 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.3444 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.3442 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.3438 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.3436 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.3440 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.3437 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.3441 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.3449 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.3447 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.3447 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.3446 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.3441 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.3440 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.3451 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.3450 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.3458 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.3458 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.3455 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.3450 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.3452 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.3454 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.3453 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.3446 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.3451 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.3451 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.3451 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.3446 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.3449 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.3446 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.3444 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.3447 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.3438 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.3440 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.3438 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.3438 [Training] 118/118 [==============================] 256.7ms/step  batch_loss: 0.3458 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:45:58 - INFO - root -   The F1-score is 0.6026405992128984
09/25/2022 12:45:58 - INFO - root -   the best eval f1 is 0.6026, saving model !!
09/25/2022 12:46:01 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:36  batch_loss: 0.3037 [Training] 2/118 [..............................] - ETA: 1:01  batch_loss: 0.2937 [Training] 3/118 [..............................] - ETA: 50s  batch_loss: 0.3236 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 0.3131 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.3143 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.3213 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.3176 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.3165 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.3190 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.3220 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.3166 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.3152 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.3201 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.3206 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.3213 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.3228 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.3265 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.3277 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.3277 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.3262 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.3261 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.3262 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.3259 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.3246 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.3242 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.3219 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.3223 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.3223 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.3234 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.3214 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.3222 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.3203 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.3187 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.3194 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.3193 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.3186 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.3188 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.3187 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.3180 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.3189 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.3189 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.3194 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.3175 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.3164 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.3170 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.3169 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.3169 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.3161 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.3155 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.3160 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.3143 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.3136 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.3135 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.3137 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.3131 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.3121 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.3125 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.3121 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.3126 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.3117 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.3107 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.3105 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.3120 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.3117 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.3117 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.3116 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.3112 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.3116 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.3116 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.3117 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.3116 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.3124 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.3124 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.3123 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.3137 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.3145 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.3139 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.3139 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.3143 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.3143 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.3135 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.3142 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.3142 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.3145 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.3142 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.3148 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.3147 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.3142 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.3136 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.3137 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.3134 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.3141 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.3140 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.3137 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.3137 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.3133 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.3135 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.3133 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.3135 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.3133 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.3127 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.3127 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.3136 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.3135 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.3132 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.3133 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.3133 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.3131 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.3130 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.3131 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.3128 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.3127 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.3125 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.3122 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.3124 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.3129 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.3124 [Training] 118/118 [==============================] 256.9ms/step  batch_loss: 0.3123 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:46:38 - INFO - root -   The F1-score is 0.6070709612481127
09/25/2022 12:46:38 - INFO - root -   the best eval f1 is 0.6071, saving model !!
09/25/2022 12:46:41 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:34  batch_loss: 0.2226 [Training] 2/118 [..............................] - ETA: 1:01  batch_loss: 0.2573 [Training] 3/118 [..............................] - ETA: 51s  batch_loss: 0.2592 [Training] 4/118 [>.............................] - ETA: 45s  batch_loss: 0.2573 [Training] 5/118 [>.............................] - ETA: 41s  batch_loss: 0.2495 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.2652 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.2612 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.2595 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.2588 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.2558 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.2561 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.2618 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.2660 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.2643 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.2641 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.2628 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.2630 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.2710 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.2721 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.2736 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.2737 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.2750 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.2767 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.2814 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.2827 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.2834 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.2839 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.2856 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.2861 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.2855 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.2859 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.2849 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.2848 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.2839 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.2833 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.2845 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2847 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2838 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.2868 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2864 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2866 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.2869 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.2879 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2883 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2895 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.2891 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2891 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2894 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.2886 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.2885 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2871 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2882 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.2878 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2867 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2874 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.2875 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.2889 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2886 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2893 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.2883 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2887 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2886 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2881 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.2879 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.2877 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2874 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2873 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.2868 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2867 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2869 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.2875 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.2879 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2879 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2887 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.2881 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.2884 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2888 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2886 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.2883 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2879 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2874 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2881 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.2877 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2876 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2869 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2879 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.2880 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2871 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2867 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2862 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.2874 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2878 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2877 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2878 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.2880 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2883 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2885 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2890 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2889 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2891 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2887 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2889 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2887 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2885 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2881 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2882 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2881 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2880 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2882 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2880 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2882 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2884 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2883 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2880 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2878 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2879 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2879 [Training] 118/118 [==============================] 255.5ms/step  batch_loss: 0.2878 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:47:18 - INFO - root -   The F1-score is 0.6182609809194519
09/25/2022 12:47:18 - INFO - root -   the best eval f1 is 0.6183, saving model !!
09/25/2022 12:47:21 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:35  batch_loss: 0.2064 [Training] 2/118 [..............................] - ETA: 1:02  batch_loss: 0.2329 [Training] 3/118 [..............................] - ETA: 50s  batch_loss: 0.2509 [Training] 4/118 [>.............................] - ETA: 45s  batch_loss: 0.2687 [Training] 5/118 [>.............................] - ETA: 41s  batch_loss: 0.2608 [Training] 6/118 [>.............................] - ETA: 39s  batch_loss: 0.2508 [Training] 7/118 [>.............................] - ETA: 37s  batch_loss: 0.2497 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.2518 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.2440 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.2457 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.2440 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.2435 [Training] 13/118 [==>...........................] - ETA: 31s  batch_loss: 0.2449 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.2488 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.2492 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.2496 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.2482 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.2518 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.2503 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.2500 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.2540 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.2544 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.2586 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.2585 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.2593 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.2580 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.2588 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.2616 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.2623 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.2609 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.2611 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.2603 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.2608 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.2592 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.2578 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.2571 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2569 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2574 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.2584 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2576 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2599 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.2601 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.2605 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2612 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2617 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.2608 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2609 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2614 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.2607 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.2616 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2623 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2619 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.2614 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2628 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2617 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.2616 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.2613 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2617 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2612 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.2624 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2635 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2638 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2639 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.2629 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.2621 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2628 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2621 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.2628 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2639 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2638 [Training] 71/118 [=================>............] - ETA: 11s  batch_loss: 0.2631 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.2638 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2636 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2630 [Training] 75/118 [==================>...........] - ETA: 10s  batch_loss: 0.2632 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.2649 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2663 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2657 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.2657 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2651 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2649 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2650 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.2647 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2641 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2646 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2640 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.2644 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2655 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2652 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2658 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.2664 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2666 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2660 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2666 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.2663 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2665 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2667 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2667 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2666 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2664 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2665 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2669 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2668 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2673 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2672 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2676 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2670 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2666 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2664 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2661 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2667 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2664 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2669 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2672 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2670 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2668 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2668 [Training] 118/118 [==============================] 249.4ms/step  batch_loss: 0.2671 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:47:57 - INFO - root -   The F1-score is 0.6262821753435264
09/25/2022 12:47:57 - INFO - root -   the best eval f1 is 0.6263, saving model !!
09/25/2022 12:48:00 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:35  batch_loss: 0.2114 [Training] 2/118 [..............................] - ETA: 1:01  batch_loss: 0.2211 [Training] 3/118 [..............................] - ETA: 49s  batch_loss: 0.2249 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 0.2287 [Training] 5/118 [>.............................] - ETA: 41s  batch_loss: 0.2297 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.2331 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.2410 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.2381 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.2445 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.2420 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.2406 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.2402 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.2444 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.2441 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.2458 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.2491 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.2508 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.2501 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.2502 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.2497 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.2486 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.2495 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.2499 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.2511 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.2511 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.2506 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.2510 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.2493 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.2500 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.2517 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.2503 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.2477 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.2480 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.2485 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.2481 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.2476 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2472 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2477 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.2472 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2486 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2488 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.2483 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.2475 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2492 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2488 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.2481 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2476 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2484 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.2474 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.2473 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2477 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2475 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.2466 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2457 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2457 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.2454 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.2459 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2454 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2462 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.2457 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2466 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2461 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2460 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.2466 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.2455 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2450 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2450 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.2451 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2448 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2444 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.2438 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.2435 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2437 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2442 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.2439 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.2436 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2438 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2434 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.2433 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2437 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2432 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2428 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.2429 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2439 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2438 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2435 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.2437 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2444 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2449 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2443 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.2442 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2446 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2442 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2444 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.2447 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2441 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2442 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2448 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2448 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2448 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2451 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2458 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2456 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2453 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2451 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2448 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2447 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2445 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2450 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2444 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2440 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2452 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2449 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2453 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2449 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2442 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2445 [Training] 118/118 [==============================] 255.7ms/step  batch_loss: 0.2459 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:48:36 - INFO - root -   The F1-score is 0.6275690535410393
09/25/2022 12:48:36 - INFO - root -   the best eval f1 is 0.6276, saving model !!
09/25/2022 12:48:39 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:33  batch_loss: 0.1807 [Training] 2/118 [..............................] - ETA: 1:01  batch_loss: 0.2140 [Training] 3/118 [..............................] - ETA: 50s  batch_loss: 0.2064 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 0.1942 [Training] 5/118 [>.............................] - ETA: 41s  batch_loss: 0.2019 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.2084 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.2183 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.2186 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.2208 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.2158 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.2209 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.2223 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.2241 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.2238 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.2240 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.2240 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.2263 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.2252 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.2242 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.2257 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.2239 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.2232 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.2232 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.2239 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.2250 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.2233 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.2251 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.2237 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.2252 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.2270 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.2259 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.2247 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.2242 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.2249 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.2246 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.2246 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2253 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2257 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.2260 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2256 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2254 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.2244 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.2253 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2245 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2248 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.2231 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2225 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2219 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.2217 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.2213 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2213 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2221 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.2226 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2219 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2222 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.2219 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.2217 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2217 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2214 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.2215 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2220 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2220 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2227 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.2226 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.2226 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2222 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2225 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.2232 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2228 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2221 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.2221 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.2220 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2225 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2230 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.2231 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.2224 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2227 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2235 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.2230 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2235 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2233 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2235 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.2240 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2241 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2241 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2243 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.2243 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2249 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2253 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2251 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.2252 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2250 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2251 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2254 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.2256 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2255 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2257 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2265 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2262 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2262 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2264 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2258 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2260 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2259 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2263 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2264 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2266 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2265 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2267 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2265 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2270 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2269 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2264 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2262 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2261 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2265 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2263 [Training] 118/118 [==============================] 255.3ms/step  batch_loss: 0.2265 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:49:16 - INFO - root -   The F1-score is 0.6311480571824422
09/25/2022 12:49:16 - INFO - root -   the best eval f1 is 0.6311, saving model !!
09/25/2022 12:49:19 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:35  batch_loss: 0.1897 [Training] 2/118 [..............................] - ETA: 1:01  batch_loss: 0.1801 [Training] 3/118 [..............................] - ETA: 50s  batch_loss: 0.1826 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 0.1797 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.1904 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.1975 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.2001 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.2033 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.2028 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.2015 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.1985 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.1991 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.1995 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.2013 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.2051 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.2030 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.2020 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.2021 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.2018 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.2033 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.2031 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.2040 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.2056 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.2069 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.2078 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.2072 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.2074 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.2084 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.2088 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.2082 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.2072 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.2064 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.2062 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.2061 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.2059 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.2059 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2058 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2046 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.2042 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2052 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2047 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.2047 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.2050 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2050 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2053 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.2051 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2048 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2058 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.2060 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.2063 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2058 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2054 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.2059 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2065 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2066 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.2071 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.2061 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2057 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2062 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.2059 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2063 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2056 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2056 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.2060 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.2060 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2057 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2062 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.2073 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2069 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2072 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.2072 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.2089 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2088 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2086 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.2082 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.2087 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2084 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2084 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.2088 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2087 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2094 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2096 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.2098 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2092 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2091 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2087 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.2092 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2095 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2095 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2103 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.2099 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2106 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2107 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2113 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.2115 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2114 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2112 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2110 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2109 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2105 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2107 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2105 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2101 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2103 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2103 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2105 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2101 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2101 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2106 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2102 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2103 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2100 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2105 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2113 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2113 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2114 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2112 [Training] 118/118 [==============================] 256.3ms/step  batch_loss: 0.2111 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:49:55 - INFO - root -   The F1-score is 0.6336721728081322
09/25/2022 12:49:55 - INFO - root -   the best eval f1 is 0.6337, saving model !!
09/25/2022 12:49:58 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:32  batch_loss: 0.2451 [Training] 2/118 [..............................] - ETA: 1:01  batch_loss: 0.2234 [Training] 3/118 [..............................] - ETA: 50s  batch_loss: 0.2180 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 0.2103 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.2053 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.2096 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.2169 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.2163 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.2150 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.2131 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.2114 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.2094 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.2072 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.2055 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.2031 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.2045 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.2032 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.2051 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.2032 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.2035 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.2016 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.2015 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.2027 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.2017 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.2024 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.2030 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.2028 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.2041 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.2023 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.2035 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.2024 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.2016 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1999 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1996 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1999 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.2001 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2012 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2006 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.2007 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2005 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2005 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.2006 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1997 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2007 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2003 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.2007 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2006 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2004 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1998 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1999 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2000 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2002 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.2010 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2000 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1992 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1995 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.2000 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1998 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1998 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.2001 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2004 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1998 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1998 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1999 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1997 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1993 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1990 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1988 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1984 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1980 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1979 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1983 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1980 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1978 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1988 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1988 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1989 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1995 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1992 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1988 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1988 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1988 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1991 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1991 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1997 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1991 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1985 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1997 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2004 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2004 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.2001 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2000 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2004 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2002 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1996 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1997 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1992 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1989 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1988 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1996 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1995 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1997 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1997 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1992 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1992 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1987 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1985 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1984 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1987 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1989 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1991 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1992 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1990 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1993 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1994 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1992 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1994 [Training] 118/118 [==============================] 252.6ms/step  batch_loss: 0.1987 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:50:35 - INFO - root -   The F1-score is 0.6318382162302308
09/25/2022 12:50:35 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:32  batch_loss: 0.1329 [Training] 2/118 [..............................] - ETA: 1:00  batch_loss: 0.1637 [Training] 3/118 [..............................] - ETA: 49s  batch_loss: 0.1735 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 0.1704 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.1676 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.1672 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.1697 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.1697 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.1710 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.1692 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.1714 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.1691 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.1690 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.1690 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.1708 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.1717 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.1716 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.1724 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.1723 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.1717 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1726 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.1722 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.1740 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1739 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.1759 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1757 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1760 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.1753 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1774 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1768 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1773 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1766 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1759 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1763 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1772 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1770 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1784 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1806 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1797 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1795 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1793 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1798 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1814 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1811 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1812 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.1823 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1817 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1826 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1829 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1830 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1831 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1843 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1841 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1844 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1842 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1848 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1843 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1844 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1854 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1849 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1852 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1856 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1855 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1854 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1857 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1863 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1861 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1861 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1857 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1858 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1866 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1864 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1866 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1863 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1859 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1861 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1863 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1864 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1863 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1863 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1864 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1863 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1870 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1868 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1868 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1869 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1865 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1874 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1872 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1873 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1869 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1867 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1867 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1869 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1872 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1877 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1881 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1881 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1878 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1881 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1884 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1885 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1881 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1879 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1878 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1879 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1879 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1880 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1879 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1880 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1877 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1878 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1879 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1879 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1881 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1879 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1880 [Training] 118/118 [==============================] 256.4ms/step  batch_loss: 0.1872 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:51:12 - INFO - root -   The F1-score is 0.6463629828115809
09/25/2022 12:51:12 - INFO - root -   the best eval f1 is 0.6464, saving model !!
09/25/2022 12:51:14 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:36  batch_loss: 0.1551 [Training] 2/118 [..............................] - ETA: 1:02  batch_loss: 0.1611 [Training] 3/118 [..............................] - ETA: 51s  batch_loss: 0.1845 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 0.1840 [Training] 5/118 [>.............................] - ETA: 41s  batch_loss: 0.1880 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.1865 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.1806 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.1871 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.1855 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.1814 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.1796 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.1841 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.1846 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.1841 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.1824 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.1810 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.1801 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.1787 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.1774 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.1756 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1753 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.1748 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.1748 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1732 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.1729 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1722 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1725 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.1734 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1727 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1709 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1699 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1701 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1701 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1709 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1704 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1697 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1707 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1708 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1713 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1717 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1714 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1708 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1710 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1714 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1721 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.1721 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1721 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1720 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1722 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1723 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1727 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1734 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1737 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1746 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1748 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1742 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.1743 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1742 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1736 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1736 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1741 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1746 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1746 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1747 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1750 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1749 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1753 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1754 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1755 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1760 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1757 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1763 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1761 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1757 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1757 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1759 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1761 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1758 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1757 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1754 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1760 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1763 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1763 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1767 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1768 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1766 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1763 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1761 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1766 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1761 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1762 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1760 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1762 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1772 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1773 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1775 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1778 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1779 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1778 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1782 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1782 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1785 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1779 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1779 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1785 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1785 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1783 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1783 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1785 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1783 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1784 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1785 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1788 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1788 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1788 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1787 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1790 [Training] 118/118 [==============================] 256.1ms/step  batch_loss: 0.1785 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:51:51 - INFO - root -   The F1-score is 0.6418610712889352
09/25/2022 12:51:51 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:46  batch_loss: 0.1459 [Training] 2/118 [..............................] - ETA: 1:08  batch_loss: 0.1676 [Training] 3/118 [..............................] - ETA: 55s  batch_loss: 0.1809 [Training] 4/118 [>.............................] - ETA: 48s  batch_loss: 0.1724 [Training] 5/118 [>.............................] - ETA: 44s  batch_loss: 0.1669 [Training] 6/118 [>.............................] - ETA: 41s  batch_loss: 0.1710 [Training] 7/118 [>.............................] - ETA: 39s  batch_loss: 0.1670 [Training] 8/118 [=>............................] - ETA: 38s  batch_loss: 0.1669 [Training] 9/118 [=>............................] - ETA: 36s  batch_loss: 0.1649 [Training] 10/118 [=>............................] - ETA: 35s  batch_loss: 0.1636 [Training] 11/118 [=>............................] - ETA: 34s  batch_loss: 0.1614 [Training] 12/118 [==>...........................] - ETA: 33s  batch_loss: 0.1633 [Training] 13/118 [==>...........................] - ETA: 32s  batch_loss: 0.1625 [Training] 14/118 [==>...........................] - ETA: 32s  batch_loss: 0.1654 [Training] 15/118 [==>...........................] - ETA: 31s  batch_loss: 0.1667 [Training] 16/118 [===>..........................] - ETA: 31s  batch_loss: 0.1666 [Training] 17/118 [===>..........................] - ETA: 30s  batch_loss: 0.1664 [Training] 18/118 [===>..........................] - ETA: 30s  batch_loss: 0.1655 [Training] 19/118 [===>..........................] - ETA: 29s  batch_loss: 0.1655 [Training] 20/118 [====>.........................] - ETA: 29s  batch_loss: 0.1645 [Training] 21/118 [====>.........................] - ETA: 28s  batch_loss: 0.1629 [Training] 22/118 [====>.........................] - ETA: 28s  batch_loss: 0.1650 [Training] 23/118 [====>.........................] - ETA: 27s  batch_loss: 0.1649 [Training] 24/118 [=====>........................] - ETA: 27s  batch_loss: 0.1646 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.1645 [Training] 26/118 [=====>........................] - ETA: 26s  batch_loss: 0.1646 [Training] 27/118 [=====>........................] - ETA: 26s  batch_loss: 0.1658 [Training] 28/118 [======>.......................] - ETA: 25s  batch_loss: 0.1662 [Training] 29/118 [======>.......................] - ETA: 25s  batch_loss: 0.1668 [Training] 30/118 [======>.......................] - ETA: 25s  batch_loss: 0.1669 [Training] 31/118 [======>.......................] - ETA: 24s  batch_loss: 0.1652 [Training] 32/118 [=======>......................] - ETA: 24s  batch_loss: 0.1637 [Training] 33/118 [=======>......................] - ETA: 24s  batch_loss: 0.1647 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 0.1655 [Training] 35/118 [=======>......................] - ETA: 23s  batch_loss: 0.1662 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.1654 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.1649 [Training] 38/118 [========>.....................] - ETA: 22s  batch_loss: 0.1640 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1639 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.1640 [Training] 41/118 [=========>....................] - ETA: 21s  batch_loss: 0.1635 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1632 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.1625 [Training] 44/118 [==========>...................] - ETA: 20s  batch_loss: 0.1628 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1630 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.1627 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.1625 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1640 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1630 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.1629 [Training] 51/118 [===========>..................] - ETA: 18s  batch_loss: 0.1631 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1628 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1627 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.1619 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1628 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1632 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.1632 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1634 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1638 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1641 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.1647 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1644 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1640 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1641 [Training] 65/118 [===============>..............] - ETA: 14s  batch_loss: 0.1645 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1649 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1657 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1655 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1652 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1646 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1643 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.1639 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1646 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1652 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1657 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1652 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1655 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1657 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1663 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1668 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1670 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1673 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1672 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1673 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1682 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1679 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1678 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1678 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1676 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1673 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.1674 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1677 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1673 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1677 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1677 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1677 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1675 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1678 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1675 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1674 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1675 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1674 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1676 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1673 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1674 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1675 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1681 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1682 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1679 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1684 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1682 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1684 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1683 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1685 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1684 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1682 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1683 [Training] 118/118 [==============================] 257.2ms/step  batch_loss: 0.1688 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:52:28 - INFO - root -   The F1-score is 0.6445049248315189
09/25/2022 12:52:28 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:29  batch_loss: 0.1339 [Training] 2/118 [..............................] - ETA: 59s  batch_loss: 0.1218 [Training] 3/118 [..............................] - ETA: 49s  batch_loss: 0.1306 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 0.1309 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.1327 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.1404 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.1420 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.1499 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.1532 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.1545 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.1597 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.1575 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.1568 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.1567 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.1562 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.1541 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.1539 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.1542 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.1538 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.1546 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1549 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.1540 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.1534 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1528 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.1534 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1521 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1517 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.1531 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1523 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1528 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1521 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1520 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1524 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1547 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1562 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1570 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1583 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1579 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1570 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1576 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1575 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1579 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1581 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1590 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1581 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1583 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1588 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1583 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1577 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1581 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1573 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1584 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1580 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1580 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1584 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1581 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1578 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1579 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1581 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1576 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1578 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1575 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1582 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1575 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1583 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1596 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1617 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1616 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1616 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1617 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1614 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1620 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1617 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1624 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1619 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1615 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1619 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1617 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1623 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1623 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1638 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1641 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1637 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1633 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1639 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1635 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1635 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1629 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1629 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1626 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1633 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1633 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1635 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1639 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1633 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1633 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1633 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1631 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1634 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1633 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1628 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1628 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1628 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1625 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1624 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1624 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1626 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1622 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1627 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1626 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1626 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1625 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1626 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1629 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1629 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1628 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1625 [Training] 118/118 [==============================] 255.9ms/step  batch_loss: 0.1631 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:53:05 - INFO - root -   The F1-score is 0.6406981155937778
09/25/2022 12:53:05 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:33  batch_loss: 0.1059 [Training] 2/118 [..............................] - ETA: 1:00  batch_loss: 0.1194 [Training] 3/118 [..............................] - ETA: 49s  batch_loss: 0.1200 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 0.1244 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.1311 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.1332 [Training] 7/118 [>.............................] - ETA: 35s  batch_loss: 0.1327 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.1336 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.1377 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.1374 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.1414 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.1421 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.1398 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.1381 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.1420 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.1416 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.1401 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.1408 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.1412 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.1414 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1405 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.1413 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.1405 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1408 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.1419 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.1434 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1435 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.1444 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.1449 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1450 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.1440 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.1449 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1451 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 0.1465 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.1464 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1465 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1473 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.1491 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1483 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1491 [Training] 41/118 [=========>....................] - ETA: 19s  batch_loss: 0.1489 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.1483 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1477 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1471 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.1479 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1489 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1487 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1496 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.1500 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1493 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1496 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1491 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.1491 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1494 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1493 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.1497 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1499 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1505 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1501 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.1502 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1497 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1496 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1502 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.1496 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1493 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1500 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1499 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1501 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1505 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1505 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1502 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1500 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1496 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1496 [Training] 75/118 [==================>...........] - ETA: 10s  batch_loss: 0.1492 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1490 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1488 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1495 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.1497 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1495 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1497 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1499 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.1499 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1497 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1498 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1495 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1496 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1493 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1491 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1491 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1492 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1493 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1493 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1489 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1487 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1495 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1498 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1500 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1496 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1507 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1509 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1519 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1525 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1529 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1528 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1526 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1526 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1529 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1530 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1534 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1533 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1533 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1531 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1527 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1526 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1525 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1525 [Training] 118/118 [==============================] 253.3ms/step  batch_loss: 0.1527 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:53:41 - INFO - root -   The F1-score is 0.6460878606706525
09/25/2022 12:53:41 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:29  batch_loss: 0.0960 [Training] 2/118 [..............................] - ETA: 59s  batch_loss: 0.1088 [Training] 3/118 [..............................] - ETA: 48s  batch_loss: 0.1171 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 0.1255 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.1290 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.1268 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.1350 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.1355 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.1394 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.1363 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.1361 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.1348 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.1390 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.1369 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.1359 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.1338 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.1322 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.1329 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.1326 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.1311 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1311 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.1356 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.1364 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1350 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.1399 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1390 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1385 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.1383 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1394 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1389 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1387 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1380 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1393 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1404 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1404 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1415 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1420 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1413 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1423 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1418 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1424 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1426 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1427 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1433 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1441 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.1437 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1437 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1440 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1447 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1445 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1447 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1439 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1445 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1452 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1446 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1453 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.1450 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1451 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1450 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1454 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1453 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1450 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1459 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1455 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1459 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1459 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1454 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1448 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1451 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1452 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1458 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.1457 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1460 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1462 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1463 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1458 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1456 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1452 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1456 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1453 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1455 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1450 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1444 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1446 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1445 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1448 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1444 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1443 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1443 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1443 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.1438 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1440 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1436 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1438 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1444 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1448 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1451 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1451 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1449 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1449 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1460 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1459 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1456 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1454 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1456 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1459 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1458 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1460 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1462 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1464 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1462 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1463 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1462 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1462 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1463 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1463 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1461 [Training] 118/118 [==============================] 256.5ms/step  batch_loss: 0.1461 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:54:18 - INFO - root -   The F1-score is 0.6481313849734901
09/25/2022 12:54:18 - INFO - root -   the best eval f1 is 0.6481, saving model !!
09/25/2022 12:54:21 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:36  batch_loss: 0.1648 [Training] 2/118 [..............................] - ETA: 1:01  batch_loss: 0.1503 [Training] 3/118 [..............................] - ETA: 50s  batch_loss: 0.1393 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 0.1294 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.1365 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.1397 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.1403 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.1410 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.1408 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.1356 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.1374 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.1376 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.1383 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.1379 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.1374 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.1356 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.1353 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.1358 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.1363 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.1361 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1380 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.1378 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.1361 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1356 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.1355 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.1343 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1341 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.1348 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1353 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1362 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1356 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1381 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1388 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1377 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1372 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1376 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1398 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1398 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1398 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1395 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1393 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1387 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1386 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1386 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1390 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1388 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1389 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1382 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1379 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1380 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1376 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1376 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1373 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1377 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1370 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1374 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1377 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1375 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1378 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1381 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1384 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1383 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1381 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1376 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1379 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1379 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1389 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1392 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1390 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1387 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1383 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1380 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1380 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1380 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1379 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1384 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1379 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1377 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1383 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1382 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1381 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1381 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1384 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1384 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1379 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1384 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1391 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1399 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1404 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1405 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1406 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1406 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1408 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1409 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1414 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1409 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1410 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1406 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1411 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1409 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1407 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1410 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1409 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1405 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1409 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1408 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1417 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1421 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1422 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1418 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1418 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1417 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1422 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1422 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1423 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1425 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1419 [Training] 118/118 [==============================] 255.0ms/step  batch_loss: 0.1425 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:54:58 - INFO - root -   The F1-score is 0.6550675894185369
09/25/2022 12:54:58 - INFO - root -   the best eval f1 is 0.6551, saving model !!
09/25/2022 12:55:00 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:38  batch_loss: 0.1251 [Training] 2/118 [..............................] - ETA: 1:03  batch_loss: 0.1224 [Training] 3/118 [..............................] - ETA: 50s  batch_loss: 0.1234 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 0.1259 [Training] 5/118 [>.............................] - ETA: 41s  batch_loss: 0.1271 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.1298 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.1283 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.1274 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.1289 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.1325 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.1343 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.1313 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.1315 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.1297 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.1345 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.1378 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.1347 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.1313 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.1322 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.1306 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1295 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.1298 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.1309 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1337 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.1333 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.1334 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1329 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.1318 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.1328 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1329 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.1328 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.1322 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1313 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 0.1316 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.1308 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1320 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1329 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.1328 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1330 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1320 [Training] 41/118 [=========>....................] - ETA: 19s  batch_loss: 0.1315 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.1321 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1319 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1317 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.1320 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1313 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1316 [Training] 48/118 [===========>..................] - ETA: 17s  batch_loss: 0.1316 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.1319 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1325 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1322 [Training] 52/118 [============>.................] - ETA: 16s  batch_loss: 0.1322 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.1316 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1314 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1313 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.1315 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1313 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1313 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1313 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.1314 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1316 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1324 [Training] 63/118 [===============>..............] - ETA: 13s  batch_loss: 0.1319 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.1314 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1325 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1331 [Training] 67/118 [================>.............] - ETA: 12s  batch_loss: 0.1334 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1336 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1334 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1335 [Training] 71/118 [=================>............] - ETA: 11s  batch_loss: 0.1335 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1334 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1337 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1336 [Training] 75/118 [==================>...........] - ETA: 10s  batch_loss: 0.1340 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1342 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1340 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1343 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.1344 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1342 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1348 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1346 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.1341 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1340 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1341 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1346 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1347 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1349 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1346 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1344 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1341 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1341 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1337 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1339 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1339 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1340 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1338 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1339 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1338 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1334 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1332 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1332 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1335 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1336 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1336 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1336 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1339 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1346 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1346 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1344 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1349 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1348 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1346 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1345 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1346 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1345 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1345 [Training] 118/118 [==============================] 250.9ms/step  batch_loss: 0.1343 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:55:37 - INFO - root -   The F1-score is 0.6500358236175341
09/25/2022 12:55:37 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:31  batch_loss: 0.1650 [Training] 2/118 [..............................] - ETA: 59s  batch_loss: 0.1453 [Training] 3/118 [..............................] - ETA: 49s  batch_loss: 0.1426 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 0.1295 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.1256 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.1259 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.1268 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.1245 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.1251 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.1272 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.1304 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.1275 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.1283 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.1275 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.1267 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.1262 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.1265 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.1260 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.1270 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.1278 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1292 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.1294 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.1289 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1292 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.1275 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1279 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1285 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.1293 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1282 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1295 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1298 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1305 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1309 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1309 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1312 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1316 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1313 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1309 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1305 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1303 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1294 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1281 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1277 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1278 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1281 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1274 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1265 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1262 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1257 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1258 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1263 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1272 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.1286 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1281 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1279 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1277 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1272 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1268 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1263 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1266 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1265 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1262 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1261 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1261 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1267 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1270 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1272 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1268 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1269 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1265 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1266 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1266 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1271 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1263 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1266 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1272 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1269 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1279 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1279 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1282 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1282 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1283 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1285 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1281 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1284 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1292 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1299 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1296 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1293 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1290 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1289 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1291 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1287 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1289 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1285 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1288 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1289 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1293 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1293 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1296 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1302 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1304 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1302 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1305 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1303 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1303 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1307 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1307 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1310 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1309 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1314 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1310 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1312 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1309 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1311 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1313 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1316 [Training] 118/118 [==============================] 255.4ms/step  batch_loss: 0.1321 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:56:13 - INFO - root -   The F1-score is 0.6415579407416928
09/25/2022 12:56:13 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:35  batch_loss: 0.1547 [Training] 2/118 [..............................] - ETA: 1:01  batch_loss: 0.1357 [Training] 3/118 [..............................] - ETA: 50s  batch_loss: 0.1336 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 0.1299 [Training] 5/118 [>.............................] - ETA: 41s  batch_loss: 0.1344 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.1363 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.1389 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.1368 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.1374 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.1350 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.1326 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.1314 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.1298 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.1262 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.1301 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.1281 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.1286 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.1282 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.1276 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.1267 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.1256 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.1253 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.1274 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1270 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.1255 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1251 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1244 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.1249 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1241 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1235 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1243 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1254 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1259 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1255 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1259 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1258 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1250 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1247 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1246 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1237 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1240 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1244 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1244 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1247 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1243 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1240 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1234 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1233 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1230 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1234 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1230 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1236 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1238 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1243 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1248 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1246 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1244 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1239 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1247 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1246 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1244 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1239 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1239 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1239 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1238 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1236 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1236 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1240 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1234 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1233 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1231 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1230 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1227 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1227 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1231 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1232 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1230 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1229 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1229 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1228 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1226 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1228 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1234 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1235 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1239 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1237 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1238 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1240 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1238 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1235 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1246 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1249 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1250 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1249 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1247 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1254 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1254 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1254 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1254 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1253 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1252 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1253 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1254 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1258 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1256 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1257 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1258 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1258 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1260 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1262 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1259 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1258 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1259 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1261 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1263 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1261 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1261 [Training] 118/118 [==============================] 256.3ms/step  batch_loss: 0.1261 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:56:50 - INFO - root -   The F1-score is 0.6496790252849469
09/25/2022 12:56:50 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:32  batch_loss: 0.1660 [Training] 2/118 [..............................] - ETA: 1:00  batch_loss: 0.1376 [Training] 3/118 [..............................] - ETA: 49s  batch_loss: 0.1232 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 0.1173 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.1160 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.1137 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.1078 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.1062 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.1056 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.1121 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.1104 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.1140 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.1137 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.1170 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.1184 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.1151 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.1174 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.1168 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.1158 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.1151 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1139 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.1176 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.1175 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1183 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.1187 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.1184 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1181 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.1173 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.1178 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1182 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1181 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1171 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1166 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1159 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1147 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1155 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1150 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1151 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1149 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1147 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1146 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.1143 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1147 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1145 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1138 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1131 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1135 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1135 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1134 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1143 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1141 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1145 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.1148 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1144 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1139 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1141 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1146 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1148 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1153 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1159 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1156 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1155 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1159 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1157 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1156 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1149 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1149 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1147 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1145 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1146 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1147 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1147 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1154 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1164 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1160 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1161 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1162 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1156 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1160 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1160 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1162 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1161 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1161 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1162 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1162 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1159 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1165 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1169 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1170 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1173 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1172 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1173 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1175 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1176 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1176 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1177 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1180 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1183 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1184 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1183 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1181 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1180 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1182 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1188 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1191 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1190 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1191 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1190 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1195 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1196 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1199 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1201 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1202 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1204 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1206 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1209 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1211 [Training] 118/118 [==============================] 254.9ms/step  batch_loss: 0.1211 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:57:27 - INFO - root -   The F1-score is 0.6545384168763305
********** predict start ***********
********** Done ***************
