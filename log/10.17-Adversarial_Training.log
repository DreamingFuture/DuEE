nohup: ignoring input
*********** data_prepare *************
开始数据预处理
开始处理tag.dict 和 tsv文件

=================DUEE 1.0 DATASET==============

=================start schema process==============
input path ./conf/DuEE1.0/event_schema.json
save trigger tag 393 at ./conf/DuEE1.0/role_tag.dict
=================end schema process===============

=================start sentence process==============

----trigger------for dir ./data/DuEE1.0 to ./data/DuEE1.0/trigger
train 6544 dev 1622 test 147541

----role------for dir ./data/DuEE1.0 to ./data/DuEE1.0/role
Traceback (most recent call last):
  File "duee_1_data_prepare.py", line 79, in <module>
    train_role = data_process("{}/duee_train.json".format(data_dir), "role", type="duee1", labels_list=train_tri)
  File "/data/qingyang/event_extration/DuEE_merge/data/data_utils.py", line 137, in data_process
    start = arg["argument_start_index"]
KeyError: 'argument_start_index'
********** train start *************
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 5, 
    "event_type": "role", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 1e-05, 
    "max_len": 150, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 30.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 48, 
    "per_gpu_train_batch_size": 64, 
    "seed": 66, 
    "stride": 100, 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/7283 [00:00<?, ?it/s]tokenizing...:   4%|▎         | 269/7283 [00:00<00:02, 2681.84it/s]tokenizing...:   8%|▊         | 565/7283 [00:00<00:02, 2842.85it/s]tokenizing...:  12%|█▏        | 850/7283 [00:00<00:02, 2758.30it/s]tokenizing...:  15%|█▌        | 1127/7283 [00:00<00:02, 2663.21it/s]tokenizing...:  19%|█▉        | 1394/7283 [00:00<00:02, 2612.99it/s]tokenizing...:  23%|██▎       | 1673/7283 [00:00<00:02, 2668.66it/s]tokenizing...:  27%|██▋       | 1941/7283 [00:00<00:02, 2617.73it/s]tokenizing...:  30%|███       | 2204/7283 [00:00<00:01, 2559.38it/s]tokenizing...:  34%|███▍      | 2462/7283 [00:00<00:01, 2561.55it/s]tokenizing...:  37%|███▋      | 2719/7283 [00:01<00:01, 2518.84it/s]tokenizing...:  41%|████      | 2989/7283 [00:01<00:01, 2570.40it/s]tokenizing...:  45%|████▍     | 3247/7283 [00:01<00:01, 2564.88it/s]tokenizing...:  48%|████▊     | 3504/7283 [00:01<00:01, 2549.12it/s]tokenizing...:  52%|█████▏    | 3760/7283 [00:01<00:01, 2544.12it/s]tokenizing...:  55%|█████▌    | 4015/7283 [00:01<00:01, 2505.78it/s]tokenizing...:  59%|█████▊    | 4266/7283 [00:01<00:01, 2477.48it/s]tokenizing...:  62%|██████▏   | 4518/7283 [00:01<00:01, 2487.94it/s]tokenizing...:  65%|██████▌   | 4767/7283 [00:01<00:01, 2465.97it/s]tokenizing...:  69%|██████▉   | 5050/7283 [00:01<00:00, 2568.64it/s]tokenizing...:  73%|███████▎  | 5334/7283 [00:02<00:00, 2644.06it/s]tokenizing...:  77%|███████▋  | 5599/7283 [00:02<00:00, 1798.29it/s]tokenizing...:  80%|████████  | 5844/7283 [00:02<00:00, 1941.50it/s]tokenizing...:  84%|████████▍ | 6104/7283 [00:02<00:00, 2100.81it/s]tokenizing...:  88%|████████▊ | 6381/7283 [00:02<00:00, 2270.79it/s]tokenizing...:  91%|█████████ | 6629/7283 [00:02<00:00, 2132.79it/s]tokenizing...:  94%|█████████▍| 6858/7283 [00:02<00:00, 1990.04it/s]tokenizing...:  97%|█████████▋| 7098/7283 [00:02<00:00, 2092.25it/s]tokenizing...: 100%|██████████| 7283/7283 [00:03<00:00, 2385.40it/s]
tokenizing...:   0%|          | 0/1913 [00:00<?, ?it/s]tokenizing...:  15%|█▌        | 289/1913 [00:00<00:00, 2881.98it/s]tokenizing...:  30%|███       | 578/1913 [00:00<00:00, 2527.88it/s]tokenizing...:  44%|████▎     | 834/1913 [00:00<00:00, 2395.41it/s]tokenizing...:  56%|█████▌    | 1076/1913 [00:00<00:00, 2384.55it/s]tokenizing...:  69%|██████▉   | 1318/1913 [00:00<00:00, 2394.56it/s]tokenizing...:  82%|████████▏ | 1572/1913 [00:00<00:00, 2441.21it/s]tokenizing...:  95%|█████████▍| 1817/1913 [00:00<00:00, 2265.98it/s]tokenizing...: 100%|██████████| 1913/1913 [00:00<00:00, 2373.78it/s]
10/17/2022 14:13:08 - INFO - root -   The nums of the train_dataset features is 7283
10/17/2022 14:13:08 - INFO - root -   The nums of the eval_dataset features is 1913
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
10/17/2022 14:13:13 - INFO - root -   ***** Running train *****
********************************************************************************
/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Traceback (most recent call last):
  File "run_ner.py", line 221, in <module>
    main()
  File "run_ner.py", line 198, in main
    train(args, train_iter, model)
  File "run_ner.py", line 91, in train
    logits = model(
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/event_extration/DuEE_merge/model/model.py", line 22, in forward
    output = self.bert(input_ids,
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1018, in forward
    encoder_outputs = self.encoder(
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 607, in forward
    layer_outputs = layer_module(
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 535, in forward
    layer_output = apply_chunking_to_forward(
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/pytorch_utils.py", line 243, in apply_chunking_to_forward
    return forward_fn(*input_tensors)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 548, in feed_forward_chunk
    layer_output = self.output(intermediate_output, attention_output)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 461, in forward
    hidden_states = self.dropout(hidden_states)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.76 GiB total capacity; 1.32 GiB already allocated; 10.44 MiB free; 1.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
********** predict start ***********
********** Done ***************
nohup: ignoring input
*********** data_prepare *************
开始数据预处理
开始处理tag.dict 和 tsv文件

=================DUEE 1.0 DATASET==============

=================start schema process==============
input path ./conf/DuEE1.0/event_schema.json
save trigger tag 393 at ./conf/DuEE1.0/role_tag.dict
=================end schema process===============

=================start sentence process==============

----trigger------for dir ./data/DuEE1.0 to ./data/DuEE1.0/trigger
train 6544 dev 1622 test 147541

----role------for dir ./data/DuEE1.0 to ./data/DuEE1.0/role
Traceback (most recent call last):
  File "duee_1_data_prepare.py", line 79, in <module>
    train_role = data_process("{}/duee_train.json".format(data_dir), "role", type="duee1", labels_list=train_tri)
  File "/data/qingyang/event_extration/DuEE_merge/data/data_utils.py", line 137, in data_process
    start = arg["argument_start_index"]
KeyError: 'argument_start_index'
********** train start *************
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 5, 
    "event_type": "role", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 1e-05, 
    "max_len": 150, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 30.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 48, 
    "per_gpu_train_batch_size": 64, 
    "seed": 66, 
    "stride": 100, 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/7283 [00:00<?, ?it/s]tokenizing...:   5%|▍         | 360/7283 [00:00<00:01, 3597.30it/s]tokenizing...:  10%|█         | 743/7283 [00:00<00:01, 3727.31it/s]tokenizing...:  15%|█▌        | 1116/7283 [00:00<00:01, 3162.93it/s]tokenizing...:  20%|█▉        | 1441/7283 [00:00<00:01, 3025.35it/s]tokenizing...:  24%|██▍       | 1749/7283 [00:00<00:01, 3042.45it/s]tokenizing...:  28%|██▊       | 2057/7283 [00:00<00:01, 2945.36it/s]tokenizing...:  32%|███▏      | 2354/7283 [00:00<00:01, 2879.61it/s]tokenizing...:  36%|███▋      | 2644/7283 [00:00<00:01, 2842.60it/s]tokenizing...:  40%|████      | 2948/7283 [00:00<00:01, 2898.17it/s]tokenizing...:  44%|████▍     | 3239/7283 [00:01<00:01, 2876.65it/s]tokenizing...:  48%|████▊     | 3528/7283 [00:01<00:01, 2839.43it/s]tokenizing...:  52%|█████▏    | 3816/7283 [00:01<00:01, 2848.61it/s]tokenizing...:  56%|█████▋    | 4102/7283 [00:01<00:01, 2770.88it/s]tokenizing...:  60%|██████    | 4380/7283 [00:01<00:01, 2686.61it/s]tokenizing...:  64%|██████▍   | 4650/7283 [00:01<00:00, 2690.02it/s]tokenizing...:  68%|██████▊   | 4944/7283 [00:01<00:00, 2760.21it/s]tokenizing...:  72%|███████▏  | 5261/7283 [00:01<00:00, 2879.94it/s]tokenizing...:  76%|███████▌  | 5550/7283 [00:02<00:00, 1926.51it/s]tokenizing...:  80%|███████▉  | 5801/7283 [00:02<00:00, 2053.78it/s]tokenizing...:  84%|████████▎ | 6082/7283 [00:02<00:00, 2233.27it/s]tokenizing...:  88%|████████▊ | 6386/7283 [00:02<00:00, 2438.16it/s]tokenizing...:  91%|█████████▏| 6654/7283 [00:02<00:00, 2425.18it/s]tokenizing...:  95%|█████████▍| 6913/7283 [00:02<00:00, 2356.94it/s]tokenizing...:  99%|█████████▉| 7239/7283 [00:02<00:00, 2598.55it/s]tokenizing...: 100%|██████████| 7283/7283 [00:02<00:00, 2675.35it/s]
tokenizing...:   0%|          | 0/1913 [00:00<?, ?it/s]tokenizing...:  18%|█▊        | 340/1913 [00:00<00:00, 3372.26it/s]tokenizing...:  35%|███▌      | 678/1913 [00:00<00:00, 2940.84it/s]tokenizing...:  51%|█████     | 976/1913 [00:00<00:00, 2725.88it/s]tokenizing...:  65%|██████▌   | 1251/1913 [00:00<00:00, 2505.26it/s]tokenizing...:  79%|███████▊  | 1504/1913 [00:00<00:00, 2447.53it/s]tokenizing...:  91%|█████████▏| 1750/1913 [00:00<00:00, 2069.42it/s]tokenizing...: 100%|██████████| 1913/1913 [00:00<00:00, 2299.22it/s]
10/17/2022 14:15:13 - INFO - root -   The nums of the train_dataset features is 7283
10/17/2022 14:15:13 - INFO - root -   The nums of the eval_dataset features is 1913
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
10/17/2022 14:15:19 - INFO - root -   ***** Running train *****
********************************************************************************
/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Traceback (most recent call last):
  File "run_ner.py", line 221, in <module>
    main()
  File "run_ner.py", line 198, in main
    train(args, train_iter, model)
  File "run_ner.py", line 91, in train
    logits = model(
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/event_extration/DuEE_merge/model/model.py", line 22, in forward
    output = self.bert(input_ids,
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1018, in forward
    encoder_outputs = self.encoder(
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 607, in forward
    layer_outputs = layer_module(
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 535, in forward
    layer_output = apply_chunking_to_forward(
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/pytorch_utils.py", line 243, in apply_chunking_to_forward
    return forward_fn(*input_tensors)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 548, in feed_forward_chunk
    layer_output = self.output(intermediate_output, attention_output)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 461, in forward
    hidden_states = self.dropout(hidden_states)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.76 GiB total capacity; 1.32 GiB already allocated; 10.44 MiB free; 1.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
********** predict start ***********
********** Done ***************
nohup: ignoring input
*********** data_prepare *************
开始数据预处理
开始处理tag.dict 和 tsv文件

=================DUEE 1.0 DATASET==============

=================start schema process==============
input path ./conf/DuEE1.0/event_schema.json
save trigger tag 393 at ./conf/DuEE1.0/role_tag.dict
=================end schema process===============

=================start sentence process==============

----trigger------for dir ./data/DuEE1.0 to ./data/DuEE1.0/trigger
train 6544 dev 1622 test 147541

----role------for dir ./data/DuEE1.0 to ./data/DuEE1.0/role
Traceback (most recent call last):
  File "duee_1_data_prepare.py", line 79, in <module>
    train_role = data_process("{}/duee_train.json".format(data_dir), "role", type="duee1", labels_list=train_tri)
  File "/data/qingyang/event_extration/DuEE_merge/data/data_utils.py", line 137, in data_process
    start = arg["argument_start_index"]
KeyError: 'argument_start_index'
********** train start *************
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 5, 
    "event_type": "role", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 1e-05, 
    "max_len": 150, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 30.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 48, 
    "per_gpu_train_batch_size": 64, 
    "seed": 66, 
    "stride": 100, 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/7283 [00:00<?, ?it/s]tokenizing...:   4%|▎         | 273/7283 [00:00<00:02, 2726.84it/s]tokenizing...:   7%|▋         | 546/7283 [00:00<00:02, 2503.37it/s]tokenizing...:  11%|█         | 798/7283 [00:00<00:02, 2371.18it/s]tokenizing...:  14%|█▍        | 1037/7283 [00:00<00:03, 1905.22it/s]tokenizing...:  17%|█▋        | 1268/7283 [00:00<00:02, 2023.56it/s]tokenizing...:  21%|██▏       | 1555/7283 [00:00<00:02, 2275.59it/s]tokenizing...:  25%|██▌       | 1830/7283 [00:00<00:02, 2417.11it/s]tokenizing...:  29%|██▊       | 2091/7283 [00:00<00:02, 2473.62it/s]tokenizing...:  32%|███▏      | 2344/7283 [00:01<00:02, 2360.64it/s]tokenizing...:  35%|███▌      | 2585/7283 [00:01<00:02, 2333.78it/s]tokenizing...:  39%|███▉      | 2868/7283 [00:01<00:01, 2475.39it/s]tokenizing...:  43%|████▎     | 3134/7283 [00:01<00:01, 2528.32it/s]tokenizing...:  47%|████▋     | 3398/7283 [00:01<00:01, 2561.07it/s]tokenizing...:  50%|█████     | 3656/7283 [00:01<00:01, 2527.93it/s]tokenizing...:  54%|█████▍    | 3931/7283 [00:01<00:01, 2592.42it/s]tokenizing...:  58%|█████▊    | 4192/7283 [00:01<00:01, 2556.48it/s]tokenizing...:  61%|██████    | 4449/7283 [00:01<00:01, 2522.66it/s]tokenizing...:  65%|██████▍   | 4702/7283 [00:01<00:01, 2518.39it/s]tokenizing...:  68%|██████▊   | 4986/7283 [00:02<00:00, 2612.20it/s]tokenizing...:  73%|███████▎  | 5285/7283 [00:02<00:00, 2722.41it/s]tokenizing...:  76%|███████▋  | 5558/7283 [00:02<00:00, 1786.43it/s]tokenizing...:  80%|███████▉  | 5803/7283 [00:02<00:00, 1930.64it/s]tokenizing...:  83%|████████▎ | 6065/7283 [00:02<00:00, 2093.93it/s]tokenizing...:  87%|████████▋ | 6337/7283 [00:02<00:00, 2250.77it/s]tokenizing...:  90%|█████████ | 6585/7283 [00:02<00:00, 2298.58it/s]tokenizing...:  94%|█████████▍| 6832/7283 [00:02<00:00, 2202.92it/s]tokenizing...:  97%|█████████▋| 7085/7283 [00:03<00:00, 2290.24it/s]tokenizing...: 100%|██████████| 7283/7283 [00:03<00:00, 2348.24it/s]
tokenizing...:   0%|          | 0/1913 [00:00<?, ?it/s]tokenizing...:  16%|█▋        | 315/1913 [00:00<00:00, 3148.60it/s]tokenizing...:  33%|███▎      | 630/1913 [00:00<00:00, 2790.92it/s]tokenizing...:  48%|████▊     | 912/1913 [00:00<00:00, 2676.23it/s]tokenizing...:  62%|██████▏   | 1181/1913 [00:00<00:00, 2626.60it/s]tokenizing...:  76%|███████▌  | 1445/1913 [00:00<00:00, 2599.39it/s]tokenizing...:  89%|████████▉ | 1706/1913 [00:00<00:00, 2403.42it/s]tokenizing...: 100%|██████████| 1913/1913 [00:00<00:00, 2547.62it/s]
10/17/2022 14:15:40 - INFO - root -   The nums of the train_dataset features is 7283
10/17/2022 14:15:40 - INFO - root -   The nums of the eval_dataset features is 1913
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
10/17/2022 14:15:46 - INFO - root -   ***** Running train *****
********************************************************************************
/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Traceback (most recent call last):
  File "run_ner.py", line 221, in <module>
    main()
  File "run_ner.py", line 198, in main
    train(args, train_iter, model)
  File "run_ner.py", line 91, in train
    logits = model(
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/event_extration/DuEE_merge/model/model.py", line 22, in forward
    output = self.bert(input_ids,
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1018, in forward
    encoder_outputs = self.encoder(
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 607, in forward
    layer_outputs = layer_module(
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 535, in forward
    layer_output = apply_chunking_to_forward(
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/pytorch_utils.py", line 243, in apply_chunking_to_forward
    return forward_fn(*input_tensors)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 548, in feed_forward_chunk
    layer_output = self.output(intermediate_output, attention_output)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 461, in forward
    hidden_states = self.dropout(hidden_states)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.76 GiB total capacity; 1.32 GiB already allocated; 10.44 MiB free; 1.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
********** predict start ***********
********** Done ***************
nohup: ignoring input
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 5, 
    "event_type": "role", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 1e-05, 
    "max_len": 150, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 30.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 48, 
    "per_gpu_train_batch_size": 64, 
    "seed": 66, 
    "stride": 100, 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/7666 [00:00<?, ?it/s]tokenizing...:   5%|▌         | 401/7666 [00:00<00:01, 4007.69it/s]tokenizing...:  11%|█         | 823/7666 [00:00<00:01, 4128.43it/s]tokenizing...:  16%|█▌        | 1236/7666 [00:00<00:01, 3515.90it/s]tokenizing...:  21%|██        | 1597/7666 [00:00<00:01, 3204.94it/s]tokenizing...:  25%|██▌       | 1925/7666 [00:00<00:01, 3093.48it/s]tokenizing...:  29%|██▉       | 2239/7666 [00:00<00:01, 2961.90it/s]tokenizing...:  33%|███▎      | 2538/7666 [00:00<00:01, 2878.44it/s]tokenizing...:  37%|███▋      | 2828/7666 [00:00<00:01, 2856.14it/s]tokenizing...:  41%|████      | 3115/7666 [00:01<00:01, 2799.66it/s]tokenizing...:  44%|████▍     | 3397/7666 [00:01<00:01, 2802.58it/s]tokenizing...:  48%|████▊     | 3678/7666 [00:01<00:01, 2795.25it/s]tokenizing...:  52%|█████▏    | 3958/7666 [00:01<00:01, 2789.16it/s]tokenizing...:  55%|█████▌    | 4237/7666 [00:01<00:01, 2769.98it/s]tokenizing...:  59%|█████▉    | 4515/7666 [00:01<00:01, 2765.33it/s]tokenizing...:  63%|██████▎   | 4792/7666 [00:01<00:01, 2738.85it/s]tokenizing...:  66%|██████▌   | 5066/7666 [00:01<00:00, 2715.19it/s]tokenizing...:  70%|██████▉   | 5343/7666 [00:01<00:01, 2073.25it/s]tokenizing...:  74%|███████▍  | 5669/7666 [00:02<00:00, 2359.89it/s]tokenizing...:  77%|███████▋  | 5940/7666 [00:02<00:00, 2443.89it/s]tokenizing...:  81%|████████  | 6202/7666 [00:02<00:00, 2287.22it/s]tokenizing...:  84%|████████▍ | 6469/7666 [00:02<00:00, 2386.05it/s]tokenizing...:  88%|████████▊ | 6758/7666 [00:02<00:00, 2522.45it/s]tokenizing...:  92%|█████████▏| 7020/7666 [00:02<00:00, 2514.88it/s]tokenizing...:  95%|█████████▍| 7278/7666 [00:02<00:00, 2383.14it/s]tokenizing...:  99%|█████████▉| 7586/7666 [00:02<00:00, 2572.96it/s]tokenizing...: 100%|██████████| 7666/7666 [00:02<00:00, 2711.87it/s]
tokenizing...:   0%|          | 0/1913 [00:00<?, ?it/s]tokenizing...:  17%|█▋        | 324/1913 [00:00<00:00, 3238.24it/s]tokenizing...:  34%|███▍      | 648/1913 [00:00<00:00, 3056.50it/s]tokenizing...:  50%|████▉     | 955/1913 [00:00<00:00, 2921.93it/s]tokenizing...:  65%|██████▌   | 1248/1913 [00:00<00:00, 2794.50it/s]tokenizing...:  80%|███████▉  | 1530/1913 [00:00<00:00, 2801.81it/s]tokenizing...:  95%|█████████▍| 1811/1913 [00:00<00:00, 2620.94it/s]tokenizing...: 100%|██████████| 1913/1913 [00:00<00:00, 2763.03it/s]
10/17/2022 14:21:09 - INFO - root -   The nums of the train_dataset features is 7666
10/17/2022 14:21:09 - INFO - root -   The nums of the eval_dataset features is 1913
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
10/17/2022 14:21:13 - INFO - root -   ***** Running train *****
********************************************************************************
/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:58  batch_loss: 6.3663 [Training] 2/120 [..............................] - ETA: 1:13  batch_loss: 6.3328 [Training] 3/120 [..............................] - ETA: 57s  batch_loss: 6.3293 [Training] 4/120 [>.............................] - ETA: 50s  batch_loss: 6.3315 [Training] 5/120 [>.............................] - ETA: 45s  batch_loss: 6.3081 [Training] 6/120 [>.............................] - ETA: 42s  batch_loss: 6.2624 [Training] 7/120 [>.............................] - ETA: 39s  batch_loss: 6.2038 [Training] 8/120 [=>............................] - ETA: 37s  batch_loss: 6.1497 [Training] 9/120 [=>............................] - ETA: 36s  batch_loss: 6.0844 [Training] 10/120 [=>............................] - ETA: 35s  batch_loss: 6.0204 [Training] 11/120 [=>............................] - ETA: 34s  batch_loss: 5.9520 [Training] 12/120 [==>...........................] - ETA: 33s  batch_loss: 5.8780 [Training] 13/120 [==>...........................] - ETA: 32s  batch_loss: 5.8081 [Training] 14/120 [==>...........................] - ETA: 31s  batch_loss: 5.7407 [Training] 15/120 [==>...........................] - ETA: 30s  batch_loss: 5.6557 [Training] 16/120 [===>..........................] - ETA: 30s  batch_loss: 5.5764 [Training] 17/120 [===>..........................] - ETA: 29s  batch_loss: 5.4971 [Training] 18/120 [===>..........................] - ETA: 29s  batch_loss: 5.4243 [Training] 19/120 [===>..........................] - ETA: 28s  batch_loss: 5.3366 [Training] 20/120 [====>.........................] - ETA: 28s  batch_loss: 5.2491 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 5.1830 [Training] 22/120 [====>.........................] - ETA: 27s  batch_loss: 5.0953 [Training] 23/120 [====>.........................] - ETA: 27s  batch_loss: 5.0137 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 4.9220 [Training] 25/120 [=====>........................] - ETA: 26s  batch_loss: 4.8338 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 4.7483 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 4.6613 [Training] 28/120 [======>.......................] - ETA: 25s  batch_loss: 4.5809 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 4.5139 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 4.4422 [Training] 31/120 [======>.......................] - ETA: 24s  batch_loss: 4.3707 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 4.2947 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 4.2309 [Training] 34/120 [=======>......................] - ETA: 23s  batch_loss: 4.1842 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 4.1395 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 4.0930 [Training] 37/120 [========>.....................] - ETA: 22s  batch_loss: 4.0501 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 4.0110 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 3.9827 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 3.9425 [Training] 41/120 [=========>....................] - ETA: 21s  batch_loss: 3.9050 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 3.8657 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 3.8357 [Training] 44/120 [==========>...................] - ETA: 20s  batch_loss: 3.8043 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 3.7772 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 3.7437 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 3.7157 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 3.6928 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 3.6646 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 3.6353 [Training] 51/120 [===========>..................] - ETA: 18s  batch_loss: 3.6050 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 3.5762 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 3.5541 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 3.5265 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 3.5032 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 3.4838 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 3.4590 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 3.4374 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 3.4211 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 3.4019 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 3.3818 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 3.3621 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 3.3469 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 3.3268 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 3.3104 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 3.2976 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 3.2795 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 3.2642 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 3.2485 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 3.2324 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 3.2139 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 3.1983 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 3.1821 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 3.1676 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 3.1559 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 3.1405 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 3.1339 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 3.1223 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 3.1121 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 3.0975 [Training] 81/120 [===================>..........] - ETA: 9s  batch_loss: 3.0910 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 3.0793 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 3.0673 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 3.0552 [Training] 85/120 [====================>.........] - ETA: 8s  batch_loss: 3.0437 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 3.0301 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 3.0197 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 3.0081 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 2.9957 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 2.9844 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 2.9753 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 2.9653 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 2.9541 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 2.9465 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 2.9361 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 2.9254 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 2.9153 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 2.9091 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 2.9021 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 2.8929 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 2.8847 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 2.8771 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 2.8674 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 2.8610 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 2.8529 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 2.8448 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 2.8386 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 2.8333 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 2.8258 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 2.8194 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 2.8133 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 2.8064 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 2.7985 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 2.7901 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 2.7848 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 2.7780 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 2.7717 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 2.7657 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 2.7593 [Training] 120/120 [==============================] 250.4ms/step  batch_loss: 2.7521 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:21:49 - INFO - root -   The F1-score is 0.03046845245651898
10/17/2022 14:21:49 - INFO - root -   the best eval f1 is 0.0305, saving model !!
10/17/2022 14:21:53 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:32  batch_loss: 2.0218 [Training] 2/120 [..............................] - ETA: 1:02  batch_loss: 2.0111 [Training] 3/120 [..............................] - ETA: 50s  batch_loss: 2.0540 [Training] 4/120 [>.............................] - ETA: 44s  batch_loss: 2.0941 [Training] 5/120 [>.............................] - ETA: 40s  batch_loss: 2.0909 [Training] 6/120 [>.............................] - ETA: 38s  batch_loss: 2.0671 [Training] 7/120 [>.............................] - ETA: 36s  batch_loss: 2.0607 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 2.0419 [Training] 9/120 [=>............................] - ETA: 33s  batch_loss: 2.0222 [Training] 10/120 [=>............................] - ETA: 32s  batch_loss: 1.9913 [Training] 11/120 [=>............................] - ETA: 31s  batch_loss: 1.9880 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 1.9713 [Training] 13/120 [==>...........................] - ETA: 30s  batch_loss: 1.9857 [Training] 14/120 [==>...........................] - ETA: 29s  batch_loss: 1.9512 [Training] 15/120 [==>...........................] - ETA: 29s  batch_loss: 1.9499 [Training] 16/120 [===>..........................] - ETA: 28s  batch_loss: 1.9320 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 1.9220 [Training] 18/120 [===>..........................] - ETA: 27s  batch_loss: 1.9101 [Training] 19/120 [===>..........................] - ETA: 27s  batch_loss: 1.9038 [Training] 20/120 [====>.........................] - ETA: 26s  batch_loss: 1.8993 [Training] 21/120 [====>.........................] - ETA: 26s  batch_loss: 1.9071 [Training] 22/120 [====>.........................] - ETA: 25s  batch_loss: 1.9170 [Training] 23/120 [====>.........................] - ETA: 25s  batch_loss: 1.9092 [Training] 24/120 [=====>........................] - ETA: 25s  batch_loss: 1.9036 [Training] 25/120 [=====>........................] - ETA: 24s  batch_loss: 1.8979 [Training] 26/120 [=====>........................] - ETA: 24s  batch_loss: 1.8957 [Training] 27/120 [=====>........................] - ETA: 24s  batch_loss: 1.8921 [Training] 28/120 [======>.......................] - ETA: 23s  batch_loss: 1.8882 [Training] 29/120 [======>.......................] - ETA: 23s  batch_loss: 1.8797 [Training] 30/120 [======>.......................] - ETA: 23s  batch_loss: 1.8767 [Training] 31/120 [======>.......................] - ETA: 22s  batch_loss: 1.8668 [Training] 32/120 [=======>......................] - ETA: 22s  batch_loss: 1.8627 [Training] 33/120 [=======>......................] - ETA: 22s  batch_loss: 1.8582 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 1.8602 [Training] 35/120 [=======>......................] - ETA: 21s  batch_loss: 1.8579 [Training] 36/120 [========>.....................] - ETA: 21s  batch_loss: 1.8578 [Training] 37/120 [========>.....................] - ETA: 21s  batch_loss: 1.8598 [Training] 38/120 [========>.....................] - ETA: 20s  batch_loss: 1.8498 [Training] 39/120 [========>.....................] - ETA: 20s  batch_loss: 1.8483 [Training] 40/120 [=========>....................] - ETA: 20s  batch_loss: 1.8423 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 1.8378 [Training] 42/120 [=========>....................] - ETA: 19s  batch_loss: 1.8377 [Training] 43/120 [=========>....................] - ETA: 19s  batch_loss: 1.8371 [Training] 44/120 [==========>...................] - ETA: 19s  batch_loss: 1.8330 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 1.8346 [Training] 46/120 [==========>...................] - ETA: 18s  batch_loss: 1.8308 [Training] 47/120 [==========>...................] - ETA: 18s  batch_loss: 1.8285 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 1.8266 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 1.8263 [Training] 50/120 [===========>..................] - ETA: 17s  batch_loss: 1.8234 [Training] 51/120 [===========>..................] - ETA: 17s  batch_loss: 1.8239 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 1.8233 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 1.8217 [Training] 54/120 [============>.................] - ETA: 16s  batch_loss: 1.8206 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 1.8181 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 1.8173 [Training] 57/120 [=============>................] - ETA: 15s  batch_loss: 1.8131 [Training] 58/120 [=============>................] - ETA: 15s  batch_loss: 1.8136 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 1.8103 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 1.8064 [Training] 61/120 [==============>...............] - ETA: 14s  batch_loss: 1.8029 [Training] 62/120 [==============>...............] - ETA: 14s  batch_loss: 1.8017 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 1.7978 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 1.7933 [Training] 65/120 [===============>..............] - ETA: 13s  batch_loss: 1.7912 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 1.7895 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 1.7839 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 1.7804 [Training] 69/120 [================>.............] - ETA: 12s  batch_loss: 1.7815 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 1.7776 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 1.7743 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 1.7728 [Training] 73/120 [=================>............] - ETA: 11s  batch_loss: 1.7696 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 1.7684 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 1.7640 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 1.7603 [Training] 77/120 [==================>...........] - ETA: 10s  batch_loss: 1.7564 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 1.7508 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 1.7451 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 1.7443 [Training] 81/120 [===================>..........] - ETA: 9s  batch_loss: 1.7431 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 1.7411 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 1.7395 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 1.7371 [Training] 85/120 [====================>.........] - ETA: 8s  batch_loss: 1.7355 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 1.7334 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 1.7281 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 1.7277 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 1.7262 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 1.7247 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 1.7214 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 1.7200 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 1.7170 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 1.7138 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 1.7121 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 1.7073 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 1.7057 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 1.7013 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 1.6997 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 1.7005 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 1.6983 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 1.6960 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 1.6944 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 1.6916 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 1.6890 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 1.6855 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 1.6825 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 1.6797 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 1.6786 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 1.6759 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 1.6777 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 1.6763 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 1.6744 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 1.6736 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 1.6719 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 1.6712 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 1.6699 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 1.6677 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 1.6670 [Training] 120/120 [==============================] 256.3ms/step  batch_loss: 1.6662 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:22:30 - INFO - root -   The F1-score is 0.08715060048889361
10/17/2022 14:22:30 - INFO - root -   the best eval f1 is 0.0872, saving model !!
10/17/2022 14:22:33 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:35  batch_loss: 1.4984 [Training] 2/120 [..............................] - ETA: 1:02  batch_loss: 1.4534 [Training] 3/120 [..............................] - ETA: 51s  batch_loss: 1.4134 [Training] 4/120 [>.............................] - ETA: 45s  batch_loss: 1.4666 [Training] 5/120 [>.............................] - ETA: 42s  batch_loss: 1.4500 [Training] 6/120 [>.............................] - ETA: 39s  batch_loss: 1.4537 [Training] 7/120 [>.............................] - ETA: 37s  batch_loss: 1.4455 [Training] 8/120 [=>............................] - ETA: 36s  batch_loss: 1.4817 [Training] 9/120 [=>............................] - ETA: 34s  batch_loss: 1.4598 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 1.4356 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 1.4222 [Training] 12/120 [==>...........................] - ETA: 32s  batch_loss: 1.4297 [Training] 13/120 [==>...........................] - ETA: 31s  batch_loss: 1.4048 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 1.4182 [Training] 15/120 [==>...........................] - ETA: 30s  batch_loss: 1.4107 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 1.3982 [Training] 17/120 [===>..........................] - ETA: 29s  batch_loss: 1.4126 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 1.4128 [Training] 19/120 [===>..........................] - ETA: 28s  batch_loss: 1.4161 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 1.4205 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 1.4141 [Training] 22/120 [====>.........................] - ETA: 27s  batch_loss: 1.4077 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 1.4067 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 1.4005 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 1.4036 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 1.4035 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 1.4020 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 1.3990 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 1.4015 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 1.3944 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 1.3899 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 1.3882 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 1.3924 [Training] 34/120 [=======>......................] - ETA: 23s  batch_loss: 1.3908 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 1.3917 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 1.3858 [Training] 37/120 [========>.....................] - ETA: 22s  batch_loss: 1.3844 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 1.3808 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 1.3799 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 1.3733 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 1.3725 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 1.3739 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 1.3703 [Training] 44/120 [==========>...................] - ETA: 19s  batch_loss: 1.3668 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 1.3637 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 1.3619 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 1.3605 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 1.3629 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 1.3587 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 1.3574 [Training] 51/120 [===========>..................] - ETA: 18s  batch_loss: 1.3541 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 1.3545 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 1.3525 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 1.3513 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 1.3516 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 1.3495 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 1.3480 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 1.3448 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 1.3411 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 1.3391 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 1.3392 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 1.3390 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 1.3396 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 1.3386 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 1.3360 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 1.3355 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 1.3351 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 1.3305 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 1.3286 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 1.3266 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 1.3266 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 1.3245 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 1.3245 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 1.3241 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 1.3240 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 1.3217 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 1.3198 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 1.3191 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 1.3202 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 1.3207 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 1.3184 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 1.3154 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 1.3131 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 1.3111 [Training] 85/120 [====================>.........] - ETA: 8s  batch_loss: 1.3106 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 1.3111 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 1.3095 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 1.3072 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 1.3057 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 1.3033 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 1.3014 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 1.3014 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 1.2995 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 1.2987 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 1.2975 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 1.2973 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 1.2965 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 1.2957 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 1.2946 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 1.2942 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 1.2923 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 1.2947 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 1.2939 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 1.2952 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 1.2950 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 1.2945 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 1.2930 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 1.2906 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 1.2890 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 1.2872 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 1.2863 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 1.2861 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 1.2851 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 1.2830 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 1.2824 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 1.2800 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 1.2799 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 1.2787 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 1.2784 [Training] 120/120 [==============================] 255.5ms/step  batch_loss: 1.2773 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:23:10 - INFO - root -   The F1-score is 0.21060480530240264
10/17/2022 14:23:10 - INFO - root -   the best eval f1 is 0.2106, saving model !!
10/17/2022 14:23:13 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:32  batch_loss: 0.9350 [Training] 2/120 [..............................] - ETA: 1:00  batch_loss: 1.0365 [Training] 3/120 [..............................] - ETA: 49s  batch_loss: 1.0832 [Training] 4/120 [>.............................] - ETA: 44s  batch_loss: 1.1380 [Training] 5/120 [>.............................] - ETA: 40s  batch_loss: 1.1379 [Training] 6/120 [>.............................] - ETA: 38s  batch_loss: 1.1508 [Training] 7/120 [>.............................] - ETA: 36s  batch_loss: 1.1316 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 1.1370 [Training] 9/120 [=>............................] - ETA: 34s  batch_loss: 1.1414 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 1.1222 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 1.1102 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 1.1051 [Training] 13/120 [==>...........................] - ETA: 30s  batch_loss: 1.1089 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 1.1047 [Training] 15/120 [==>...........................] - ETA: 29s  batch_loss: 1.1018 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 1.1046 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 1.1064 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 1.1126 [Training] 19/120 [===>..........................] - ETA: 27s  batch_loss: 1.1093 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 1.1109 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 1.1122 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 1.1177 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 1.1231 [Training] 24/120 [=====>........................] - ETA: 25s  batch_loss: 1.1178 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 1.1097 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 1.1112 [Training] 27/120 [=====>........................] - ETA: 24s  batch_loss: 1.1119 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 1.1105 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 1.1114 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 1.1101 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 1.1085 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 1.1071 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 1.1074 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 1.1085 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 1.1054 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 1.1073 [Training] 37/120 [========>.....................] - ETA: 21s  batch_loss: 1.1018 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 1.0979 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 1.0969 [Training] 40/120 [=========>....................] - ETA: 20s  batch_loss: 1.0973 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 1.0959 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 1.0932 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 1.0959 [Training] 44/120 [==========>...................] - ETA: 19s  batch_loss: 1.0924 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 1.0932 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 1.0910 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 1.0917 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 1.0887 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 1.0889 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 1.0884 [Training] 51/120 [===========>..................] - ETA: 17s  batch_loss: 1.0878 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 1.0873 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 1.0860 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 1.0837 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 1.0813 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 1.0814 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 1.0810 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 1.0807 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 1.0808 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 1.0797 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 1.0791 [Training] 62/120 [==============>...............] - ETA: 14s  batch_loss: 1.0766 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 1.0758 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 1.0766 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 1.0737 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 1.0741 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 1.0738 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 1.0747 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 1.0729 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 1.0695 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 1.0681 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 1.0712 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 1.0715 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 1.0705 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 1.0689 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 1.0696 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 1.0691 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 1.0686 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 1.0701 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 1.0680 [Training] 81/120 [===================>..........] - ETA: 9s  batch_loss: 1.0675 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 1.0668 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 1.0673 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 1.0667 [Training] 85/120 [====================>.........] - ETA: 8s  batch_loss: 1.0643 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 1.0643 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 1.0628 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 1.0641 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 1.0629 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 1.0622 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 1.0623 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 1.0612 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 1.0596 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 1.0594 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 1.0568 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 1.0546 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 1.0528 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 1.0523 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 1.0500 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 1.0495 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 1.0488 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 1.0479 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 1.0460 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 1.0461 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 1.0452 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 1.0441 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 1.0445 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 1.0427 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 1.0409 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 1.0399 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 1.0399 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 1.0395 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 1.0376 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 1.0360 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 1.0354 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 1.0352 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 1.0343 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 1.0339 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 1.0333 [Training] 120/120 [==============================] 255.0ms/step  batch_loss: 1.0321 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:23:50 - INFO - root -   The F1-score is 0.3544738725841088
10/17/2022 14:23:50 - INFO - root -   the best eval f1 is 0.3545, saving model !!
10/17/2022 14:23:53 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:32  batch_loss: 0.9208 [Training] 2/120 [..............................] - ETA: 1:01  batch_loss: 0.9126 [Training] 3/120 [..............................] - ETA: 50s  batch_loss: 0.9236 [Training] 4/120 [>.............................] - ETA: 44s  batch_loss: 0.9268 [Training] 5/120 [>.............................] - ETA: 41s  batch_loss: 0.9400 [Training] 6/120 [>.............................] - ETA: 38s  batch_loss: 0.9312 [Training] 7/120 [>.............................] - ETA: 37s  batch_loss: 0.9390 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 0.9475 [Training] 9/120 [=>............................] - ETA: 34s  batch_loss: 0.9397 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 0.9444 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.9449 [Training] 12/120 [==>...........................] - ETA: 32s  batch_loss: 0.9420 [Training] 13/120 [==>...........................] - ETA: 31s  batch_loss: 0.9379 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.9410 [Training] 15/120 [==>...........................] - ETA: 30s  batch_loss: 0.9393 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.9338 [Training] 17/120 [===>..........................] - ETA: 29s  batch_loss: 0.9354 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.9289 [Training] 19/120 [===>..........................] - ETA: 28s  batch_loss: 0.9277 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.9208 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.9252 [Training] 22/120 [====>.........................] - ETA: 27s  batch_loss: 0.9191 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.9132 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.9075 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.9054 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.9111 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 0.9098 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.9072 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.9030 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 0.9025 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.9019 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.9078 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.9071 [Training] 34/120 [=======>......................] - ETA: 23s  batch_loss: 0.9045 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.9031 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.9031 [Training] 37/120 [========>.....................] - ETA: 22s  batch_loss: 0.9032 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.9069 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.9083 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.9078 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.9035 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.9014 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.9031 [Training] 44/120 [==========>...................] - ETA: 20s  batch_loss: 0.9021 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.8977 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.8952 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.8917 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.8906 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.8927 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.8926 [Training] 51/120 [===========>..................] - ETA: 18s  batch_loss: 0.8938 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.8926 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.8919 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.8880 [Training] 55/120 [============>.................] - ETA: 17s  batch_loss: 0.8873 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.8857 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.8845 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.8836 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.8825 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.8824 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.8806 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.8813 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.8818 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.8802 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.8800 [Training] 66/120 [===============>..............] - ETA: 14s  batch_loss: 0.8784 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.8797 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.8787 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.8790 [Training] 70/120 [================>.............] - ETA: 13s  batch_loss: 0.8776 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.8782 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.8763 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.8775 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.8773 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.8767 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.8764 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.8759 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.8761 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.8738 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.8728 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.8725 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.8723 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.8719 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.8721 [Training] 85/120 [====================>.........] - ETA: 9s  batch_loss: 0.8709 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.8702 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.8698 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.8705 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.8694 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.8681 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.8671 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.8664 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.8660 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.8654 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.8630 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.8613 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.8607 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.8604 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.8613 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.8608 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.8617 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.8610 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.8604 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.8605 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.8602 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.8591 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.8593 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.8592 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.8587 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.8579 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.8581 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.8577 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.8587 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.8580 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.8580 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.8590 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.8584 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.8585 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.8583 [Training] 120/120 [==============================] 253.6ms/step  batch_loss: 0.8584 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:24:30 - INFO - root -   The F1-score is 0.4414619037786495
10/17/2022 14:24:30 - INFO - root -   the best eval f1 is 0.4415, saving model !!
10/17/2022 14:24:33 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:34  batch_loss: 0.8124 [Training] 2/120 [..............................] - ETA: 1:02  batch_loss: 0.7599 [Training] 3/120 [..............................] - ETA: 51s  batch_loss: 0.7916 [Training] 4/120 [>.............................] - ETA: 45s  batch_loss: 0.7562 [Training] 5/120 [>.............................] - ETA: 41s  batch_loss: 0.7490 [Training] 6/120 [>.............................] - ETA: 39s  batch_loss: 0.7445 [Training] 7/120 [>.............................] - ETA: 37s  batch_loss: 0.7497 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 0.7580 [Training] 9/120 [=>............................] - ETA: 34s  batch_loss: 0.7560 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 0.7614 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.7727 [Training] 12/120 [==>...........................] - ETA: 32s  batch_loss: 0.7654 [Training] 13/120 [==>...........................] - ETA: 31s  batch_loss: 0.7595 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.7627 [Training] 15/120 [==>...........................] - ETA: 30s  batch_loss: 0.7596 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.7678 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 0.7631 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.7583 [Training] 19/120 [===>..........................] - ETA: 28s  batch_loss: 0.7535 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.7533 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.7505 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.7578 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.7573 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.7587 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.7555 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.7556 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 0.7524 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.7563 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.7571 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 0.7620 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.7631 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.7624 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.7600 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.7582 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.7614 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.7646 [Training] 37/120 [========>.....................] - ETA: 21s  batch_loss: 0.7646 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.7612 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.7614 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.7596 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.7611 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.7613 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.7652 [Training] 44/120 [==========>...................] - ETA: 19s  batch_loss: 0.7627 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.7637 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.7651 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.7643 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.7631 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.7640 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.7634 [Training] 51/120 [===========>..................] - ETA: 17s  batch_loss: 0.7620 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.7624 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.7625 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.7643 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.7639 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.7634 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.7627 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.7621 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.7608 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.7637 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.7637 [Training] 62/120 [==============>...............] - ETA: 14s  batch_loss: 0.7618 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.7610 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.7603 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.7598 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 0.7597 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.7595 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.7590 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.7590 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.7580 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.7572 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.7587 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.7578 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.7582 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.7570 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.7555 [Training] 77/120 [==================>...........] - ETA: 10s  batch_loss: 0.7541 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.7525 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.7520 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.7515 [Training] 81/120 [===================>..........] - ETA: 9s  batch_loss: 0.7504 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.7495 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.7483 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.7474 [Training] 85/120 [====================>.........] - ETA: 8s  batch_loss: 0.7472 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.7469 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.7469 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.7468 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.7454 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.7464 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.7450 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.7455 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.7462 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.7471 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.7465 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.7447 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.7439 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.7436 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.7425 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.7411 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.7405 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.7390 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.7386 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.7385 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.7376 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.7369 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.7364 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.7356 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.7353 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.7349 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.7349 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.7352 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.7357 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.7354 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.7348 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.7339 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.7329 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.7315 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.7306 [Training] 120/120 [==============================] 254.1ms/step  batch_loss: 0.7303 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:25:10 - INFO - root -   The F1-score is 0.4851614680162501
10/17/2022 14:25:10 - INFO - root -   the best eval f1 is 0.4852, saving model !!
10/17/2022 14:25:13 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:33  batch_loss: 0.7354 [Training] 2/120 [..............................] - ETA: 1:00  batch_loss: 0.7457 [Training] 3/120 [..............................] - ETA: 50s  batch_loss: 0.7202 [Training] 4/120 [>.............................] - ETA: 44s  batch_loss: 0.7011 [Training] 5/120 [>.............................] - ETA: 41s  batch_loss: 0.7006 [Training] 6/120 [>.............................] - ETA: 38s  batch_loss: 0.6969 [Training] 7/120 [>.............................] - ETA: 36s  batch_loss: 0.6917 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 0.6694 [Training] 9/120 [=>............................] - ETA: 34s  batch_loss: 0.6713 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 0.6755 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.6740 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.6794 [Training] 13/120 [==>...........................] - ETA: 31s  batch_loss: 0.6714 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.6757 [Training] 15/120 [==>...........................] - ETA: 29s  batch_loss: 0.6811 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.6896 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 0.6837 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.6832 [Training] 19/120 [===>..........................] - ETA: 28s  batch_loss: 0.6824 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.6797 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.6720 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.6687 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.6732 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.6712 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.6674 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.6663 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 0.6638 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.6630 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.6667 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 0.6689 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.6707 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.6703 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.6695 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.6663 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.6660 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.6647 [Training] 37/120 [========>.....................] - ETA: 22s  batch_loss: 0.6640 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.6649 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.6642 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.6617 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.6606 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.6606 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.6612 [Training] 44/120 [==========>...................] - ETA: 19s  batch_loss: 0.6591 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.6580 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.6584 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.6590 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.6605 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.6590 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.6572 [Training] 51/120 [===========>..................] - ETA: 18s  batch_loss: 0.6568 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.6547 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.6515 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.6499 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.6491 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.6500 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.6516 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.6499 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.6491 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.6503 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.6504 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.6493 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.6485 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.6504 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.6499 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 0.6528 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.6514 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.6502 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.6489 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.6475 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.6472 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.6481 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.6502 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.6499 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.6502 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.6487 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.6481 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.6471 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.6459 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.6461 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.6454 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.6449 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.6437 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.6433 [Training] 85/120 [====================>.........] - ETA: 8s  batch_loss: 0.6418 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.6421 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.6428 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.6413 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.6405 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.6394 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.6398 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.6404 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.6399 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.6387 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.6388 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.6378 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.6373 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.6372 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.6367 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.6364 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.6351 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.6351 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.6348 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.6354 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.6355 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.6358 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.6349 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.6340 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.6329 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.6325 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.6316 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.6298 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.6298 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.6289 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.6287 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.6280 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.6282 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.6287 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.6283 [Training] 120/120 [==============================] 255.2ms/step  batch_loss: 0.6291 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:25:51 - INFO - root -   The F1-score is 0.5232078390923156
10/17/2022 14:25:51 - INFO - root -   the best eval f1 is 0.5232, saving model !!
10/17/2022 14:25:54 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:28  batch_loss: 0.5654 [Training] 2/120 [..............................] - ETA: 59s  batch_loss: 0.6331 [Training] 3/120 [..............................] - ETA: 49s  batch_loss: 0.5939 [Training] 4/120 [>.............................] - ETA: 44s  batch_loss: 0.5795 [Training] 5/120 [>.............................] - ETA: 40s  batch_loss: 0.5514 [Training] 6/120 [>.............................] - ETA: 38s  batch_loss: 0.5553 [Training] 7/120 [>.............................] - ETA: 36s  batch_loss: 0.5731 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 0.5714 [Training] 9/120 [=>............................] - ETA: 33s  batch_loss: 0.5692 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 0.5740 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.5652 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.5641 [Training] 13/120 [==>...........................] - ETA: 30s  batch_loss: 0.5691 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.5652 [Training] 15/120 [==>...........................] - ETA: 29s  batch_loss: 0.5665 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.5600 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 0.5625 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.5610 [Training] 19/120 [===>..........................] - ETA: 27s  batch_loss: 0.5584 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.5575 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.5625 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.5635 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.5654 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.5668 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.5680 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.5688 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 0.5695 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.5671 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.5643 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 0.5633 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.5593 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.5592 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.5621 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.5630 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.5643 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.5640 [Training] 37/120 [========>.....................] - ETA: 21s  batch_loss: 0.5617 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.5667 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.5657 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.5658 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.5658 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.5634 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.5646 [Training] 44/120 [==========>...................] - ETA: 19s  batch_loss: 0.5647 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.5627 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.5607 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.5590 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.5572 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.5558 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.5551 [Training] 51/120 [===========>..................] - ETA: 17s  batch_loss: 0.5555 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.5552 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.5557 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.5568 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.5562 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.5558 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.5568 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.5590 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.5582 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.5602 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.5591 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.5592 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.5579 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.5573 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.5572 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 0.5567 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.5553 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.5559 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.5564 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.5554 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.5557 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.5560 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.5561 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.5560 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.5546 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.5551 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.5554 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.5552 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.5542 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.5535 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.5532 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.5530 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.5536 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.5534 [Training] 85/120 [====================>.........] - ETA: 8s  batch_loss: 0.5532 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.5532 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.5536 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.5527 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.5529 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.5528 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.5519 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.5520 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.5518 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.5514 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.5510 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.5506 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.5504 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.5502 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.5492 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.5499 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.5505 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.5503 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.5500 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.5492 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.5491 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.5487 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.5486 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.5479 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.5475 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.5465 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.5464 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.5466 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.5464 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.5465 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.5466 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.5461 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.5459 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.5465 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.5463 [Training] 120/120 [==============================] 255.2ms/step  batch_loss: 0.5455 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:26:31 - INFO - root -   The F1-score is 0.5451263537906137
10/17/2022 14:26:31 - INFO - root -   the best eval f1 is 0.5451, saving model !!
10/17/2022 14:26:34 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:30  batch_loss: 0.4941 [Training] 2/120 [..............................] - ETA: 1:00  batch_loss: 0.5347 [Training] 3/120 [..............................] - ETA: 49s  batch_loss: 0.5078 [Training] 4/120 [>.............................] - ETA: 44s  batch_loss: 0.4915 [Training] 5/120 [>.............................] - ETA: 40s  batch_loss: 0.4891 [Training] 6/120 [>.............................] - ETA: 38s  batch_loss: 0.4882 [Training] 7/120 [>.............................] - ETA: 36s  batch_loss: 0.4791 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 0.4857 [Training] 9/120 [=>............................] - ETA: 34s  batch_loss: 0.4847 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 0.4821 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.4789 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.4807 [Training] 13/120 [==>...........................] - ETA: 30s  batch_loss: 0.4841 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.4848 [Training] 15/120 [==>...........................] - ETA: 29s  batch_loss: 0.4832 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.4827 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 0.4832 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.4904 [Training] 19/120 [===>..........................] - ETA: 27s  batch_loss: 0.4903 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.4905 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.4956 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.4913 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.4920 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.4930 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.4925 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.4934 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 0.4917 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.4914 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.4949 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 0.4959 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.4953 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.4930 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.4974 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.4962 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.4955 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.4986 [Training] 37/120 [========>.....................] - ETA: 21s  batch_loss: 0.4979 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.4964 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.4963 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.4955 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.4999 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.4999 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.4981 [Training] 44/120 [==========>...................] - ETA: 19s  batch_loss: 0.4981 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.4976 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.4981 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.4987 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.4978 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.4986 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.4976 [Training] 51/120 [===========>..................] - ETA: 17s  batch_loss: 0.4973 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.4970 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.4956 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.4962 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.4958 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.4956 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.4959 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.4955 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.4960 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.4965 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.4968 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.4960 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.4950 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.4945 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.4938 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 0.4934 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.4916 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.4900 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.4901 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.4896 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.4882 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.4879 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.4872 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.4868 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.4869 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.4879 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.4868 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.4867 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.4863 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.4866 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.4864 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.4862 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.4859 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.4858 [Training] 85/120 [====================>.........] - ETA: 8s  batch_loss: 0.4856 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.4860 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.4863 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.4859 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.4871 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.4862 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.4851 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.4853 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.4844 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.4839 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.4835 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.4844 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.4849 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.4840 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.4837 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.4833 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.4830 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.4826 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.4829 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.4823 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.4816 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.4809 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.4807 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.4804 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.4797 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.4799 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.4792 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.4789 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.4797 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.4793 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.4796 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.4790 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.4783 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.4790 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.4789 [Training] 120/120 [==============================] 255.1ms/step  batch_loss: 0.4782 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:27:11 - INFO - root -   The F1-score is 0.5636917372881356
10/17/2022 14:27:11 - INFO - root -   the best eval f1 is 0.5637, saving model !!
10/17/2022 14:27:14 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:33  batch_loss: 0.4186 [Training] 2/120 [..............................] - ETA: 1:01  batch_loss: 0.3860 [Training] 3/120 [..............................] - ETA: 50s  batch_loss: 0.4111 [Training] 4/120 [>.............................] - ETA: 44s  batch_loss: 0.4154 [Training] 5/120 [>.............................] - ETA: 41s  batch_loss: 0.4084 [Training] 6/120 [>.............................] - ETA: 38s  batch_loss: 0.4044 [Training] 7/120 [>.............................] - ETA: 37s  batch_loss: 0.4052 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 0.4116 [Training] 9/120 [=>............................] - ETA: 34s  batch_loss: 0.4112 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 0.4142 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.4195 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.4245 [Training] 13/120 [==>...........................] - ETA: 31s  batch_loss: 0.4215 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.4198 [Training] 15/120 [==>...........................] - ETA: 30s  batch_loss: 0.4222 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.4264 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 0.4272 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.4210 [Training] 19/120 [===>..........................] - ETA: 28s  batch_loss: 0.4237 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.4232 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.4246 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.4219 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.4202 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.4206 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.4163 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.4181 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 0.4175 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.4157 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.4145 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 0.4173 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.4166 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.4172 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.4195 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.4205 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.4221 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.4222 [Training] 37/120 [========>.....................] - ETA: 22s  batch_loss: 0.4232 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.4219 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.4210 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.4234 [Training] 41/120 [=========>....................] - ETA: 21s  batch_loss: 0.4262 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.4255 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.4249 [Training] 44/120 [==========>...................] - ETA: 20s  batch_loss: 0.4266 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.4275 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.4261 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.4282 [Training] 48/120 [===========>..................] - ETA: 19s  batch_loss: 0.4278 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.4270 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.4291 [Training] 51/120 [===========>..................] - ETA: 18s  batch_loss: 0.4285 [Training] 52/120 [============>.................] - ETA: 18s  batch_loss: 0.4289 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.4284 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.4303 [Training] 55/120 [============>.................] - ETA: 17s  batch_loss: 0.4289 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.4307 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.4300 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.4306 [Training] 59/120 [=============>................] - ETA: 16s  batch_loss: 0.4288 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.4294 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.4302 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.4297 [Training] 63/120 [==============>...............] - ETA: 15s  batch_loss: 0.4300 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.4300 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.4305 [Training] 66/120 [===============>..............] - ETA: 14s  batch_loss: 0.4309 [Training] 67/120 [===============>..............] - ETA: 14s  batch_loss: 0.4319 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.4315 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.4307 [Training] 70/120 [================>.............] - ETA: 13s  batch_loss: 0.4295 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.4293 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.4288 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.4279 [Training] 74/120 [=================>............] - ETA: 12s  batch_loss: 0.4277 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.4287 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.4274 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.4285 [Training] 78/120 [==================>...........] - ETA: 11s  batch_loss: 0.4286 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.4289 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.4294 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.4288 [Training] 82/120 [===================>..........] - ETA: 10s  batch_loss: 0.4289 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.4296 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.4292 [Training] 85/120 [====================>.........] - ETA: 9s  batch_loss: 0.4287 [Training] 86/120 [====================>.........] - ETA: 9s  batch_loss: 0.4285 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.4287 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.4292 [Training] 89/120 [=====================>........] - ETA: 8s  batch_loss: 0.4309 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.4305 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.4306 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.4297 [Training] 93/120 [======================>.......] - ETA: 7s  batch_loss: 0.4302 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.4300 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.4292 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.4288 [Training] 97/120 [=======================>......] - ETA: 6s  batch_loss: 0.4281 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.4281 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.4282 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.4276 [Training] 101/120 [========================>.....] - ETA: 5s  batch_loss: 0.4280 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.4275 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.4277 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.4271 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.4266 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.4260 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.4253 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.4249 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.4250 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.4261 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.4261 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.4268 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.4269 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.4263 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.4264 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.4277 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.4278 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.4274 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.4275 [Training] 120/120 [==============================] 264.5ms/step  batch_loss: 0.4274 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:27:53 - INFO - root -   The F1-score is 0.5830823621841038
10/17/2022 14:27:53 - INFO - root -   the best eval f1 is 0.5831, saving model !!
10/17/2022 14:27:56 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:33  batch_loss: 0.3446 [Training] 2/120 [..............................] - ETA: 1:01  batch_loss: 0.3716 [Training] 3/120 [..............................] - ETA: 50s  batch_loss: 0.3582 [Training] 4/120 [>.............................] - ETA: 44s  batch_loss: 0.3573 [Training] 5/120 [>.............................] - ETA: 41s  batch_loss: 0.3711 [Training] 6/120 [>.............................] - ETA: 38s  batch_loss: 0.3715 [Training] 7/120 [>.............................] - ETA: 36s  batch_loss: 0.3699 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 0.3818 [Training] 9/120 [=>............................] - ETA: 34s  batch_loss: 0.3800 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 0.3746 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.3760 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.3748 [Training] 13/120 [==>...........................] - ETA: 31s  batch_loss: 0.3711 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.3774 [Training] 15/120 [==>...........................] - ETA: 29s  batch_loss: 0.3796 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.3785 [Training] 17/120 [===>..........................] - ETA: 29s  batch_loss: 0.3772 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.3794 [Training] 19/120 [===>..........................] - ETA: 28s  batch_loss: 0.3838 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.3849 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.3847 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.3830 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.3822 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.3824 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.3817 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.3817 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 0.3819 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.3835 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.3828 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 0.3843 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.3849 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.3853 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.3849 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.3844 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.3858 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.3858 [Training] 37/120 [========>.....................] - ETA: 22s  batch_loss: 0.3892 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.3887 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.3876 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.3874 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.3863 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.3899 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.3894 [Training] 44/120 [==========>...................] - ETA: 20s  batch_loss: 0.3889 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.3897 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.3897 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.3897 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.3885 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.3887 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.3891 [Training] 51/120 [===========>..................] - ETA: 18s  batch_loss: 0.3904 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.3917 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.3922 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.3912 [Training] 55/120 [============>.................] - ETA: 17s  batch_loss: 0.3906 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.3895 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.3885 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.3872 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.3883 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.3872 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.3868 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.3871 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.3875 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.3869 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.3865 [Training] 66/120 [===============>..............] - ETA: 14s  batch_loss: 0.3867 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.3864 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.3867 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.3862 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.3866 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.3863 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.3867 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.3867 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.3865 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.3866 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.3866 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.3873 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.3864 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.3867 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.3864 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.3859 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.3860 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.3857 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.3848 [Training] 85/120 [====================>.........] - ETA: 9s  batch_loss: 0.3845 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.3839 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.3838 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.3836 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.3842 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.3838 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.3840 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.3839 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.3836 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.3835 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.3834 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.3826 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.3825 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.3823 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.3819 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.3814 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.3814 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.3816 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.3815 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.3812 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.3809 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.3806 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.3810 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.3804 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.3800 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.3795 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.3793 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.3789 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.3782 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.3784 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.3781 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.3784 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.3781 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.3771 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.3765 [Training] 120/120 [==============================] 256.7ms/step  batch_loss: 0.3768 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:28:33 - INFO - root -   The F1-score is 0.5981248394554328
10/17/2022 14:28:33 - INFO - root -   the best eval f1 is 0.5981, saving model !!
10/17/2022 14:28:36 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:31  batch_loss: 0.3179 [Training] 2/120 [..............................] - ETA: 1:01  batch_loss: 0.3486 [Training] 3/120 [..............................] - ETA: 50s  batch_loss: 0.3624 [Training] 4/120 [>.............................] - ETA: 44s  batch_loss: 0.3630 [Training] 5/120 [>.............................] - ETA: 40s  batch_loss: 0.3641 [Training] 6/120 [>.............................] - ETA: 38s  batch_loss: 0.3694 [Training] 7/120 [>.............................] - ETA: 36s  batch_loss: 0.3673 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 0.3694 [Training] 9/120 [=>............................] - ETA: 34s  batch_loss: 0.3644 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 0.3723 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.3663 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.3624 [Training] 13/120 [==>...........................] - ETA: 31s  batch_loss: 0.3591 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.3635 [Training] 15/120 [==>...........................] - ETA: 29s  batch_loss: 0.3659 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.3636 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 0.3633 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.3639 [Training] 19/120 [===>..........................] - ETA: 28s  batch_loss: 0.3601 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.3595 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.3601 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.3564 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.3557 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.3580 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.3554 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.3533 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 0.3523 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.3537 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.3505 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 0.3479 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.3453 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.3449 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.3448 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.3463 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.3455 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.3443 [Training] 37/120 [========>.....................] - ETA: 22s  batch_loss: 0.3429 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.3424 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.3424 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.3404 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.3414 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.3409 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.3436 [Training] 44/120 [==========>...................] - ETA: 19s  batch_loss: 0.3447 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.3439 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.3444 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.3443 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.3441 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.3444 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.3446 [Training] 51/120 [===========>..................] - ETA: 17s  batch_loss: 0.3441 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.3441 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.3462 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.3460 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.3451 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.3442 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.3444 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.3452 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.3459 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.3453 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.3457 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.3457 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.3456 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.3454 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.3456 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 0.3466 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.3466 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.3469 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.3467 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.3461 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.3459 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.3457 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.3459 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.3459 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.3461 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.3467 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.3475 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.3473 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.3470 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.3467 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.3469 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.3458 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.3456 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.3448 [Training] 85/120 [====================>.........] - ETA: 8s  batch_loss: 0.3456 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.3459 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.3457 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.3457 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.3456 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.3447 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.3448 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.3450 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.3446 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.3446 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.3448 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.3447 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.3437 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.3431 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.3435 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.3433 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.3426 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.3429 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.3428 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.3432 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.3434 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.3430 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.3424 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.3423 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.3420 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.3416 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.3420 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.3420 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.3421 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.3423 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.3426 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.3424 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.3425 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.3422 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.3419 [Training] 120/120 [==============================] 256.9ms/step  batch_loss: 0.3419 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:29:13 - INFO - root -   The F1-score is 0.6027606386362191
10/17/2022 14:29:13 - INFO - root -   the best eval f1 is 0.6028, saving model !!
10/17/2022 14:29:16 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:32  batch_loss: 0.3701 [Training] 2/120 [..............................] - ETA: 1:00  batch_loss: 0.3089 [Training] 3/120 [..............................] - ETA: 49s  batch_loss: 0.3029 [Training] 4/120 [>.............................] - ETA: 44s  batch_loss: 0.3015 [Training] 5/120 [>.............................] - ETA: 40s  batch_loss: 0.3102 [Training] 6/120 [>.............................] - ETA: 38s  batch_loss: 0.3129 [Training] 7/120 [>.............................] - ETA: 36s  batch_loss: 0.3067 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 0.3061 [Training] 9/120 [=>............................] - ETA: 34s  batch_loss: 0.3079 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 0.3106 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.3066 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.3073 [Training] 13/120 [==>...........................] - ETA: 31s  batch_loss: 0.3082 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.3066 [Training] 15/120 [==>...........................] - ETA: 29s  batch_loss: 0.3037 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.3057 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 0.3056 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.3084 [Training] 19/120 [===>..........................] - ETA: 28s  batch_loss: 0.3092 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.3084 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.3072 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.3061 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.3068 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.3107 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.3100 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.3102 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 0.3097 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.3082 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.3070 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 0.3068 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.3051 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.3076 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.3080 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.3067 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.3069 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.3070 [Training] 37/120 [========>.....................] - ETA: 21s  batch_loss: 0.3081 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.3079 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.3083 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.3096 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.3118 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.3119 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.3112 [Training] 44/120 [==========>...................] - ETA: 19s  batch_loss: 0.3116 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.3134 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.3125 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.3122 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.3110 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.3115 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.3122 [Training] 51/120 [===========>..................] - ETA: 18s  batch_loss: 0.3119 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.3124 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.3118 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.3111 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.3115 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.3126 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.3118 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.3112 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.3111 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.3114 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.3114 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.3130 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.3120 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.3119 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.3122 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 0.3116 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.3112 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.3100 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.3102 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.3102 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.3091 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.3101 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.3107 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.3106 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.3112 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.3111 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.3106 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.3102 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.3104 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.3096 [Training] 81/120 [===================>..........] - ETA: 9s  batch_loss: 0.3100 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.3102 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.3094 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.3098 [Training] 85/120 [====================>.........] - ETA: 8s  batch_loss: 0.3096 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.3099 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.3099 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.3102 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.3111 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.3103 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.3102 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.3106 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.3109 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.3114 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.3114 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.3110 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.3106 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.3107 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.3107 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.3104 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.3098 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.3097 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.3098 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.3103 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.3100 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.3101 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.3103 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.3106 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.3104 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.3101 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.3097 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.3097 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.3096 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.3094 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.3092 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.3093 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.3098 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.3104 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.3101 [Training] 120/120 [==============================] 255.1ms/step  batch_loss: 0.3097 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:29:53 - INFO - root -   The F1-score is 0.606189281348517
10/17/2022 14:29:53 - INFO - root -   the best eval f1 is 0.6062, saving model !!
10/17/2022 14:29:57 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:33  batch_loss: 0.2959 [Training] 2/120 [..............................] - ETA: 1:00  batch_loss: 0.2911 [Training] 3/120 [..............................] - ETA: 50s  batch_loss: 0.2706 [Training] 4/120 [>.............................] - ETA: 44s  batch_loss: 0.2475 [Training] 5/120 [>.............................] - ETA: 41s  batch_loss: 0.2559 [Training] 6/120 [>.............................] - ETA: 38s  batch_loss: 0.2569 [Training] 7/120 [>.............................] - ETA: 37s  batch_loss: 0.2686 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 0.2647 [Training] 9/120 [=>............................] - ETA: 34s  batch_loss: 0.2657 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 0.2683 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.2765 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.2775 [Training] 13/120 [==>...........................] - ETA: 31s  batch_loss: 0.2779 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.2784 [Training] 15/120 [==>...........................] - ETA: 30s  batch_loss: 0.2755 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.2789 [Training] 17/120 [===>..........................] - ETA: 29s  batch_loss: 0.2796 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.2761 [Training] 19/120 [===>..........................] - ETA: 28s  batch_loss: 0.2767 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.2790 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.2799 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.2825 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.2842 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.2904 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.2921 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.2942 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 0.2923 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.2921 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.2924 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 0.2933 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.2933 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.2923 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.2919 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.2899 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.2908 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.2901 [Training] 37/120 [========>.....................] - ETA: 22s  batch_loss: 0.2882 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.2888 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.2879 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.2877 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.2874 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.2867 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.2876 [Training] 44/120 [==========>...................] - ETA: 20s  batch_loss: 0.2875 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.2862 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.2859 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.2861 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.2862 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.2858 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.2856 [Training] 51/120 [===========>..................] - ETA: 18s  batch_loss: 0.2860 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.2854 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.2855 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.2850 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.2855 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.2854 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.2852 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.2862 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.2861 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.2863 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.2871 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.2879 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.2888 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.2882 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.2884 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 0.2885 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.2897 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.2902 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.2892 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.2896 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.2898 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.2888 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.2885 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.2898 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.2913 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.2906 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.2902 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.2900 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.2894 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.2893 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.2898 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.2889 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.2891 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.2894 [Training] 85/120 [====================>.........] - ETA: 8s  batch_loss: 0.2888 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.2886 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.2878 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.2877 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.2873 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.2872 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.2881 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.2879 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.2881 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.2873 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.2870 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.2868 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.2868 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.2866 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.2859 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.2855 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.2855 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.2850 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.2853 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.2852 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.2852 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.2856 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.2859 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.2857 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.2857 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.2861 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.2863 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.2862 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.2866 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.2860 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.2857 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.2850 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.2849 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.2847 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.2848 [Training] 120/120 [==============================] 254.9ms/step  batch_loss: 0.2842 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:30:34 - INFO - root -   The F1-score is 0.615394385875778
10/17/2022 14:30:34 - INFO - root -   the best eval f1 is 0.6154, saving model !!
10/17/2022 14:30:37 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:28  batch_loss: 0.2668 [Training] 2/120 [..............................] - ETA: 59s  batch_loss: 0.2851 [Training] 3/120 [..............................] - ETA: 49s  batch_loss: 0.2686 [Training] 4/120 [>.............................] - ETA: 43s  batch_loss: 0.2818 [Training] 5/120 [>.............................] - ETA: 40s  batch_loss: 0.2674 [Training] 6/120 [>.............................] - ETA: 38s  batch_loss: 0.2726 [Training] 7/120 [>.............................] - ETA: 36s  batch_loss: 0.2650 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 0.2629 [Training] 9/120 [=>............................] - ETA: 34s  batch_loss: 0.2621 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 0.2570 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.2572 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.2573 [Training] 13/120 [==>...........................] - ETA: 30s  batch_loss: 0.2598 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.2620 [Training] 15/120 [==>...........................] - ETA: 29s  batch_loss: 0.2600 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.2636 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 0.2616 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.2672 [Training] 19/120 [===>..........................] - ETA: 27s  batch_loss: 0.2685 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.2701 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.2717 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.2706 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.2690 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.2706 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.2691 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.2700 [Training] 27/120 [=====>........................] - ETA: 24s  batch_loss: 0.2687 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.2660 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.2658 [Training] 30/120 [======>.......................] - ETA: 23s  batch_loss: 0.2626 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.2628 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.2628 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.2626 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.2624 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.2614 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.2597 [Training] 37/120 [========>.....................] - ETA: 21s  batch_loss: 0.2599 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.2606 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.2602 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.2586 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.2603 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.2600 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.2588 [Training] 44/120 [==========>...................] - ETA: 19s  batch_loss: 0.2594 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.2577 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.2567 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.2567 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.2570 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.2575 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.2569 [Training] 51/120 [===========>..................] - ETA: 17s  batch_loss: 0.2573 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.2571 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.2575 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.2580 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.2583 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.2586 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.2587 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.2585 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.2594 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.2597 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.2595 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.2595 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.2589 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.2597 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.2594 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 0.2598 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.2603 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.2600 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.2596 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.2593 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.2589 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.2607 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.2608 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.2603 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.2603 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.2598 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.2599 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.2610 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.2620 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.2625 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.2637 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.2635 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.2634 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.2633 [Training] 85/120 [====================>.........] - ETA: 8s  batch_loss: 0.2640 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.2644 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.2653 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.2652 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.2650 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.2648 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.2646 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.2649 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.2650 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.2643 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.2640 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.2642 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.2639 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.2637 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.2634 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.2638 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.2636 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.2633 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.2639 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.2631 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.2625 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.2625 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.2626 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.2628 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.2630 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.2632 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.2629 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.2629 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.2628 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.2627 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.2627 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.2626 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.2627 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.2626 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.2630 [Training] 120/120 [==============================] 255.6ms/step  batch_loss: 0.2629 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:31:14 - INFO - root -   The F1-score is 0.6292931245569946
10/17/2022 14:31:14 - INFO - root -   the best eval f1 is 0.6293, saving model !!
10/17/2022 14:31:17 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:34  batch_loss: 0.2295 [Training] 2/120 [..............................] - ETA: 1:02  batch_loss: 0.2324 [Training] 3/120 [..............................] - ETA: 51s  batch_loss: 0.2307 [Training] 4/120 [>.............................] - ETA: 45s  batch_loss: 0.2417 [Training] 5/120 [>.............................] - ETA: 42s  batch_loss: 0.2439 [Training] 6/120 [>.............................] - ETA: 39s  batch_loss: 0.2465 [Training] 7/120 [>.............................] - ETA: 37s  batch_loss: 0.2480 [Training] 8/120 [=>............................] - ETA: 36s  batch_loss: 0.2447 [Training] 9/120 [=>............................] - ETA: 35s  batch_loss: 0.2403 [Training] 10/120 [=>............................] - ETA: 34s  batch_loss: 0.2414 [Training] 11/120 [=>............................] - ETA: 33s  batch_loss: 0.2395 [Training] 12/120 [==>...........................] - ETA: 33s  batch_loss: 0.2407 [Training] 13/120 [==>...........................] - ETA: 32s  batch_loss: 0.2424 [Training] 14/120 [==>...........................] - ETA: 31s  batch_loss: 0.2451 [Training] 15/120 [==>...........................] - ETA: 31s  batch_loss: 0.2413 [Training] 16/120 [===>..........................] - ETA: 30s  batch_loss: 0.2441 [Training] 17/120 [===>..........................] - ETA: 30s  batch_loss: 0.2428 [Training] 18/120 [===>..........................] - ETA: 29s  batch_loss: 0.2415 [Training] 19/120 [===>..........................] - ETA: 29s  batch_loss: 0.2420 [Training] 20/120 [====>.........................] - ETA: 28s  batch_loss: 0.2406 [Training] 21/120 [====>.........................] - ETA: 28s  batch_loss: 0.2402 [Training] 22/120 [====>.........................] - ETA: 28s  batch_loss: 0.2391 [Training] 23/120 [====>.........................] - ETA: 27s  batch_loss: 0.2379 [Training] 24/120 [=====>........................] - ETA: 27s  batch_loss: 0.2376 [Training] 25/120 [=====>........................] - ETA: 26s  batch_loss: 0.2414 [Training] 26/120 [=====>........................] - ETA: 26s  batch_loss: 0.2415 [Training] 27/120 [=====>........................] - ETA: 26s  batch_loss: 0.2423 [Training] 28/120 [======>.......................] - ETA: 25s  batch_loss: 0.2440 [Training] 29/120 [======>.......................] - ETA: 25s  batch_loss: 0.2447 [Training] 30/120 [======>.......................] - ETA: 25s  batch_loss: 0.2442 [Training] 31/120 [======>.......................] - ETA: 24s  batch_loss: 0.2437 [Training] 32/120 [=======>......................] - ETA: 24s  batch_loss: 0.2431 [Training] 33/120 [=======>......................] - ETA: 24s  batch_loss: 0.2436 [Training] 34/120 [=======>......................] - ETA: 23s  batch_loss: 0.2428 [Training] 35/120 [=======>......................] - ETA: 23s  batch_loss: 0.2422 [Training] 36/120 [========>.....................] - ETA: 23s  batch_loss: 0.2423 [Training] 37/120 [========>.....................] - ETA: 23s  batch_loss: 0.2420 [Training] 38/120 [========>.....................] - ETA: 22s  batch_loss: 0.2412 [Training] 39/120 [========>.....................] - ETA: 22s  batch_loss: 0.2406 [Training] 40/120 [=========>....................] - ETA: 22s  batch_loss: 0.2397 [Training] 41/120 [=========>....................] - ETA: 21s  batch_loss: 0.2392 [Training] 42/120 [=========>....................] - ETA: 21s  batch_loss: 0.2406 [Training] 43/120 [=========>....................] - ETA: 21s  batch_loss: 0.2401 [Training] 44/120 [==========>...................] - ETA: 20s  batch_loss: 0.2400 [Training] 45/120 [==========>...................] - ETA: 20s  batch_loss: 0.2391 [Training] 46/120 [==========>...................] - ETA: 20s  batch_loss: 0.2386 [Training] 47/120 [==========>...................] - ETA: 20s  batch_loss: 0.2394 [Training] 48/120 [===========>..................] - ETA: 19s  batch_loss: 0.2415 [Training] 49/120 [===========>..................] - ETA: 19s  batch_loss: 0.2413 [Training] 50/120 [===========>..................] - ETA: 19s  batch_loss: 0.2413 [Training] 51/120 [===========>..................] - ETA: 18s  batch_loss: 0.2422 [Training] 52/120 [============>.................] - ETA: 18s  batch_loss: 0.2420 [Training] 53/120 [============>.................] - ETA: 18s  batch_loss: 0.2411 [Training] 54/120 [============>.................] - ETA: 18s  batch_loss: 0.2400 [Training] 55/120 [============>.................] - ETA: 17s  batch_loss: 0.2403 [Training] 56/120 [=============>................] - ETA: 17s  batch_loss: 0.2408 [Training] 57/120 [=============>................] - ETA: 17s  batch_loss: 0.2410 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.2404 [Training] 59/120 [=============>................] - ETA: 16s  batch_loss: 0.2411 [Training] 60/120 [==============>...............] - ETA: 16s  batch_loss: 0.2415 [Training] 61/120 [==============>...............] - ETA: 16s  batch_loss: 0.2413 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.2405 [Training] 63/120 [==============>...............] - ETA: 15s  batch_loss: 0.2415 [Training] 64/120 [===============>..............] - ETA: 15s  batch_loss: 0.2415 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.2410 [Training] 66/120 [===============>..............] - ETA: 14s  batch_loss: 0.2405 [Training] 67/120 [===============>..............] - ETA: 14s  batch_loss: 0.2398 [Training] 68/120 [================>.............] - ETA: 14s  batch_loss: 0.2395 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.2407 [Training] 70/120 [================>.............] - ETA: 13s  batch_loss: 0.2403 [Training] 71/120 [================>.............] - ETA: 13s  batch_loss: 0.2400 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.2400 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.2403 [Training] 74/120 [=================>............] - ETA: 12s  batch_loss: 0.2401 [Training] 75/120 [=================>............] - ETA: 12s  batch_loss: 0.2405 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.2399 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.2401 [Training] 78/120 [==================>...........] - ETA: 11s  batch_loss: 0.2404 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.2397 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.2393 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.2393 [Training] 82/120 [===================>..........] - ETA: 10s  batch_loss: 0.2393 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.2396 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.2395 [Training] 85/120 [====================>.........] - ETA: 9s  batch_loss: 0.2393 [Training] 86/120 [====================>.........] - ETA: 9s  batch_loss: 0.2386 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.2389 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.2398 [Training] 89/120 [=====================>........] - ETA: 8s  batch_loss: 0.2394 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.2398 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.2395 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.2395 [Training] 93/120 [======================>.......] - ETA: 7s  batch_loss: 0.2392 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.2389 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.2383 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.2385 [Training] 97/120 [=======================>......] - ETA: 6s  batch_loss: 0.2387 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.2390 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.2391 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.2392 [Training] 101/120 [========================>.....] - ETA: 5s  batch_loss: 0.2393 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.2392 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.2390 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.2393 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.2393 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.2399 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.2406 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.2403 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.2410 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.2414 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.2411 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.2410 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.2406 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.2405 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.2416 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.2418 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.2418 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.2414 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.2418 [Training] 120/120 [==============================] 262.1ms/step  batch_loss: 0.2420 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:31:55 - INFO - root -   The F1-score is 0.6318057932433289
10/17/2022 14:31:55 - INFO - root -   the best eval f1 is 0.6318, saving model !!
10/17/2022 14:31:58 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:29  batch_loss: 0.2222 [Training] 2/120 [..............................] - ETA: 59s  batch_loss: 0.2117 [Training] 3/120 [..............................] - ETA: 49s  batch_loss: 0.2038 [Training] 4/120 [>.............................] - ETA: 43s  batch_loss: 0.2027 [Training] 5/120 [>.............................] - ETA: 40s  batch_loss: 0.2092 [Training] 6/120 [>.............................] - ETA: 38s  batch_loss: 0.2162 [Training] 7/120 [>.............................] - ETA: 36s  batch_loss: 0.2245 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 0.2274 [Training] 9/120 [=>............................] - ETA: 34s  batch_loss: 0.2209 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 0.2269 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.2232 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.2218 [Training] 13/120 [==>...........................] - ETA: 31s  batch_loss: 0.2204 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.2208 [Training] 15/120 [==>...........................] - ETA: 29s  batch_loss: 0.2215 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.2235 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 0.2215 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.2211 [Training] 19/120 [===>..........................] - ETA: 27s  batch_loss: 0.2209 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.2211 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.2211 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.2227 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.2212 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.2224 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.2209 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.2212 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 0.2226 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.2214 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.2195 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 0.2185 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.2193 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.2189 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.2186 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.2195 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.2199 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.2205 [Training] 37/120 [========>.....................] - ETA: 21s  batch_loss: 0.2208 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.2211 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.2219 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.2224 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.2230 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.2224 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.2220 [Training] 44/120 [==========>...................] - ETA: 19s  batch_loss: 0.2221 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.2233 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.2230 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.2227 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.2219 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.2218 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.2215 [Training] 51/120 [===========>..................] - ETA: 17s  batch_loss: 0.2209 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.2232 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.2234 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.2235 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.2228 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.2249 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.2242 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.2245 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.2244 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.2244 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.2255 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.2249 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.2251 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.2245 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.2238 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 0.2240 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.2243 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.2240 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.2243 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.2243 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.2241 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.2250 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.2252 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.2249 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.2244 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.2239 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.2236 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.2241 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.2237 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.2233 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.2239 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.2246 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.2250 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.2254 [Training] 85/120 [====================>.........] - ETA: 8s  batch_loss: 0.2252 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.2259 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.2262 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.2261 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.2260 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.2261 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.2258 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.2261 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.2260 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.2259 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.2260 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.2256 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.2266 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.2263 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.2262 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.2259 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.2261 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.2260 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.2262 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.2255 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.2259 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.2264 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.2267 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.2267 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.2262 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.2263 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.2264 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.2256 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.2257 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.2255 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.2251 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.2249 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.2248 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.2248 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.2246 [Training] 120/120 [==============================] 255.3ms/step  batch_loss: 0.2247 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:32:35 - INFO - root -   The F1-score is 0.6338974292727038
10/17/2022 14:32:35 - INFO - root -   the best eval f1 is 0.6339, saving model !!
10/17/2022 14:32:38 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:36  batch_loss: 0.2191 [Training] 2/120 [..............................] - ETA: 1:01  batch_loss: 0.2157 [Training] 3/120 [..............................] - ETA: 50s  batch_loss: 0.2016 [Training] 4/120 [>.............................] - ETA: 45s  batch_loss: 0.2033 [Training] 5/120 [>.............................] - ETA: 41s  batch_loss: 0.1945 [Training] 6/120 [>.............................] - ETA: 39s  batch_loss: 0.1940 [Training] 7/120 [>.............................] - ETA: 37s  batch_loss: 0.1940 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 0.1931 [Training] 9/120 [=>............................] - ETA: 34s  batch_loss: 0.1986 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 0.1987 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.1976 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.1988 [Training] 13/120 [==>...........................] - ETA: 30s  batch_loss: 0.1979 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.1990 [Training] 15/120 [==>...........................] - ETA: 29s  batch_loss: 0.1993 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.1982 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 0.1975 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.1981 [Training] 19/120 [===>..........................] - ETA: 27s  batch_loss: 0.1990 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.1984 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.1988 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.1990 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.1985 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.2001 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.2000 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.2002 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 0.1982 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.1979 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.1986 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 0.1988 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.2006 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.2014 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.2007 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.2009 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.2012 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.2019 [Training] 37/120 [========>.....................] - ETA: 22s  batch_loss: 0.2026 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.2023 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.2022 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.2016 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.2008 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.2020 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.2016 [Training] 44/120 [==========>...................] - ETA: 19s  batch_loss: 0.2014 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.2014 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.2013 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.2009 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.2001 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.1993 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.2003 [Training] 51/120 [===========>..................] - ETA: 18s  batch_loss: 0.2006 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.2001 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.1998 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.1995 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.1995 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.1992 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.1998 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.2007 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.2016 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.2014 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.2008 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.2007 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.2018 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.2021 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.2021 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 0.2019 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.2020 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.2016 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.2024 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.2029 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.2036 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.2034 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.2034 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.2032 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.2034 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.2030 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.2036 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.2039 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.2043 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.2050 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.2052 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.2055 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.2053 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.2052 [Training] 85/120 [====================>.........] - ETA: 9s  batch_loss: 0.2060 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.2059 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.2055 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.2058 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.2062 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.2059 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.2058 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.2059 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.2063 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.2063 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.2065 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.2071 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.2072 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.2070 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.2072 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.2088 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.2089 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.2083 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.2080 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.2082 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.2082 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.2085 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.2086 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.2086 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.2089 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.2089 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.2088 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.2088 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.2088 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.2091 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.2088 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.2090 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.2093 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.2092 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.2091 [Training] 120/120 [==============================] 256.3ms/step  batch_loss: 0.2089 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:33:16 - INFO - root -   The F1-score is 0.6426503698938566
10/17/2022 14:33:16 - INFO - root -   the best eval f1 is 0.6427, saving model !!
10/17/2022 14:33:19 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:29  batch_loss: 0.1777 [Training] 2/120 [..............................] - ETA: 59s  batch_loss: 0.1704 [Training] 3/120 [..............................] - ETA: 49s  batch_loss: 0.1658 [Training] 4/120 [>.............................] - ETA: 43s  batch_loss: 0.1658 [Training] 5/120 [>.............................] - ETA: 40s  batch_loss: 0.1676 [Training] 6/120 [>.............................] - ETA: 38s  batch_loss: 0.1713 [Training] 7/120 [>.............................] - ETA: 36s  batch_loss: 0.1793 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 0.1809 [Training] 9/120 [=>............................] - ETA: 33s  batch_loss: 0.1824 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 0.1812 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.1789 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.1793 [Training] 13/120 [==>...........................] - ETA: 30s  batch_loss: 0.1801 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.1835 [Training] 15/120 [==>...........................] - ETA: 29s  batch_loss: 0.1839 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.1845 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 0.1849 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.1838 [Training] 19/120 [===>..........................] - ETA: 27s  batch_loss: 0.1832 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.1843 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.1877 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.1879 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.1887 [Training] 24/120 [=====>........................] - ETA: 25s  batch_loss: 0.1893 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.1899 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.1909 [Training] 27/120 [=====>........................] - ETA: 24s  batch_loss: 0.1910 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.1899 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.1920 [Training] 30/120 [======>.......................] - ETA: 23s  batch_loss: 0.1932 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.1935 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.1931 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.1934 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.1939 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.1934 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.1935 [Training] 37/120 [========>.....................] - ETA: 21s  batch_loss: 0.1933 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.1939 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.1947 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.1937 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.1931 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.1926 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.1921 [Training] 44/120 [==========>...................] - ETA: 19s  batch_loss: 0.1912 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.1911 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.1911 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.1898 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.1913 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.1915 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.1912 [Training] 51/120 [===========>..................] - ETA: 17s  batch_loss: 0.1903 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.1901 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.1899 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.1894 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.1899 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.1899 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.1896 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.1903 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.1901 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.1901 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.1895 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.1899 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.1901 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.1899 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.1906 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 0.1918 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.1926 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.1940 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.1942 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.1941 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.1943 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.1946 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.1949 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.1951 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.1950 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.1953 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.1956 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.1958 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.1959 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.1964 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.1969 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.1969 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.1966 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.1963 [Training] 85/120 [====================>.........] - ETA: 8s  batch_loss: 0.1961 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.1961 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.1959 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.1957 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.1958 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.1961 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.1960 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.1954 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.1955 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.1950 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.1958 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.1963 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.1965 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.1967 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.1968 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.1971 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.1969 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.1965 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.1969 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.1969 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.1968 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.1968 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.1968 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.1964 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.1967 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.1965 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.1965 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.1965 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.1967 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.1972 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.1973 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.1971 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.1972 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.1976 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.1980 [Training] 120/120 [==============================] 255.6ms/step  batch_loss: 0.1977 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:33:56 - INFO - root -   The F1-score is 0.6439900630613415
10/17/2022 14:33:56 - INFO - root -   the best eval f1 is 0.6440, saving model !!
10/17/2022 14:33:59 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:32  batch_loss: 0.1965 [Training] 2/120 [..............................] - ETA: 1:01  batch_loss: 0.1875 [Training] 3/120 [..............................] - ETA: 50s  batch_loss: 0.1861 [Training] 4/120 [>.............................] - ETA: 44s  batch_loss: 0.1836 [Training] 5/120 [>.............................] - ETA: 41s  batch_loss: 0.1738 [Training] 6/120 [>.............................] - ETA: 38s  batch_loss: 0.1712 [Training] 7/120 [>.............................] - ETA: 37s  batch_loss: 0.1718 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 0.1671 [Training] 9/120 [=>............................] - ETA: 34s  batch_loss: 0.1692 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 0.1658 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.1690 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.1752 [Training] 13/120 [==>...........................] - ETA: 31s  batch_loss: 0.1725 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.1761 [Training] 15/120 [==>...........................] - ETA: 29s  batch_loss: 0.1789 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.1804 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 0.1826 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.1807 [Training] 19/120 [===>..........................] - ETA: 27s  batch_loss: 0.1810 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.1808 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.1800 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.1796 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.1796 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.1807 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.1800 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.1813 [Training] 27/120 [=====>........................] - ETA: 24s  batch_loss: 0.1815 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.1795 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.1790 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 0.1781 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.1780 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.1777 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.1779 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.1779 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.1805 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.1803 [Training] 37/120 [========>.....................] - ETA: 21s  batch_loss: 0.1820 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.1834 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.1846 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.1835 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.1829 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.1851 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.1854 [Training] 44/120 [==========>...................] - ETA: 19s  batch_loss: 0.1844 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.1837 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.1838 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.1851 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.1858 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.1862 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.1858 [Training] 51/120 [===========>..................] - ETA: 17s  batch_loss: 0.1866 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.1886 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.1892 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.1900 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.1904 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.1899 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.1903 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.1894 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.1894 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.1888 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.1885 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.1887 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.1894 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.1894 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.1897 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 0.1894 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.1891 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.1883 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.1874 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.1872 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.1868 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.1871 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.1871 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.1874 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.1869 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.1872 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.1871 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.1874 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.1873 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.1868 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.1862 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.1859 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.1856 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.1856 [Training] 85/120 [====================>.........] - ETA: 8s  batch_loss: 0.1855 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.1851 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.1851 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.1852 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.1848 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.1853 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.1848 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.1846 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.1846 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.1844 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.1848 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.1844 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.1847 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.1850 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.1846 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.1846 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.1844 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.1844 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.1840 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.1847 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.1846 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.1848 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.1845 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.1847 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.1847 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.1845 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.1846 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.1851 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.1848 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.1848 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.1849 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.1848 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.1850 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.1857 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.1854 [Training] 120/120 [==============================] 258.2ms/step  batch_loss: 0.1853 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:34:36 - INFO - root -   The F1-score is 0.6429224937916612
10/17/2022 14:34:36 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:28  batch_loss: 0.1588 [Training] 2/120 [..............................] - ETA: 58s  batch_loss: 0.1469 [Training] 3/120 [..............................] - ETA: 48s  batch_loss: 0.1555 [Training] 4/120 [>.............................] - ETA: 43s  batch_loss: 0.1503 [Training] 5/120 [>.............................] - ETA: 40s  batch_loss: 0.1656 [Training] 6/120 [>.............................] - ETA: 37s  batch_loss: 0.1597 [Training] 7/120 [>.............................] - ETA: 36s  batch_loss: 0.1564 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 0.1555 [Training] 9/120 [=>............................] - ETA: 33s  batch_loss: 0.1528 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 0.1560 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.1599 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.1608 [Training] 13/120 [==>...........................] - ETA: 30s  batch_loss: 0.1660 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.1654 [Training] 15/120 [==>...........................] - ETA: 29s  batch_loss: 0.1649 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.1629 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 0.1641 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.1655 [Training] 19/120 [===>..........................] - ETA: 28s  batch_loss: 0.1644 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.1641 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.1646 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.1651 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.1647 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.1656 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.1645 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.1666 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 0.1655 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.1660 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.1672 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 0.1669 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.1660 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.1663 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.1670 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.1674 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.1675 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.1667 [Training] 37/120 [========>.....................] - ETA: 22s  batch_loss: 0.1672 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.1679 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.1676 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.1673 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.1673 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.1668 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.1680 [Training] 44/120 [==========>...................] - ETA: 20s  batch_loss: 0.1678 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.1676 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.1682 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.1685 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.1683 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.1685 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.1690 [Training] 51/120 [===========>..................] - ETA: 18s  batch_loss: 0.1693 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.1703 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.1712 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.1720 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.1727 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.1726 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.1727 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.1731 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.1739 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.1749 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.1747 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.1749 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.1747 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.1750 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.1750 [Training] 66/120 [===============>..............] - ETA: 14s  batch_loss: 0.1747 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.1753 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.1748 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.1749 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.1746 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.1747 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.1757 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.1757 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.1761 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.1763 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.1760 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.1758 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.1758 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.1755 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.1756 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.1756 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.1752 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.1750 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.1747 [Training] 85/120 [====================>.........] - ETA: 9s  batch_loss: 0.1742 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.1744 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.1742 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.1746 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.1740 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.1740 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.1745 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.1742 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.1745 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.1743 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.1741 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.1747 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.1749 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.1745 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.1750 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.1747 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.1749 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.1756 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.1755 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.1756 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.1760 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.1758 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.1755 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.1753 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.1749 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.1752 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.1757 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.1759 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.1759 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.1761 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.1758 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.1762 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.1759 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.1757 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.1755 [Training] 120/120 [==============================] 256.5ms/step  batch_loss: 0.1756 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:35:13 - INFO - root -   The F1-score is 0.6470777341991899
10/17/2022 14:35:13 - INFO - root -   the best eval f1 is 0.6471, saving model !!
10/17/2022 14:35:16 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:30  batch_loss: 0.1533 [Training] 2/120 [..............................] - ETA: 1:01  batch_loss: 0.1437 [Training] 3/120 [..............................] - ETA: 50s  batch_loss: 0.1471 [Training] 4/120 [>.............................] - ETA: 45s  batch_loss: 0.1592 [Training] 5/120 [>.............................] - ETA: 42s  batch_loss: 0.1522 [Training] 6/120 [>.............................] - ETA: 39s  batch_loss: 0.1578 [Training] 7/120 [>.............................] - ETA: 38s  batch_loss: 0.1590 [Training] 8/120 [=>............................] - ETA: 36s  batch_loss: 0.1626 [Training] 9/120 [=>............................] - ETA: 35s  batch_loss: 0.1596 [Training] 10/120 [=>............................] - ETA: 34s  batch_loss: 0.1593 [Training] 11/120 [=>............................] - ETA: 33s  batch_loss: 0.1581 [Training] 12/120 [==>...........................] - ETA: 33s  batch_loss: 0.1575 [Training] 13/120 [==>...........................] - ETA: 32s  batch_loss: 0.1554 [Training] 14/120 [==>...........................] - ETA: 31s  batch_loss: 0.1558 [Training] 15/120 [==>...........................] - ETA: 31s  batch_loss: 0.1548 [Training] 16/120 [===>..........................] - ETA: 30s  batch_loss: 0.1575 [Training] 17/120 [===>..........................] - ETA: 30s  batch_loss: 0.1569 [Training] 18/120 [===>..........................] - ETA: 29s  batch_loss: 0.1569 [Training] 19/120 [===>..........................] - ETA: 29s  batch_loss: 0.1577 [Training] 20/120 [====>.........................] - ETA: 28s  batch_loss: 0.1567 [Training] 21/120 [====>.........................] - ETA: 28s  batch_loss: 0.1586 [Training] 22/120 [====>.........................] - ETA: 28s  batch_loss: 0.1590 [Training] 23/120 [====>.........................] - ETA: 27s  batch_loss: 0.1585 [Training] 24/120 [=====>........................] - ETA: 27s  batch_loss: 0.1578 [Training] 25/120 [=====>........................] - ETA: 26s  batch_loss: 0.1597 [Training] 26/120 [=====>........................] - ETA: 26s  batch_loss: 0.1594 [Training] 27/120 [=====>........................] - ETA: 26s  batch_loss: 0.1597 [Training] 28/120 [======>.......................] - ETA: 25s  batch_loss: 0.1611 [Training] 29/120 [======>.......................] - ETA: 25s  batch_loss: 0.1625 [Training] 30/120 [======>.......................] - ETA: 25s  batch_loss: 0.1619 [Training] 31/120 [======>.......................] - ETA: 24s  batch_loss: 0.1636 [Training] 32/120 [=======>......................] - ETA: 24s  batch_loss: 0.1625 [Training] 33/120 [=======>......................] - ETA: 24s  batch_loss: 0.1643 [Training] 34/120 [=======>......................] - ETA: 23s  batch_loss: 0.1641 [Training] 35/120 [=======>......................] - ETA: 23s  batch_loss: 0.1636 [Training] 36/120 [========>.....................] - ETA: 23s  batch_loss: 0.1639 [Training] 37/120 [========>.....................] - ETA: 22s  batch_loss: 0.1638 [Training] 38/120 [========>.....................] - ETA: 22s  batch_loss: 0.1633 [Training] 39/120 [========>.....................] - ETA: 22s  batch_loss: 0.1629 [Training] 40/120 [=========>....................] - ETA: 22s  batch_loss: 0.1629 [Training] 41/120 [=========>....................] - ETA: 21s  batch_loss: 0.1630 [Training] 42/120 [=========>....................] - ETA: 21s  batch_loss: 0.1634 [Training] 43/120 [=========>....................] - ETA: 21s  batch_loss: 0.1624 [Training] 44/120 [==========>...................] - ETA: 20s  batch_loss: 0.1621 [Training] 45/120 [==========>...................] - ETA: 20s  batch_loss: 0.1621 [Training] 46/120 [==========>...................] - ETA: 20s  batch_loss: 0.1620 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.1618 [Training] 48/120 [===========>..................] - ETA: 19s  batch_loss: 0.1611 [Training] 49/120 [===========>..................] - ETA: 19s  batch_loss: 0.1609 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.1613 [Training] 51/120 [===========>..................] - ETA: 18s  batch_loss: 0.1617 [Training] 52/120 [============>.................] - ETA: 18s  batch_loss: 0.1610 [Training] 53/120 [============>.................] - ETA: 18s  batch_loss: 0.1608 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.1617 [Training] 55/120 [============>.................] - ETA: 17s  batch_loss: 0.1614 [Training] 56/120 [=============>................] - ETA: 17s  batch_loss: 0.1615 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.1619 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.1614 [Training] 59/120 [=============>................] - ETA: 16s  batch_loss: 0.1618 [Training] 60/120 [==============>...............] - ETA: 16s  batch_loss: 0.1614 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.1608 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.1619 [Training] 63/120 [==============>...............] - ETA: 15s  batch_loss: 0.1618 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.1621 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.1614 [Training] 66/120 [===============>..............] - ETA: 14s  batch_loss: 0.1613 [Training] 67/120 [===============>..............] - ETA: 14s  batch_loss: 0.1613 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.1613 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.1613 [Training] 70/120 [================>.............] - ETA: 13s  batch_loss: 0.1612 [Training] 71/120 [================>.............] - ETA: 13s  batch_loss: 0.1613 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.1611 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.1611 [Training] 74/120 [=================>............] - ETA: 12s  batch_loss: 0.1607 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.1615 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.1615 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.1615 [Training] 78/120 [==================>...........] - ETA: 11s  batch_loss: 0.1621 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.1623 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.1621 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.1621 [Training] 82/120 [===================>..........] - ETA: 10s  batch_loss: 0.1622 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.1626 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.1623 [Training] 85/120 [====================>.........] - ETA: 9s  batch_loss: 0.1621 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.1620 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.1618 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.1618 [Training] 89/120 [=====================>........] - ETA: 8s  batch_loss: 0.1626 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.1631 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.1640 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.1641 [Training] 93/120 [======================>.......] - ETA: 7s  batch_loss: 0.1643 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.1643 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.1645 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.1645 [Training] 97/120 [=======================>......] - ETA: 6s  batch_loss: 0.1645 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.1648 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.1650 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.1647 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.1653 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.1658 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.1654 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.1658 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.1655 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.1654 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.1651 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.1652 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.1653 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.1652 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.1647 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.1649 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.1647 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.1653 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.1658 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.1656 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.1656 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.1657 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.1660 [Training] 120/120 [==============================] 260.5ms/step  batch_loss: 0.1659 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:35:54 - INFO - root -   The F1-score is 0.6469041672144078
10/17/2022 14:35:54 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:32  batch_loss: 0.1064 [Training] 2/120 [..............................] - ETA: 1:00  batch_loss: 0.1329 [Training] 3/120 [..............................] - ETA: 50s  batch_loss: 0.1234 [Training] 4/120 [>.............................] - ETA: 44s  batch_loss: 0.1266 [Training] 5/120 [>.............................] - ETA: 41s  batch_loss: 0.1290 [Training] 6/120 [>.............................] - ETA: 38s  batch_loss: 0.1300 [Training] 7/120 [>.............................] - ETA: 36s  batch_loss: 0.1277 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 0.1345 [Training] 9/120 [=>............................] - ETA: 34s  batch_loss: 0.1407 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 0.1429 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.1446 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.1418 [Training] 13/120 [==>...........................] - ETA: 31s  batch_loss: 0.1433 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.1466 [Training] 15/120 [==>...........................] - ETA: 30s  batch_loss: 0.1496 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.1503 [Training] 17/120 [===>..........................] - ETA: 29s  batch_loss: 0.1512 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.1547 [Training] 19/120 [===>..........................] - ETA: 28s  batch_loss: 0.1534 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.1562 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.1535 [Training] 22/120 [====>.........................] - ETA: 27s  batch_loss: 0.1530 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.1517 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.1529 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.1518 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.1514 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 0.1556 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.1557 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.1541 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 0.1545 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.1551 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.1549 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.1537 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.1528 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.1521 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.1522 [Training] 37/120 [========>.....................] - ETA: 22s  batch_loss: 0.1520 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.1525 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.1531 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.1529 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.1526 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.1526 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.1530 [Training] 44/120 [==========>...................] - ETA: 20s  batch_loss: 0.1530 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.1523 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.1543 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.1544 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.1547 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.1545 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.1548 [Training] 51/120 [===========>..................] - ETA: 18s  batch_loss: 0.1542 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.1548 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.1548 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.1549 [Training] 55/120 [============>.................] - ETA: 17s  batch_loss: 0.1555 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.1553 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.1552 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.1564 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.1579 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.1578 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.1584 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.1580 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.1580 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.1582 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.1579 [Training] 66/120 [===============>..............] - ETA: 14s  batch_loss: 0.1574 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.1570 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.1568 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.1565 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.1568 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.1562 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.1569 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.1563 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.1562 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.1563 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.1561 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.1554 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.1553 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.1555 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.1558 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.1559 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.1556 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.1554 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.1560 [Training] 85/120 [====================>.........] - ETA: 9s  batch_loss: 0.1561 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.1567 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.1574 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.1576 [Training] 89/120 [=====================>........] - ETA: 8s  batch_loss: 0.1575 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.1571 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.1569 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.1575 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.1577 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.1583 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.1587 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.1584 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.1581 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.1580 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.1583 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.1583 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.1586 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.1591 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.1592 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.1589 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.1588 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.1590 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.1591 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.1593 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.1592 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.1595 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.1596 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.1599 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.1594 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.1595 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.1596 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.1601 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.1598 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.1595 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.1595 [Training] 120/120 [==============================] 256.5ms/step  batch_loss: 0.1595 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:36:31 - INFO - root -   The F1-score is 0.6428097198247246
10/17/2022 14:36:31 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:33  batch_loss: 0.1670 [Training] 2/120 [..............................] - ETA: 1:01  batch_loss: 0.1575 [Training] 3/120 [..............................] - ETA: 51s  batch_loss: 0.1542 [Training] 4/120 [>.............................] - ETA: 45s  batch_loss: 0.1682 [Training] 5/120 [>.............................] - ETA: 42s  batch_loss: 0.1576 [Training] 6/120 [>.............................] - ETA: 39s  batch_loss: 0.1546 [Training] 7/120 [>.............................] - ETA: 38s  batch_loss: 0.1539 [Training] 8/120 [=>............................] - ETA: 36s  batch_loss: 0.1500 [Training] 9/120 [=>............................] - ETA: 35s  batch_loss: 0.1487 [Training] 10/120 [=>............................] - ETA: 34s  batch_loss: 0.1465 [Training] 11/120 [=>............................] - ETA: 33s  batch_loss: 0.1429 [Training] 12/120 [==>...........................] - ETA: 32s  batch_loss: 0.1433 [Training] 13/120 [==>...........................] - ETA: 31s  batch_loss: 0.1500 [Training] 14/120 [==>...........................] - ETA: 31s  batch_loss: 0.1505 [Training] 15/120 [==>...........................] - ETA: 30s  batch_loss: 0.1491 [Training] 16/120 [===>..........................] - ETA: 30s  batch_loss: 0.1508 [Training] 17/120 [===>..........................] - ETA: 29s  batch_loss: 0.1514 [Training] 18/120 [===>..........................] - ETA: 29s  batch_loss: 0.1520 [Training] 19/120 [===>..........................] - ETA: 28s  batch_loss: 0.1518 [Training] 20/120 [====>.........................] - ETA: 28s  batch_loss: 0.1509 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.1507 [Training] 22/120 [====>.........................] - ETA: 27s  batch_loss: 0.1521 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.1510 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.1503 [Training] 25/120 [=====>........................] - ETA: 26s  batch_loss: 0.1513 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.1519 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 0.1520 [Training] 28/120 [======>.......................] - ETA: 25s  batch_loss: 0.1509 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.1505 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 0.1506 [Training] 31/120 [======>.......................] - ETA: 24s  batch_loss: 0.1519 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.1540 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.1530 [Training] 34/120 [=======>......................] - ETA: 23s  batch_loss: 0.1526 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.1522 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.1516 [Training] 37/120 [========>.....................] - ETA: 22s  batch_loss: 0.1526 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.1515 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.1502 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.1503 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.1506 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.1513 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.1510 [Training] 44/120 [==========>...................] - ETA: 20s  batch_loss: 0.1506 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.1505 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.1494 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.1503 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.1499 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.1499 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.1497 [Training] 51/120 [===========>..................] - ETA: 18s  batch_loss: 0.1491 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.1493 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.1487 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.1484 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.1482 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.1478 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.1486 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.1491 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.1488 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.1488 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.1488 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.1484 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.1484 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.1485 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.1486 [Training] 66/120 [===============>..............] - ETA: 14s  batch_loss: 0.1481 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.1475 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.1466 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.1473 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.1474 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.1474 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.1470 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.1475 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.1481 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.1481 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.1485 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.1482 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.1485 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.1488 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.1484 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.1487 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.1489 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.1492 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.1488 [Training] 85/120 [====================>.........] - ETA: 9s  batch_loss: 0.1490 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.1487 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.1484 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.1481 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.1483 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.1487 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.1487 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.1484 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.1484 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.1489 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.1488 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.1489 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.1487 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.1483 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.1484 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.1489 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.1487 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.1489 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.1489 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.1494 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.1495 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.1498 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.1496 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.1497 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.1495 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.1495 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.1496 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.1498 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.1499 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.1498 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.1502 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.1505 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.1506 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.1510 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.1513 [Training] 120/120 [==============================] 255.9ms/step  batch_loss: 0.1507 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:37:09 - INFO - root -   The F1-score is 0.6493066857626457
10/17/2022 14:37:09 - INFO - root -   the best eval f1 is 0.6493, saving model !!
10/17/2022 14:37:12 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:31  batch_loss: 0.1776 [Training] 2/120 [..............................] - ETA: 1:00  batch_loss: 0.1559 [Training] 3/120 [..............................] - ETA: 49s  batch_loss: 0.1462 [Training] 4/120 [>.............................] - ETA: 44s  batch_loss: 0.1345 [Training] 5/120 [>.............................] - ETA: 40s  batch_loss: 0.1370 [Training] 6/120 [>.............................] - ETA: 38s  batch_loss: 0.1365 [Training] 7/120 [>.............................] - ETA: 36s  batch_loss: 0.1375 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 0.1463 [Training] 9/120 [=>............................] - ETA: 33s  batch_loss: 0.1495 [Training] 10/120 [=>............................] - ETA: 32s  batch_loss: 0.1466 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.1454 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.1445 [Training] 13/120 [==>...........................] - ETA: 30s  batch_loss: 0.1460 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.1427 [Training] 15/120 [==>...........................] - ETA: 29s  batch_loss: 0.1450 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.1430 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 0.1438 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.1410 [Training] 19/120 [===>..........................] - ETA: 27s  batch_loss: 0.1392 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.1391 [Training] 21/120 [====>.........................] - ETA: 26s  batch_loss: 0.1400 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.1398 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.1396 [Training] 24/120 [=====>........................] - ETA: 25s  batch_loss: 0.1392 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.1386 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.1379 [Training] 27/120 [=====>........................] - ETA: 24s  batch_loss: 0.1371 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.1378 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.1373 [Training] 30/120 [======>.......................] - ETA: 23s  batch_loss: 0.1378 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.1370 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.1363 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.1360 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.1356 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.1352 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.1348 [Training] 37/120 [========>.....................] - ETA: 21s  batch_loss: 0.1354 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.1381 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.1387 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.1378 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.1377 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.1384 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.1383 [Training] 44/120 [==========>...................] - ETA: 19s  batch_loss: 0.1389 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.1393 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.1393 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.1395 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.1388 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.1401 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.1399 [Training] 51/120 [===========>..................] - ETA: 17s  batch_loss: 0.1401 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.1398 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.1397 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.1394 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.1403 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.1405 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.1403 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.1399 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.1399 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.1398 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.1402 [Training] 62/120 [==============>...............] - ETA: 14s  batch_loss: 0.1404 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.1404 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.1404 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.1406 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 0.1403 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.1403 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.1408 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.1400 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.1403 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.1403 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.1407 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.1409 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.1409 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.1412 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.1413 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.1416 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.1416 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.1421 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.1425 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.1427 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.1431 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.1431 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.1429 [Training] 85/120 [====================>.........] - ETA: 8s  batch_loss: 0.1430 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.1430 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.1428 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.1426 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.1428 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.1429 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.1430 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.1428 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.1430 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.1427 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.1430 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.1430 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.1430 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.1432 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.1429 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.1430 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.1428 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.1425 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.1423 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.1425 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.1429 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.1430 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.1436 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.1434 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.1434 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.1434 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.1432 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.1435 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.1435 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.1435 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.1437 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.1437 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.1436 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.1435 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.1436 [Training] 120/120 [==============================] 255.1ms/step  batch_loss: 0.1436 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:37:49 - INFO - root -   The F1-score is 0.642134314627415
10/17/2022 14:37:49 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:27  batch_loss: 0.0781 [Training] 2/120 [..............................] - ETA: 59s  batch_loss: 0.1115 [Training] 3/120 [..............................] - ETA: 49s  batch_loss: 0.1052 [Training] 4/120 [>.............................] - ETA: 43s  batch_loss: 0.1117 [Training] 5/120 [>.............................] - ETA: 40s  batch_loss: 0.1162 [Training] 6/120 [>.............................] - ETA: 37s  batch_loss: 0.1180 [Training] 7/120 [>.............................] - ETA: 36s  batch_loss: 0.1194 [Training] 8/120 [=>............................] - ETA: 34s  batch_loss: 0.1179 [Training] 9/120 [=>............................] - ETA: 33s  batch_loss: 0.1191 [Training] 10/120 [=>............................] - ETA: 32s  batch_loss: 0.1205 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.1238 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.1241 [Training] 13/120 [==>...........................] - ETA: 30s  batch_loss: 0.1272 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.1301 [Training] 15/120 [==>...........................] - ETA: 29s  batch_loss: 0.1307 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.1282 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 0.1281 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.1299 [Training] 19/120 [===>..........................] - ETA: 27s  batch_loss: 0.1333 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.1330 [Training] 21/120 [====>.........................] - ETA: 26s  batch_loss: 0.1317 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.1311 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.1310 [Training] 24/120 [=====>........................] - ETA: 25s  batch_loss: 0.1313 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.1304 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.1301 [Training] 27/120 [=====>........................] - ETA: 24s  batch_loss: 0.1284 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.1269 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.1259 [Training] 30/120 [======>.......................] - ETA: 23s  batch_loss: 0.1252 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.1251 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.1264 [Training] 33/120 [=======>......................] - ETA: 22s  batch_loss: 0.1265 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.1279 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.1278 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.1294 [Training] 37/120 [========>.....................] - ETA: 21s  batch_loss: 0.1318 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.1322 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.1327 [Training] 40/120 [=========>....................] - ETA: 20s  batch_loss: 0.1330 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.1334 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.1336 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.1344 [Training] 44/120 [==========>...................] - ETA: 19s  batch_loss: 0.1341 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.1343 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.1356 [Training] 47/120 [==========>...................] - ETA: 18s  batch_loss: 0.1359 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.1358 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.1349 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.1345 [Training] 51/120 [===========>..................] - ETA: 17s  batch_loss: 0.1355 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.1361 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.1360 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.1363 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.1364 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.1363 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.1363 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.1368 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.1365 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.1355 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.1353 [Training] 62/120 [==============>...............] - ETA: 14s  batch_loss: 0.1352 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.1353 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.1353 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.1356 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 0.1354 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.1354 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.1354 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.1351 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.1362 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.1365 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.1365 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.1378 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.1372 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.1373 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.1383 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.1385 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.1385 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.1382 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.1379 [Training] 81/120 [===================>..........] - ETA: 9s  batch_loss: 0.1380 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.1383 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.1381 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.1381 [Training] 85/120 [====================>.........] - ETA: 8s  batch_loss: 0.1387 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.1384 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.1380 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.1380 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.1379 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.1378 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.1380 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.1383 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.1383 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.1384 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.1390 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.1387 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.1386 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.1385 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.1385 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.1382 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.1378 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.1377 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.1376 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.1374 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.1374 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.1373 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.1374 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.1372 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.1370 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.1370 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.1372 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.1379 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.1379 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.1376 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.1375 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.1378 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.1376 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.1380 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.1377 [Training] 120/120 [==============================] 254.6ms/step  batch_loss: 0.1378 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:38:26 - INFO - root -   The F1-score is 0.6518923037846075
10/17/2022 14:38:26 - INFO - root -   the best eval f1 is 0.6519, saving model !!
10/17/2022 14:38:29 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:30  batch_loss: 0.1501 [Training] 2/120 [..............................] - ETA: 1:00  batch_loss: 0.1423 [Training] 3/120 [..............................] - ETA: 49s  batch_loss: 0.1341 [Training] 4/120 [>.............................] - ETA: 44s  batch_loss: 0.1313 [Training] 5/120 [>.............................] - ETA: 40s  batch_loss: 0.1302 [Training] 6/120 [>.............................] - ETA: 38s  batch_loss: 0.1314 [Training] 7/120 [>.............................] - ETA: 36s  batch_loss: 0.1302 [Training] 8/120 [=>............................] - ETA: 35s  batch_loss: 0.1317 [Training] 9/120 [=>............................] - ETA: 34s  batch_loss: 0.1287 [Training] 10/120 [=>............................] - ETA: 33s  batch_loss: 0.1255 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.1258 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.1259 [Training] 13/120 [==>...........................] - ETA: 31s  batch_loss: 0.1263 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.1256 [Training] 15/120 [==>...........................] - ETA: 29s  batch_loss: 0.1272 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.1284 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 0.1288 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.1281 [Training] 19/120 [===>..........................] - ETA: 28s  batch_loss: 0.1282 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.1277 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.1274 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.1275 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.1269 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.1257 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.1251 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.1255 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 0.1251 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.1250 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.1250 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 0.1241 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.1237 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.1236 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.1243 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.1249 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.1264 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.1251 [Training] 37/120 [========>.....................] - ETA: 22s  batch_loss: 0.1257 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.1262 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.1263 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.1274 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.1285 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.1281 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.1290 [Training] 44/120 [==========>...................] - ETA: 20s  batch_loss: 0.1278 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.1278 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.1278 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.1277 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.1276 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.1273 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.1287 [Training] 51/120 [===========>..................] - ETA: 18s  batch_loss: 0.1290 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.1287 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.1283 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.1278 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.1282 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.1276 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.1270 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.1270 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.1272 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.1276 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.1280 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.1277 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.1274 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.1271 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.1267 [Training] 66/120 [===============>..............] - ETA: 14s  batch_loss: 0.1270 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.1268 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.1266 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.1266 [Training] 70/120 [================>.............] - ETA: 13s  batch_loss: 0.1266 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.1265 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.1268 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.1274 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.1282 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.1286 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.1286 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.1288 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.1289 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.1293 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.1293 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.1292 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.1293 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.1297 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.1297 [Training] 85/120 [====================>.........] - ETA: 9s  batch_loss: 0.1298 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.1296 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.1302 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.1299 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.1299 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.1297 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.1296 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.1297 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.1294 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.1294 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.1291 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.1294 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.1293 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.1296 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.1293 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.1295 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.1299 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.1300 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.1298 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.1297 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.1302 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.1301 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.1304 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.1309 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.1309 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.1307 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.1305 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.1306 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.1310 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.1311 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.1309 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.1307 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.1307 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.1315 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.1315 [Training] 120/120 [==============================] 256.0ms/step  batch_loss: 0.1314 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:39:06 - INFO - root -   The F1-score is 0.6533843857485884
10/17/2022 14:39:06 - INFO - root -   the best eval f1 is 0.6534, saving model !!
10/17/2022 14:39:09 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:29  batch_loss: 0.1272 [Training] 2/120 [..............................] - ETA: 58s  batch_loss: 0.1248 [Training] 3/120 [..............................] - ETA: 48s  batch_loss: 0.1145 [Training] 4/120 [>.............................] - ETA: 43s  batch_loss: 0.1134 [Training] 5/120 [>.............................] - ETA: 39s  batch_loss: 0.1127 [Training] 6/120 [>.............................] - ETA: 37s  batch_loss: 0.1073 [Training] 7/120 [>.............................] - ETA: 36s  batch_loss: 0.1110 [Training] 8/120 [=>............................] - ETA: 34s  batch_loss: 0.1098 [Training] 9/120 [=>............................] - ETA: 33s  batch_loss: 0.1108 [Training] 10/120 [=>............................] - ETA: 32s  batch_loss: 0.1090 [Training] 11/120 [=>............................] - ETA: 31s  batch_loss: 0.1119 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.1114 [Training] 13/120 [==>...........................] - ETA: 30s  batch_loss: 0.1148 [Training] 14/120 [==>...........................] - ETA: 29s  batch_loss: 0.1146 [Training] 15/120 [==>...........................] - ETA: 29s  batch_loss: 0.1139 [Training] 16/120 [===>..........................] - ETA: 28s  batch_loss: 0.1141 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 0.1171 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.1163 [Training] 19/120 [===>..........................] - ETA: 27s  batch_loss: 0.1186 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.1183 [Training] 21/120 [====>.........................] - ETA: 26s  batch_loss: 0.1175 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.1165 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.1154 [Training] 24/120 [=====>........................] - ETA: 25s  batch_loss: 0.1157 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.1169 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.1171 [Training] 27/120 [=====>........................] - ETA: 24s  batch_loss: 0.1192 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.1210 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.1214 [Training] 30/120 [======>.......................] - ETA: 23s  batch_loss: 0.1209 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.1195 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.1212 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.1203 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.1208 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.1203 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.1205 [Training] 37/120 [========>.....................] - ETA: 21s  batch_loss: 0.1203 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.1197 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.1204 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.1207 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.1207 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.1204 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.1223 [Training] 44/120 [==========>...................] - ETA: 19s  batch_loss: 0.1221 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.1222 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.1221 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.1224 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.1229 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.1225 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.1226 [Training] 51/120 [===========>..................] - ETA: 17s  batch_loss: 0.1230 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.1238 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.1245 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.1240 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.1240 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.1247 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.1247 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.1245 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.1246 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.1243 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.1245 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.1245 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.1250 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.1249 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.1247 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 0.1250 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.1248 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.1250 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.1259 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.1255 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.1252 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.1248 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.1246 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.1246 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.1246 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.1245 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.1244 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.1248 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.1250 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.1247 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.1251 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.1253 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.1254 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.1255 [Training] 85/120 [====================>.........] - ETA: 9s  batch_loss: 0.1261 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.1264 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.1261 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.1263 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.1265 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.1265 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.1266 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.1264 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.1260 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.1259 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.1260 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.1255 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.1254 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.1255 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.1257 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.1259 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.1258 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.1259 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.1257 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.1257 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.1260 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.1260 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.1259 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.1263 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.1262 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.1259 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.1258 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.1260 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.1262 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.1265 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.1262 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.1259 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.1258 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.1258 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.1257 [Training] 120/120 [==============================] 256.4ms/step  batch_loss: 0.1266 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:39:46 - INFO - root -   The F1-score is 0.6545973655181364
10/17/2022 14:39:46 - INFO - root -   the best eval f1 is 0.6546, saving model !!
10/17/2022 14:39:49 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:28  batch_loss: 0.1238 [Training] 2/120 [..............................] - ETA: 58s  batch_loss: 0.1225 [Training] 3/120 [..............................] - ETA: 48s  batch_loss: 0.1137 [Training] 4/120 [>.............................] - ETA: 43s  batch_loss: 0.1120 [Training] 5/120 [>.............................] - ETA: 40s  batch_loss: 0.1101 [Training] 6/120 [>.............................] - ETA: 37s  batch_loss: 0.1140 [Training] 7/120 [>.............................] - ETA: 36s  batch_loss: 0.1155 [Training] 8/120 [=>............................] - ETA: 34s  batch_loss: 0.1151 [Training] 9/120 [=>............................] - ETA: 33s  batch_loss: 0.1155 [Training] 10/120 [=>............................] - ETA: 32s  batch_loss: 0.1158 [Training] 11/120 [=>............................] - ETA: 32s  batch_loss: 0.1163 [Training] 12/120 [==>...........................] - ETA: 31s  batch_loss: 0.1150 [Training] 13/120 [==>...........................] - ETA: 30s  batch_loss: 0.1159 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.1177 [Training] 15/120 [==>...........................] - ETA: 29s  batch_loss: 0.1196 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.1211 [Training] 17/120 [===>..........................] - ETA: 28s  batch_loss: 0.1209 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.1196 [Training] 19/120 [===>..........................] - ETA: 27s  batch_loss: 0.1190 [Training] 20/120 [====>.........................] - ETA: 27s  batch_loss: 0.1197 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.1189 [Training] 22/120 [====>.........................] - ETA: 26s  batch_loss: 0.1188 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.1193 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.1199 [Training] 25/120 [=====>........................] - ETA: 25s  batch_loss: 0.1214 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.1212 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 0.1223 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.1214 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.1218 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 0.1217 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.1214 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.1212 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.1223 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.1227 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.1218 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.1218 [Training] 37/120 [========>.....................] - ETA: 21s  batch_loss: 0.1216 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.1222 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.1214 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.1218 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.1208 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.1211 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.1213 [Training] 44/120 [==========>...................] - ETA: 19s  batch_loss: 0.1207 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.1205 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.1207 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.1198 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.1195 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.1193 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.1201 [Training] 51/120 [===========>..................] - ETA: 17s  batch_loss: 0.1201 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.1198 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.1200 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.1206 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.1210 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.1205 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.1211 [Training] 58/120 [=============>................] - ETA: 16s  batch_loss: 0.1209 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.1214 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.1219 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.1216 [Training] 62/120 [==============>...............] - ETA: 15s  batch_loss: 0.1214 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.1217 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.1210 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.1211 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 0.1205 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.1213 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.1212 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.1212 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.1215 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.1218 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.1215 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.1215 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.1215 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.1211 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.1211 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.1213 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.1209 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.1213 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.1214 [Training] 81/120 [===================>..........] - ETA: 10s  batch_loss: 0.1213 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.1216 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.1219 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.1222 [Training] 85/120 [====================>.........] - ETA: 8s  batch_loss: 0.1218 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.1222 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.1230 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.1230 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.1230 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.1232 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.1230 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.1241 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.1244 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.1247 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.1249 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.1245 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.1249 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.1251 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.1249 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.1253 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.1251 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.1252 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.1250 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.1253 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.1253 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.1252 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.1252 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.1250 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.1247 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.1246 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.1247 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.1246 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.1243 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.1243 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.1241 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.1241 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.1243 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.1244 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.1244 [Training] 120/120 [==============================] 255.0ms/step  batch_loss: 0.1239 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:40:26 - INFO - root -   The F1-score is 0.6592554291623578
10/17/2022 14:40:26 - INFO - root -   the best eval f1 is 0.6593, saving model !!
10/17/2022 14:40:29 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/120 [..............................] - ETA: 1:40  batch_loss: 0.1131 [Training] 2/120 [..............................] - ETA: 1:04  batch_loss: 0.1078 [Training] 3/120 [..............................] - ETA: 52s  batch_loss: 0.1010 [Training] 4/120 [>.............................] - ETA: 46s  batch_loss: 0.1074 [Training] 5/120 [>.............................] - ETA: 42s  batch_loss: 0.1096 [Training] 6/120 [>.............................] - ETA: 39s  batch_loss: 0.1098 [Training] 7/120 [>.............................] - ETA: 37s  batch_loss: 0.1063 [Training] 8/120 [=>............................] - ETA: 36s  batch_loss: 0.1086 [Training] 9/120 [=>............................] - ETA: 35s  batch_loss: 0.1089 [Training] 10/120 [=>............................] - ETA: 34s  batch_loss: 0.1070 [Training] 11/120 [=>............................] - ETA: 33s  batch_loss: 0.1079 [Training] 12/120 [==>...........................] - ETA: 32s  batch_loss: 0.1067 [Training] 13/120 [==>...........................] - ETA: 31s  batch_loss: 0.1089 [Training] 14/120 [==>...........................] - ETA: 30s  batch_loss: 0.1104 [Training] 15/120 [==>...........................] - ETA: 30s  batch_loss: 0.1141 [Training] 16/120 [===>..........................] - ETA: 29s  batch_loss: 0.1137 [Training] 17/120 [===>..........................] - ETA: 29s  batch_loss: 0.1107 [Training] 18/120 [===>..........................] - ETA: 28s  batch_loss: 0.1117 [Training] 19/120 [===>..........................] - ETA: 28s  batch_loss: 0.1120 [Training] 20/120 [====>.........................] - ETA: 28s  batch_loss: 0.1131 [Training] 21/120 [====>.........................] - ETA: 27s  batch_loss: 0.1112 [Training] 22/120 [====>.........................] - ETA: 27s  batch_loss: 0.1104 [Training] 23/120 [====>.........................] - ETA: 26s  batch_loss: 0.1098 [Training] 24/120 [=====>........................] - ETA: 26s  batch_loss: 0.1098 [Training] 25/120 [=====>........................] - ETA: 26s  batch_loss: 0.1097 [Training] 26/120 [=====>........................] - ETA: 25s  batch_loss: 0.1096 [Training] 27/120 [=====>........................] - ETA: 25s  batch_loss: 0.1096 [Training] 28/120 [======>.......................] - ETA: 24s  batch_loss: 0.1100 [Training] 29/120 [======>.......................] - ETA: 24s  batch_loss: 0.1100 [Training] 30/120 [======>.......................] - ETA: 24s  batch_loss: 0.1093 [Training] 31/120 [======>.......................] - ETA: 23s  batch_loss: 0.1091 [Training] 32/120 [=======>......................] - ETA: 23s  batch_loss: 0.1094 [Training] 33/120 [=======>......................] - ETA: 23s  batch_loss: 0.1092 [Training] 34/120 [=======>......................] - ETA: 22s  batch_loss: 0.1086 [Training] 35/120 [=======>......................] - ETA: 22s  batch_loss: 0.1092 [Training] 36/120 [========>.....................] - ETA: 22s  batch_loss: 0.1100 [Training] 37/120 [========>.....................] - ETA: 22s  batch_loss: 0.1095 [Training] 38/120 [========>.....................] - ETA: 21s  batch_loss: 0.1108 [Training] 39/120 [========>.....................] - ETA: 21s  batch_loss: 0.1121 [Training] 40/120 [=========>....................] - ETA: 21s  batch_loss: 0.1129 [Training] 41/120 [=========>....................] - ETA: 20s  batch_loss: 0.1137 [Training] 42/120 [=========>....................] - ETA: 20s  batch_loss: 0.1132 [Training] 43/120 [=========>....................] - ETA: 20s  batch_loss: 0.1133 [Training] 44/120 [==========>...................] - ETA: 19s  batch_loss: 0.1133 [Training] 45/120 [==========>...................] - ETA: 19s  batch_loss: 0.1127 [Training] 46/120 [==========>...................] - ETA: 19s  batch_loss: 0.1130 [Training] 47/120 [==========>...................] - ETA: 19s  batch_loss: 0.1127 [Training] 48/120 [===========>..................] - ETA: 18s  batch_loss: 0.1129 [Training] 49/120 [===========>..................] - ETA: 18s  batch_loss: 0.1139 [Training] 50/120 [===========>..................] - ETA: 18s  batch_loss: 0.1134 [Training] 51/120 [===========>..................] - ETA: 17s  batch_loss: 0.1141 [Training] 52/120 [============>.................] - ETA: 17s  batch_loss: 0.1148 [Training] 53/120 [============>.................] - ETA: 17s  batch_loss: 0.1144 [Training] 54/120 [============>.................] - ETA: 17s  batch_loss: 0.1143 [Training] 55/120 [============>.................] - ETA: 16s  batch_loss: 0.1144 [Training] 56/120 [=============>................] - ETA: 16s  batch_loss: 0.1145 [Training] 57/120 [=============>................] - ETA: 16s  batch_loss: 0.1150 [Training] 58/120 [=============>................] - ETA: 15s  batch_loss: 0.1147 [Training] 59/120 [=============>................] - ETA: 15s  batch_loss: 0.1145 [Training] 60/120 [==============>...............] - ETA: 15s  batch_loss: 0.1149 [Training] 61/120 [==============>...............] - ETA: 15s  batch_loss: 0.1144 [Training] 62/120 [==============>...............] - ETA: 14s  batch_loss: 0.1147 [Training] 63/120 [==============>...............] - ETA: 14s  batch_loss: 0.1150 [Training] 64/120 [===============>..............] - ETA: 14s  batch_loss: 0.1146 [Training] 65/120 [===============>..............] - ETA: 14s  batch_loss: 0.1146 [Training] 66/120 [===============>..............] - ETA: 13s  batch_loss: 0.1148 [Training] 67/120 [===============>..............] - ETA: 13s  batch_loss: 0.1152 [Training] 68/120 [================>.............] - ETA: 13s  batch_loss: 0.1152 [Training] 69/120 [================>.............] - ETA: 13s  batch_loss: 0.1154 [Training] 70/120 [================>.............] - ETA: 12s  batch_loss: 0.1158 [Training] 71/120 [================>.............] - ETA: 12s  batch_loss: 0.1156 [Training] 72/120 [=================>............] - ETA: 12s  batch_loss: 0.1159 [Training] 73/120 [=================>............] - ETA: 12s  batch_loss: 0.1152 [Training] 74/120 [=================>............] - ETA: 11s  batch_loss: 0.1158 [Training] 75/120 [=================>............] - ETA: 11s  batch_loss: 0.1153 [Training] 76/120 [==================>...........] - ETA: 11s  batch_loss: 0.1154 [Training] 77/120 [==================>...........] - ETA: 11s  batch_loss: 0.1154 [Training] 78/120 [==================>...........] - ETA: 10s  batch_loss: 0.1155 [Training] 79/120 [==================>...........] - ETA: 10s  batch_loss: 0.1159 [Training] 80/120 [===================>..........] - ETA: 10s  batch_loss: 0.1157 [Training] 81/120 [===================>..........] - ETA: 9s  batch_loss: 0.1155 [Training] 82/120 [===================>..........] - ETA: 9s  batch_loss: 0.1155 [Training] 83/120 [===================>..........] - ETA: 9s  batch_loss: 0.1153 [Training] 84/120 [====================>.........] - ETA: 9s  batch_loss: 0.1156 [Training] 85/120 [====================>.........] - ETA: 8s  batch_loss: 0.1158 [Training] 86/120 [====================>.........] - ETA: 8s  batch_loss: 0.1160 [Training] 87/120 [====================>.........] - ETA: 8s  batch_loss: 0.1166 [Training] 88/120 [=====================>........] - ETA: 8s  batch_loss: 0.1167 [Training] 89/120 [=====================>........] - ETA: 7s  batch_loss: 0.1171 [Training] 90/120 [=====================>........] - ETA: 7s  batch_loss: 0.1171 [Training] 91/120 [=====================>........] - ETA: 7s  batch_loss: 0.1166 [Training] 92/120 [======================>.......] - ETA: 7s  batch_loss: 0.1167 [Training] 93/120 [======================>.......] - ETA: 6s  batch_loss: 0.1171 [Training] 94/120 [======================>.......] - ETA: 6s  batch_loss: 0.1166 [Training] 95/120 [======================>.......] - ETA: 6s  batch_loss: 0.1167 [Training] 96/120 [=======================>......] - ETA: 6s  batch_loss: 0.1168 [Training] 97/120 [=======================>......] - ETA: 5s  batch_loss: 0.1166 [Training] 98/120 [=======================>......] - ETA: 5s  batch_loss: 0.1177 [Training] 99/120 [=======================>......] - ETA: 5s  batch_loss: 0.1177 [Training] 100/120 [========================>.....] - ETA: 5s  batch_loss: 0.1176 [Training] 101/120 [========================>.....] - ETA: 4s  batch_loss: 0.1175 [Training] 102/120 [========================>.....] - ETA: 4s  batch_loss: 0.1176 [Training] 103/120 [========================>.....] - ETA: 4s  batch_loss: 0.1181 [Training] 104/120 [=========================>....] - ETA: 4s  batch_loss: 0.1178 [Training] 105/120 [=========================>....] - ETA: 3s  batch_loss: 0.1177 [Training] 106/120 [=========================>....] - ETA: 3s  batch_loss: 0.1178 [Training] 107/120 [=========================>....] - ETA: 3s  batch_loss: 0.1181 [Training] 108/120 [==========================>...] - ETA: 3s  batch_loss: 0.1181 [Training] 109/120 [==========================>...] - ETA: 2s  batch_loss: 0.1181 [Training] 110/120 [==========================>...] - ETA: 2s  batch_loss: 0.1181 [Training] 111/120 [==========================>...] - ETA: 2s  batch_loss: 0.1182 [Training] 112/120 [===========================>..] - ETA: 2s  batch_loss: 0.1182 [Training] 113/120 [===========================>..] - ETA: 1s  batch_loss: 0.1184 [Training] 114/120 [===========================>..] - ETA: 1s  batch_loss: 0.1186 [Training] 115/120 [===========================>..] - ETA: 1s  batch_loss: 0.1184 [Training] 116/120 [============================>.] - ETA: 1s  batch_loss: 0.1183 [Training] 117/120 [============================>.] - ETA: 0s  batch_loss: 0.1189 [Training] 118/120 [============================>.] - ETA: 0s  batch_loss: 0.1189 [Training] 119/120 [============================>.] - ETA: 0s  batch_loss: 0.1189 [Training] 120/120 [==============================] 254.4ms/step  batch_loss: 0.1188 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/17/2022 14:41:06 - INFO - root -   The F1-score is 0.6552714161714629
