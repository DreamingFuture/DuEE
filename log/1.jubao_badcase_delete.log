nohup: ignoring input
*********** data_prepare *************
开始数据预处理
Traceback (most recent call last):
  File "/data/qingyang/event_extration/DuEE_merge/data/DuEE1.0/our_test.py", line 7, in <module>
    df = pd.read_excel('新闻_8k_带摘要.xlsx')
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/pandas/util/_decorators.py", line 311, in wrapper
    return func(*args, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/pandas/io/excel/_base.py", line 457, in read_excel
    io = ExcelFile(io, storage_options=storage_options, engine=engine)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/pandas/io/excel/_base.py", line 1376, in __init__
    ext = inspect_excel_format(
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/pandas/io/excel/_base.py", line 1250, in inspect_excel_format
    with get_handle(
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/pandas/io/common.py", line 795, in get_handle
    handle = open(handle, ioargs.mode)
FileNotFoundError: [Errno 2] No such file or directory: '新闻_8k_带摘要.xlsx'
开始处理tag.dict 和 tsv文件

=================DUEE 1.0 DATASET==============

=================start schema process==============
input path ./conf/DuEE1.0/event_schema.json
save trigger tag 393 at ./conf/DuEE1.0/role_tag.dict
=================end schema process===============

=================start sentence process==============

----trigger------for dir ./data/DuEE1.0 to ./data/DuEE1.0/trigger
train 6379 dev 1633 test 147541

----role------for dir ./data/DuEE1.0 to ./data/DuEE1.0/role
train 7502 dev 1925 test 147541
=================end schema process==============
********** train start *************
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 5, 
    "event_type": "role", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 1e-05, 
    "max_len": 150, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 30.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 48, 
    "per_gpu_train_batch_size": 64, 
    "seed": 66, 
    "stride": 100, 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/7501 [00:00<?, ?it/s]tokenizing...:   4%|▎         | 272/7501 [00:00<00:02, 2716.27it/s]tokenizing...:   9%|▊         | 653/7501 [00:00<00:02, 3358.39it/s]tokenizing...:  13%|█▎        | 994/7501 [00:00<00:01, 3381.04it/s]tokenizing...:  18%|█▊        | 1333/7501 [00:00<00:02, 2967.40it/s]tokenizing...:  22%|██▏       | 1637/7501 [00:00<00:02, 2910.12it/s]tokenizing...:  26%|██▌       | 1933/7501 [00:00<00:01, 2898.76it/s]tokenizing...:  30%|██▉       | 2226/7501 [00:00<00:01, 2763.09it/s]tokenizing...:  33%|███▎      | 2505/7501 [00:00<00:01, 2708.93it/s]tokenizing...:  37%|███▋      | 2778/7501 [00:00<00:01, 2689.71it/s]tokenizing...:  41%|████      | 3060/7501 [00:01<00:01, 2727.63it/s]tokenizing...:  44%|████▍     | 3334/7501 [00:01<00:01, 2689.66it/s]tokenizing...:  48%|████▊     | 3612/7501 [00:01<00:01, 2715.13it/s]tokenizing...:  52%|█████▏    | 3885/7501 [00:01<00:01, 2648.72it/s]tokenizing...:  55%|█████▌    | 4160/7501 [00:01<00:01, 2676.56it/s]tokenizing...:  59%|█████▉    | 4429/7501 [00:01<00:01, 2571.07it/s]tokenizing...:  63%|██████▎   | 4708/7501 [00:01<00:01, 2631.75it/s]tokenizing...:  66%|██████▋   | 4973/7501 [00:01<00:00, 2615.68it/s]tokenizing...:  70%|███████   | 5271/7501 [00:01<00:00, 2719.48it/s]tokenizing...:  74%|███████▍  | 5544/7501 [00:02<00:01, 1935.67it/s]tokenizing...:  77%|███████▋  | 5779/7501 [00:02<00:00, 2025.81it/s]tokenizing...:  80%|████████  | 6007/7501 [00:02<00:00, 1994.69it/s]tokenizing...:  83%|████████▎ | 6241/7501 [00:02<00:00, 2081.61it/s]tokenizing...:  87%|████████▋ | 6500/7501 [00:02<00:00, 2215.33it/s]tokenizing...:  90%|█████████ | 6776/7501 [00:02<00:00, 2363.25it/s]tokenizing...:  94%|█████████▎| 7022/7501 [00:02<00:00, 2271.51it/s]tokenizing...:  97%|█████████▋| 7268/7501 [00:02<00:00, 2323.06it/s]tokenizing...: 100%|██████████| 7501/7501 [00:02<00:00, 2529.52it/s]
tokenizing...:   0%|          | 0/1924 [00:00<?, ?it/s]tokenizing...:  16%|█▋        | 313/1924 [00:00<00:00, 2991.10it/s]tokenizing...:  32%|███▏      | 613/1924 [00:00<00:00, 2843.68it/s]tokenizing...:  47%|████▋     | 898/1924 [00:00<00:00, 2711.78it/s]tokenizing...:  61%|██████    | 1170/1924 [00:00<00:00, 2661.32it/s]tokenizing...:  75%|███████▍  | 1437/1924 [00:00<00:00, 2582.86it/s]tokenizing...:  88%|████████▊ | 1696/1924 [00:00<00:00, 2457.68it/s]tokenizing...: 100%|██████████| 1924/1924 [00:00<00:00, 2573.73it/s]
09/25/2022 10:35:44 - INFO - root -   The nums of the train_dataset features is 7501
09/25/2022 10:35:44 - INFO - root -   The nums of the eval_dataset features is 1924
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
09/25/2022 10:35:48 - INFO - root -   ***** Running train *****
********************************************************************************
/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 2:14  batch_loss: 6.3595 [Training] 2/118 [..............................] - ETA: 1:19  batch_loss: 6.3496 [Training] 3/118 [..............................] - ETA: 1:01  batch_loss: 6.3427 [Training] 4/118 [>.............................] - ETA: 53s  batch_loss: 6.3387 [Training] 5/118 [>.............................] - ETA: 47s  batch_loss: 6.3086 [Training] 6/118 [>.............................] - ETA: 43s  batch_loss: 6.2622 [Training] 7/118 [>.............................] - ETA: 41s  batch_loss: 6.2080 [Training] 8/118 [=>............................] - ETA: 39s  batch_loss: 6.1466 [Training] 9/118 [=>............................] - ETA: 37s  batch_loss: 6.0844 [Training] 10/118 [=>............................] - ETA: 36s  batch_loss: 6.0195 [Training] 11/118 [=>............................] - ETA: 34s  batch_loss: 5.9613 [Training] 12/118 [==>...........................] - ETA: 33s  batch_loss: 5.8884 [Training] 13/118 [==>...........................] - ETA: 32s  batch_loss: 5.8213 [Training] 14/118 [==>...........................] - ETA: 32s  batch_loss: 5.7505 [Training] 15/118 [==>...........................] - ETA: 31s  batch_loss: 5.6768 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 5.5993 [Training] 17/118 [===>..........................] - ETA: 30s  batch_loss: 5.5196 [Training] 18/118 [===>..........................] - ETA: 29s  batch_loss: 5.4566 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 5.3849 [Training] 20/118 [====>.........................] - ETA: 28s  batch_loss: 5.2905 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 5.2091 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 5.1184 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 5.0338 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 4.9491 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 4.8649 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 4.7728 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 4.6899 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 4.5983 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 4.5237 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 4.4407 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 4.3816 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 4.3260 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 4.2589 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 4.2070 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 4.1559 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 4.1051 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 4.0593 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 4.0110 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 3.9649 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 3.9187 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 3.8823 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 3.8438 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 3.8106 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 3.7737 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 3.7418 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 3.7176 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 3.6872 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 3.6571 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 3.6306 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 3.6028 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 3.5753 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 3.5457 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 3.5202 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 3.4932 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 3.4758 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 3.4592 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 3.4383 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 3.4176 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 3.3960 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 3.3822 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 3.3641 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 3.3400 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 3.3225 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 3.3063 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 3.2924 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 3.2775 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 3.2603 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 3.2468 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 3.2354 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 3.2188 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 3.2067 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 3.1909 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 3.1794 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 3.1657 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 3.1541 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 3.1423 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 3.1291 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 3.1155 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 3.1030 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 3.0887 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 3.0738 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 3.0637 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 3.0521 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 3.0406 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 3.0285 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 3.0174 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 3.0040 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 2.9989 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 2.9844 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 2.9722 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 2.9654 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 2.9566 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 2.9451 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 2.9363 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 2.9270 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 2.9183 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 2.9091 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 2.9009 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 2.8925 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 2.8824 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 2.8734 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 2.8650 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 2.8567 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 2.8509 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 2.8431 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 2.8346 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 2.8281 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 2.8198 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 2.8137 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 2.8058 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 2.7989 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 2.7928 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 2.7870 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 2.7797 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 2.7702 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 2.7627 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 2.7546 [Training] 118/118 [==============================] 254.1ms/step  batch_loss: 2.7498 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:36:25 - INFO - root -   The F1-score is 0.03605408112168252
09/25/2022 10:36:25 - INFO - root -   the best eval f1 is 0.0361, saving model !!
09/25/2022 10:36:26 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:36  batch_loss: 1.7934 [Training] 2/118 [..............................] - ETA: 1:02  batch_loss: 1.9376 [Training] 3/118 [..............................] - ETA: 51s  batch_loss: 1.9127 [Training] 4/118 [>.............................] - ETA: 45s  batch_loss: 1.8895 [Training] 5/118 [>.............................] - ETA: 41s  batch_loss: 1.9074 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 1.9107 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 1.8735 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 1.8610 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 1.8464 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 1.8572 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 1.8459 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 1.8789 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 1.8732 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 1.8747 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 1.8517 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 1.8534 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 1.8530 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 1.8464 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 1.8360 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 1.8410 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 1.8537 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 1.8471 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 1.8615 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 1.8582 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 1.8588 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 1.8487 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 1.8516 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 1.8613 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 1.8527 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 1.8554 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 1.8454 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 1.8408 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 1.8339 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 1.8369 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 1.8307 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 1.8232 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 1.8207 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 1.8215 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 1.8253 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 1.8262 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 1.8243 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 1.8242 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 1.8183 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 1.8181 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 1.8211 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 1.8198 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 1.8226 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 1.8210 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 1.8192 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 1.8163 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 1.8147 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 1.8150 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 1.8133 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 1.8052 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 1.8014 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 1.8030 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 1.8019 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 1.7968 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 1.7916 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 1.7881 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 1.7862 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 1.7855 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 1.7859 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 1.7869 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 1.7881 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 1.7811 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 1.7766 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 1.7727 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 1.7695 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 1.7689 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 1.7668 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 1.7645 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 1.7615 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 1.7615 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 1.7608 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 1.7594 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 1.7560 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 1.7577 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 1.7548 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 1.7478 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 1.7459 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 1.7450 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 1.7404 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 1.7377 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 1.7364 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 1.7355 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 1.7340 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 1.7337 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 1.7322 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 1.7302 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 1.7277 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 1.7248 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 1.7245 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 1.7205 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 1.7184 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 1.7154 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 1.7145 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 1.7114 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 1.7090 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 1.7052 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 1.7016 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 1.7000 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 1.6979 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 1.6956 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 1.6934 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 1.6928 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 1.6904 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 1.6883 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 1.6853 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 1.6822 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 1.6805 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 1.6772 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 1.6748 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 1.6732 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 1.6710 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 1.6697 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 1.6711 [Training] 118/118 [==============================] 254.9ms/step  batch_loss: 1.6678 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:37:02 - INFO - root -   The F1-score is 0.13478303457733373
09/25/2022 10:37:02 - INFO - root -   the best eval f1 is 0.1348, saving model !!
09/25/2022 10:37:05 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:40  batch_loss: 1.3971 [Training] 2/118 [..............................] - ETA: 1:04  batch_loss: 1.4175 [Training] 3/118 [..............................] - ETA: 52s  batch_loss: 1.4002 [Training] 4/118 [>.............................] - ETA: 46s  batch_loss: 1.4051 [Training] 5/118 [>.............................] - ETA: 42s  batch_loss: 1.4442 [Training] 6/118 [>.............................] - ETA: 39s  batch_loss: 1.4370 [Training] 7/118 [>.............................] - ETA: 37s  batch_loss: 1.4066 [Training] 8/118 [=>............................] - ETA: 36s  batch_loss: 1.4039 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 1.3913 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 1.4000 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 1.3963 [Training] 12/118 [==>...........................] - ETA: 32s  batch_loss: 1.3858 [Training] 13/118 [==>...........................] - ETA: 31s  batch_loss: 1.3830 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 1.3751 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 1.3752 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 1.3837 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 1.3869 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 1.3788 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 1.3824 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 1.3916 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 1.3931 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 1.3864 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 1.3836 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 1.3745 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 1.3734 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 1.3730 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 1.3683 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 1.3774 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 1.3789 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 1.3795 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 1.3842 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 1.3847 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 1.3824 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 1.3805 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 1.3837 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 1.3816 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 1.3767 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 1.3750 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 1.3744 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 1.3755 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 1.3729 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 1.3672 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 1.3677 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 1.3677 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 1.3637 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 1.3624 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 1.3568 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 1.3562 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 1.3572 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 1.3566 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 1.3553 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 1.3550 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 1.3532 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 1.3520 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 1.3499 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 1.3542 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 1.3520 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 1.3493 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 1.3475 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 1.3438 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 1.3423 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 1.3408 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 1.3402 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 1.3421 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 1.3388 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 1.3374 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 1.3359 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 1.3341 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 1.3355 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 1.3344 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 1.3338 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 1.3310 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 1.3286 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 1.3274 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 1.3257 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 1.3259 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 1.3239 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 1.3244 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 1.3224 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 1.3233 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 1.3231 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 1.3209 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 1.3209 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 1.3196 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 1.3194 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 1.3194 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 1.3189 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 1.3192 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 1.3197 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 1.3189 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 1.3168 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 1.3168 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 1.3168 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 1.3164 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 1.3157 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 1.3140 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 1.3125 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 1.3115 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 1.3099 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 1.3070 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 1.3040 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 1.3035 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 1.3035 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 1.3031 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 1.3011 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 1.3001 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 1.2993 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 1.2998 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 1.2994 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 1.2973 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 1.2963 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 1.2958 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 1.2954 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 1.2940 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 1.2916 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 1.2907 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 1.2904 [Training] 118/118 [==============================] 255.3ms/step  batch_loss: 1.2886 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:37:42 - INFO - root -   The F1-score is 0.21512032809719103
09/25/2022 10:37:42 - INFO - root -   the best eval f1 is 0.2151, saving model !!
09/25/2022 10:37:44 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:33  batch_loss: 1.2597 [Training] 2/118 [..............................] - ETA: 1:02  batch_loss: 1.2566 [Training] 3/118 [..............................] - ETA: 50s  batch_loss: 1.2213 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 1.2093 [Training] 5/118 [>.............................] - ETA: 41s  batch_loss: 1.1822 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 1.1451 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 1.1299 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 1.1261 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 1.1374 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 1.1547 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 1.1451 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 1.1464 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 1.1424 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 1.1459 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 1.1396 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 1.1315 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 1.1400 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 1.1416 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 1.1312 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 1.1332 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 1.1258 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 1.1274 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 1.1273 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 1.1252 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 1.1259 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 1.1293 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 1.1244 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 1.1234 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 1.1209 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 1.1214 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 1.1157 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 1.1136 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 1.1116 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 1.1131 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 1.1114 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 1.1112 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 1.1140 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 1.1151 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 1.1128 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 1.1122 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 1.1091 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 1.1077 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 1.1083 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 1.1077 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 1.1069 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 1.1082 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 1.1081 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 1.1106 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 1.1080 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 1.1076 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 1.1084 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 1.1110 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 1.1115 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 1.1085 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 1.1069 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 1.1076 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 1.1076 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 1.1040 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 1.1041 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 1.1048 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 1.1027 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 1.1035 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 1.1030 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 1.1031 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 1.1025 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 1.1018 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 1.1019 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 1.1011 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 1.0992 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 1.0986 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 1.0971 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 1.0956 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 1.0947 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 1.0957 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 1.0970 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 1.0960 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 1.0959 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 1.0969 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 1.0954 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 1.0943 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 1.0911 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 1.0892 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 1.0878 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 1.0879 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 1.0860 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 1.0846 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 1.0855 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 1.0846 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 1.0838 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 1.0834 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 1.0813 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 1.0804 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 1.0781 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 1.0754 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 1.0742 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 1.0724 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 1.0715 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 1.0707 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 1.0698 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 1.0695 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 1.0689 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 1.0689 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 1.0678 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 1.0669 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 1.0653 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 1.0654 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 1.0637 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 1.0629 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 1.0609 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 1.0606 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 1.0594 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 1.0583 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 1.0564 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 1.0544 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 1.0532 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 1.0519 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 1.0509 [Training] 118/118 [==============================] 254.5ms/step  batch_loss: 1.0504 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:38:21 - INFO - root -   The F1-score is 0.3354762432572231
09/25/2022 10:38:21 - INFO - root -   the best eval f1 is 0.3355, saving model !!
09/25/2022 10:38:23 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:42  batch_loss: 1.0136 [Training] 2/118 [..............................] - ETA: 1:06  batch_loss: 0.9570 [Training] 3/118 [..............................] - ETA: 53s  batch_loss: 0.9463 [Training] 4/118 [>.............................] - ETA: 46s  batch_loss: 0.9658 [Training] 5/118 [>.............................] - ETA: 42s  batch_loss: 0.9763 [Training] 6/118 [>.............................] - ETA: 40s  batch_loss: 0.9307 [Training] 7/118 [>.............................] - ETA: 37s  batch_loss: 0.9496 [Training] 8/118 [=>............................] - ETA: 36s  batch_loss: 0.9389 [Training] 9/118 [=>............................] - ETA: 35s  batch_loss: 0.9345 [Training] 10/118 [=>............................] - ETA: 34s  batch_loss: 0.9376 [Training] 11/118 [=>............................] - ETA: 33s  batch_loss: 0.9424 [Training] 12/118 [==>...........................] - ETA: 32s  batch_loss: 0.9443 [Training] 13/118 [==>...........................] - ETA: 31s  batch_loss: 0.9422 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.9374 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 0.9364 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.9410 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.9384 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.9350 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.9271 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.9193 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.9261 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.9311 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.9314 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.9303 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.9300 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.9322 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.9352 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.9300 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.9311 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.9283 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.9231 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.9251 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.9248 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.9236 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.9232 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.9183 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.9219 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.9190 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.9189 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.9216 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.9242 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.9197 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.9177 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.9153 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.9177 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.9184 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.9183 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.9208 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.9183 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.9150 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.9132 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.9120 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.9122 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.9094 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.9069 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.9052 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.9047 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.9037 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.9023 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.9029 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.9012 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.9021 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.9035 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.9027 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.9019 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.9028 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.8997 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.8984 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.8974 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.8965 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.8954 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.8949 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.8945 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.8936 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.8931 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.8916 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.8901 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.8907 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.8898 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.8889 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.8897 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.8890 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.8872 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.8861 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.8865 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.8871 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.8877 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.8862 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.8863 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.8863 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.8877 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.8877 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.8877 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.8857 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.8859 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.8845 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.8833 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.8824 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.8806 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.8800 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.8792 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.8797 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.8793 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.8781 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.8776 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.8769 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.8771 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.8766 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.8767 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.8760 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.8763 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.8784 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.8775 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.8775 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.8770 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.8772 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.8767 [Training] 118/118 [==============================] 256.2ms/step  batch_loss: 0.8766 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:39:00 - INFO - root -   The F1-score is 0.4321738536185334
09/25/2022 10:39:00 - INFO - root -   the best eval f1 is 0.4322, saving model !!
09/25/2022 10:39:03 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:36  batch_loss: 0.9120 [Training] 2/118 [..............................] - ETA: 1:03  batch_loss: 0.8391 [Training] 3/118 [..............................] - ETA: 51s  batch_loss: 0.8618 [Training] 4/118 [>.............................] - ETA: 45s  batch_loss: 0.8452 [Training] 5/118 [>.............................] - ETA: 41s  batch_loss: 0.8192 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.8352 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.8353 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.8269 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.8220 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.8166 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.8108 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.8063 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.8113 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.8097 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.8081 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.8058 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.8082 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.8002 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.8000 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.8034 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.8052 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.8086 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.8098 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.8018 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.7963 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.7947 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.7920 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.7902 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.7881 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.7859 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.7849 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.7827 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.7811 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.7797 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.7774 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.7754 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.7700 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.7706 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.7707 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.7686 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.7698 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.7706 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.7713 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.7711 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.7733 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.7724 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.7736 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.7764 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.7757 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.7737 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.7696 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.7675 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.7667 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.7655 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.7645 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.7637 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.7608 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.7598 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.7590 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.7628 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.7627 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.7631 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.7629 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.7614 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.7607 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.7621 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.7622 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.7613 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.7613 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.7616 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.7612 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.7599 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.7596 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.7581 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.7571 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.7559 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.7559 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.7557 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.7568 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.7551 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.7553 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.7544 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.7532 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.7521 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.7509 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.7500 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.7499 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.7495 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.7488 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.7478 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.7469 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.7462 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.7467 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.7448 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.7449 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.7455 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.7441 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.7458 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.7453 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.7441 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.7451 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.7453 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.7442 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.7441 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.7450 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.7448 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.7444 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.7443 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.7446 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.7446 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.7448 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.7448 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.7445 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.7434 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.7431 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.7426 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.7434 [Training] 118/118 [==============================] 256.0ms/step  batch_loss: 0.7466 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:39:40 - INFO - root -   The F1-score is 0.4891156462585034
09/25/2022 10:39:40 - INFO - root -   the best eval f1 is 0.4891, saving model !!
09/25/2022 10:39:42 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:40  batch_loss: 0.6651 [Training] 2/118 [..............................] - ETA: 1:04  batch_loss: 0.6619 [Training] 3/118 [..............................] - ETA: 52s  batch_loss: 0.6277 [Training] 4/118 [>.............................] - ETA: 46s  batch_loss: 0.6340 [Training] 5/118 [>.............................] - ETA: 42s  batch_loss: 0.6451 [Training] 6/118 [>.............................] - ETA: 39s  batch_loss: 0.6543 [Training] 7/118 [>.............................] - ETA: 37s  batch_loss: 0.6673 [Training] 8/118 [=>............................] - ETA: 36s  batch_loss: 0.6828 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.6738 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.6738 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.6821 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.6798 [Training] 13/118 [==>...........................] - ETA: 31s  batch_loss: 0.6856 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.6797 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 0.6816 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.6850 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.6819 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.6850 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.6844 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.6827 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.6811 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.6783 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.6772 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.6754 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.6720 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.6732 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.6724 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.6720 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.6677 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.6682 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.6702 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.6694 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.6695 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.6718 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.6725 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.6732 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.6727 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.6729 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.6725 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.6698 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.6698 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.6669 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.6654 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.6658 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.6649 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.6634 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.6625 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.6636 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.6614 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.6585 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.6610 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.6623 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.6603 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.6592 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.6595 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.6571 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.6576 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.6564 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.6543 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.6553 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.6552 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.6517 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.6506 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.6514 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.6496 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.6495 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.6505 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.6518 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.6517 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.6510 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.6495 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.6487 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.6486 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.6486 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.6481 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.6476 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.6467 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.6449 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.6449 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.6445 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.6448 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.6442 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.6438 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.6452 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.6444 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.6455 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.6448 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.6443 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.6443 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.6443 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.6432 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.6424 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.6445 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.6446 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.6449 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.6449 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.6450 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.6438 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.6444 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.6448 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.6444 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.6447 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.6441 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.6433 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.6430 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.6434 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.6432 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.6423 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.6419 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.6409 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.6405 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.6407 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.6403 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.6399 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.6393 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.6391 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.6386 [Training] 118/118 [==============================] 256.3ms/step  batch_loss: 0.6440 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:40:19 - INFO - root -   The F1-score is 0.5094945172506018
09/25/2022 10:40:19 - INFO - root -   the best eval f1 is 0.5095, saving model !!
09/25/2022 10:40:21 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:56  batch_loss: 0.5909 [Training] 2/118 [..............................] - ETA: 1:13  batch_loss: 0.6605 [Training] 3/118 [..............................] - ETA: 58s  batch_loss: 0.6085 [Training] 4/118 [>.............................] - ETA: 51s  batch_loss: 0.5937 [Training] 5/118 [>.............................] - ETA: 46s  batch_loss: 0.5844 [Training] 6/118 [>.............................] - ETA: 43s  batch_loss: 0.6215 [Training] 7/118 [>.............................] - ETA: 41s  batch_loss: 0.6167 [Training] 8/118 [=>............................] - ETA: 39s  batch_loss: 0.6117 [Training] 9/118 [=>............................] - ETA: 37s  batch_loss: 0.6086 [Training] 10/118 [=>............................] - ETA: 36s  batch_loss: 0.6094 [Training] 11/118 [=>............................] - ETA: 35s  batch_loss: 0.6030 [Training] 12/118 [==>...........................] - ETA: 34s  batch_loss: 0.5960 [Training] 13/118 [==>...........................] - ETA: 33s  batch_loss: 0.5990 [Training] 14/118 [==>...........................] - ETA: 32s  batch_loss: 0.5995 [Training] 15/118 [==>...........................] - ETA: 31s  batch_loss: 0.5991 [Training] 16/118 [===>..........................] - ETA: 31s  batch_loss: 0.6005 [Training] 17/118 [===>..........................] - ETA: 30s  batch_loss: 0.5952 [Training] 18/118 [===>..........................] - ETA: 30s  batch_loss: 0.5894 [Training] 19/118 [===>..........................] - ETA: 29s  batch_loss: 0.5890 [Training] 20/118 [====>.........................] - ETA: 29s  batch_loss: 0.5894 [Training] 21/118 [====>.........................] - ETA: 28s  batch_loss: 0.5872 [Training] 22/118 [====>.........................] - ETA: 28s  batch_loss: 0.5844 [Training] 23/118 [====>.........................] - ETA: 27s  batch_loss: 0.5842 [Training] 24/118 [=====>........................] - ETA: 27s  batch_loss: 0.5810 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.5785 [Training] 26/118 [=====>........................] - ETA: 26s  batch_loss: 0.5762 [Training] 27/118 [=====>........................] - ETA: 26s  batch_loss: 0.5739 [Training] 28/118 [======>.......................] - ETA: 25s  batch_loss: 0.5778 [Training] 29/118 [======>.......................] - ETA: 25s  batch_loss: 0.5772 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.5767 [Training] 31/118 [======>.......................] - ETA: 24s  batch_loss: 0.5763 [Training] 32/118 [=======>......................] - ETA: 24s  batch_loss: 0.5805 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.5777 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 0.5774 [Training] 35/118 [=======>......................] - ETA: 23s  batch_loss: 0.5763 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.5765 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.5762 [Training] 38/118 [========>.....................] - ETA: 22s  batch_loss: 0.5778 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.5784 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.5800 [Training] 41/118 [=========>....................] - ETA: 21s  batch_loss: 0.5797 [Training] 42/118 [=========>....................] - ETA: 21s  batch_loss: 0.5787 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.5790 [Training] 44/118 [==========>...................] - ETA: 20s  batch_loss: 0.5789 [Training] 45/118 [==========>...................] - ETA: 20s  batch_loss: 0.5785 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.5794 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.5783 [Training] 48/118 [===========>..................] - ETA: 19s  batch_loss: 0.5768 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.5763 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.5767 [Training] 51/118 [===========>..................] - ETA: 18s  batch_loss: 0.5756 [Training] 52/118 [============>.................] - ETA: 18s  batch_loss: 0.5765 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.5757 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.5764 [Training] 55/118 [============>.................] - ETA: 17s  batch_loss: 0.5751 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.5740 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.5727 [Training] 58/118 [=============>................] - ETA: 16s  batch_loss: 0.5724 [Training] 59/118 [==============>...............] - ETA: 16s  batch_loss: 0.5719 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.5716 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.5708 [Training] 62/118 [==============>...............] - ETA: 15s  batch_loss: 0.5709 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.5720 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.5729 [Training] 65/118 [===============>..............] - ETA: 14s  batch_loss: 0.5723 [Training] 66/118 [===============>..............] - ETA: 14s  batch_loss: 0.5705 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.5696 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.5701 [Training] 69/118 [================>.............] - ETA: 13s  batch_loss: 0.5695 [Training] 70/118 [================>.............] - ETA: 13s  batch_loss: 0.5686 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.5685 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.5673 [Training] 73/118 [=================>............] - ETA: 12s  batch_loss: 0.5667 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.5662 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.5658 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.5656 [Training] 77/118 [==================>...........] - ETA: 11s  batch_loss: 0.5650 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.5644 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.5638 [Training] 80/118 [===================>..........] - ETA: 10s  batch_loss: 0.5631 [Training] 81/118 [===================>..........] - ETA: 10s  batch_loss: 0.5633 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.5626 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.5616 [Training] 84/118 [====================>.........] - ETA: 9s  batch_loss: 0.5616 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.5614 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.5604 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.5601 [Training] 88/118 [=====================>........] - ETA: 8s  batch_loss: 0.5591 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.5589 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.5590 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.5595 [Training] 92/118 [======================>.......] - ETA: 7s  batch_loss: 0.5592 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.5584 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.5585 [Training] 95/118 [=======================>......] - ETA: 6s  batch_loss: 0.5584 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.5582 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.5578 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.5574 [Training] 99/118 [========================>.....] - ETA: 5s  batch_loss: 0.5575 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.5569 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.5565 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.5560 [Training] 103/118 [=========================>....] - ETA: 4s  batch_loss: 0.5566 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.5572 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.5568 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.5556 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.5558 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.5562 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.5561 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.5561 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.5576 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.5570 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.5573 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.5564 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.5560 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.5561 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.5556 [Training] 118/118 [==============================] 266.4ms/step  batch_loss: 0.5561 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:41:00 - INFO - root -   The F1-score is 0.5393185645809043
09/25/2022 10:41:00 - INFO - root -   the best eval f1 is 0.5393, saving model !!
09/25/2022 10:41:02 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:38  batch_loss: 0.5551 [Training] 2/118 [..............................] - ETA: 1:03  batch_loss: 0.5459 [Training] 3/118 [..............................] - ETA: 51s  batch_loss: 0.5383 [Training] 4/118 [>.............................] - ETA: 45s  batch_loss: 0.5129 [Training] 5/118 [>.............................] - ETA: 41s  batch_loss: 0.5164 [Training] 6/118 [>.............................] - ETA: 39s  batch_loss: 0.5246 [Training] 7/118 [>.............................] - ETA: 37s  batch_loss: 0.5308 [Training] 8/118 [=>............................] - ETA: 36s  batch_loss: 0.5205 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.5233 [Training] 10/118 [=>............................] - ETA: 34s  batch_loss: 0.5156 [Training] 11/118 [=>............................] - ETA: 33s  batch_loss: 0.5243 [Training] 12/118 [==>...........................] - ETA: 32s  batch_loss: 0.5358 [Training] 13/118 [==>...........................] - ETA: 31s  batch_loss: 0.5271 [Training] 14/118 [==>...........................] - ETA: 31s  batch_loss: 0.5213 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 0.5218 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 0.5191 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.5203 [Training] 18/118 [===>..........................] - ETA: 29s  batch_loss: 0.5237 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.5196 [Training] 20/118 [====>.........................] - ETA: 28s  batch_loss: 0.5213 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.5168 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 0.5215 [Training] 23/118 [====>.........................] - ETA: 27s  batch_loss: 0.5197 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.5206 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.5172 [Training] 26/118 [=====>........................] - ETA: 26s  batch_loss: 0.5170 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.5145 [Training] 28/118 [======>.......................] - ETA: 25s  batch_loss: 0.5147 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.5140 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.5153 [Training] 31/118 [======>.......................] - ETA: 24s  batch_loss: 0.5142 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.5123 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.5122 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 0.5130 [Training] 35/118 [=======>......................] - ETA: 23s  batch_loss: 0.5124 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.5124 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.5133 [Training] 38/118 [========>.....................] - ETA: 22s  batch_loss: 0.5141 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.5120 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.5109 [Training] 41/118 [=========>....................] - ETA: 21s  batch_loss: 0.5107 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.5117 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.5104 [Training] 44/118 [==========>...................] - ETA: 20s  batch_loss: 0.5105 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.5094 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.5096 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.5072 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.5047 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.5041 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.5030 [Training] 51/118 [===========>..................] - ETA: 18s  batch_loss: 0.5022 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.5022 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.5033 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.5035 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.5050 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.5048 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.5042 [Training] 58/118 [=============>................] - ETA: 16s  batch_loss: 0.5032 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.5021 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.5020 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.4998 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.4991 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.4986 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.4983 [Training] 65/118 [===============>..............] - ETA: 14s  batch_loss: 0.4976 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.4971 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.4974 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.4967 [Training] 69/118 [================>.............] - ETA: 13s  batch_loss: 0.4971 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.4964 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.4975 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.4969 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.4966 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.4960 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.4966 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.4964 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.4960 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.4956 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.4952 [Training] 80/118 [===================>..........] - ETA: 10s  batch_loss: 0.4942 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.4944 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.4938 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.4933 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.4923 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.4921 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.4908 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.4906 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.4897 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.4899 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.4893 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.4892 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.4899 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.4889 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.4890 [Training] 95/118 [=======================>......] - ETA: 6s  batch_loss: 0.4888 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.4883 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.4886 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.4881 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.4871 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.4868 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.4867 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.4867 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.4869 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.4872 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.4877 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.4873 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.4880 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.4885 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.4878 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.4871 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.4861 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.4856 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.4858 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.4854 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.4858 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.4856 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.4860 [Training] 118/118 [==============================] 258.5ms/step  batch_loss: 0.4852 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:41:39 - INFO - root -   The F1-score is 0.572678594749902
09/25/2022 10:41:39 - INFO - root -   the best eval f1 is 0.5727, saving model !!
09/25/2022 10:41:42 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:37  batch_loss: 0.3489 [Training] 2/118 [..............................] - ETA: 1:02  batch_loss: 0.4006 [Training] 3/118 [..............................] - ETA: 50s  batch_loss: 0.3899 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 0.3861 [Training] 5/118 [>.............................] - ETA: 41s  batch_loss: 0.3926 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.4074 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.4075 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.4057 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.4016 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.4091 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.4128 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.4093 [Training] 13/118 [==>...........................] - ETA: 31s  batch_loss: 0.4103 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.4108 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.4147 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.4202 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.4217 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.4254 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.4275 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.4250 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.4243 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.4231 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.4221 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.4254 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.4280 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.4297 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.4298 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.4284 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.4323 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.4356 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.4388 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.4379 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.4380 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.4357 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.4363 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.4364 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.4382 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.4368 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.4368 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.4367 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.4346 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.4341 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.4355 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.4369 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.4350 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.4358 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.4359 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.4353 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.4368 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.4371 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.4366 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.4386 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.4376 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.4359 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.4363 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.4360 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.4356 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.4338 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.4351 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.4348 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.4351 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.4361 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.4361 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.4359 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.4360 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.4364 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.4381 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.4379 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.4369 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.4376 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.4373 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.4372 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.4371 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.4365 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.4366 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.4363 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.4364 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.4362 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.4358 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.4351 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.4363 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.4352 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.4353 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.4364 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.4368 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.4361 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.4357 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.4355 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.4353 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.4358 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.4354 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.4356 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.4350 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.4346 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.4345 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.4341 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.4336 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.4337 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.4332 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.4327 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.4326 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.4319 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.4323 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.4321 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.4321 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.4311 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.4310 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.4305 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.4304 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.4297 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.4294 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.4302 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.4304 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.4310 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.4317 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.4310 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.4305 [Training] 118/118 [==============================] 255.8ms/step  batch_loss: 0.4309 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:42:19 - INFO - root -   The F1-score is 0.5792698826597132
09/25/2022 10:42:19 - INFO - root -   the best eval f1 is 0.5793, saving model !!
09/25/2022 10:42:21 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 2:03  batch_loss: 0.4384 [Training] 2/118 [..............................] - ETA: 1:16  batch_loss: 0.4018 [Training] 3/118 [..............................] - ETA: 1:00  batch_loss: 0.4076 [Training] 4/118 [>.............................] - ETA: 52s  batch_loss: 0.3995 [Training] 5/118 [>.............................] - ETA: 46s  batch_loss: 0.4073 [Training] 6/118 [>.............................] - ETA: 43s  batch_loss: 0.4234 [Training] 7/118 [>.............................] - ETA: 41s  batch_loss: 0.4258 [Training] 8/118 [=>............................] - ETA: 39s  batch_loss: 0.4109 [Training] 9/118 [=>............................] - ETA: 37s  batch_loss: 0.4075 [Training] 10/118 [=>............................] - ETA: 36s  batch_loss: 0.4024 [Training] 11/118 [=>............................] - ETA: 35s  batch_loss: 0.4057 [Training] 12/118 [==>...........................] - ETA: 34s  batch_loss: 0.4017 [Training] 13/118 [==>...........................] - ETA: 33s  batch_loss: 0.4047 [Training] 14/118 [==>...........................] - ETA: 32s  batch_loss: 0.4046 [Training] 15/118 [==>...........................] - ETA: 32s  batch_loss: 0.4013 [Training] 16/118 [===>..........................] - ETA: 31s  batch_loss: 0.4084 [Training] 17/118 [===>..........................] - ETA: 30s  batch_loss: 0.4095 [Training] 18/118 [===>..........................] - ETA: 30s  batch_loss: 0.4079 [Training] 19/118 [===>..........................] - ETA: 29s  batch_loss: 0.4054 [Training] 20/118 [====>.........................] - ETA: 29s  batch_loss: 0.4031 [Training] 21/118 [====>.........................] - ETA: 28s  batch_loss: 0.4023 [Training] 22/118 [====>.........................] - ETA: 28s  batch_loss: 0.4034 [Training] 23/118 [====>.........................] - ETA: 27s  batch_loss: 0.4012 [Training] 24/118 [=====>........................] - ETA: 27s  batch_loss: 0.3997 [Training] 25/118 [=====>........................] - ETA: 27s  batch_loss: 0.4005 [Training] 26/118 [=====>........................] - ETA: 26s  batch_loss: 0.4003 [Training] 27/118 [=====>........................] - ETA: 26s  batch_loss: 0.3983 [Training] 28/118 [======>.......................] - ETA: 25s  batch_loss: 0.3972 [Training] 29/118 [======>.......................] - ETA: 25s  batch_loss: 0.3976 [Training] 30/118 [======>.......................] - ETA: 25s  batch_loss: 0.3959 [Training] 31/118 [======>.......................] - ETA: 24s  batch_loss: 0.3950 [Training] 32/118 [=======>......................] - ETA: 24s  batch_loss: 0.3955 [Training] 33/118 [=======>......................] - ETA: 24s  batch_loss: 0.3939 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 0.3921 [Training] 35/118 [=======>......................] - ETA: 23s  batch_loss: 0.3914 [Training] 36/118 [========>.....................] - ETA: 23s  batch_loss: 0.3926 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.3934 [Training] 38/118 [========>.....................] - ETA: 22s  batch_loss: 0.3928 [Training] 39/118 [========>.....................] - ETA: 22s  batch_loss: 0.3926 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.3919 [Training] 41/118 [=========>....................] - ETA: 21s  batch_loss: 0.3922 [Training] 42/118 [=========>....................] - ETA: 21s  batch_loss: 0.3933 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.3927 [Training] 44/118 [==========>...................] - ETA: 20s  batch_loss: 0.3918 [Training] 45/118 [==========>...................] - ETA: 20s  batch_loss: 0.3912 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.3917 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.3910 [Training] 48/118 [===========>..................] - ETA: 19s  batch_loss: 0.3913 [Training] 49/118 [===========>..................] - ETA: 19s  batch_loss: 0.3917 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.3936 [Training] 51/118 [===========>..................] - ETA: 18s  batch_loss: 0.3940 [Training] 52/118 [============>.................] - ETA: 18s  batch_loss: 0.3920 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.3903 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.3906 [Training] 55/118 [============>.................] - ETA: 17s  batch_loss: 0.3902 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.3888 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.3899 [Training] 58/118 [=============>................] - ETA: 16s  batch_loss: 0.3895 [Training] 59/118 [==============>...............] - ETA: 16s  batch_loss: 0.3922 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.3934 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.3932 [Training] 62/118 [==============>...............] - ETA: 15s  batch_loss: 0.3929 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.3937 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.3928 [Training] 65/118 [===============>..............] - ETA: 14s  batch_loss: 0.3932 [Training] 66/118 [===============>..............] - ETA: 14s  batch_loss: 0.3931 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.3921 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.3913 [Training] 69/118 [================>.............] - ETA: 13s  batch_loss: 0.3908 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.3907 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.3911 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.3910 [Training] 73/118 [=================>............] - ETA: 12s  batch_loss: 0.3900 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.3907 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.3910 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.3918 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.3915 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.3923 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.3924 [Training] 80/118 [===================>..........] - ETA: 10s  batch_loss: 0.3930 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.3929 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.3926 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.3922 [Training] 84/118 [====================>.........] - ETA: 9s  batch_loss: 0.3920 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.3912 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.3917 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.3919 [Training] 88/118 [=====================>........] - ETA: 8s  batch_loss: 0.3909 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.3909 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.3911 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.3905 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.3894 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.3895 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.3893 [Training] 95/118 [=======================>......] - ETA: 6s  batch_loss: 0.3890 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.3883 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.3880 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.3879 [Training] 99/118 [========================>.....] - ETA: 5s  batch_loss: 0.3878 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.3877 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.3870 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.3867 [Training] 103/118 [=========================>....] - ETA: 4s  batch_loss: 0.3863 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.3857 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.3860 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.3856 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.3852 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.3856 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.3853 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.3849 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.3850 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.3855 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.3854 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.3847 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.3848 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.3852 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.3848 [Training] 118/118 [==============================] 265.0ms/step  batch_loss: 0.3844 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:42:59 - INFO - root -   The F1-score is 0.5915992900808519
09/25/2022 10:42:59 - INFO - root -   the best eval f1 is 0.5916, saving model !!
09/25/2022 10:43:01 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:38  batch_loss: 0.3521 [Training] 2/118 [..............................] - ETA: 1:04  batch_loss: 0.3648 [Training] 3/118 [..............................] - ETA: 52s  batch_loss: 0.3523 [Training] 4/118 [>.............................] - ETA: 45s  batch_loss: 0.3401 [Training] 5/118 [>.............................] - ETA: 42s  batch_loss: 0.3386 [Training] 6/118 [>.............................] - ETA: 39s  batch_loss: 0.3458 [Training] 7/118 [>.............................] - ETA: 37s  batch_loss: 0.3440 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.3403 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.3410 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.3427 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.3409 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.3406 [Training] 13/118 [==>...........................] - ETA: 31s  batch_loss: 0.3425 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.3401 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.3396 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.3431 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.3439 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.3436 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.3420 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.3429 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.3415 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.3440 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.3441 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.3421 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.3435 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.3439 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.3442 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.3448 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.3454 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.3432 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.3435 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.3428 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.3453 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.3475 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.3471 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.3475 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.3481 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.3498 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.3493 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.3481 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.3471 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.3478 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.3484 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.3473 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.3476 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.3484 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.3485 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.3488 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.3486 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.3481 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.3482 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.3485 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.3471 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.3456 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.3475 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.3484 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.3471 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.3471 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.3470 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.3471 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.3467 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.3466 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.3465 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.3459 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.3467 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.3467 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.3471 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.3465 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.3454 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.3459 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.3453 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.3449 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.3445 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.3442 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.3437 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.3436 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.3436 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.3437 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.3434 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.3434 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.3427 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.3433 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.3444 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.3442 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.3438 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.3436 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.3440 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.3437 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.3441 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.3449 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.3447 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.3447 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.3446 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.3441 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.3440 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.3451 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.3450 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.3458 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.3458 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.3455 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.3450 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.3452 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.3454 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.3453 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.3446 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.3451 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.3451 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.3451 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.3446 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.3449 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.3446 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.3444 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.3447 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.3438 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.3440 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.3438 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.3438 [Training] 118/118 [==============================] 256.0ms/step  batch_loss: 0.3458 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:43:38 - INFO - root -   The F1-score is 0.6026405992128984
09/25/2022 10:43:38 - INFO - root -   the best eval f1 is 0.6026, saving model !!
09/25/2022 10:43:41 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:53  batch_loss: 0.3037 [Training] 2/118 [..............................] - ETA: 1:10  batch_loss: 0.2937 [Training] 3/118 [..............................] - ETA: 56s  batch_loss: 0.3236 [Training] 4/118 [>.............................] - ETA: 49s  batch_loss: 0.3131 [Training] 5/118 [>.............................] - ETA: 44s  batch_loss: 0.3143 [Training] 6/118 [>.............................] - ETA: 41s  batch_loss: 0.3213 [Training] 7/118 [>.............................] - ETA: 39s  batch_loss: 0.3176 [Training] 8/118 [=>............................] - ETA: 37s  batch_loss: 0.3165 [Training] 9/118 [=>............................] - ETA: 36s  batch_loss: 0.3190 [Training] 10/118 [=>............................] - ETA: 35s  batch_loss: 0.3220 [Training] 11/118 [=>............................] - ETA: 34s  batch_loss: 0.3166 [Training] 12/118 [==>...........................] - ETA: 33s  batch_loss: 0.3152 [Training] 13/118 [==>...........................] - ETA: 32s  batch_loss: 0.3201 [Training] 14/118 [==>...........................] - ETA: 31s  batch_loss: 0.3206 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 0.3213 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 0.3228 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.3265 [Training] 18/118 [===>..........................] - ETA: 29s  batch_loss: 0.3277 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.3277 [Training] 20/118 [====>.........................] - ETA: 28s  batch_loss: 0.3262 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.3261 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 0.3262 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.3259 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.3246 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.3242 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.3219 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.3223 [Training] 28/118 [======>.......................] - ETA: 25s  batch_loss: 0.3223 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.3234 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.3214 [Training] 31/118 [======>.......................] - ETA: 24s  batch_loss: 0.3222 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.3203 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.3187 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 0.3194 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.3193 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.3186 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.3188 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.3187 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.3180 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.3189 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.3189 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.3194 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.3175 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.3164 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.3170 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.3169 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.3169 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.3161 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.3155 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.3160 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.3143 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.3136 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.3135 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.3137 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.3131 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.3121 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.3125 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.3121 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.3126 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.3117 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.3107 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.3105 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.3120 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.3117 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.3117 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.3116 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.3112 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.3116 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.3116 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.3117 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.3116 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.3124 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.3124 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.3123 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.3137 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.3145 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.3139 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.3139 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.3143 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.3143 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.3135 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.3142 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.3142 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.3145 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.3142 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.3148 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.3147 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.3142 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.3136 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.3137 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.3134 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.3141 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.3140 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.3137 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.3137 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.3133 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.3135 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.3133 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.3135 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.3133 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.3127 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.3127 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.3136 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.3135 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.3132 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.3133 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.3133 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.3131 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.3130 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.3131 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.3128 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.3127 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.3125 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.3122 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.3124 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.3129 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.3124 [Training] 118/118 [==============================] 257.4ms/step  batch_loss: 0.3123 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:44:18 - INFO - root -   The F1-score is 0.6070709612481127
09/25/2022 10:44:18 - INFO - root -   the best eval f1 is 0.6071, saving model !!
09/25/2022 10:44:20 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:39  batch_loss: 0.2226 [Training] 2/118 [..............................] - ETA: 1:05  batch_loss: 0.2573 [Training] 3/118 [..............................] - ETA: 52s  batch_loss: 0.2592 [Training] 4/118 [>.............................] - ETA: 46s  batch_loss: 0.2573 [Training] 5/118 [>.............................] - ETA: 42s  batch_loss: 0.2495 [Training] 6/118 [>.............................] - ETA: 39s  batch_loss: 0.2652 [Training] 7/118 [>.............................] - ETA: 37s  batch_loss: 0.2612 [Training] 8/118 [=>............................] - ETA: 36s  batch_loss: 0.2595 [Training] 9/118 [=>............................] - ETA: 35s  batch_loss: 0.2588 [Training] 10/118 [=>............................] - ETA: 34s  batch_loss: 0.2558 [Training] 11/118 [=>............................] - ETA: 33s  batch_loss: 0.2561 [Training] 12/118 [==>...........................] - ETA: 32s  batch_loss: 0.2618 [Training] 13/118 [==>...........................] - ETA: 31s  batch_loss: 0.2660 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.2643 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 0.2641 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.2628 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.2630 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.2710 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.2721 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.2736 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.2737 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.2750 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.2767 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.2814 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.2827 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.2834 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.2839 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.2856 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.2861 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.2855 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.2859 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.2849 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.2848 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.2839 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.2833 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.2845 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2847 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2838 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.2868 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2864 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2866 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.2869 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.2879 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2883 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2895 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.2891 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2891 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2894 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.2886 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.2885 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2871 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2882 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.2878 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2867 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2874 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.2875 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.2889 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2886 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2893 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.2883 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2887 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2886 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2881 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.2879 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.2877 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2874 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2873 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.2868 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2867 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2869 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.2875 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.2879 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2879 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2887 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.2881 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.2884 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2888 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2886 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.2883 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2879 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2874 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2881 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.2877 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2876 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2869 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2879 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.2880 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2871 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2867 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2862 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.2874 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2878 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2877 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2878 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.2880 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2883 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2885 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2890 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2889 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2891 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2887 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2889 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2887 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2885 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2881 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2882 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2881 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2880 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2882 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2880 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2882 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2884 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2883 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2880 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2878 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2879 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2879 [Training] 118/118 [==============================] 255.8ms/step  batch_loss: 0.2878 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:44:57 - INFO - root -   The F1-score is 0.6182609809194519
09/25/2022 10:44:57 - INFO - root -   the best eval f1 is 0.6183, saving model !!
09/25/2022 10:45:00 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:45  batch_loss: 0.2064 [Training] 2/118 [..............................] - ETA: 1:06  batch_loss: 0.2329 [Training] 3/118 [..............................] - ETA: 53s  batch_loss: 0.2509 [Training] 4/118 [>.............................] - ETA: 47s  batch_loss: 0.2687 [Training] 5/118 [>.............................] - ETA: 43s  batch_loss: 0.2608 [Training] 6/118 [>.............................] - ETA: 40s  batch_loss: 0.2508 [Training] 7/118 [>.............................] - ETA: 38s  batch_loss: 0.2497 [Training] 8/118 [=>............................] - ETA: 36s  batch_loss: 0.2518 [Training] 9/118 [=>............................] - ETA: 35s  batch_loss: 0.2440 [Training] 10/118 [=>............................] - ETA: 34s  batch_loss: 0.2457 [Training] 11/118 [=>............................] - ETA: 33s  batch_loss: 0.2440 [Training] 12/118 [==>...........................] - ETA: 32s  batch_loss: 0.2435 [Training] 13/118 [==>...........................] - ETA: 31s  batch_loss: 0.2449 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.2488 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 0.2492 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.2496 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.2482 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.2518 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.2503 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.2500 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.2540 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.2544 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.2586 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.2585 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.2593 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.2580 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.2588 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.2616 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.2623 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.2609 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.2611 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.2603 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.2608 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.2592 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.2578 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.2571 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2569 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2574 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.2584 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2576 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2599 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.2601 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.2605 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2612 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2617 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.2608 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2609 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2614 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.2607 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.2616 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2623 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2619 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.2614 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2628 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2617 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.2616 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.2613 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2617 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2612 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.2624 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2635 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2638 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2639 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.2629 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.2621 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2628 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2621 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.2628 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2639 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2638 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.2631 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.2638 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2636 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2630 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.2632 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.2649 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2663 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2657 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.2657 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2651 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2649 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2650 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.2647 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2641 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2646 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2640 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.2644 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2655 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2652 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2658 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.2664 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2666 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2660 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2666 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.2663 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2665 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2667 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2667 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2666 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2664 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2665 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2669 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2668 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2673 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2672 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2676 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2670 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2666 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2664 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2661 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2667 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2664 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2669 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2672 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2670 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2668 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2668 [Training] 118/118 [==============================] 256.2ms/step  batch_loss: 0.2671 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:45:37 - INFO - root -   The F1-score is 0.6262821753435264
09/25/2022 10:45:37 - INFO - root -   the best eval f1 is 0.6263, saving model !!
09/25/2022 10:45:39 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:39  batch_loss: 0.2114 [Training] 2/118 [..............................] - ETA: 1:04  batch_loss: 0.2211 [Training] 3/118 [..............................] - ETA: 52s  batch_loss: 0.2249 [Training] 4/118 [>.............................] - ETA: 46s  batch_loss: 0.2287 [Training] 5/118 [>.............................] - ETA: 42s  batch_loss: 0.2297 [Training] 6/118 [>.............................] - ETA: 39s  batch_loss: 0.2331 [Training] 7/118 [>.............................] - ETA: 37s  batch_loss: 0.2410 [Training] 8/118 [=>............................] - ETA: 36s  batch_loss: 0.2381 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.2445 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.2420 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.2406 [Training] 12/118 [==>...........................] - ETA: 32s  batch_loss: 0.2402 [Training] 13/118 [==>...........................] - ETA: 31s  batch_loss: 0.2444 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.2441 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 0.2458 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.2491 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.2508 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.2501 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.2502 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.2497 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.2486 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.2495 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.2499 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.2511 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.2511 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.2506 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.2510 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.2493 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.2500 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.2517 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.2503 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.2477 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.2480 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.2485 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.2481 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.2476 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2472 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2477 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.2472 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2486 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2488 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.2483 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.2475 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2492 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2488 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.2481 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2476 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2484 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.2474 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.2473 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2477 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2475 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.2466 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2457 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2457 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.2454 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.2459 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2454 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2462 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.2457 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2466 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2461 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2460 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.2466 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.2455 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2450 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2450 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.2451 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2448 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2444 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.2438 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.2435 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2437 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2442 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.2439 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.2436 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2438 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2434 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.2433 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2437 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2432 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2428 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.2429 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2439 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2438 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2435 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.2437 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2444 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2449 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2443 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.2442 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2446 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2442 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2444 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.2447 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2441 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2442 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2448 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2448 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2448 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2451 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2458 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2456 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2453 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2451 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2448 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2447 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2445 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2450 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2444 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2440 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2452 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2449 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2453 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2449 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2442 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2445 [Training] 118/118 [==============================] 255.6ms/step  batch_loss: 0.2459 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:46:16 - INFO - root -   The F1-score is 0.6275690535410393
09/25/2022 10:46:16 - INFO - root -   the best eval f1 is 0.6276, saving model !!
09/25/2022 10:46:19 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:59  batch_loss: 0.1807 [Training] 2/118 [..............................] - ETA: 1:14  batch_loss: 0.2140 [Training] 3/118 [..............................] - ETA: 58s  batch_loss: 0.2064 [Training] 4/118 [>.............................] - ETA: 51s  batch_loss: 0.1942 [Training] 5/118 [>.............................] - ETA: 46s  batch_loss: 0.2019 [Training] 6/118 [>.............................] - ETA: 42s  batch_loss: 0.2084 [Training] 7/118 [>.............................] - ETA: 40s  batch_loss: 0.2183 [Training] 8/118 [=>............................] - ETA: 38s  batch_loss: 0.2186 [Training] 9/118 [=>............................] - ETA: 37s  batch_loss: 0.2208 [Training] 10/118 [=>............................] - ETA: 35s  batch_loss: 0.2158 [Training] 11/118 [=>............................] - ETA: 34s  batch_loss: 0.2209 [Training] 12/118 [==>...........................] - ETA: 33s  batch_loss: 0.2223 [Training] 13/118 [==>...........................] - ETA: 32s  batch_loss: 0.2241 [Training] 14/118 [==>...........................] - ETA: 32s  batch_loss: 0.2238 [Training] 15/118 [==>...........................] - ETA: 31s  batch_loss: 0.2240 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 0.2240 [Training] 17/118 [===>..........................] - ETA: 30s  batch_loss: 0.2263 [Training] 18/118 [===>..........................] - ETA: 29s  batch_loss: 0.2252 [Training] 19/118 [===>..........................] - ETA: 29s  batch_loss: 0.2242 [Training] 20/118 [====>.........................] - ETA: 28s  batch_loss: 0.2257 [Training] 21/118 [====>.........................] - ETA: 28s  batch_loss: 0.2239 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 0.2232 [Training] 23/118 [====>.........................] - ETA: 27s  batch_loss: 0.2232 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.2239 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.2250 [Training] 26/118 [=====>........................] - ETA: 26s  batch_loss: 0.2233 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.2251 [Training] 28/118 [======>.......................] - ETA: 25s  batch_loss: 0.2237 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.2252 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.2270 [Training] 31/118 [======>.......................] - ETA: 24s  batch_loss: 0.2259 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.2247 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.2242 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 0.2249 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.2246 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.2246 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.2253 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2257 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.2260 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.2256 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2254 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.2244 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.2253 [Training] 44/118 [==========>...................] - ETA: 20s  batch_loss: 0.2245 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2248 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.2231 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.2225 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2219 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.2217 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.2213 [Training] 51/118 [===========>..................] - ETA: 18s  batch_loss: 0.2213 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2221 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.2226 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.2219 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2222 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.2219 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.2217 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2217 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2214 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.2215 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.2220 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2220 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2227 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.2226 [Training] 65/118 [===============>..............] - ETA: 14s  batch_loss: 0.2226 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2222 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2225 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.2232 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2228 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2221 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.2221 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.2220 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2225 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2230 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.2231 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.2224 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2227 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2235 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.2230 [Training] 80/118 [===================>..........] - ETA: 10s  batch_loss: 0.2235 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2233 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2235 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.2240 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2241 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2241 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2243 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.2243 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2249 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2253 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2251 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.2252 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2250 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2251 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2254 [Training] 95/118 [=======================>......] - ETA: 6s  batch_loss: 0.2256 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2255 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2257 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2265 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2262 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2262 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2264 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2258 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2260 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2259 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2263 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2264 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2266 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2265 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2267 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2265 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2270 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2269 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2264 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2262 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2261 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2265 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2263 [Training] 118/118 [==============================] 258.2ms/step  batch_loss: 0.2265 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:46:56 - INFO - root -   The F1-score is 0.6311480571824422
09/25/2022 10:46:56 - INFO - root -   the best eval f1 is 0.6311, saving model !!
09/25/2022 10:46:59 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:56  batch_loss: 0.1897 [Training] 2/118 [..............................] - ETA: 1:12  batch_loss: 0.1801 [Training] 3/118 [..............................] - ETA: 56s  batch_loss: 0.1826 [Training] 4/118 [>.............................] - ETA: 49s  batch_loss: 0.1797 [Training] 5/118 [>.............................] - ETA: 44s  batch_loss: 0.1904 [Training] 6/118 [>.............................] - ETA: 41s  batch_loss: 0.1975 [Training] 7/118 [>.............................] - ETA: 39s  batch_loss: 0.2001 [Training] 8/118 [=>............................] - ETA: 37s  batch_loss: 0.2033 [Training] 9/118 [=>............................] - ETA: 36s  batch_loss: 0.2028 [Training] 10/118 [=>............................] - ETA: 35s  batch_loss: 0.2015 [Training] 11/118 [=>............................] - ETA: 34s  batch_loss: 0.1985 [Training] 12/118 [==>...........................] - ETA: 33s  batch_loss: 0.1991 [Training] 13/118 [==>...........................] - ETA: 32s  batch_loss: 0.1995 [Training] 14/118 [==>...........................] - ETA: 31s  batch_loss: 0.2013 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 0.2051 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 0.2030 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.2020 [Training] 18/118 [===>..........................] - ETA: 29s  batch_loss: 0.2021 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.2018 [Training] 20/118 [====>.........................] - ETA: 28s  batch_loss: 0.2033 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.2031 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 0.2040 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.2056 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.2069 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.2078 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.2072 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.2074 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.2084 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.2088 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.2082 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.2072 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.2064 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.2062 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.2061 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.2059 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.2059 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2058 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2046 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.2042 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.2052 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2047 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.2047 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.2050 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2050 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2053 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.2051 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.2048 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2058 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.2060 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.2063 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2058 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2054 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.2059 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.2065 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2066 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.2071 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.2061 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2057 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2062 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.2059 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.2063 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2056 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2056 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.2060 [Training] 65/118 [===============>..............] - ETA: 14s  batch_loss: 0.2060 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2057 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2062 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.2073 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2069 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2072 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.2072 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.2089 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2088 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2086 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.2082 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.2087 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2084 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2084 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.2088 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2087 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2094 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2096 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.2098 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2092 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2091 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2087 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.2092 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2095 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2095 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2103 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.2099 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2106 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2107 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2113 [Training] 95/118 [=======================>......] - ETA: 6s  batch_loss: 0.2115 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2114 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2112 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2110 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2109 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2105 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2107 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2105 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2101 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2103 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2103 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2105 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2101 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2101 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2106 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2102 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2103 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2100 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2105 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2113 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2113 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2114 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2112 [Training] 118/118 [==============================] 258.6ms/step  batch_loss: 0.2111 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:47:36 - INFO - root -   The F1-score is 0.6336721728081322
09/25/2022 10:47:36 - INFO - root -   the best eval f1 is 0.6337, saving model !!
09/25/2022 10:47:38 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:57  batch_loss: 0.2451 [Training] 2/118 [..............................] - ETA: 1:13  batch_loss: 0.2234 [Training] 3/118 [..............................] - ETA: 58s  batch_loss: 0.2180 [Training] 4/118 [>.............................] - ETA: 50s  batch_loss: 0.2103 [Training] 5/118 [>.............................] - ETA: 45s  batch_loss: 0.2053 [Training] 6/118 [>.............................] - ETA: 42s  batch_loss: 0.2096 [Training] 7/118 [>.............................] - ETA: 40s  batch_loss: 0.2169 [Training] 8/118 [=>............................] - ETA: 38s  batch_loss: 0.2163 [Training] 9/118 [=>............................] - ETA: 36s  batch_loss: 0.2150 [Training] 10/118 [=>............................] - ETA: 35s  batch_loss: 0.2131 [Training] 11/118 [=>............................] - ETA: 34s  batch_loss: 0.2114 [Training] 12/118 [==>...........................] - ETA: 33s  batch_loss: 0.2094 [Training] 13/118 [==>...........................] - ETA: 32s  batch_loss: 0.2072 [Training] 14/118 [==>...........................] - ETA: 32s  batch_loss: 0.2055 [Training] 15/118 [==>...........................] - ETA: 31s  batch_loss: 0.2031 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 0.2045 [Training] 17/118 [===>..........................] - ETA: 30s  batch_loss: 0.2032 [Training] 18/118 [===>..........................] - ETA: 29s  batch_loss: 0.2051 [Training] 19/118 [===>..........................] - ETA: 29s  batch_loss: 0.2032 [Training] 20/118 [====>.........................] - ETA: 28s  batch_loss: 0.2035 [Training] 21/118 [====>.........................] - ETA: 28s  batch_loss: 0.2016 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 0.2015 [Training] 23/118 [====>.........................] - ETA: 27s  batch_loss: 0.2027 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.2017 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.2024 [Training] 26/118 [=====>........................] - ETA: 26s  batch_loss: 0.2030 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.2028 [Training] 28/118 [======>.......................] - ETA: 25s  batch_loss: 0.2041 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.2023 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.2035 [Training] 31/118 [======>.......................] - ETA: 24s  batch_loss: 0.2024 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.2016 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.1999 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 0.1996 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1999 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.2001 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.2012 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2006 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.2007 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.2005 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2005 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.2006 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.1997 [Training] 44/118 [==========>...................] - ETA: 20s  batch_loss: 0.2007 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2003 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.2007 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.2006 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2004 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1998 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.1999 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2000 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2002 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.2010 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.2000 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1992 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1995 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.2000 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1998 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1998 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.2001 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.2004 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1998 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1998 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1999 [Training] 65/118 [===============>..............] - ETA: 14s  batch_loss: 0.1997 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1993 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1990 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1988 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1984 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1980 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1979 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.1983 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1980 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1978 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1988 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.1988 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1989 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1995 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1992 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1988 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1988 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1988 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1991 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1991 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1997 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1991 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1985 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1997 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2004 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2004 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.2001 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2000 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2004 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2002 [Training] 95/118 [=======================>......] - ETA: 6s  batch_loss: 0.1996 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1997 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1992 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1989 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1988 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1996 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1995 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1997 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1997 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1992 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1992 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1987 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1985 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1984 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1987 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1989 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1991 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1992 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1990 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1993 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1994 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1992 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1994 [Training] 118/118 [==============================] 259.1ms/step  batch_loss: 0.1987 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:48:16 - INFO - root -   The F1-score is 0.6318382162302308
09/25/2022 10:48:16 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:52  batch_loss: 0.1329 [Training] 2/118 [..............................] - ETA: 1:11  batch_loss: 0.1637 [Training] 3/118 [..............................] - ETA: 56s  batch_loss: 0.1735 [Training] 4/118 [>.............................] - ETA: 49s  batch_loss: 0.1704 [Training] 5/118 [>.............................] - ETA: 44s  batch_loss: 0.1676 [Training] 6/118 [>.............................] - ETA: 41s  batch_loss: 0.1672 [Training] 7/118 [>.............................] - ETA: 39s  batch_loss: 0.1697 [Training] 8/118 [=>............................] - ETA: 37s  batch_loss: 0.1697 [Training] 9/118 [=>............................] - ETA: 36s  batch_loss: 0.1710 [Training] 10/118 [=>............................] - ETA: 35s  batch_loss: 0.1692 [Training] 11/118 [=>............................] - ETA: 34s  batch_loss: 0.1714 [Training] 12/118 [==>...........................] - ETA: 33s  batch_loss: 0.1691 [Training] 13/118 [==>...........................] - ETA: 32s  batch_loss: 0.1690 [Training] 14/118 [==>...........................] - ETA: 31s  batch_loss: 0.1690 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 0.1708 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 0.1717 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.1716 [Training] 18/118 [===>..........................] - ETA: 29s  batch_loss: 0.1724 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.1723 [Training] 20/118 [====>.........................] - ETA: 28s  batch_loss: 0.1717 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.1726 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 0.1722 [Training] 23/118 [====>.........................] - ETA: 27s  batch_loss: 0.1740 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.1739 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.1759 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1757 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.1760 [Training] 28/118 [======>.......................] - ETA: 25s  batch_loss: 0.1753 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1774 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.1768 [Training] 31/118 [======>.......................] - ETA: 24s  batch_loss: 0.1773 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1766 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.1759 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 0.1763 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1772 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.1770 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.1784 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1806 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1797 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.1795 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1793 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1798 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.1814 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1811 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1812 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.1823 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1817 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1826 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1829 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.1830 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1831 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1843 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1841 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1844 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1842 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1848 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.1843 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1844 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1854 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1849 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1852 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1856 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1855 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1854 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1857 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1863 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1861 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1861 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1857 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1858 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1866 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1864 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1866 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1863 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1859 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1861 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1863 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1864 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1863 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1863 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1864 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1863 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1870 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1868 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1868 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1869 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1865 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1874 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1872 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1873 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1869 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1867 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1867 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1869 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1872 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1877 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1881 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1881 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1878 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1881 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1884 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1885 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1881 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1879 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1878 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1879 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1879 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1880 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1879 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1880 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1877 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1878 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1879 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1879 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1881 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1879 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1880 [Training] 118/118 [==============================] 255.8ms/step  batch_loss: 0.1872 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:48:53 - INFO - root -   The F1-score is 0.6463629828115809
09/25/2022 10:48:53 - INFO - root -   the best eval f1 is 0.6464, saving model !!
09/25/2022 10:48:55 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:55  batch_loss: 0.1551 [Training] 2/118 [..............................] - ETA: 1:12  batch_loss: 0.1611 [Training] 3/118 [..............................] - ETA: 57s  batch_loss: 0.1845 [Training] 4/118 [>.............................] - ETA: 49s  batch_loss: 0.1840 [Training] 5/118 [>.............................] - ETA: 45s  batch_loss: 0.1880 [Training] 6/118 [>.............................] - ETA: 42s  batch_loss: 0.1865 [Training] 7/118 [>.............................] - ETA: 39s  batch_loss: 0.1806 [Training] 8/118 [=>............................] - ETA: 38s  batch_loss: 0.1871 [Training] 9/118 [=>............................] - ETA: 36s  batch_loss: 0.1855 [Training] 10/118 [=>............................] - ETA: 35s  batch_loss: 0.1814 [Training] 11/118 [=>............................] - ETA: 34s  batch_loss: 0.1796 [Training] 12/118 [==>...........................] - ETA: 33s  batch_loss: 0.1841 [Training] 13/118 [==>...........................] - ETA: 32s  batch_loss: 0.1846 [Training] 14/118 [==>...........................] - ETA: 31s  batch_loss: 0.1841 [Training] 15/118 [==>...........................] - ETA: 31s  batch_loss: 0.1824 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 0.1810 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.1801 [Training] 18/118 [===>..........................] - ETA: 29s  batch_loss: 0.1787 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.1774 [Training] 20/118 [====>.........................] - ETA: 28s  batch_loss: 0.1756 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.1753 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 0.1748 [Training] 23/118 [====>.........................] - ETA: 27s  batch_loss: 0.1748 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.1732 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.1729 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1722 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.1725 [Training] 28/118 [======>.......................] - ETA: 25s  batch_loss: 0.1734 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1727 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.1709 [Training] 31/118 [======>.......................] - ETA: 24s  batch_loss: 0.1699 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1701 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.1701 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 0.1709 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1704 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.1697 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.1707 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1708 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1713 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.1717 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1714 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1708 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.1710 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1714 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1721 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.1721 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.1721 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1720 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1722 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.1723 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1727 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1734 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1737 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.1746 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1748 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1742 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.1743 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1742 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1736 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1736 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.1741 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1746 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1746 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1747 [Training] 65/118 [===============>..............] - ETA: 14s  batch_loss: 0.1750 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1749 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1753 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1754 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1755 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1760 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1757 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.1763 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1761 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1757 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1757 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.1759 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1761 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1758 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1757 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1754 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1760 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1763 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1763 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1767 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1768 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1766 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1763 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1761 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1766 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1761 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.1762 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1760 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1762 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1772 [Training] 95/118 [=======================>......] - ETA: 6s  batch_loss: 0.1773 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1775 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1778 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1779 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1778 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1782 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1782 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1785 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1779 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1779 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1785 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1785 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1783 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1783 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1785 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1783 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1784 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1785 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1788 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1788 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1788 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1787 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1790 [Training] 118/118 [==============================] 258.4ms/step  batch_loss: 0.1785 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:49:32 - INFO - root -   The F1-score is 0.6418610712889352
09/25/2022 10:49:32 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:52  batch_loss: 0.1459 [Training] 2/118 [..............................] - ETA: 1:10  batch_loss: 0.1676 [Training] 3/118 [..............................] - ETA: 56s  batch_loss: 0.1809 [Training] 4/118 [>.............................] - ETA: 49s  batch_loss: 0.1724 [Training] 5/118 [>.............................] - ETA: 44s  batch_loss: 0.1669 [Training] 6/118 [>.............................] - ETA: 41s  batch_loss: 0.1710 [Training] 7/118 [>.............................] - ETA: 39s  batch_loss: 0.1670 [Training] 8/118 [=>............................] - ETA: 37s  batch_loss: 0.1669 [Training] 9/118 [=>............................] - ETA: 36s  batch_loss: 0.1649 [Training] 10/118 [=>............................] - ETA: 34s  batch_loss: 0.1636 [Training] 11/118 [=>............................] - ETA: 33s  batch_loss: 0.1614 [Training] 12/118 [==>...........................] - ETA: 32s  batch_loss: 0.1633 [Training] 13/118 [==>...........................] - ETA: 32s  batch_loss: 0.1625 [Training] 14/118 [==>...........................] - ETA: 31s  batch_loss: 0.1654 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 0.1667 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 0.1666 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.1664 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.1655 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.1655 [Training] 20/118 [====>.........................] - ETA: 28s  batch_loss: 0.1645 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.1629 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 0.1650 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.1649 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.1646 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.1645 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1646 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.1658 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.1662 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1668 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.1669 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1652 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1637 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.1647 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1655 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1662 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.1654 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.1649 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1640 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1639 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.1640 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1635 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1632 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.1625 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1628 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1630 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.1627 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.1625 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1640 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1630 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.1629 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1631 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1628 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1627 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.1619 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1628 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1632 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.1632 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1634 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1638 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1641 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.1647 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1644 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1640 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1641 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1645 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1649 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1657 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1655 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1652 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1646 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1643 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.1639 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1646 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1652 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1657 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.1652 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1655 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1657 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1663 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1668 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1670 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1673 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1672 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1673 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1682 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1679 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1678 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1678 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1676 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1673 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.1674 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1677 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1673 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1677 [Training] 95/118 [=======================>......] - ETA: 6s  batch_loss: 0.1677 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1677 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1675 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1678 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1675 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1674 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1675 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1674 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1676 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1673 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1674 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1675 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1681 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1682 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1679 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1684 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1682 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1684 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1683 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1685 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1684 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1682 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1683 [Training] 118/118 [==============================] 258.2ms/step  batch_loss: 0.1688 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:50:10 - INFO - root -   The F1-score is 0.6445049248315189
09/25/2022 10:50:10 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:51  batch_loss: 0.1339 [Training] 2/118 [..............................] - ETA: 1:10  batch_loss: 0.1218 [Training] 3/118 [..............................] - ETA: 56s  batch_loss: 0.1306 [Training] 4/118 [>.............................] - ETA: 49s  batch_loss: 0.1309 [Training] 5/118 [>.............................] - ETA: 44s  batch_loss: 0.1327 [Training] 6/118 [>.............................] - ETA: 41s  batch_loss: 0.1404 [Training] 7/118 [>.............................] - ETA: 39s  batch_loss: 0.1420 [Training] 8/118 [=>............................] - ETA: 37s  batch_loss: 0.1499 [Training] 9/118 [=>............................] - ETA: 36s  batch_loss: 0.1532 [Training] 10/118 [=>............................] - ETA: 35s  batch_loss: 0.1545 [Training] 11/118 [=>............................] - ETA: 34s  batch_loss: 0.1597 [Training] 12/118 [==>...........................] - ETA: 33s  batch_loss: 0.1575 [Training] 13/118 [==>...........................] - ETA: 32s  batch_loss: 0.1568 [Training] 14/118 [==>...........................] - ETA: 31s  batch_loss: 0.1567 [Training] 15/118 [==>...........................] - ETA: 31s  batch_loss: 0.1562 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 0.1541 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.1539 [Training] 18/118 [===>..........................] - ETA: 29s  batch_loss: 0.1542 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.1538 [Training] 20/118 [====>.........................] - ETA: 28s  batch_loss: 0.1546 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.1549 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 0.1540 [Training] 23/118 [====>.........................] - ETA: 27s  batch_loss: 0.1534 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.1528 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.1534 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1521 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.1517 [Training] 28/118 [======>.......................] - ETA: 25s  batch_loss: 0.1531 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1523 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.1528 [Training] 31/118 [======>.......................] - ETA: 24s  batch_loss: 0.1521 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1520 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.1524 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 0.1547 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1562 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.1570 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.1583 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1579 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1570 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.1576 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1575 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1579 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.1581 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1590 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1581 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.1583 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.1588 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1583 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1577 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.1581 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1573 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1584 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1580 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.1580 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1584 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1581 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.1578 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1579 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1581 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1576 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.1578 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1575 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1582 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1575 [Training] 65/118 [===============>..............] - ETA: 14s  batch_loss: 0.1583 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1596 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1617 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1616 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1616 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1617 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1614 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.1620 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1617 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1624 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1619 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.1615 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1619 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1617 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1623 [Training] 80/118 [===================>..........] - ETA: 10s  batch_loss: 0.1623 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1638 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1641 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1637 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1633 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1639 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1635 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1635 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1629 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1629 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1626 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.1633 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1633 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1635 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1639 [Training] 95/118 [=======================>......] - ETA: 6s  batch_loss: 0.1633 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1633 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1633 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1631 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1634 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1633 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1628 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1628 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1628 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1625 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1624 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1624 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1626 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1622 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1627 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1626 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1626 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1625 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1626 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1629 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1629 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1628 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1625 [Training] 118/118 [==============================] 258.6ms/step  batch_loss: 0.1631 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:50:47 - INFO - root -   The F1-score is 0.6406981155937778
09/25/2022 10:50:47 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:52  batch_loss: 0.1059 [Training] 2/118 [..............................] - ETA: 1:10  batch_loss: 0.1194 [Training] 3/118 [..............................] - ETA: 56s  batch_loss: 0.1200 [Training] 4/118 [>.............................] - ETA: 49s  batch_loss: 0.1244 [Training] 5/118 [>.............................] - ETA: 44s  batch_loss: 0.1311 [Training] 6/118 [>.............................] - ETA: 42s  batch_loss: 0.1332 [Training] 7/118 [>.............................] - ETA: 39s  batch_loss: 0.1327 [Training] 8/118 [=>............................] - ETA: 37s  batch_loss: 0.1336 [Training] 9/118 [=>............................] - ETA: 36s  batch_loss: 0.1377 [Training] 10/118 [=>............................] - ETA: 35s  batch_loss: 0.1374 [Training] 11/118 [=>............................] - ETA: 34s  batch_loss: 0.1414 [Training] 12/118 [==>...........................] - ETA: 33s  batch_loss: 0.1421 [Training] 13/118 [==>...........................] - ETA: 32s  batch_loss: 0.1398 [Training] 14/118 [==>...........................] - ETA: 31s  batch_loss: 0.1381 [Training] 15/118 [==>...........................] - ETA: 31s  batch_loss: 0.1420 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 0.1416 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.1401 [Training] 18/118 [===>..........................] - ETA: 29s  batch_loss: 0.1408 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.1412 [Training] 20/118 [====>.........................] - ETA: 28s  batch_loss: 0.1414 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.1405 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 0.1413 [Training] 23/118 [====>.........................] - ETA: 27s  batch_loss: 0.1405 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.1408 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.1419 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1434 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.1435 [Training] 28/118 [======>.......................] - ETA: 25s  batch_loss: 0.1444 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1449 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.1450 [Training] 31/118 [======>.......................] - ETA: 24s  batch_loss: 0.1440 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1449 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.1451 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 0.1465 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1464 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.1465 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.1473 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1491 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1483 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.1491 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1489 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1483 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.1477 [Training] 44/118 [==========>...................] - ETA: 20s  batch_loss: 0.1471 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1479 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.1489 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.1487 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1496 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1500 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.1493 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1496 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1491 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1491 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.1494 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1493 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1497 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.1499 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1505 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1501 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1502 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.1497 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1496 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1502 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1496 [Training] 65/118 [===============>..............] - ETA: 14s  batch_loss: 0.1493 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1500 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1499 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1501 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1505 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1505 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1502 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.1500 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1496 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1496 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1492 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.1490 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1488 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1495 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1497 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1495 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1497 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1499 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1499 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1497 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1498 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1495 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1496 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1493 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1491 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1491 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.1492 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1493 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1493 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1489 [Training] 95/118 [=======================>......] - ETA: 6s  batch_loss: 0.1487 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1495 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1498 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1500 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1496 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1507 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1509 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1519 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1525 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1529 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1528 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1526 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1526 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1529 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1530 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1534 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1533 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1533 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1531 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1527 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1526 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1525 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1525 [Training] 118/118 [==============================] 258.7ms/step  batch_loss: 0.1527 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:51:24 - INFO - root -   The F1-score is 0.6460878606706525
09/25/2022 10:51:24 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:55  batch_loss: 0.0960 [Training] 2/118 [..............................] - ETA: 1:12  batch_loss: 0.1088 [Training] 3/118 [..............................] - ETA: 57s  batch_loss: 0.1171 [Training] 4/118 [>.............................] - ETA: 49s  batch_loss: 0.1255 [Training] 5/118 [>.............................] - ETA: 45s  batch_loss: 0.1290 [Training] 6/118 [>.............................] - ETA: 42s  batch_loss: 0.1268 [Training] 7/118 [>.............................] - ETA: 39s  batch_loss: 0.1350 [Training] 8/118 [=>............................] - ETA: 38s  batch_loss: 0.1355 [Training] 9/118 [=>............................] - ETA: 36s  batch_loss: 0.1394 [Training] 10/118 [=>............................] - ETA: 35s  batch_loss: 0.1363 [Training] 11/118 [=>............................] - ETA: 34s  batch_loss: 0.1361 [Training] 12/118 [==>...........................] - ETA: 33s  batch_loss: 0.1348 [Training] 13/118 [==>...........................] - ETA: 32s  batch_loss: 0.1390 [Training] 14/118 [==>...........................] - ETA: 31s  batch_loss: 0.1369 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 0.1359 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 0.1338 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.1322 [Training] 18/118 [===>..........................] - ETA: 29s  batch_loss: 0.1329 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.1326 [Training] 20/118 [====>.........................] - ETA: 28s  batch_loss: 0.1311 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.1311 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 0.1356 [Training] 23/118 [====>.........................] - ETA: 27s  batch_loss: 0.1364 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.1350 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.1399 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1390 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.1385 [Training] 28/118 [======>.......................] - ETA: 25s  batch_loss: 0.1383 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1394 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.1389 [Training] 31/118 [======>.......................] - ETA: 24s  batch_loss: 0.1387 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1380 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.1393 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 0.1404 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1404 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.1415 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.1420 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1413 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1423 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.1418 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1424 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1426 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.1427 [Training] 44/118 [==========>...................] - ETA: 20s  batch_loss: 0.1433 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1441 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.1437 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.1437 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1440 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1447 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.1445 [Training] 51/118 [===========>..................] - ETA: 18s  batch_loss: 0.1447 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1439 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1445 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.1452 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1446 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1453 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.1450 [Training] 58/118 [=============>................] - ETA: 16s  batch_loss: 0.1451 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1450 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1454 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.1453 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1450 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1459 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1455 [Training] 65/118 [===============>..............] - ETA: 14s  batch_loss: 0.1459 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1459 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1454 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1448 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1451 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1452 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1458 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.1457 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1460 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1462 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1463 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.1458 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1456 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1452 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1456 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1453 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1455 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1450 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1444 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1446 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1445 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1448 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1444 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1443 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1443 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1443 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.1438 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1440 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1436 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1438 [Training] 95/118 [=======================>......] - ETA: 6s  batch_loss: 0.1444 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1448 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1451 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1451 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1449 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1449 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1460 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1459 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1456 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1454 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1456 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1459 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1458 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1460 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1462 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1464 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1462 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1463 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1462 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1462 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1463 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1463 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1461 [Training] 118/118 [==============================] 256.9ms/step  batch_loss: 0.1461 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:52:01 - INFO - root -   The F1-score is 0.6481313849734901
09/25/2022 10:52:01 - INFO - root -   the best eval f1 is 0.6481, saving model !!
09/25/2022 10:52:04 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:53  batch_loss: 0.1648 [Training] 2/118 [..............................] - ETA: 1:11  batch_loss: 0.1503 [Training] 3/118 [..............................] - ETA: 57s  batch_loss: 0.1393 [Training] 4/118 [>.............................] - ETA: 49s  batch_loss: 0.1294 [Training] 5/118 [>.............................] - ETA: 45s  batch_loss: 0.1365 [Training] 6/118 [>.............................] - ETA: 41s  batch_loss: 0.1397 [Training] 7/118 [>.............................] - ETA: 39s  batch_loss: 0.1403 [Training] 8/118 [=>............................] - ETA: 37s  batch_loss: 0.1410 [Training] 9/118 [=>............................] - ETA: 36s  batch_loss: 0.1408 [Training] 10/118 [=>............................] - ETA: 35s  batch_loss: 0.1356 [Training] 11/118 [=>............................] - ETA: 34s  batch_loss: 0.1374 [Training] 12/118 [==>...........................] - ETA: 33s  batch_loss: 0.1376 [Training] 13/118 [==>...........................] - ETA: 32s  batch_loss: 0.1383 [Training] 14/118 [==>...........................] - ETA: 31s  batch_loss: 0.1379 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 0.1374 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 0.1356 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.1353 [Training] 18/118 [===>..........................] - ETA: 29s  batch_loss: 0.1358 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.1363 [Training] 20/118 [====>.........................] - ETA: 28s  batch_loss: 0.1361 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.1380 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 0.1378 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.1361 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.1356 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.1355 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1343 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.1341 [Training] 28/118 [======>.......................] - ETA: 25s  batch_loss: 0.1348 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1353 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.1362 [Training] 31/118 [======>.......................] - ETA: 24s  batch_loss: 0.1356 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1381 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.1388 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 0.1377 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1372 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.1376 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.1398 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1398 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1398 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.1395 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1393 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1387 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.1386 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1386 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1390 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.1388 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.1389 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1382 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1379 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.1380 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1376 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1376 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1373 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.1377 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1370 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1374 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.1377 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1375 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1378 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1381 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.1384 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1383 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1381 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1376 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1379 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1379 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1389 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1392 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1390 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1387 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1383 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.1380 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1380 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1380 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1379 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1384 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1379 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1377 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1383 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1382 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1381 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1381 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1384 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1384 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1379 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1384 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1391 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1399 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1404 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1405 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.1406 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1406 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1408 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1409 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1414 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1409 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1410 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1406 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1411 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1409 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1407 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1410 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1409 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1405 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1409 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1408 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1417 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1421 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1422 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1418 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1418 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1417 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1422 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1422 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1423 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1425 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1419 [Training] 118/118 [==============================] 257.6ms/step  batch_loss: 0.1425 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:52:41 - INFO - root -   The F1-score is 0.6550675894185369
09/25/2022 10:52:41 - INFO - root -   the best eval f1 is 0.6551, saving model !!
09/25/2022 10:52:44 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:59  batch_loss: 0.1251 [Training] 2/118 [..............................] - ETA: 1:13  batch_loss: 0.1224 [Training] 3/118 [..............................] - ETA: 57s  batch_loss: 0.1234 [Training] 4/118 [>.............................] - ETA: 50s  batch_loss: 0.1259 [Training] 5/118 [>.............................] - ETA: 45s  batch_loss: 0.1271 [Training] 6/118 [>.............................] - ETA: 42s  batch_loss: 0.1298 [Training] 7/118 [>.............................] - ETA: 39s  batch_loss: 0.1283 [Training] 8/118 [=>............................] - ETA: 37s  batch_loss: 0.1274 [Training] 9/118 [=>............................] - ETA: 36s  batch_loss: 0.1289 [Training] 10/118 [=>............................] - ETA: 34s  batch_loss: 0.1325 [Training] 11/118 [=>............................] - ETA: 33s  batch_loss: 0.1343 [Training] 12/118 [==>...........................] - ETA: 32s  batch_loss: 0.1313 [Training] 13/118 [==>...........................] - ETA: 32s  batch_loss: 0.1315 [Training] 14/118 [==>...........................] - ETA: 31s  batch_loss: 0.1297 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 0.1345 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 0.1378 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.1347 [Training] 18/118 [===>..........................] - ETA: 29s  batch_loss: 0.1313 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.1322 [Training] 20/118 [====>.........................] - ETA: 28s  batch_loss: 0.1306 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.1295 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 0.1298 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.1309 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.1337 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.1333 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1334 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.1329 [Training] 28/118 [======>.......................] - ETA: 25s  batch_loss: 0.1318 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1328 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.1329 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1328 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1322 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.1313 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 0.1316 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1308 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.1320 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.1329 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1328 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1330 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.1320 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1315 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1321 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.1319 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1317 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1320 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.1313 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.1316 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1316 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1319 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.1325 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1322 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1322 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1316 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.1314 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1313 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1315 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.1313 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1313 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1313 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1314 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.1316 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1324 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1319 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1314 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1325 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1331 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1334 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1336 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1334 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1335 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1335 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.1334 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1337 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1336 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1340 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.1342 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1340 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1343 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1344 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1342 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1348 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1346 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1341 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1340 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1341 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1346 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1347 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1349 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1346 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1344 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.1341 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1341 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1337 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1339 [Training] 95/118 [=======================>......] - ETA: 6s  batch_loss: 0.1339 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1340 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1338 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1339 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1338 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1334 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1332 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1332 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1335 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1336 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1336 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1336 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1339 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1346 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1346 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1344 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1349 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1348 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1346 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1345 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1346 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1345 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1345 [Training] 118/118 [==============================] 258.4ms/step  batch_loss: 0.1343 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:53:21 - INFO - root -   The F1-score is 0.6500358236175341
09/25/2022 10:53:21 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:53  batch_loss: 0.1650 [Training] 2/118 [..............................] - ETA: 1:12  batch_loss: 0.1453 [Training] 3/118 [..............................] - ETA: 57s  batch_loss: 0.1426 [Training] 4/118 [>.............................] - ETA: 49s  batch_loss: 0.1295 [Training] 5/118 [>.............................] - ETA: 45s  batch_loss: 0.1256 [Training] 6/118 [>.............................] - ETA: 42s  batch_loss: 0.1259 [Training] 7/118 [>.............................] - ETA: 39s  batch_loss: 0.1268 [Training] 8/118 [=>............................] - ETA: 38s  batch_loss: 0.1245 [Training] 9/118 [=>............................] - ETA: 36s  batch_loss: 0.1251 [Training] 10/118 [=>............................] - ETA: 35s  batch_loss: 0.1272 [Training] 11/118 [=>............................] - ETA: 34s  batch_loss: 0.1304 [Training] 12/118 [==>...........................] - ETA: 33s  batch_loss: 0.1275 [Training] 13/118 [==>...........................] - ETA: 32s  batch_loss: 0.1283 [Training] 14/118 [==>...........................] - ETA: 31s  batch_loss: 0.1275 [Training] 15/118 [==>...........................] - ETA: 31s  batch_loss: 0.1267 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 0.1262 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.1265 [Training] 18/118 [===>..........................] - ETA: 29s  batch_loss: 0.1260 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.1270 [Training] 20/118 [====>.........................] - ETA: 28s  batch_loss: 0.1278 [Training] 21/118 [====>.........................] - ETA: 28s  batch_loss: 0.1292 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 0.1294 [Training] 23/118 [====>.........................] - ETA: 27s  batch_loss: 0.1289 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.1292 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.1275 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1279 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.1285 [Training] 28/118 [======>.......................] - ETA: 25s  batch_loss: 0.1293 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1282 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.1295 [Training] 31/118 [======>.......................] - ETA: 24s  batch_loss: 0.1298 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1305 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.1309 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 0.1309 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1312 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.1316 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.1313 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1309 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1305 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.1303 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1294 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1281 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.1277 [Training] 44/118 [==========>...................] - ETA: 20s  batch_loss: 0.1278 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1281 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.1274 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.1265 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1262 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1257 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.1258 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1263 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1272 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1286 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.1281 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1279 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1277 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.1272 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1268 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1263 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1266 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.1265 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1262 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1261 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1261 [Training] 65/118 [===============>..............] - ETA: 14s  batch_loss: 0.1267 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1270 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1272 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1268 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1269 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1265 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1266 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.1266 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1271 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1263 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1266 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.1272 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1269 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1279 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1279 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1282 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1282 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1283 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1285 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1281 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1284 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1292 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1299 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1296 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1293 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1290 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.1289 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1291 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1287 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1289 [Training] 95/118 [=======================>......] - ETA: 6s  batch_loss: 0.1285 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1288 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1289 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1293 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1293 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1296 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1302 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1304 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1302 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1305 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1303 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1303 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1307 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1307 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1310 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1309 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1314 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1310 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1312 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1309 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1311 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1313 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1316 [Training] 118/118 [==============================] 258.4ms/step  batch_loss: 0.1321 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:53:58 - INFO - root -   The F1-score is 0.6415579407416928
09/25/2022 10:53:58 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:55  batch_loss: 0.1547 [Training] 2/118 [..............................] - ETA: 1:12  batch_loss: 0.1357 [Training] 3/118 [..............................] - ETA: 57s  batch_loss: 0.1336 [Training] 4/118 [>.............................] - ETA: 50s  batch_loss: 0.1299 [Training] 5/118 [>.............................] - ETA: 45s  batch_loss: 0.1344 [Training] 6/118 [>.............................] - ETA: 42s  batch_loss: 0.1363 [Training] 7/118 [>.............................] - ETA: 40s  batch_loss: 0.1389 [Training] 8/118 [=>............................] - ETA: 38s  batch_loss: 0.1368 [Training] 9/118 [=>............................] - ETA: 36s  batch_loss: 0.1374 [Training] 10/118 [=>............................] - ETA: 35s  batch_loss: 0.1350 [Training] 11/118 [=>............................] - ETA: 34s  batch_loss: 0.1326 [Training] 12/118 [==>...........................] - ETA: 33s  batch_loss: 0.1314 [Training] 13/118 [==>...........................] - ETA: 32s  batch_loss: 0.1298 [Training] 14/118 [==>...........................] - ETA: 31s  batch_loss: 0.1262 [Training] 15/118 [==>...........................] - ETA: 31s  batch_loss: 0.1301 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 0.1281 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.1286 [Training] 18/118 [===>..........................] - ETA: 29s  batch_loss: 0.1282 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.1276 [Training] 20/118 [====>.........................] - ETA: 28s  batch_loss: 0.1267 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.1256 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 0.1253 [Training] 23/118 [====>.........................] - ETA: 27s  batch_loss: 0.1274 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.1270 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.1255 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1251 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.1244 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.1249 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1241 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.1235 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1243 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1254 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.1259 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1255 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1259 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.1258 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.1250 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1247 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1246 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.1237 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1240 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1244 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.1244 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1247 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1243 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.1240 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1234 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1233 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1230 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.1234 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1230 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1236 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1238 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1243 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1248 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1246 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.1244 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1239 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1247 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1246 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.1244 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1239 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1239 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1239 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1238 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1236 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1236 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1240 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1234 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1233 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1231 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.1230 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1227 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1227 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1231 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1232 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1230 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1229 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1229 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1228 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1226 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1228 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1234 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1235 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1239 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1237 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1238 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1240 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1238 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1235 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.1246 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1249 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1250 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1249 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1247 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1254 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1254 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1254 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1254 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1253 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1252 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1253 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1254 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1258 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1256 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1257 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1258 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1258 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1260 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1262 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1259 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1258 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1259 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1261 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1263 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1261 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1261 [Training] 118/118 [==============================] 257.6ms/step  batch_loss: 0.1261 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:54:36 - INFO - root -   The F1-score is 0.6496790252849469
09/25/2022 10:54:36 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:52  batch_loss: 0.1660 [Training] 2/118 [..............................] - ETA: 1:10  batch_loss: 0.1376 [Training] 3/118 [..............................] - ETA: 56s  batch_loss: 0.1232 [Training] 4/118 [>.............................] - ETA: 48s  batch_loss: 0.1173 [Training] 5/118 [>.............................] - ETA: 44s  batch_loss: 0.1160 [Training] 6/118 [>.............................] - ETA: 41s  batch_loss: 0.1137 [Training] 7/118 [>.............................] - ETA: 39s  batch_loss: 0.1078 [Training] 8/118 [=>............................] - ETA: 37s  batch_loss: 0.1062 [Training] 9/118 [=>............................] - ETA: 36s  batch_loss: 0.1056 [Training] 10/118 [=>............................] - ETA: 34s  batch_loss: 0.1121 [Training] 11/118 [=>............................] - ETA: 33s  batch_loss: 0.1104 [Training] 12/118 [==>...........................] - ETA: 32s  batch_loss: 0.1140 [Training] 13/118 [==>...........................] - ETA: 31s  batch_loss: 0.1137 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.1170 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 0.1184 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.1151 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.1174 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.1168 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.1158 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.1151 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.1139 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 0.1176 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.1175 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.1183 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.1187 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1184 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.1181 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.1173 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1178 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.1182 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1181 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1171 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.1166 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1159 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1147 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.1155 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1150 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1151 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1149 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.1147 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1146 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1143 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.1147 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1145 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1138 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.1131 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1135 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1135 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1134 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.1143 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1141 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1145 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1148 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.1144 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1139 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1141 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.1146 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1148 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1153 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1159 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.1156 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1155 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1159 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1157 [Training] 65/118 [===============>..............] - ETA: 14s  batch_loss: 0.1156 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1149 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1149 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1147 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1145 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1146 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1147 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.1147 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1154 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1164 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1160 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.1161 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1162 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1156 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1160 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1160 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1162 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1161 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1161 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1162 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1162 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1159 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1165 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1169 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1170 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1173 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.1172 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1173 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1175 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1176 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1176 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1177 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1180 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1183 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1184 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1183 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1181 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1180 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1182 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1188 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1191 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1190 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1191 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1190 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1195 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1196 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1199 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1201 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1202 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1204 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1206 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1209 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1211 [Training] 118/118 [==============================] 257.8ms/step  batch_loss: 0.1211 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 10:55:13 - INFO - root -   The F1-score is 0.6545384168763305
./all.sh: line 6: h: command not found
********** predict start ***********
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 8, 
    "event_type": "role", 
    "fine_tunning_model_path": "./output/DuEE1.0/role/best_model.pkl", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 0.001, 
    "max_len": 250, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 50.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 128, 
    "per_gpu_train_batch_size": 8, 
    "predict_save_path": "./output/DuEE1.0/role/test_result.json", 
    "seed": 66, 
    "stride": 100, 
    "test_json": "./data/DuEE1.0/subor_news.json", 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/147540 [00:00<?, ?it/s]tokenizing...:   0%|          | 147/147540 [00:00<01:40, 1463.48it/s]tokenizing...:   0%|          | 305/147540 [00:00<01:36, 1526.81it/s]tokenizing...:   0%|          | 517/147540 [00:00<01:21, 1795.86it/s]tokenizing...:   1%|          | 738/147540 [00:00<01:14, 1957.84it/s]tokenizing...:   1%|          | 934/147540 [00:00<01:15, 1933.11it/s]tokenizing...:   1%|          | 1156/147540 [00:00<01:12, 2022.65it/s]tokenizing...:   1%|          | 1359/147540 [00:00<01:31, 1598.09it/s]tokenizing...:   1%|          | 1551/147540 [00:00<01:26, 1683.29it/s]tokenizing...:   1%|          | 1783/147540 [00:00<01:18, 1858.44it/s]tokenizing...:   1%|▏         | 1979/147540 [00:01<03:29, 696.26it/s] tokenizing...:   1%|▏         | 2198/147540 [00:01<02:43, 889.49it/s]tokenizing...:   2%|▏         | 2412/147540 [00:01<02:13, 1084.94it/s]tokenizing...:   2%|▏         | 2620/147540 [00:01<01:54, 1266.41it/s]tokenizing...:   2%|▏         | 2814/147540 [00:02<01:42, 1405.49it/s]tokenizing...:   2%|▏         | 3011/147540 [00:02<01:34, 1532.61it/s]tokenizing...:   2%|▏         | 3221/147540 [00:02<01:26, 1670.92it/s]tokenizing...:   2%|▏         | 3455/147540 [00:02<01:18, 1844.56it/s]tokenizing...:   2%|▏         | 3664/147540 [00:02<01:17, 1849.67it/s]tokenizing...:   3%|▎         | 3875/147540 [00:02<01:14, 1920.42it/s]tokenizing...:   3%|▎         | 4080/147540 [00:02<01:16, 1865.14it/s]tokenizing...:   3%|▎         | 4280/147540 [00:02<01:15, 1901.37it/s]tokenizing...:   3%|▎         | 4488/147540 [00:02<01:13, 1949.21it/s]tokenizing...:   3%|▎         | 4688/147540 [00:03<01:24, 1699.09it/s]tokenizing...:   3%|▎         | 4878/147540 [00:03<01:21, 1749.46it/s]tokenizing...:   3%|▎         | 5060/147540 [00:03<01:22, 1725.01it/s]tokenizing...:   4%|▎         | 5238/147540 [00:03<01:26, 1646.57it/s]tokenizing...:   4%|▎         | 5407/147540 [00:03<01:27, 1627.98it/s]tokenizing...:   4%|▍         | 5573/147540 [00:03<01:28, 1606.94it/s]tokenizing...:   4%|▍         | 5736/147540 [00:03<01:28, 1598.86it/s]tokenizing...:   4%|▍         | 5897/147540 [00:03<01:30, 1573.54it/s]tokenizing...:   4%|▍         | 6056/147540 [00:03<01:29, 1574.29it/s]tokenizing...:   4%|▍         | 6214/147540 [00:04<01:32, 1535.92it/s]tokenizing...:   4%|▍         | 6369/147540 [00:04<01:32, 1526.89it/s]tokenizing...:   4%|▍         | 6528/147540 [00:04<01:31, 1540.79it/s]tokenizing...:   5%|▍         | 6684/147540 [00:04<01:31, 1541.80it/s]tokenizing...:   5%|▍         | 6843/147540 [00:04<01:30, 1553.34it/s]tokenizing...:   5%|▍         | 6999/147540 [00:04<01:31, 1536.22it/s]tokenizing...:   5%|▍         | 7157/147540 [00:04<01:32, 1510.76it/s]tokenizing...:   5%|▍         | 7317/147540 [00:04<01:31, 1533.57it/s]tokenizing...:   5%|▌         | 7473/147540 [00:04<01:31, 1537.11it/s]tokenizing...:   5%|▌         | 7629/147540 [00:04<01:30, 1543.00it/s]tokenizing...:   5%|▌         | 7790/147540 [00:05<01:29, 1561.11it/s]tokenizing...:   5%|▌         | 7947/147540 [00:05<01:30, 1547.71it/s]tokenizing...:   5%|▌         | 8105/147540 [00:05<01:29, 1556.42it/s]tokenizing...:   6%|▌         | 8261/147540 [00:05<01:33, 1496.82it/s]tokenizing...:   6%|▌         | 8418/147540 [00:05<01:31, 1517.67it/s]tokenizing...:   6%|▌         | 8571/147540 [00:05<01:31, 1520.66it/s]tokenizing...:   6%|▌         | 8729/147540 [00:05<01:30, 1536.82it/s]tokenizing...:   6%|▌         | 8941/147540 [00:05<01:21, 1708.38it/s]tokenizing...:   6%|▌         | 9123/147540 [00:05<01:19, 1741.30it/s]tokenizing...:   6%|▋         | 9298/147540 [00:05<01:22, 1673.31it/s]tokenizing...:   6%|▋         | 9499/147540 [00:06<01:17, 1770.55it/s]tokenizing...:   7%|▋         | 9721/147540 [00:06<01:12, 1896.80it/s]tokenizing...:   7%|▋         | 9912/147540 [00:06<01:18, 1749.85it/s]tokenizing...:   7%|▋         | 10116/147540 [00:06<01:15, 1829.51it/s]tokenizing...:   7%|▋         | 10324/147540 [00:06<01:12, 1900.49it/s]tokenizing...:   7%|▋         | 10517/147540 [00:06<01:14, 1846.45it/s]tokenizing...:   7%|▋         | 10704/147540 [00:06<01:15, 1820.13it/s]tokenizing...:   7%|▋         | 10896/147540 [00:06<01:13, 1847.94it/s]tokenizing...:   8%|▊         | 11116/147540 [00:06<01:09, 1949.89it/s]tokenizing...:   8%|▊         | 11333/147540 [00:07<01:09, 1972.45it/s]tokenizing...:   8%|▊         | 11557/147540 [00:07<01:06, 2049.83it/s]tokenizing...:   8%|▊         | 11771/147540 [00:07<01:05, 2075.90it/s]tokenizing...:   8%|▊         | 11986/147540 [00:07<01:04, 2095.34it/s]tokenizing...:   8%|▊         | 12196/147540 [00:07<01:11, 1895.05it/s]tokenizing...:   8%|▊         | 12390/147540 [00:07<01:12, 1874.70it/s]tokenizing...:   9%|▊         | 12602/147540 [00:07<01:09, 1942.16it/s]tokenizing...:   9%|▊         | 12799/147540 [00:07<01:09, 1946.22it/s]tokenizing...:   9%|▉         | 13016/147540 [00:07<01:06, 2009.97it/s]tokenizing...:   9%|▉         | 13219/147540 [00:07<01:06, 2005.42it/s]tokenizing...:   9%|▉         | 13427/147540 [00:08<01:06, 2025.56it/s]tokenizing...:   9%|▉         | 13649/147540 [00:08<01:04, 2081.96it/s]tokenizing...:   9%|▉         | 13863/147540 [00:08<01:03, 2098.05it/s]tokenizing...:  10%|▉         | 14074/147540 [00:08<01:03, 2099.35it/s]tokenizing...:  10%|▉         | 14285/147540 [00:08<01:16, 1730.98it/s]tokenizing...:  10%|▉         | 14469/147540 [00:08<01:19, 1666.13it/s]tokenizing...:  10%|▉         | 14675/147540 [00:08<01:15, 1767.24it/s]tokenizing...:  10%|█         | 14870/147540 [00:08<01:13, 1816.01it/s]tokenizing...:  10%|█         | 15095/147540 [00:08<01:08, 1934.98it/s]tokenizing...:  10%|█         | 15308/147540 [00:09<01:06, 1989.60it/s]tokenizing...:  11%|█         | 15511/147540 [00:09<01:09, 1911.73it/s]tokenizing...:  11%|█         | 15711/147540 [00:09<01:08, 1935.30it/s]tokenizing...:  11%|█         | 15914/147540 [00:09<01:07, 1959.86it/s]tokenizing...:  11%|█         | 16112/147540 [00:09<01:09, 1901.01it/s]tokenizing...:  11%|█         | 16335/147540 [00:09<01:05, 1994.85it/s]tokenizing...:  11%|█         | 16536/147540 [00:09<01:09, 1893.14it/s]tokenizing...:  11%|█▏        | 16728/147540 [00:09<01:14, 1766.51it/s]tokenizing...:  11%|█▏        | 16908/147540 [00:09<01:13, 1773.25it/s]tokenizing...:  12%|█▏        | 17088/147540 [00:10<01:16, 1699.64it/s]tokenizing...:  12%|█▏        | 17260/147540 [00:10<01:24, 1544.72it/s]tokenizing...:  12%|█▏        | 17428/147540 [00:10<01:22, 1580.31it/s]tokenizing...:  12%|█▏        | 17596/147540 [00:10<01:22, 1567.06it/s]tokenizing...:  12%|█▏        | 17766/147540 [00:10<01:21, 1601.52it/s]tokenizing...:  12%|█▏        | 17941/147540 [00:10<01:18, 1642.56it/s]tokenizing...:  12%|█▏        | 18109/147540 [00:10<01:18, 1651.88it/s]tokenizing...:  12%|█▏        | 18278/147540 [00:10<01:17, 1661.75it/s]tokenizing...:  13%|█▎        | 18473/147540 [00:10<01:13, 1744.59it/s]tokenizing...:  13%|█▎        | 18649/147540 [00:11<01:16, 1695.08it/s]tokenizing...:  13%|█▎        | 18820/147540 [00:11<01:16, 1692.39it/s]tokenizing...:  13%|█▎        | 18991/147540 [00:11<01:15, 1696.72it/s]tokenizing...:  13%|█▎        | 19162/147540 [00:11<01:15, 1696.53it/s]tokenizing...:  13%|█▎        | 19332/147540 [00:11<01:15, 1692.81it/s]tokenizing...:  13%|█▎        | 19502/147540 [00:11<01:15, 1691.42it/s]tokenizing...:  13%|█▎        | 19672/147540 [00:11<01:15, 1690.03it/s]tokenizing...:  13%|█▎        | 19842/147540 [00:11<01:17, 1640.39it/s]tokenizing...:  14%|█▎        | 20030/147540 [00:11<01:14, 1709.51it/s]tokenizing...:  14%|█▎        | 20245/147540 [00:11<01:09, 1838.89it/s]tokenizing...:  14%|█▍        | 20441/147540 [00:12<01:07, 1874.63it/s]tokenizing...:  14%|█▍        | 20637/147540 [00:12<01:06, 1898.25it/s]tokenizing...:  14%|█▍        | 20828/147540 [00:13<04:53, 431.09it/s] tokenizing...:  14%|█▍        | 20998/147540 [00:13<03:53, 542.37it/s]tokenizing...:  14%|█▍        | 21166/147540 [00:13<03:08, 669.13it/s]tokenizing...:  14%|█▍        | 21334/147540 [00:13<02:36, 807.85it/s]tokenizing...:  15%|█▍        | 21504/147540 [00:13<02:12, 953.42it/s]tokenizing...:  15%|█▍        | 21666/147540 [00:13<01:57, 1072.50it/s]tokenizing...:  15%|█▍        | 21832/147540 [00:14<01:45, 1195.00it/s]tokenizing...:  15%|█▍        | 21994/147540 [00:14<01:38, 1274.30it/s]tokenizing...:  15%|█▌        | 22159/147540 [00:14<01:31, 1364.09it/s]tokenizing...:  15%|█▌        | 22321/147540 [00:14<01:27, 1428.52it/s]tokenizing...:  15%|█▌        | 22489/147540 [00:14<01:23, 1494.69it/s]tokenizing...:  15%|█▌        | 22656/147540 [00:14<01:20, 1543.06it/s]tokenizing...:  15%|█▌        | 22820/147540 [00:14<01:20, 1555.22it/s]tokenizing...:  16%|█▌        | 22983/147540 [00:14<01:20, 1556.64it/s]tokenizing...:  16%|█▌        | 23147/147540 [00:14<01:18, 1579.58it/s]tokenizing...:  16%|█▌        | 23311/147540 [00:14<01:17, 1595.48it/s]tokenizing...:  16%|█▌        | 23478/147540 [00:15<01:16, 1617.25it/s]tokenizing...:  16%|█▌        | 23642/147540 [00:15<01:17, 1604.69it/s]tokenizing...:  16%|█▌        | 23805/147540 [00:15<01:16, 1609.78it/s]tokenizing...:  16%|█▌        | 23967/147540 [00:15<01:18, 1574.77it/s]tokenizing...:  16%|█▋        | 24127/147540 [00:15<01:18, 1575.38it/s]tokenizing...:  16%|█▋        | 24293/147540 [00:15<01:17, 1599.42it/s]tokenizing...:  17%|█▋        | 24466/147540 [00:15<01:15, 1636.78it/s]tokenizing...:  17%|█▋        | 24683/147540 [00:15<01:08, 1794.88it/s]tokenizing...:  17%|█▋        | 24890/147540 [00:15<01:05, 1872.66it/s]tokenizing...:  17%|█▋        | 25078/147540 [00:15<01:07, 1809.75it/s]tokenizing...:  17%|█▋        | 25260/147540 [00:16<01:08, 1776.03it/s]tokenizing...:  17%|█▋        | 25439/147540 [00:16<01:09, 1746.21it/s]tokenizing...:  17%|█▋        | 25645/147540 [00:16<01:06, 1834.81it/s]tokenizing...:  18%|█▊        | 25830/147540 [00:16<01:06, 1834.29it/s]tokenizing...:  18%|█▊        | 26023/147540 [00:16<01:05, 1862.35it/s]tokenizing...:  18%|█▊        | 26210/147540 [00:16<01:06, 1832.38it/s]tokenizing...:  18%|█▊        | 26414/147540 [00:16<01:04, 1891.18it/s]tokenizing...:  18%|█▊        | 26621/147540 [00:16<01:02, 1942.73it/s]tokenizing...:  18%|█▊        | 26832/147540 [00:16<01:00, 1991.38it/s]tokenizing...:  18%|█▊        | 27053/147540 [00:16<00:58, 2054.57it/s]tokenizing...:  18%|█▊        | 27259/147540 [00:17<00:59, 2032.99it/s]tokenizing...:  19%|█▊        | 27476/147540 [00:17<00:57, 2072.79it/s]tokenizing...:  19%|█▉        | 27684/147540 [00:17<00:57, 2072.76it/s]tokenizing...:  19%|█▉        | 27921/147540 [00:17<00:55, 2160.67it/s]tokenizing...:  19%|█▉        | 28138/147540 [00:17<01:03, 1871.54it/s]tokenizing...:  19%|█▉        | 28347/147540 [00:17<01:01, 1929.20it/s]tokenizing...:  19%|█▉        | 28558/147540 [00:17<01:00, 1978.16it/s]tokenizing...:  20%|█▉        | 28778/147540 [00:17<00:58, 2040.64it/s]tokenizing...:  20%|█▉        | 29002/147540 [00:17<00:56, 2096.86it/s]tokenizing...:  20%|█▉        | 29215/147540 [00:18<00:57, 2063.66it/s]tokenizing...:  20%|█▉        | 29426/147540 [00:18<00:56, 2072.63it/s]tokenizing...:  20%|██        | 29635/147540 [00:18<00:57, 2066.59it/s]tokenizing...:  20%|██        | 29849/147540 [00:18<00:56, 2086.13it/s]tokenizing...:  20%|██        | 30062/147540 [00:18<00:56, 2097.57it/s]tokenizing...:  21%|██        | 30273/147540 [00:18<00:56, 2061.81it/s]tokenizing...:  21%|██        | 30480/147540 [00:18<00:57, 2035.33it/s]tokenizing...:  21%|██        | 30698/147540 [00:18<00:56, 2077.57it/s]tokenizing...:  21%|██        | 30917/147540 [00:18<00:55, 2108.84it/s]tokenizing...:  21%|██        | 31130/147540 [00:18<00:55, 2112.54it/s]tokenizing...:  21%|██        | 31342/147540 [00:19<00:55, 2088.47it/s]tokenizing...:  21%|██▏       | 31568/147540 [00:19<00:54, 2137.02it/s]tokenizing...:  22%|██▏       | 31782/147540 [00:19<00:57, 2010.16it/s]tokenizing...:  22%|██▏       | 31985/147540 [00:19<00:58, 1991.92it/s]tokenizing...:  22%|██▏       | 32186/147540 [00:19<00:57, 1993.40it/s]tokenizing...:  22%|██▏       | 32387/147540 [00:19<01:00, 1908.49it/s]tokenizing...:  22%|██▏       | 32582/147540 [00:19<00:59, 1919.16it/s]tokenizing...:  22%|██▏       | 32779/147540 [00:19<00:59, 1933.26it/s]tokenizing...:  22%|██▏       | 32973/147540 [00:19<01:01, 1869.09it/s]tokenizing...:  22%|██▏       | 33161/147540 [00:20<01:02, 1824.13it/s]tokenizing...:  23%|██▎       | 33345/147540 [00:20<01:10, 1609.45it/s]tokenizing...:  23%|██▎       | 33511/147540 [00:20<01:11, 1595.77it/s]tokenizing...:  23%|██▎       | 33674/147540 [00:20<01:11, 1595.24it/s]tokenizing...:  23%|██▎       | 33885/147540 [00:20<01:05, 1735.18it/s]tokenizing...:  23%|██▎       | 34062/147540 [00:20<01:05, 1735.34it/s]tokenizing...:  23%|██▎       | 34238/147540 [00:20<01:05, 1728.25it/s]tokenizing...:  23%|██▎       | 34413/147540 [00:20<01:07, 1670.02it/s]tokenizing...:  23%|██▎       | 34583/147540 [00:20<01:07, 1677.08it/s]tokenizing...:  24%|██▎       | 34764/147540 [00:21<01:05, 1714.30it/s]tokenizing...:  24%|██▎       | 34937/147540 [00:21<01:05, 1715.79it/s]tokenizing...:  24%|██▍       | 35110/147540 [00:21<01:05, 1707.19it/s]tokenizing...:  24%|██▍       | 35282/147540 [00:21<01:05, 1702.61it/s]tokenizing...:  24%|██▍       | 35453/147540 [00:21<01:07, 1671.17it/s]tokenizing...:  24%|██▍       | 35622/147540 [00:21<01:06, 1674.72it/s]tokenizing...:  24%|██▍       | 35791/147540 [00:21<01:06, 1677.41it/s]tokenizing...:  24%|██▍       | 35962/147540 [00:21<01:06, 1684.50it/s]tokenizing...:  24%|██▍       | 36131/147540 [00:21<01:06, 1685.96it/s]tokenizing...:  25%|██▍       | 36300/147540 [00:21<01:06, 1684.63it/s]tokenizing...:  25%|██▍       | 36484/147540 [00:22<01:04, 1729.22it/s]tokenizing...:  25%|██▍       | 36692/147540 [00:22<01:00, 1833.08it/s]tokenizing...:  25%|██▌       | 36897/147540 [00:22<00:58, 1896.25it/s]tokenizing...:  25%|██▌       | 37087/147540 [00:22<01:08, 1619.13it/s]tokenizing...:  25%|██▌       | 37256/147540 [00:22<01:08, 1619.32it/s]tokenizing...:  25%|██▌       | 37423/147540 [00:22<01:08, 1610.72it/s]tokenizing...:  25%|██▌       | 37588/147540 [00:22<01:10, 1562.09it/s]tokenizing...:  26%|██▌       | 37750/147540 [00:22<01:09, 1574.80it/s]tokenizing...:  26%|██▌       | 37910/147540 [00:22<01:09, 1573.45it/s]tokenizing...:  26%|██▌       | 38069/147540 [00:23<01:09, 1567.45it/s]tokenizing...:  26%|██▌       | 38227/147540 [00:23<01:09, 1569.18it/s]tokenizing...:  26%|██▌       | 38387/147540 [00:23<01:09, 1577.23it/s]tokenizing...:  26%|██▌       | 38548/147540 [00:23<01:08, 1584.30it/s]tokenizing...:  26%|██▌       | 38707/147540 [00:23<01:10, 1543.93it/s]tokenizing...:  26%|██▋       | 38866/147540 [00:23<01:09, 1556.21it/s]tokenizing...:  26%|██▋       | 39025/147540 [00:23<01:09, 1565.04it/s]tokenizing...:  27%|██▋       | 39184/147540 [00:23<01:09, 1570.29it/s]tokenizing...:  27%|██▋       | 39343/147540 [00:23<01:08, 1575.75it/s]tokenizing...:  27%|██▋       | 39502/147540 [00:23<01:08, 1579.60it/s]tokenizing...:  27%|██▋       | 39661/147540 [00:24<01:10, 1526.08it/s]tokenizing...:  27%|██▋       | 39822/147540 [00:24<01:09, 1548.16it/s]tokenizing...:  27%|██▋       | 39982/147540 [00:24<01:08, 1560.17it/s]tokenizing...:  27%|██▋       | 40141/147540 [00:24<01:08, 1567.79it/s]tokenizing...:  27%|██▋       | 40304/147540 [00:24<01:07, 1585.78it/s]tokenizing...:  27%|██▋       | 40463/147540 [00:24<01:07, 1586.66it/s]tokenizing...:  28%|██▊       | 40622/147540 [00:24<01:07, 1587.16it/s]tokenizing...:  28%|██▊       | 40781/147540 [00:24<01:07, 1575.23it/s]tokenizing...:  28%|██▊       | 40964/147540 [00:24<01:04, 1646.52it/s]tokenizing...:  28%|██▊       | 41129/147540 [00:24<01:07, 1580.23it/s]tokenizing...:  28%|██▊       | 41296/147540 [00:25<01:06, 1605.55it/s]tokenizing...:  28%|██▊       | 41484/147540 [00:25<01:02, 1685.61it/s]tokenizing...:  28%|██▊       | 41654/147540 [00:25<01:03, 1671.44it/s]tokenizing...:  28%|██▊       | 41822/147540 [00:25<01:04, 1629.41it/s]tokenizing...:  28%|██▊       | 42043/147540 [00:25<00:58, 1794.91it/s]tokenizing...:  29%|██▊       | 42224/147540 [00:25<01:01, 1715.23it/s]tokenizing...:  29%|██▊       | 42401/147540 [00:25<01:00, 1729.06it/s]tokenizing...:  29%|██▉       | 42599/147540 [00:25<00:58, 1798.32it/s]tokenizing...:  29%|██▉       | 42780/147540 [00:25<00:59, 1754.47it/s]tokenizing...:  29%|██▉       | 42970/147540 [00:25<00:58, 1792.90it/s]tokenizing...:  29%|██▉       | 43174/147540 [00:26<00:56, 1863.65it/s]tokenizing...:  29%|██▉       | 43388/147540 [00:26<00:53, 1943.02it/s]tokenizing...:  30%|██▉       | 43611/147540 [00:26<00:51, 2021.34it/s]tokenizing...:  30%|██▉       | 43814/147540 [00:26<00:58, 1759.06it/s]tokenizing...:  30%|██▉       | 43997/147540 [00:27<04:13, 407.96it/s] tokenizing...:  30%|██▉       | 44243/147540 [00:27<02:59, 574.09it/s]tokenizing...:  30%|███       | 44461/147540 [00:27<02:19, 740.37it/s]tokenizing...:  30%|███       | 44677/147540 [00:28<01:51, 923.46it/s]tokenizing...:  30%|███       | 44869/147540 [00:28<01:38, 1042.18it/s]tokenizing...:  31%|███       | 45051/147540 [00:28<01:27, 1174.72it/s]tokenizing...:  31%|███       | 45281/147540 [00:28<01:13, 1400.78it/s]tokenizing...:  31%|███       | 45514/147540 [00:28<01:03, 1607.18it/s]tokenizing...:  31%|███       | 45734/147540 [00:28<00:58, 1749.84it/s]tokenizing...:  31%|███       | 45945/147540 [00:28<00:56, 1811.12it/s]tokenizing...:  31%|███▏      | 46152/147540 [00:28<00:55, 1810.90it/s]tokenizing...:  31%|███▏      | 46351/147540 [00:28<00:55, 1838.21it/s]tokenizing...:  32%|███▏      | 46548/147540 [00:29<00:59, 1687.75it/s]tokenizing...:  32%|███▏      | 46745/147540 [00:29<00:57, 1760.13it/s]tokenizing...:  32%|███▏      | 46947/147540 [00:29<00:55, 1828.34it/s]tokenizing...:  32%|███▏      | 47192/147540 [00:29<00:50, 2000.08it/s]tokenizing...:  32%|███▏      | 47421/147540 [00:29<00:48, 2082.49it/s]tokenizing...:  32%|███▏      | 47634/147540 [00:29<00:54, 1831.02it/s]tokenizing...:  32%|███▏      | 47826/147540 [00:29<01:01, 1626.07it/s]tokenizing...:  33%|███▎      | 48027/147540 [00:29<00:57, 1721.44it/s]tokenizing...:  33%|███▎      | 48211/147540 [00:29<00:56, 1745.66it/s]tokenizing...:  33%|███▎      | 48421/147540 [00:30<00:53, 1841.70it/s]tokenizing...:  33%|███▎      | 48638/147540 [00:30<00:51, 1928.97it/s]tokenizing...:  33%|███▎      | 48836/147540 [00:30<00:54, 1811.43it/s]tokenizing...:  33%|███▎      | 49022/147540 [00:30<00:56, 1748.19it/s]tokenizing...:  33%|███▎      | 49201/147540 [00:30<00:55, 1759.58it/s]tokenizing...:  33%|███▎      | 49418/147540 [00:30<00:52, 1873.08it/s]tokenizing...:  34%|███▎      | 49608/147540 [00:30<01:04, 1522.08it/s]tokenizing...:  34%|███▎      | 49781/147540 [00:30<01:02, 1573.50it/s]tokenizing...:  34%|███▍      | 50001/147540 [00:31<00:56, 1737.41it/s]tokenizing...:  34%|███▍      | 50200/147540 [00:31<00:53, 1805.80it/s]tokenizing...:  34%|███▍      | 50388/147540 [00:31<00:58, 1649.85it/s]tokenizing...:  34%|███▍      | 50606/147540 [00:31<00:54, 1789.75it/s]tokenizing...:  34%|███▍      | 50795/147540 [00:31<00:53, 1816.65it/s]tokenizing...:  35%|███▍      | 51001/147540 [00:31<00:51, 1884.05it/s]tokenizing...:  35%|███▍      | 51194/147540 [00:31<00:54, 1783.78it/s]tokenizing...:  35%|███▍      | 51418/147540 [00:31<00:50, 1909.74it/s]tokenizing...:  35%|███▍      | 51613/147540 [00:31<00:51, 1854.75it/s]tokenizing...:  35%|███▌      | 51802/147540 [00:31<00:53, 1804.88it/s]tokenizing...:  35%|███▌      | 52011/147540 [00:32<00:50, 1880.68it/s]tokenizing...:  35%|███▌      | 52201/147540 [00:32<00:52, 1823.30it/s]tokenizing...:  36%|███▌      | 52385/147540 [00:32<00:54, 1742.58it/s]tokenizing...:  36%|███▌      | 52564/147540 [00:32<00:54, 1755.07it/s]tokenizing...:  36%|███▌      | 52766/147540 [00:32<00:51, 1829.13it/s]tokenizing...:  36%|███▌      | 52951/147540 [00:32<00:53, 1764.70it/s]tokenizing...:  36%|███▌      | 53129/147540 [00:32<00:53, 1753.77it/s]tokenizing...:  36%|███▌      | 53306/147540 [00:32<00:55, 1690.88it/s]tokenizing...:  36%|███▌      | 53476/147540 [00:32<00:55, 1691.01it/s]tokenizing...:  36%|███▋      | 53669/147540 [00:33<00:53, 1758.83it/s]tokenizing...:  36%|███▋      | 53846/147540 [00:33<00:53, 1741.13it/s]tokenizing...:  37%|███▋      | 54021/147540 [00:33<00:54, 1724.61it/s]tokenizing...:  37%|███▋      | 54194/147540 [00:33<00:56, 1660.09it/s]tokenizing...:  37%|███▋      | 54361/147540 [00:33<00:57, 1625.98it/s]tokenizing...:  37%|███▋      | 54529/147540 [00:33<00:56, 1640.33it/s]tokenizing...:  37%|███▋      | 54694/147540 [00:33<00:56, 1633.11it/s]tokenizing...:  37%|███▋      | 54863/147540 [00:33<00:56, 1648.91it/s]tokenizing...:  37%|███▋      | 55032/147540 [00:33<00:55, 1659.18it/s]tokenizing...:  37%|███▋      | 55232/147540 [00:33<00:52, 1757.82it/s]tokenizing...:  38%|███▊      | 55421/147540 [00:34<00:51, 1796.72it/s]tokenizing...:  38%|███▊      | 55623/147540 [00:34<00:49, 1860.50it/s]tokenizing...:  38%|███▊      | 55810/147540 [00:34<00:52, 1737.48it/s]tokenizing...:  38%|███▊      | 55986/147540 [00:34<00:53, 1719.24it/s]tokenizing...:  38%|███▊      | 56160/147540 [00:34<00:53, 1721.30it/s]tokenizing...:  38%|███▊      | 56333/147540 [00:34<00:53, 1715.96it/s]tokenizing...:  38%|███▊      | 56506/147540 [00:34<00:54, 1656.62it/s]tokenizing...:  38%|███▊      | 56673/147540 [00:34<00:54, 1660.08it/s]tokenizing...:  39%|███▊      | 56843/147540 [00:34<00:54, 1669.63it/s]tokenizing...:  39%|███▊      | 57011/147540 [00:35<00:54, 1656.93it/s]tokenizing...:  39%|███▉      | 57179/147540 [00:35<00:54, 1662.14it/s]tokenizing...:  39%|███▉      | 57348/147540 [00:35<00:54, 1668.96it/s]tokenizing...:  39%|███▉      | 57516/147540 [00:35<00:55, 1629.40it/s]tokenizing...:  39%|███▉      | 57684/147540 [00:35<00:54, 1641.89it/s]tokenizing...:  39%|███▉      | 57852/147540 [00:35<00:54, 1652.46it/s]tokenizing...:  39%|███▉      | 58023/147540 [00:35<00:53, 1666.27it/s]tokenizing...:  39%|███▉      | 58190/147540 [00:35<00:54, 1650.18it/s]tokenizing...:  40%|███▉      | 58361/147540 [00:35<00:53, 1665.54it/s]tokenizing...:  40%|███▉      | 58528/147540 [00:35<00:54, 1629.99it/s]tokenizing...:  40%|███▉      | 58697/147540 [00:36<00:53, 1646.74it/s]tokenizing...:  40%|███▉      | 58866/147540 [00:36<00:53, 1655.64it/s]tokenizing...:  40%|████      | 59035/147540 [00:36<00:53, 1665.31it/s]tokenizing...:  40%|████      | 59204/147540 [00:36<00:52, 1670.32it/s]tokenizing...:  40%|████      | 59373/147540 [00:36<00:52, 1671.66it/s]tokenizing...:  40%|████      | 59543/147540 [00:36<00:52, 1672.28it/s]tokenizing...:  41%|████      | 59758/147540 [00:36<00:48, 1811.79it/s]tokenizing...:  41%|████      | 59942/147540 [00:36<00:48, 1817.29it/s]tokenizing...:  41%|████      | 60132/147540 [00:36<00:47, 1838.95it/s]tokenizing...:  41%|████      | 60340/147540 [00:36<00:45, 1910.68it/s]tokenizing...:  41%|████      | 60558/147540 [00:37<00:43, 1989.49it/s]tokenizing...:  41%|████      | 60758/147540 [00:37<00:45, 1918.05it/s]tokenizing...:  41%|████▏     | 60951/147540 [00:37<00:45, 1891.12it/s]tokenizing...:  41%|████▏     | 61141/147540 [00:37<00:49, 1733.80it/s]tokenizing...:  42%|████▏     | 61317/147540 [00:37<00:58, 1481.76it/s]tokenizing...:  42%|████▏     | 61483/147540 [00:37<00:56, 1525.26it/s]tokenizing...:  42%|████▏     | 61656/147540 [00:37<00:54, 1578.10it/s]tokenizing...:  42%|████▏     | 61825/147540 [00:37<00:53, 1604.15it/s]tokenizing...:  42%|████▏     | 62046/147540 [00:37<00:48, 1770.30it/s]tokenizing...:  42%|████▏     | 62268/147540 [00:38<00:44, 1898.26it/s]tokenizing...:  42%|████▏     | 62500/147540 [00:38<00:42, 2020.05it/s]tokenizing...:  43%|████▎     | 62705/147540 [00:38<00:43, 1931.89it/s]tokenizing...:  43%|████▎     | 62901/147540 [00:38<00:44, 1911.26it/s]tokenizing...:  43%|████▎     | 63111/147540 [00:38<00:42, 1963.58it/s]tokenizing...:  43%|████▎     | 63312/147540 [00:38<00:42, 1975.33it/s]tokenizing...:  43%|████▎     | 63511/147540 [00:38<00:42, 1956.79it/s]tokenizing...:  43%|████▎     | 63708/147540 [00:38<00:43, 1928.67it/s]tokenizing...:  43%|████▎     | 63902/147540 [00:38<00:43, 1911.02it/s]tokenizing...:  43%|████▎     | 64116/147540 [00:39<00:42, 1970.72it/s]tokenizing...:  44%|████▎     | 64375/147540 [00:39<00:38, 2150.70it/s]tokenizing...:  44%|████▍     | 64640/147540 [00:39<00:36, 2296.64it/s]tokenizing...:  44%|████▍     | 64871/147540 [00:39<00:37, 2176.46it/s]tokenizing...:  44%|████▍     | 65128/147540 [00:39<00:36, 2286.07it/s]tokenizing...:  44%|████▍     | 65365/147540 [00:39<00:35, 2309.18it/s]tokenizing...:  45%|████▍     | 65673/147540 [00:39<00:32, 2532.74it/s]tokenizing...:  45%|████▍     | 65928/147540 [00:39<00:33, 2445.76it/s]tokenizing...:  45%|████▍     | 66175/147540 [00:39<00:33, 2404.92it/s]tokenizing...:  45%|████▌     | 66417/147540 [00:39<00:34, 2337.08it/s]tokenizing...:  45%|████▌     | 66652/147540 [00:40<00:35, 2271.47it/s]tokenizing...:  45%|████▌     | 66880/147540 [00:40<00:37, 2169.68it/s]tokenizing...:  45%|████▌     | 67099/147540 [00:40<00:37, 2172.49it/s]tokenizing...:  46%|████▌     | 67318/147540 [00:40<00:37, 2162.80it/s]tokenizing...:  46%|████▌     | 67543/147540 [00:40<00:36, 2187.22it/s]tokenizing...:  46%|████▌     | 67763/147540 [00:40<00:37, 2132.88it/s]tokenizing...:  46%|████▌     | 67977/147540 [00:40<00:40, 1944.80it/s]tokenizing...:  46%|████▌     | 68175/147540 [00:40<00:41, 1898.75it/s]tokenizing...:  46%|████▋     | 68367/147540 [00:40<00:43, 1831.76it/s]tokenizing...:  46%|████▋     | 68552/147540 [00:41<00:43, 1815.53it/s]tokenizing...:  47%|████▋     | 68735/147540 [00:41<00:44, 1769.61it/s]tokenizing...:  47%|████▋     | 68913/147540 [00:41<00:44, 1761.34it/s]tokenizing...:  47%|████▋     | 69090/147540 [00:41<00:45, 1736.30it/s]tokenizing...:  47%|████▋     | 69286/147540 [00:41<00:43, 1797.20it/s]tokenizing...:  47%|████▋     | 69467/147540 [00:41<00:44, 1763.08it/s]tokenizing...:  47%|████▋     | 69644/147540 [00:41<00:44, 1738.19it/s]tokenizing...:  47%|████▋     | 69819/147540 [00:41<00:45, 1705.06it/s]tokenizing...:  47%|████▋     | 69990/147540 [00:41<00:46, 1667.91it/s]tokenizing...:  48%|████▊     | 70159/147540 [00:42<00:46, 1673.45it/s]tokenizing...:  48%|████▊     | 70343/147540 [00:42<00:44, 1719.71it/s]tokenizing...:  48%|████▊     | 70536/147540 [00:42<00:43, 1780.85it/s]tokenizing...:  48%|████▊     | 70715/147540 [00:42<00:43, 1751.72it/s]tokenizing...:  48%|████▊     | 70891/147540 [00:42<00:44, 1728.79it/s]tokenizing...:  48%|████▊     | 71065/147540 [00:42<00:45, 1666.36it/s]tokenizing...:  48%|████▊     | 71235/147540 [00:42<00:45, 1675.10it/s]tokenizing...:  48%|████▊     | 71405/147540 [00:42<00:45, 1680.30it/s]tokenizing...:  49%|████▊     | 71574/147540 [00:42<00:45, 1682.41it/s]tokenizing...:  49%|████▊     | 71744/147540 [00:42<00:44, 1686.56it/s]tokenizing...:  49%|████▊     | 71916/147540 [00:43<00:44, 1693.43it/s]tokenizing...:  49%|████▉     | 72086/147540 [00:43<00:46, 1609.66it/s]tokenizing...:  49%|████▉     | 72264/147540 [00:43<00:45, 1657.43it/s]tokenizing...:  49%|████▉     | 72433/147540 [00:43<00:45, 1666.07it/s]tokenizing...:  49%|████▉     | 72601/147540 [00:43<00:44, 1666.02it/s]tokenizing...:  49%|████▉     | 72769/147540 [00:43<00:44, 1667.39it/s]tokenizing...:  49%|████▉     | 72937/147540 [00:43<00:44, 1668.91it/s]tokenizing...:  50%|████▉     | 73105/147540 [00:43<00:46, 1603.87it/s]tokenizing...:  50%|████▉     | 73267/147540 [00:45<04:17, 287.92it/s] tokenizing...:  50%|████▉     | 73433/147540 [00:45<03:13, 382.27it/s]tokenizing...:  50%|████▉     | 73598/147540 [00:45<02:29, 495.42it/s]tokenizing...:  50%|████▉     | 73755/147540 [00:45<01:59, 617.18it/s]tokenizing...:  50%|█████     | 73923/147540 [00:45<01:36, 761.69it/s]tokenizing...:  50%|█████     | 74087/147540 [00:45<01:21, 906.47it/s]tokenizing...:  50%|█████     | 74242/147540 [00:46<01:12, 1015.02it/s]tokenizing...:  50%|█████     | 74405/147540 [00:46<01:03, 1144.65it/s]tokenizing...:  51%|█████     | 74567/147540 [00:46<00:58, 1252.91it/s]tokenizing...:  51%|█████     | 74724/147540 [00:46<00:54, 1328.91it/s]tokenizing...:  51%|█████     | 74897/147540 [00:46<00:50, 1432.89it/s]tokenizing...:  51%|█████     | 75059/147540 [00:46<00:49, 1472.87it/s]tokenizing...:  51%|█████     | 75224/147540 [00:46<00:47, 1520.43it/s]tokenizing...:  51%|█████     | 75397/147540 [00:46<00:45, 1579.85it/s]tokenizing...:  51%|█████     | 75562/147540 [00:46<00:50, 1424.53it/s]tokenizing...:  51%|█████▏    | 75750/147540 [00:47<00:46, 1545.76it/s]tokenizing...:  51%|█████▏    | 75948/147540 [00:47<00:42, 1665.33it/s]tokenizing...:  52%|█████▏    | 76176/147540 [00:47<00:38, 1836.90it/s]tokenizing...:  52%|█████▏    | 76365/147540 [00:47<00:44, 1599.60it/s]tokenizing...:  52%|█████▏    | 76534/147540 [00:47<00:45, 1575.93it/s]tokenizing...:  52%|█████▏    | 76746/147540 [00:47<00:41, 1718.70it/s]tokenizing...:  52%|█████▏    | 76951/147540 [00:47<00:39, 1809.48it/s]tokenizing...:  52%|█████▏    | 77150/147540 [00:47<00:37, 1859.48it/s]tokenizing...:  52%|█████▏    | 77355/147540 [00:47<00:36, 1898.15it/s]tokenizing...:  53%|█████▎    | 77548/147540 [00:48<00:39, 1756.02it/s]tokenizing...:  53%|█████▎    | 77754/147540 [00:48<00:37, 1838.20it/s]tokenizing...:  53%|█████▎    | 77982/147540 [00:48<00:35, 1960.86it/s]tokenizing...:  53%|█████▎    | 78182/147540 [00:48<00:35, 1970.01it/s]tokenizing...:  53%|█████▎    | 78382/147540 [00:48<00:35, 1934.97it/s]tokenizing...:  53%|█████▎    | 78578/147540 [00:48<00:36, 1876.52it/s]tokenizing...:  53%|█████▎    | 78768/147540 [00:48<00:37, 1838.45it/s]tokenizing...:  54%|█████▎    | 78970/147540 [00:48<00:36, 1888.53it/s]tokenizing...:  54%|█████▎    | 79165/147540 [00:48<00:35, 1902.89it/s]tokenizing...:  54%|█████▍    | 79357/147540 [00:48<00:38, 1776.16it/s]tokenizing...:  54%|█████▍    | 79537/147540 [00:49<00:40, 1680.07it/s]tokenizing...:  54%|█████▍    | 79708/147540 [00:49<00:40, 1669.32it/s]tokenizing...:  54%|█████▍    | 79906/147540 [00:49<00:38, 1753.89it/s]tokenizing...:  54%|█████▍    | 80084/147540 [00:49<00:38, 1758.63it/s]tokenizing...:  54%|█████▍    | 80261/147540 [00:49<00:39, 1682.68it/s]tokenizing...:  55%|█████▍    | 80431/147540 [00:49<00:44, 1513.54it/s]tokenizing...:  55%|█████▍    | 80633/147540 [00:49<00:40, 1647.13it/s]tokenizing...:  55%|█████▍    | 80859/147540 [00:49<00:36, 1813.88it/s]tokenizing...:  55%|█████▍    | 81088/147540 [00:49<00:34, 1947.08it/s]tokenizing...:  55%|█████▌    | 81308/147540 [00:50<00:32, 2010.09it/s]tokenizing...:  55%|█████▌    | 81512/147540 [00:50<00:37, 1773.64it/s]tokenizing...:  55%|█████▌    | 81697/147540 [00:50<00:38, 1729.83it/s]tokenizing...:  56%|█████▌    | 81905/147540 [00:50<00:36, 1822.11it/s]tokenizing...:  56%|█████▌    | 82092/147540 [00:50<00:35, 1823.62it/s]tokenizing...:  56%|█████▌    | 82313/147540 [00:50<00:33, 1931.90it/s]tokenizing...:  56%|█████▌    | 82509/147540 [00:50<00:35, 1845.50it/s]tokenizing...:  56%|█████▌    | 82754/147540 [00:50<00:32, 2013.47it/s]tokenizing...:  56%|█████▋    | 83013/147540 [00:50<00:29, 2177.46it/s]tokenizing...:  56%|█████▋    | 83234/147540 [00:51<00:29, 2176.40it/s]tokenizing...:  57%|█████▋    | 83483/147540 [00:51<00:28, 2267.69it/s]tokenizing...:  57%|█████▋    | 83712/147540 [00:51<00:28, 2264.51it/s]tokenizing...:  57%|█████▋    | 84026/147540 [00:51<00:25, 2521.70it/s]tokenizing...:  57%|█████▋    | 84280/147540 [00:51<00:26, 2405.92it/s]tokenizing...:  57%|█████▋    | 84523/147540 [00:51<00:26, 2399.23it/s]tokenizing...:  57%|█████▋    | 84765/147540 [00:51<00:27, 2256.17it/s]tokenizing...:  58%|█████▊    | 84993/147540 [00:51<00:28, 2195.67it/s]tokenizing...:  58%|█████▊    | 85215/147540 [00:51<00:28, 2156.19it/s]tokenizing...:  58%|█████▊    | 85432/147540 [00:52<00:28, 2150.46it/s]tokenizing...:  58%|█████▊    | 85648/147540 [00:52<00:28, 2152.62it/s]tokenizing...:  58%|█████▊    | 85864/147540 [00:52<00:29, 2110.39it/s]tokenizing...:  58%|█████▊    | 86076/147540 [00:52<00:30, 2016.56it/s]tokenizing...:  58%|█████▊    | 86279/147540 [00:52<00:32, 1876.61it/s]tokenizing...:  59%|█████▊    | 86469/147540 [00:52<00:33, 1845.52it/s]tokenizing...:  59%|█████▊    | 86655/147540 [00:52<00:32, 1845.19it/s]tokenizing...:  59%|█████▉    | 86841/147540 [00:52<00:33, 1809.35it/s]tokenizing...:  59%|█████▉    | 87052/147540 [00:52<00:31, 1893.21it/s]tokenizing...:  59%|█████▉    | 87243/147540 [00:53<00:32, 1857.18it/s]tokenizing...:  59%|█████▉    | 87436/147540 [00:53<00:32, 1875.26it/s]tokenizing...:  59%|█████▉    | 87625/147540 [00:53<00:33, 1801.10it/s]tokenizing...:  60%|█████▉    | 87806/147540 [00:53<00:33, 1783.08it/s]tokenizing...:  60%|█████▉    | 87985/147540 [00:53<00:34, 1715.03it/s]tokenizing...:  60%|█████▉    | 88170/147540 [00:53<00:33, 1751.62it/s]tokenizing...:  60%|█████▉    | 88352/147540 [00:53<00:33, 1770.10it/s]tokenizing...:  60%|██████    | 88530/147540 [00:53<00:35, 1661.79it/s]tokenizing...:  60%|██████    | 88698/147540 [00:53<00:36, 1634.35it/s]tokenizing...:  60%|██████    | 88865/147540 [00:53<00:35, 1642.02it/s]tokenizing...:  60%|██████    | 89037/147540 [00:54<00:35, 1659.71it/s]tokenizing...:  60%|██████    | 89227/147540 [00:54<00:33, 1729.26it/s]tokenizing...:  61%|██████    | 89421/147540 [00:54<00:32, 1789.84it/s]tokenizing...:  61%|██████    | 89605/147540 [00:54<00:32, 1804.09it/s]tokenizing...:  61%|██████    | 89799/147540 [00:54<00:31, 1842.36it/s]tokenizing...:  61%|██████    | 89984/147540 [00:54<00:31, 1836.99it/s]tokenizing...:  61%|██████    | 90168/147540 [00:54<00:31, 1814.18it/s]tokenizing...:  61%|██████    | 90350/147540 [00:54<00:31, 1795.34it/s]tokenizing...:  61%|██████▏   | 90530/147540 [00:54<00:33, 1718.44it/s]tokenizing...:  61%|██████▏   | 90703/147540 [00:55<00:34, 1632.37it/s]tokenizing...:  62%|██████▏   | 90868/147540 [00:55<00:37, 1495.13it/s]tokenizing...:  62%|██████▏   | 91052/147540 [00:55<00:35, 1583.11it/s]tokenizing...:  62%|██████▏   | 91264/147540 [00:55<00:32, 1731.15it/s]tokenizing...:  62%|██████▏   | 91477/147540 [00:55<00:30, 1842.88it/s]tokenizing...:  62%|██████▏   | 91667/147540 [00:55<00:30, 1858.44it/s]tokenizing...:  62%|██████▏   | 91862/147540 [00:55<00:29, 1883.85it/s]tokenizing...:  62%|██████▏   | 92066/147540 [00:55<00:28, 1926.84it/s]tokenizing...:  63%|██████▎   | 92274/147540 [00:55<00:28, 1970.48it/s]tokenizing...:  63%|██████▎   | 92485/147540 [00:55<00:27, 2011.64it/s]tokenizing...:  63%|██████▎   | 92687/147540 [00:56<00:27, 1999.26it/s]tokenizing...:  63%|██████▎   | 92926/147540 [00:56<00:25, 2114.77it/s]tokenizing...:  63%|██████▎   | 93138/147540 [00:56<00:26, 2078.52it/s]tokenizing...:  63%|██████▎   | 93357/147540 [00:56<00:25, 2110.77it/s]tokenizing...:  63%|██████▎   | 93578/147540 [00:56<00:25, 2139.62it/s]tokenizing...:  64%|██████▎   | 93793/147540 [00:56<00:25, 2135.05it/s]tokenizing...:  64%|██████▎   | 94007/147540 [00:56<00:25, 2120.86it/s]tokenizing...:  64%|██████▍   | 94220/147540 [00:56<00:25, 2108.93it/s]tokenizing...:  64%|██████▍   | 94432/147540 [00:56<00:25, 2095.27it/s]tokenizing...:  64%|██████▍   | 94650/147540 [00:56<00:24, 2116.38it/s]tokenizing...:  64%|██████▍   | 94871/147540 [00:57<00:24, 2142.15it/s]tokenizing...:  64%|██████▍   | 95086/147540 [00:57<00:24, 2101.12it/s]tokenizing...:  65%|██████▍   | 95306/147540 [00:57<00:24, 2128.83it/s]tokenizing...:  65%|██████▍   | 95534/147540 [00:57<00:23, 2172.22it/s]tokenizing...:  65%|██████▍   | 95752/147540 [00:57<00:24, 2094.10it/s]tokenizing...:  65%|██████▌   | 95972/147540 [00:57<00:24, 2122.13it/s]tokenizing...:  65%|██████▌   | 96185/147540 [00:57<00:24, 2079.00it/s]tokenizing...:  65%|██████▌   | 96401/147540 [00:57<00:24, 2102.28it/s]tokenizing...:  65%|██████▌   | 96612/147540 [00:57<00:25, 2003.91it/s]tokenizing...:  66%|██████▌   | 96814/147540 [00:58<00:25, 1994.11it/s]tokenizing...:  66%|██████▌   | 97015/147540 [00:58<00:26, 1891.20it/s]tokenizing...:  66%|██████▌   | 97206/147540 [00:58<00:28, 1770.98it/s]tokenizing...:  66%|██████▌   | 97386/147540 [00:58<00:28, 1752.81it/s]tokenizing...:  66%|██████▌   | 97563/147540 [00:58<00:29, 1713.17it/s]tokenizing...:  66%|██████▌   | 97736/147540 [00:58<00:29, 1706.23it/s]tokenizing...:  66%|██████▋   | 97908/147540 [00:58<00:29, 1697.20it/s]tokenizing...:  66%|██████▋   | 98079/147540 [00:58<00:29, 1692.97it/s]tokenizing...:  67%|██████▋   | 98249/147540 [00:58<00:29, 1645.64it/s]tokenizing...:  67%|██████▋   | 98420/147540 [00:58<00:29, 1661.92it/s]tokenizing...:  67%|██████▋   | 98587/147540 [00:59<00:29, 1648.78it/s]tokenizing...:  67%|██████▋   | 98756/147540 [00:59<00:29, 1655.59it/s]tokenizing...:  67%|██████▋   | 98924/147540 [00:59<00:29, 1660.67it/s]tokenizing...:  67%|██████▋   | 99092/147540 [00:59<00:29, 1665.89it/s]tokenizing...:  67%|██████▋   | 99259/147540 [00:59<00:29, 1653.76it/s]tokenizing...:  67%|██████▋   | 99425/147540 [00:59<00:30, 1601.78it/s]tokenizing...:  68%|██████▊   | 99593/147540 [00:59<00:29, 1623.09it/s]tokenizing...:  68%|██████▊   | 99761/147540 [00:59<00:29, 1639.20it/s]tokenizing...:  68%|██████▊   | 99926/147540 [00:59<00:29, 1640.28it/s]tokenizing...:  68%|██████▊   | 100095/147540 [01:00<00:28, 1651.44it/s]tokenizing...:  68%|██████▊   | 100261/147540 [01:00<00:28, 1645.06it/s]tokenizing...:  68%|██████▊   | 100426/147540 [01:00<00:28, 1624.76it/s]tokenizing...:  68%|██████▊   | 100647/147540 [01:00<00:26, 1796.46it/s]tokenizing...:  68%|██████▊   | 100827/147540 [01:00<00:28, 1661.25it/s]tokenizing...:  68%|██████▊   | 100996/147540 [01:00<00:33, 1386.28it/s]tokenizing...:  69%|██████▊   | 101144/147540 [01:00<00:33, 1403.39it/s]tokenizing...:  69%|██████▊   | 101332/147540 [01:00<00:30, 1527.90it/s]tokenizing...:  69%|██████▉   | 101527/147540 [01:00<00:28, 1641.07it/s]tokenizing...:  69%|██████▉   | 101730/147540 [01:01<00:26, 1746.55it/s]tokenizing...:  69%|██████▉   | 101934/147540 [01:01<00:24, 1827.72it/s]tokenizing...:  69%|██████▉   | 102135/147540 [01:01<00:24, 1879.38it/s]tokenizing...:  69%|██████▉   | 102354/147540 [01:01<00:22, 1969.75it/s]tokenizing...:  70%|██████▉   | 102554/147540 [01:01<00:23, 1937.39it/s]tokenizing...:  70%|██████▉   | 102776/147540 [01:01<00:22, 2015.67it/s]tokenizing...:  70%|██████▉   | 102988/147540 [01:01<00:21, 2045.45it/s]tokenizing...:  70%|██████▉   | 103197/147540 [01:01<00:21, 2055.68it/s]tokenizing...:  70%|███████   | 103408/147540 [01:01<00:21, 2070.30it/s]tokenizing...:  70%|███████   | 103631/147540 [01:01<00:20, 2113.75it/s]tokenizing...:  70%|███████   | 103843/147540 [01:02<00:21, 2009.87it/s]tokenizing...:  71%|███████   | 104072/147540 [01:02<00:20, 2086.70it/s]tokenizing...:  71%|███████   | 104293/147540 [01:02<00:20, 2122.12it/s]tokenizing...:  71%|███████   | 104540/147540 [01:02<00:19, 2224.17it/s]tokenizing...:  71%|███████   | 104764/147540 [01:02<00:19, 2186.67it/s]tokenizing...:  71%|███████   | 104984/147540 [01:02<00:19, 2187.87it/s]tokenizing...:  71%|███████▏  | 105234/147540 [01:02<00:18, 2278.93it/s]tokenizing...:  71%|███████▏  | 105474/147540 [01:02<00:18, 2311.90it/s]tokenizing...:  72%|███████▏  | 105733/147540 [01:02<00:17, 2393.82it/s]tokenizing...:  72%|███████▏  | 106007/147540 [01:02<00:16, 2495.91it/s]tokenizing...:  72%|███████▏  | 106257/147540 [01:03<00:16, 2460.21it/s]tokenizing...:  72%|███████▏  | 106504/147540 [01:03<00:18, 2229.00it/s]tokenizing...:  72%|███████▏  | 106732/147540 [01:03<00:18, 2148.18it/s]tokenizing...:  72%|███████▏  | 106950/147540 [01:03<00:18, 2137.25it/s]tokenizing...:  73%|███████▎  | 107166/147540 [01:03<00:18, 2131.36it/s]tokenizing...:  73%|███████▎  | 107388/147540 [01:03<00:18, 2154.35it/s]tokenizing...:  73%|███████▎  | 107605/147540 [01:03<00:18, 2152.70it/s]tokenizing...:  73%|███████▎  | 107822/147540 [01:03<00:18, 2114.67it/s]tokenizing...:  73%|███████▎  | 108035/147540 [01:03<00:18, 2095.33it/s]tokenizing...:  73%|███████▎  | 108245/147540 [01:04<00:20, 1890.03it/s]tokenizing...:  74%|███████▎  | 108474/147540 [01:04<00:19, 1997.30it/s]tokenizing...:  74%|███████▎  | 108678/147540 [01:04<00:19, 1981.97it/s]tokenizing...:  74%|███████▍  | 108879/147540 [01:04<00:19, 1952.87it/s]tokenizing...:  74%|███████▍  | 109085/147540 [01:04<00:19, 1982.32it/s]tokenizing...:  74%|███████▍  | 109285/147540 [01:04<00:19, 1944.86it/s]tokenizing...:  74%|███████▍  | 109481/147540 [01:04<00:20, 1857.92it/s]tokenizing...:  74%|███████▍  | 109669/147540 [01:04<00:20, 1839.25it/s]tokenizing...:  74%|███████▍  | 109854/147540 [01:07<02:41, 232.90it/s] tokenizing...:  75%|███████▍  | 110021/147540 [01:07<02:03, 302.91it/s]tokenizing...:  75%|███████▍  | 110189/147540 [01:07<01:35, 392.46it/s]tokenizing...:  75%|███████▍  | 110400/147540 [01:07<01:09, 538.02it/s]tokenizing...:  75%|███████▍  | 110580/147540 [01:07<00:54, 674.27it/s]tokenizing...:  75%|███████▌  | 110751/147540 [01:07<00:45, 810.26it/s]tokenizing...:  75%|███████▌  | 110921/147540 [01:07<00:39, 935.54it/s]tokenizing...:  75%|███████▌  | 111088/147540 [01:08<00:34, 1070.10it/s]tokenizing...:  75%|███████▌  | 111257/147540 [01:08<00:30, 1197.98it/s]tokenizing...:  76%|███████▌  | 111441/147540 [01:08<00:26, 1344.19it/s]tokenizing...:  76%|███████▌  | 111624/147540 [01:08<00:24, 1462.50it/s]tokenizing...:  76%|███████▌  | 111799/147540 [01:08<00:23, 1520.05it/s]tokenizing...:  76%|███████▌  | 111972/147540 [01:08<00:23, 1528.83it/s]tokenizing...:  76%|███████▌  | 112140/147540 [01:08<00:22, 1569.51it/s]tokenizing...:  76%|███████▌  | 112308/147540 [01:08<00:22, 1575.82it/s]tokenizing...:  76%|███████▌  | 112475/147540 [01:08<00:21, 1599.57it/s]tokenizing...:  76%|███████▋  | 112646/147540 [01:08<00:21, 1627.89it/s]tokenizing...:  76%|███████▋  | 112814/147540 [01:09<00:21, 1640.31it/s]tokenizing...:  77%|███████▋  | 112981/147540 [01:09<00:21, 1609.50it/s]tokenizing...:  77%|███████▋  | 113188/147540 [01:09<00:19, 1742.12it/s]tokenizing...:  77%|███████▋  | 113382/147540 [01:09<00:19, 1796.95it/s]tokenizing...:  77%|███████▋  | 113584/147540 [01:09<00:18, 1861.63it/s]tokenizing...:  77%|███████▋  | 113772/147540 [01:09<00:18, 1847.38it/s]tokenizing...:  77%|███████▋  | 113958/147540 [01:09<00:18, 1815.69it/s]tokenizing...:  77%|███████▋  | 114141/147540 [01:09<00:18, 1767.48it/s]tokenizing...:  77%|███████▋  | 114319/147540 [01:09<00:19, 1714.84it/s]tokenizing...:  78%|███████▊  | 114492/147540 [01:10<00:19, 1703.91it/s]tokenizing...:  78%|███████▊  | 114663/147540 [01:10<00:19, 1700.43it/s]tokenizing...:  78%|███████▊  | 114834/147540 [01:10<00:19, 1702.01it/s]tokenizing...:  78%|███████▊  | 115005/147540 [01:10<00:19, 1675.39it/s]tokenizing...:  78%|███████▊  | 115173/147540 [01:10<00:19, 1671.61it/s]tokenizing...:  78%|███████▊  | 115342/147540 [01:10<00:19, 1673.20it/s]tokenizing...:  78%|███████▊  | 115513/147540 [01:10<00:19, 1682.06it/s]tokenizing...:  78%|███████▊  | 115682/147540 [01:10<00:19, 1660.50it/s]tokenizing...:  79%|███████▊  | 115852/147540 [01:10<00:18, 1668.37it/s]tokenizing...:  79%|███████▊  | 116020/147540 [01:10<00:18, 1671.48it/s]tokenizing...:  79%|███████▉  | 116188/147540 [01:11<00:19, 1638.16it/s]tokenizing...:  79%|███████▉  | 116356/147540 [01:11<00:18, 1646.38it/s]tokenizing...:  79%|███████▉  | 116521/147540 [01:11<00:18, 1636.27it/s]tokenizing...:  79%|███████▉  | 116689/147540 [01:11<00:18, 1649.09it/s]tokenizing...:  79%|███████▉  | 116858/147540 [01:11<00:18, 1659.89it/s]tokenizing...:  79%|███████▉  | 117027/147540 [01:11<00:18, 1668.12it/s]tokenizing...:  79%|███████▉  | 117194/147540 [01:11<00:18, 1633.31it/s]tokenizing...:  80%|███████▉  | 117358/147540 [01:11<00:18, 1626.07it/s]tokenizing...:  80%|███████▉  | 117527/147540 [01:11<00:18, 1644.44it/s]tokenizing...:  80%|███████▉  | 117699/147540 [01:11<00:17, 1665.82it/s]tokenizing...:  80%|███████▉  | 117906/147540 [01:12<00:16, 1784.99it/s]tokenizing...:  80%|████████  | 118100/147540 [01:12<00:16, 1830.43it/s]tokenizing...:  80%|████████  | 118284/147540 [01:12<00:18, 1571.03it/s]tokenizing...:  80%|████████  | 118479/147540 [01:12<00:17, 1670.70it/s]tokenizing...:  80%|████████  | 118703/147540 [01:12<00:15, 1826.41it/s]tokenizing...:  81%|████████  | 118923/147540 [01:12<00:14, 1931.08it/s]tokenizing...:  81%|████████  | 119137/147540 [01:12<00:14, 1990.44it/s]tokenizing...:  81%|████████  | 119340/147540 [01:12<00:19, 1482.38it/s]tokenizing...:  81%|████████  | 119510/147540 [01:13<00:19, 1430.91it/s]tokenizing...:  81%|████████  | 119669/147540 [01:13<00:18, 1467.91it/s]tokenizing...:  81%|████████  | 119861/147540 [01:13<00:17, 1581.93it/s]tokenizing...:  81%|████████▏ | 120029/147540 [01:13<00:17, 1541.92it/s]tokenizing...:  81%|████████▏ | 120213/147540 [01:13<00:16, 1620.00it/s]tokenizing...:  82%|████████▏ | 120381/147540 [01:13<00:17, 1592.35it/s]tokenizing...:  82%|████████▏ | 120545/147540 [01:13<00:17, 1540.98it/s]tokenizing...:  82%|████████▏ | 120711/147540 [01:13<00:17, 1573.07it/s]tokenizing...:  82%|████████▏ | 120897/147540 [01:13<00:16, 1652.35it/s]tokenizing...:  82%|████████▏ | 121132/147540 [01:14<00:14, 1851.51it/s]tokenizing...:  82%|████████▏ | 121331/147540 [01:14<00:13, 1891.53it/s]tokenizing...:  82%|████████▏ | 121552/147540 [01:14<00:13, 1983.40it/s]tokenizing...:  83%|████████▎ | 121786/147540 [01:14<00:12, 2078.42it/s]tokenizing...:  83%|████████▎ | 122004/147540 [01:14<00:12, 2104.93it/s]tokenizing...:  83%|████████▎ | 122223/147540 [01:14<00:11, 2126.67it/s]tokenizing...:  83%|████████▎ | 122437/147540 [01:14<00:11, 2096.13it/s]tokenizing...:  83%|████████▎ | 122648/147540 [01:14<00:11, 2076.54it/s]tokenizing...:  83%|████████▎ | 122922/147540 [01:14<00:10, 2270.85it/s]tokenizing...:  83%|████████▎ | 123181/147540 [01:14<00:10, 2364.55it/s]tokenizing...:  84%|████████▎ | 123419/147540 [01:15<00:10, 2254.41it/s]tokenizing...:  84%|████████▍ | 123667/147540 [01:15<00:10, 2317.69it/s]tokenizing...:  84%|████████▍ | 123918/147540 [01:15<00:09, 2370.58it/s]tokenizing...:  84%|████████▍ | 124176/147540 [01:15<00:09, 2429.44it/s]tokenizing...:  84%|████████▍ | 124475/147540 [01:15<00:08, 2593.58it/s]tokenizing...:  85%|████████▍ | 124736/147540 [01:15<00:09, 2486.14it/s]tokenizing...:  85%|████████▍ | 124987/147540 [01:15<00:09, 2344.99it/s]tokenizing...:  85%|████████▍ | 125224/147540 [01:15<00:09, 2235.88it/s]tokenizing...:  85%|████████▌ | 125450/147540 [01:15<00:09, 2221.15it/s]tokenizing...:  85%|████████▌ | 125674/147540 [01:16<00:10, 2168.93it/s]tokenizing...:  85%|████████▌ | 125897/147540 [01:16<00:09, 2182.28it/s]tokenizing...:  85%|████████▌ | 126116/147540 [01:16<00:09, 2183.04it/s]tokenizing...:  86%|████████▌ | 126347/147540 [01:16<00:09, 2216.96it/s]tokenizing...:  86%|████████▌ | 126570/147540 [01:16<00:09, 2145.13it/s]tokenizing...:  86%|████████▌ | 126786/147540 [01:16<00:09, 2133.78it/s]tokenizing...:  86%|████████▌ | 127000/147540 [01:16<00:09, 2066.08it/s]tokenizing...:  86%|████████▌ | 127208/147540 [01:16<00:10, 2026.84it/s]tokenizing...:  86%|████████▋ | 127412/147540 [01:16<00:10, 2010.30it/s]tokenizing...:  86%|████████▋ | 127614/147540 [01:16<00:10, 1941.97it/s]tokenizing...:  87%|████████▋ | 127809/147540 [01:17<00:10, 1898.66it/s]tokenizing...:  87%|████████▋ | 128001/147540 [01:17<00:10, 1900.16it/s]tokenizing...:  87%|████████▋ | 128192/147540 [01:17<00:10, 1874.72it/s]tokenizing...:  87%|████████▋ | 128380/147540 [01:17<00:10, 1798.47it/s]tokenizing...:  87%|████████▋ | 128563/147540 [01:17<00:10, 1805.75it/s]tokenizing...:  87%|████████▋ | 128755/147540 [01:17<00:10, 1833.98it/s]tokenizing...:  87%|████████▋ | 128939/147540 [01:17<00:10, 1790.53it/s]tokenizing...:  88%|████████▊ | 129119/147540 [01:17<00:10, 1760.18it/s]tokenizing...:  88%|████████▊ | 129296/147540 [01:17<00:10, 1740.40it/s]tokenizing...:  88%|████████▊ | 129471/147540 [01:18<00:10, 1712.06it/s]tokenizing...:  88%|████████▊ | 129643/147540 [01:18<00:10, 1689.60it/s]tokenizing...:  88%|████████▊ | 129813/147540 [01:18<00:10, 1691.30it/s]tokenizing...:  88%|████████▊ | 129984/147540 [01:18<00:10, 1695.75it/s]tokenizing...:  88%|████████▊ | 130154/147540 [01:18<00:10, 1681.23it/s]tokenizing...:  88%|████████▊ | 130324/147540 [01:18<00:10, 1686.29it/s]tokenizing...:  88%|████████▊ | 130493/147540 [01:18<00:10, 1682.04it/s]tokenizing...:  89%|████████▊ | 130662/147540 [01:18<00:10, 1680.55it/s]tokenizing...:  89%|████████▊ | 130831/147540 [01:18<00:10, 1643.84it/s]tokenizing...:  89%|████████▉ | 130999/147540 [01:18<00:10, 1653.79it/s]tokenizing...:  89%|████████▉ | 131165/147540 [01:19<00:09, 1655.43it/s]tokenizing...:  89%|████████▉ | 131373/147540 [01:19<00:09, 1780.68it/s]tokenizing...:  89%|████████▉ | 131558/147540 [01:19<00:08, 1793.33it/s]tokenizing...:  89%|████████▉ | 131752/147540 [01:19<00:08, 1830.17it/s]tokenizing...:  89%|████████▉ | 131946/147540 [01:19<00:08, 1861.94it/s]tokenizing...:  90%|████████▉ | 132133/147540 [01:19<00:08, 1812.13it/s]tokenizing...:  90%|████████▉ | 132315/147540 [01:19<00:08, 1781.34it/s]tokenizing...:  90%|████████▉ | 132494/147540 [01:19<00:08, 1752.06it/s]tokenizing...:  90%|████████▉ | 132670/147540 [01:19<00:08, 1718.52it/s]tokenizing...:  90%|█████████ | 132843/147540 [01:19<00:08, 1695.14it/s]tokenizing...:  90%|█████████ | 133013/147540 [01:20<00:08, 1688.76it/s]tokenizing...:  90%|█████████ | 133182/147540 [01:20<00:08, 1686.80it/s]tokenizing...:  90%|█████████ | 133351/147540 [01:20<00:08, 1684.72it/s]tokenizing...:  90%|█████████ | 133520/147540 [01:20<00:08, 1682.66it/s]tokenizing...:  91%|█████████ | 133690/147540 [01:20<00:08, 1686.48it/s]tokenizing...:  91%|█████████ | 133859/147540 [01:20<00:08, 1644.50it/s]tokenizing...:  91%|█████████ | 134024/147540 [01:20<00:08, 1637.35it/s]tokenizing...:  91%|█████████ | 134195/147540 [01:20<00:08, 1657.26it/s]tokenizing...:  91%|█████████ | 134364/147540 [01:20<00:07, 1666.73it/s]tokenizing...:  91%|█████████ | 134533/147540 [01:20<00:07, 1672.91it/s]tokenizing...:  91%|█████████▏| 134701/147540 [01:21<00:07, 1674.17it/s]tokenizing...:  91%|█████████▏| 134869/147540 [01:21<00:07, 1640.12it/s]tokenizing...:  92%|█████████▏| 135038/147540 [01:21<00:07, 1651.76it/s]tokenizing...:  92%|█████████▏| 135206/147540 [01:21<00:07, 1659.35it/s]tokenizing...:  92%|█████████▏| 135379/147540 [01:21<00:07, 1679.94it/s]tokenizing...:  92%|█████████▏| 135548/147540 [01:21<00:07, 1676.32it/s]tokenizing...:  92%|█████████▏| 135718/147540 [01:21<00:07, 1682.26it/s]tokenizing...:  92%|█████████▏| 135902/147540 [01:21<00:06, 1728.77it/s]tokenizing...:  92%|█████████▏| 136113/147540 [01:21<00:06, 1842.16it/s]tokenizing...:  92%|█████████▏| 136336/147540 [01:22<00:05, 1957.06it/s]tokenizing...:  93%|█████████▎| 136536/147540 [01:22<00:05, 1969.44it/s]tokenizing...:  93%|█████████▎| 136734/147540 [01:22<00:06, 1676.97it/s]tokenizing...:  93%|█████████▎| 136937/147540 [01:22<00:05, 1770.28it/s]tokenizing...:  93%|█████████▎| 137121/147540 [01:22<00:05, 1776.23it/s]tokenizing...:  93%|█████████▎| 137325/147540 [01:22<00:05, 1808.47it/s]tokenizing...:  93%|█████████▎| 137510/147540 [01:22<00:07, 1362.42it/s]tokenizing...:  93%|█████████▎| 137665/147540 [01:22<00:07, 1328.62it/s]tokenizing...:  93%|█████████▎| 137835/147540 [01:23<00:06, 1416.07it/s]tokenizing...:  94%|█████████▎| 138013/147540 [01:23<00:06, 1508.33it/s]tokenizing...:  94%|█████████▎| 138244/147540 [01:23<00:05, 1720.64it/s]tokenizing...:  94%|█████████▍| 138465/147540 [01:23<00:04, 1854.92it/s]tokenizing...:  94%|█████████▍| 138668/147540 [01:23<00:04, 1903.78it/s]tokenizing...:  94%|█████████▍| 138907/147540 [01:23<00:04, 2040.61it/s]tokenizing...:  94%|█████████▍| 139119/147540 [01:23<00:04, 2059.61it/s]tokenizing...:  94%|█████████▍| 139338/147540 [01:23<00:03, 2094.35it/s]tokenizing...:  95%|█████████▍| 139550/147540 [01:23<00:03, 2040.76it/s]tokenizing...:  95%|█████████▍| 139775/147540 [01:23<00:03, 2100.01it/s]tokenizing...:  95%|█████████▍| 139987/147540 [01:24<00:03, 2080.72it/s]tokenizing...:  95%|█████████▌| 140218/147540 [01:24<00:03, 2145.31it/s]tokenizing...:  95%|█████████▌| 140434/147540 [01:24<00:03, 2149.39it/s]tokenizing...:  95%|█████████▌| 140650/147540 [01:24<00:03, 2113.25it/s]tokenizing...:  96%|█████████▌| 140963/147540 [01:24<00:02, 2409.46it/s]tokenizing...:  96%|█████████▌| 141205/147540 [01:24<00:02, 2315.59it/s]tokenizing...:  96%|█████████▌| 141448/147540 [01:24<00:02, 2346.19it/s]tokenizing...:  96%|█████████▌| 141700/147540 [01:24<00:02, 2395.25it/s]tokenizing...:  96%|█████████▌| 141980/147540 [01:24<00:02, 2512.89it/s]tokenizing...:  96%|█████████▋| 142245/147540 [01:24<00:02, 2551.09it/s]tokenizing...:  97%|█████████▋| 142501/147540 [01:25<00:02, 2460.15it/s]tokenizing...:  97%|█████████▋| 142749/147540 [01:25<00:02, 2389.72it/s]tokenizing...:  97%|█████████▋| 142990/147540 [01:25<00:01, 2278.76it/s]tokenizing...:  97%|█████████▋| 143220/147540 [01:25<00:01, 2179.28it/s]tokenizing...:  97%|█████████▋| 143440/147540 [01:25<00:01, 2169.90it/s]tokenizing...:  97%|█████████▋| 143665/147540 [01:25<00:01, 2191.95it/s]tokenizing...:  98%|█████████▊| 143886/147540 [01:25<00:01, 1902.00it/s]tokenizing...:  98%|█████████▊| 144084/147540 [01:25<00:01, 1886.80it/s]tokenizing...:  98%|█████████▊| 144289/147540 [01:25<00:01, 1927.38it/s]tokenizing...:  98%|█████████▊| 144486/147540 [01:26<00:01, 1765.06it/s]tokenizing...:  98%|█████████▊| 144687/147540 [01:26<00:01, 1829.18it/s]tokenizing...:  98%|█████████▊| 144874/147540 [01:26<00:01, 1810.46it/s]tokenizing...:  98%|█████████▊| 145061/147540 [01:26<00:01, 1826.47it/s]tokenizing...:  98%|█████████▊| 145271/147540 [01:26<00:01, 1901.09it/s]tokenizing...:  99%|█████████▊| 145468/147540 [01:26<00:01, 1920.72it/s]tokenizing...:  99%|█████████▊| 145662/147540 [01:26<00:01, 1842.90it/s]tokenizing...:  99%|█████████▉| 145853/147540 [01:26<00:00, 1861.08it/s]tokenizing...:  99%|█████████▉| 146041/147540 [01:26<00:00, 1826.22it/s]tokenizing...:  99%|█████████▉| 146225/147540 [01:27<00:00, 1783.63it/s]tokenizing...:  99%|█████████▉| 146405/147540 [01:27<00:00, 1715.45it/s]tokenizing...:  99%|█████████▉| 146611/147540 [01:27<00:00, 1809.73it/s]tokenizing...:  99%|█████████▉| 146794/147540 [01:27<00:00, 1766.62it/s]tokenizing...: 100%|█████████▉| 146972/147540 [01:27<00:00, 1731.47it/s]tokenizing...: 100%|█████████▉| 147146/147540 [01:27<00:00, 1722.14it/s]tokenizing...: 100%|█████████▉| 147319/147540 [01:27<00:00, 1717.29it/s]tokenizing...: 100%|█████████▉| 147491/147540 [01:27<00:00, 1701.86it/s]tokenizing...: 100%|██████████| 147540/147540 [01:27<00:00, 1679.98it/s]
09/25/2022 10:56:46 - INFO - root -   The nums of the test_dataset features is 147540
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 0/1153 [00:00<?, ?it/s]  0%|          | 1/1153 [00:01<34:31,  1.80s/it]  0%|          | 2/1153 [00:07<1:16:17,  3.98s/it]  0%|          | 3/1153 [00:08<53:24,  2.79s/it]    0%|          | 4/1153 [00:10<42:22,  2.21s/it]  0%|          | 5/1153 [00:11<36:01,  1.88s/it]  1%|          | 6/1153 [00:12<31:07,  1.63s/it]  1%|          | 7/1153 [00:13<28:50,  1.51s/it]  1%|          | 8/1153 [00:14<27:22,  1.43s/it]  1%|          | 9/1153 [00:16<26:09,  1.37s/it]  1%|          | 10/1153 [00:17<25:30,  1.34s/it]  1%|          | 11/1153 [00:18<25:20,  1.33s/it]  1%|          | 12/1153 [00:19<23:11,  1.22s/it]  1%|          | 13/1153 [00:21<23:24,  1.23s/it]  1%|          | 14/1153 [00:22<24:04,  1.27s/it]  1%|▏         | 15/1153 [00:23<25:04,  1.32s/it]  1%|▏         | 16/1153 [00:25<25:27,  1.34s/it]  1%|▏         | 17/1153 [00:26<25:47,  1.36s/it]  2%|▏         | 18/1153 [00:32<51:48,  2.74s/it]  2%|▏         | 19/1153 [00:33<43:30,  2.30s/it]  2%|▏         | 20/1153 [00:35<37:41,  2.00s/it]  2%|▏         | 21/1153 [00:36<33:23,  1.77s/it]  2%|▏         | 22/1153 [00:37<30:26,  1.61s/it]  2%|▏         | 23/1153 [00:38<28:26,  1.51s/it]  2%|▏         | 24/1153 [00:40<27:18,  1.45s/it]  2%|▏         | 25/1153 [00:41<26:17,  1.40s/it]  2%|▏         | 26/1153 [00:42<26:35,  1.42s/it]  2%|▏         | 27/1153 [00:44<25:00,  1.33s/it]  2%|▏         | 28/1153 [00:45<25:38,  1.37s/it]  3%|▎         | 29/1153 [00:46<24:57,  1.33s/it]  3%|▎         | 30/1153 [00:48<24:55,  1.33s/it]  3%|▎         | 31/1153 [00:49<26:07,  1.40s/it]  3%|▎         | 32/1153 [00:50<25:36,  1.37s/it]  3%|▎         | 33/1153 [00:52<25:13,  1.35s/it]  3%|▎         | 34/1153 [00:57<45:00,  2.41s/it]  3%|▎         | 35/1153 [00:58<38:40,  2.08s/it]  3%|▎         | 36/1153 [00:59<34:10,  1.84s/it]  3%|▎         | 37/1153 [01:01<31:05,  1.67s/it]  3%|▎         | 38/1153 [01:02<29:05,  1.57s/it]  3%|▎         | 39/1153 [01:03<27:43,  1.49s/it]  3%|▎         | 40/1153 [01:04<25:33,  1.38s/it]  4%|▎         | 41/1153 [01:05<24:34,  1.33s/it]  4%|▎         | 42/1153 [01:07<23:16,  1.26s/it]  4%|▎         | 43/1153 [01:08<22:24,  1.21s/it]  4%|▍         | 44/1153 [01:09<21:40,  1.17s/it]  4%|▍         | 45/1153 [01:10<21:18,  1.15s/it]  4%|▍         | 46/1153 [01:11<20:55,  1.13s/it]  4%|▍         | 47/1153 [01:12<20:37,  1.12s/it]  4%|▍         | 48/1153 [01:13<20:25,  1.11s/it]  4%|▍         | 49/1153 [01:14<20:26,  1.11s/it]  4%|▍         | 50/1153 [01:19<42:41,  2.32s/it]  4%|▍         | 51/1153 [01:21<36:04,  1.96s/it]  5%|▍         | 52/1153 [01:22<31:46,  1.73s/it]  5%|▍         | 53/1153 [01:23<28:22,  1.55s/it]  5%|▍         | 54/1153 [01:24<26:07,  1.43s/it]  5%|▍         | 55/1153 [01:25<24:21,  1.33s/it]  5%|▍         | 56/1153 [01:26<23:01,  1.26s/it]  5%|▍         | 57/1153 [01:27<22:18,  1.22s/it]  5%|▌         | 58/1153 [01:28<21:45,  1.19s/it]  5%|▌         | 59/1153 [01:30<21:26,  1.18s/it]  5%|▌         | 60/1153 [01:31<21:09,  1.16s/it]  5%|▌         | 61/1153 [01:32<20:51,  1.15s/it]  5%|▌         | 62/1153 [01:33<20:59,  1.15s/it]  5%|▌         | 63/1153 [01:34<20:40,  1.14s/it]  6%|▌         | 64/1153 [01:35<20:34,  1.13s/it]  6%|▌         | 65/1153 [01:36<20:31,  1.13s/it]  6%|▌         | 66/1153 [01:37<20:26,  1.13s/it]  6%|▌         | 67/1153 [01:39<20:27,  1.13s/it]  6%|▌         | 68/1153 [01:43<36:40,  2.03s/it]  6%|▌         | 69/1153 [01:44<32:46,  1.81s/it]  6%|▌         | 70/1153 [01:45<27:18,  1.51s/it]  6%|▌         | 71/1153 [01:46<25:53,  1.44s/it]  6%|▌         | 72/1153 [01:47<25:05,  1.39s/it]  6%|▋         | 73/1153 [01:49<24:31,  1.36s/it]  6%|▋         | 74/1153 [01:50<23:26,  1.30s/it]  7%|▋         | 75/1153 [01:51<23:13,  1.29s/it]  7%|▋         | 76/1153 [01:52<23:02,  1.28s/it]  7%|▋         | 77/1153 [01:54<22:58,  1.28s/it]  7%|▋         | 78/1153 [01:55<23:12,  1.30s/it]  7%|▋         | 79/1153 [01:56<23:02,  1.29s/it]  7%|▋         | 80/1153 [01:57<22:53,  1.28s/it]  7%|▋         | 81/1153 [01:59<21:58,  1.23s/it]  7%|▋         | 82/1153 [02:00<22:06,  1.24s/it]  7%|▋         | 83/1153 [02:01<22:10,  1.24s/it]  7%|▋         | 84/1153 [02:02<22:15,  1.25s/it]  7%|▋         | 85/1153 [02:06<36:11,  2.03s/it]  7%|▋         | 86/1153 [02:08<32:13,  1.81s/it]  8%|▊         | 87/1153 [02:09<29:25,  1.66s/it]  8%|▊         | 88/1153 [02:10<27:17,  1.54s/it]  8%|▊         | 89/1153 [02:11<25:48,  1.46s/it]  8%|▊         | 90/1153 [02:13<24:49,  1.40s/it]  8%|▊         | 91/1153 [02:14<24:09,  1.37s/it]  8%|▊         | 92/1153 [02:15<23:44,  1.34s/it]  8%|▊         | 93/1153 [02:17<23:28,  1.33s/it]  8%|▊         | 94/1153 [02:18<23:03,  1.31s/it]  8%|▊         | 95/1153 [02:19<22:50,  1.30s/it]  8%|▊         | 96/1153 [02:20<22:41,  1.29s/it]  8%|▊         | 97/1153 [02:22<22:37,  1.29s/it]  8%|▊         | 98/1153 [02:23<22:35,  1.28s/it]  9%|▊         | 99/1153 [02:24<22:36,  1.29s/it]  9%|▊         | 100/1153 [02:28<36:17,  2.07s/it]  9%|▉         | 101/1153 [02:29<32:06,  1.83s/it]  9%|▉         | 102/1153 [02:31<29:08,  1.66s/it]  9%|▉         | 103/1153 [02:32<27:06,  1.55s/it]  9%|▉         | 104/1153 [02:33<25:42,  1.47s/it]  9%|▉         | 105/1153 [02:34<24:35,  1.41s/it]  9%|▉         | 106/1153 [02:36<23:47,  1.36s/it]  9%|▉         | 107/1153 [02:37<23:16,  1.33s/it]  9%|▉         | 108/1153 [02:38<22:57,  1.32s/it]  9%|▉         | 109/1153 [02:39<21:47,  1.25s/it] 10%|▉         | 110/1153 [02:41<21:51,  1.26s/it] 10%|▉         | 111/1153 [02:42<21:58,  1.27s/it] 10%|▉         | 112/1153 [02:43<22:01,  1.27s/it] 10%|▉         | 113/1153 [02:44<22:10,  1.28s/it] 10%|▉         | 114/1153 [02:46<23:18,  1.35s/it] 10%|▉         | 115/1153 [02:47<23:03,  1.33s/it] 10%|█         | 116/1153 [02:51<36:03,  2.09s/it] 10%|█         | 117/1153 [02:52<31:30,  1.82s/it] 10%|█         | 118/1153 [02:54<28:34,  1.66s/it] 10%|█         | 119/1153 [02:55<26:33,  1.54s/it] 10%|█         | 120/1153 [02:56<25:11,  1.46s/it] 10%|█         | 121/1153 [02:57<24:16,  1.41s/it] 11%|█         | 122/1153 [02:59<22:42,  1.32s/it] 11%|█         | 123/1153 [03:00<23:53,  1.39s/it] 11%|█         | 124/1153 [03:01<23:28,  1.37s/it] 11%|█         | 125/1153 [03:03<23:52,  1.39s/it] 11%|█         | 126/1153 [03:04<23:00,  1.34s/it] 11%|█         | 127/1153 [03:05<21:31,  1.26s/it] 11%|█         | 128/1153 [03:06<21:00,  1.23s/it] 11%|█         | 129/1153 [03:08<21:23,  1.25s/it] 11%|█▏        | 130/1153 [03:09<21:49,  1.28s/it] 11%|█▏        | 131/1153 [03:10<22:03,  1.29s/it] 11%|█▏        | 132/1153 [03:15<38:54,  2.29s/it] 12%|█▏        | 133/1153 [03:16<33:01,  1.94s/it] 12%|█▏        | 134/1153 [03:17<28:57,  1.71s/it] 12%|█▏        | 135/1153 [03:18<26:09,  1.54s/it] 12%|█▏        | 136/1153 [03:19<24:04,  1.42s/it] 12%|█▏        | 137/1153 [03:21<22:57,  1.36s/it] 12%|█▏        | 138/1153 [03:22<21:55,  1.30s/it] 12%|█▏        | 139/1153 [03:23<21:05,  1.25s/it] 12%|█▏        | 140/1153 [03:24<20:32,  1.22s/it] 12%|█▏        | 141/1153 [03:25<20:01,  1.19s/it] 12%|█▏        | 142/1153 [03:26<20:14,  1.20s/it] 12%|█▏        | 143/1153 [03:28<21:37,  1.28s/it] 12%|█▏        | 144/1153 [03:29<21:36,  1.29s/it] 13%|█▎        | 145/1153 [03:31<22:07,  1.32s/it] 13%|█▎        | 146/1153 [03:32<21:24,  1.28s/it] 13%|█▎        | 147/1153 [03:33<20:35,  1.23s/it] 13%|█▎        | 148/1153 [03:34<20:06,  1.20s/it] 13%|█▎        | 149/1153 [03:35<19:43,  1.18s/it] 13%|█▎        | 150/1153 [03:36<19:31,  1.17s/it] 13%|█▎        | 151/1153 [03:41<35:25,  2.12s/it] 13%|█▎        | 152/1153 [03:42<30:59,  1.86s/it] 13%|█▎        | 153/1153 [03:43<27:16,  1.64s/it] 13%|█▎        | 154/1153 [03:44<24:40,  1.48s/it] 13%|█▎        | 155/1153 [03:45<22:52,  1.37s/it] 14%|█▎        | 156/1153 [03:47<22:20,  1.34s/it] 14%|█▎        | 157/1153 [03:48<21:58,  1.32s/it] 14%|█▎        | 158/1153 [03:49<21:47,  1.31s/it] 14%|█▍        | 159/1153 [03:50<21:38,  1.31s/it] 14%|█▍        | 160/1153 [03:52<21:33,  1.30s/it] 14%|█▍        | 161/1153 [03:53<21:31,  1.30s/it] 14%|█▍        | 162/1153 [03:54<20:37,  1.25s/it] 14%|█▍        | 163/1153 [03:55<20:53,  1.27s/it] 14%|█▍        | 164/1153 [03:57<20:18,  1.23s/it] 14%|█▍        | 165/1153 [03:58<19:46,  1.20s/it] 14%|█▍        | 166/1153 [03:59<19:24,  1.18s/it] 14%|█▍        | 167/1153 [04:00<19:24,  1.18s/it] 15%|█▍        | 168/1153 [04:05<35:44,  2.18s/it] 15%|█▍        | 169/1153 [04:06<30:35,  1.87s/it] 15%|█▍        | 170/1153 [04:07<27:45,  1.69s/it] 15%|█▍        | 171/1153 [04:08<25:13,  1.54s/it] 15%|█▍        | 172/1153 [04:09<23:59,  1.47s/it] 15%|█▌        | 173/1153 [04:11<23:40,  1.45s/it] 15%|█▌        | 174/1153 [04:12<23:09,  1.42s/it] 15%|█▌        | 175/1153 [04:13<21:35,  1.32s/it] 15%|█▌        | 176/1153 [04:14<20:42,  1.27s/it] 15%|█▌        | 177/1153 [04:16<20:10,  1.24s/it] 15%|█▌        | 178/1153 [04:17<19:44,  1.21s/it] 16%|█▌        | 179/1153 [04:18<19:38,  1.21s/it] 16%|█▌        | 180/1153 [04:19<19:40,  1.21s/it] 16%|█▌        | 181/1153 [04:20<19:29,  1.20s/it] 16%|█▌        | 182/1153 [04:22<19:06,  1.18s/it] 16%|█▌        | 183/1153 [04:23<19:04,  1.18s/it] 16%|█▌        | 184/1153 [04:24<19:25,  1.20s/it] 16%|█▌        | 185/1153 [04:25<19:12,  1.19s/it] 16%|█▌        | 186/1153 [04:29<34:36,  2.15s/it] 16%|█▌        | 187/1153 [04:31<29:47,  1.85s/it] 16%|█▋        | 188/1153 [04:32<26:18,  1.64s/it] 16%|█▋        | 189/1153 [04:33<23:57,  1.49s/it] 16%|█▋        | 190/1153 [04:34<22:07,  1.38s/it] 17%|█▋        | 191/1153 [04:35<22:00,  1.37s/it] 17%|█▋        | 192/1153 [04:37<21:37,  1.35s/it] 17%|█▋        | 193/1153 [04:38<21:28,  1.34s/it] 17%|█▋        | 194/1153 [04:39<21:35,  1.35s/it] 17%|█▋        | 195/1153 [04:41<21:13,  1.33s/it] 17%|█▋        | 196/1153 [04:42<21:00,  1.32s/it] 17%|█▋        | 197/1153 [04:43<20:52,  1.31s/it] 17%|█▋        | 198/1153 [04:45<21:38,  1.36s/it] 17%|█▋        | 199/1153 [04:46<22:22,  1.41s/it] 17%|█▋        | 200/1153 [04:48<21:57,  1.38s/it] 17%|█▋        | 201/1153 [04:49<22:07,  1.39s/it] 18%|█▊        | 202/1153 [04:50<21:41,  1.37s/it] 18%|█▊        | 203/1153 [04:56<40:51,  2.58s/it] 18%|█▊        | 204/1153 [04:57<34:50,  2.20s/it] 18%|█▊        | 205/1153 [04:58<31:08,  1.97s/it] 18%|█▊        | 206/1153 [05:00<28:11,  1.79s/it] 18%|█▊        | 207/1153 [05:01<25:48,  1.64s/it] 18%|█▊        | 208/1153 [05:02<24:26,  1.55s/it] 18%|█▊        | 209/1153 [05:04<23:13,  1.48s/it] 18%|█▊        | 210/1153 [05:05<21:04,  1.34s/it] 18%|█▊        | 211/1153 [05:06<20:53,  1.33s/it] 18%|█▊        | 212/1153 [05:07<18:28,  1.18s/it] 18%|█▊        | 213/1153 [05:08<20:01,  1.28s/it] 19%|█▊        | 214/1153 [05:10<20:13,  1.29s/it] 19%|█▊        | 215/1153 [05:11<20:19,  1.30s/it] 19%|█▊        | 216/1153 [05:13<21:00,  1.35s/it] 19%|█▉        | 217/1153 [05:14<20:44,  1.33s/it] 19%|█▉        | 218/1153 [05:15<20:14,  1.30s/it] 19%|█▉        | 219/1153 [05:20<36:34,  2.35s/it] 19%|█▉        | 220/1153 [05:21<32:02,  2.06s/it] 19%|█▉        | 221/1153 [05:23<28:33,  1.84s/it] 19%|█▉        | 222/1153 [05:24<26:14,  1.69s/it] 19%|█▉        | 223/1153 [05:25<24:21,  1.57s/it] 19%|█▉        | 224/1153 [05:27<23:05,  1.49s/it] 20%|█▉        | 225/1153 [05:28<22:14,  1.44s/it] 20%|█▉        | 226/1153 [05:29<21:42,  1.41s/it] 20%|█▉        | 227/1153 [05:30<21:07,  1.37s/it] 20%|█▉        | 228/1153 [05:32<20:44,  1.35s/it] 20%|█▉        | 229/1153 [05:33<19:48,  1.29s/it] 20%|█▉        | 230/1153 [05:34<19:51,  1.29s/it] 20%|██        | 231/1153 [05:35<19:09,  1.25s/it] 20%|██        | 232/1153 [05:37<19:30,  1.27s/it] 20%|██        | 233/1153 [05:38<19:41,  1.28s/it] 20%|██        | 234/1153 [05:39<19:45,  1.29s/it] 20%|██        | 235/1153 [05:41<19:52,  1.30s/it] 20%|██        | 236/1153 [05:45<34:46,  2.28s/it] 21%|██        | 237/1153 [05:46<30:26,  1.99s/it] 21%|██        | 238/1153 [05:48<27:09,  1.78s/it] 21%|██        | 239/1153 [05:49<25:47,  1.69s/it] 21%|██        | 240/1153 [05:51<23:56,  1.57s/it] 21%|██        | 241/1153 [05:52<22:41,  1.49s/it] 21%|██        | 242/1153 [05:53<21:49,  1.44s/it] 21%|██        | 243/1153 [05:55<21:32,  1.42s/it] 21%|██        | 244/1153 [05:56<21:21,  1.41s/it] 21%|██        | 245/1153 [05:57<19:22,  1.28s/it] 21%|██▏       | 246/1153 [05:58<18:27,  1.22s/it] 21%|██▏       | 247/1153 [05:59<18:53,  1.25s/it] 22%|██▏       | 248/1153 [06:01<19:06,  1.27s/it] 22%|██▏       | 249/1153 [06:02<19:22,  1.29s/it] 22%|██▏       | 250/1153 [06:03<19:44,  1.31s/it] 22%|██▏       | 251/1153 [06:05<20:06,  1.34s/it] 22%|██▏       | 252/1153 [06:09<34:30,  2.30s/it] 22%|██▏       | 253/1153 [06:11<30:01,  2.00s/it] 22%|██▏       | 254/1153 [06:12<26:49,  1.79s/it] 22%|██▏       | 255/1153 [06:13<24:01,  1.60s/it] 22%|██▏       | 256/1153 [06:14<22:37,  1.51s/it] 22%|██▏       | 257/1153 [06:16<21:38,  1.45s/it] 22%|██▏       | 258/1153 [06:17<20:27,  1.37s/it] 22%|██▏       | 259/1153 [06:18<20:46,  1.39s/it] 23%|██▎       | 260/1153 [06:20<20:43,  1.39s/it] 23%|██▎       | 261/1153 [06:21<20:38,  1.39s/it] 23%|██▎       | 262/1153 [06:22<19:18,  1.30s/it] 23%|██▎       | 263/1153 [06:23<18:43,  1.26s/it] 23%|██▎       | 264/1153 [06:24<18:24,  1.24s/it] 23%|██▎       | 265/1153 [06:26<18:42,  1.26s/it] 23%|██▎       | 266/1153 [06:27<18:11,  1.23s/it] 23%|██▎       | 267/1153 [06:28<18:19,  1.24s/it] 23%|██▎       | 268/1153 [06:30<18:51,  1.28s/it] 23%|██▎       | 269/1153 [06:31<18:14,  1.24s/it] 23%|██▎       | 270/1153 [06:35<32:50,  2.23s/it] 24%|██▎       | 271/1153 [06:36<28:02,  1.91s/it] 24%|██▎       | 272/1153 [06:38<25:19,  1.73s/it] 24%|██▎       | 273/1153 [06:39<22:50,  1.56s/it] 24%|██▍       | 274/1153 [06:40<21:11,  1.45s/it] 24%|██▍       | 275/1153 [06:41<19:48,  1.35s/it] 24%|██▍       | 276/1153 [06:42<18:54,  1.29s/it] 24%|██▍       | 277/1153 [06:44<18:29,  1.27s/it] 24%|██▍       | 278/1153 [06:45<18:13,  1.25s/it] 24%|██▍       | 279/1153 [06:46<17:49,  1.22s/it] 24%|██▍       | 280/1153 [06:47<17:24,  1.20s/it] 24%|██▍       | 281/1153 [06:48<17:16,  1.19s/it] 24%|██▍       | 282/1153 [06:49<17:11,  1.18s/it] 25%|██▍       | 283/1153 [06:51<17:01,  1.17s/it] 25%|██▍       | 284/1153 [06:52<17:39,  1.22s/it] 25%|██▍       | 285/1153 [06:53<18:03,  1.25s/it] 25%|██▍       | 286/1153 [06:55<18:21,  1.27s/it] 25%|██▍       | 287/1153 [06:56<18:41,  1.29s/it] 25%|██▍       | 288/1153 [07:01<33:07,  2.30s/it] 25%|██▌       | 289/1153 [07:02<28:56,  2.01s/it] 25%|██▌       | 290/1153 [07:03<26:20,  1.83s/it] 25%|██▌       | 291/1153 [07:05<24:41,  1.72s/it] 25%|██▌       | 292/1153 [07:06<22:36,  1.58s/it] 25%|██▌       | 293/1153 [07:07<20:53,  1.46s/it] 25%|██▌       | 294/1153 [07:08<20:17,  1.42s/it] 26%|██▌       | 295/1153 [07:10<19:32,  1.37s/it] 26%|██▌       | 296/1153 [07:11<18:49,  1.32s/it] 26%|██▌       | 297/1153 [07:12<18:11,  1.28s/it] 26%|██▌       | 298/1153 [07:13<17:56,  1.26s/it] 26%|██▌       | 299/1153 [07:14<17:28,  1.23s/it] 26%|██▌       | 300/1153 [07:16<17:09,  1.21s/it] 26%|██▌       | 301/1153 [07:17<17:46,  1.25s/it] 26%|██▌       | 302/1153 [07:18<17:45,  1.25s/it] 26%|██▋       | 303/1153 [07:19<17:22,  1.23s/it] 26%|██▋       | 304/1153 [07:21<17:12,  1.22s/it] 26%|██▋       | 305/1153 [07:22<17:06,  1.21s/it] 27%|██▋       | 306/1153 [07:23<17:01,  1.21s/it] 27%|██▋       | 307/1153 [07:28<31:30,  2.23s/it] 27%|██▋       | 308/1153 [07:29<27:14,  1.93s/it] 27%|██▋       | 309/1153 [07:30<24:09,  1.72s/it] 27%|██▋       | 310/1153 [07:31<22:31,  1.60s/it] 27%|██▋       | 311/1153 [07:33<21:22,  1.52s/it] 27%|██▋       | 312/1153 [07:34<20:01,  1.43s/it] 27%|██▋       | 313/1153 [07:35<18:54,  1.35s/it] 27%|██▋       | 314/1153 [07:36<18:48,  1.35s/it] 27%|██▋       | 315/1153 [07:38<18:11,  1.30s/it] 27%|██▋       | 316/1153 [07:39<17:36,  1.26s/it] 27%|██▋       | 317/1153 [07:40<17:17,  1.24s/it] 28%|██▊       | 318/1153 [07:41<16:54,  1.21s/it] 28%|██▊       | 319/1153 [07:43<17:22,  1.25s/it] 28%|██▊       | 320/1153 [07:44<17:41,  1.27s/it] 28%|██▊       | 321/1153 [07:45<17:34,  1.27s/it] 28%|██▊       | 322/1153 [07:47<18:04,  1.30s/it] 28%|██▊       | 323/1153 [07:48<18:26,  1.33s/it] 28%|██▊       | 324/1153 [07:49<18:23,  1.33s/it] 28%|██▊       | 325/1153 [07:55<34:54,  2.53s/it] 28%|██▊       | 326/1153 [07:56<30:18,  2.20s/it] 28%|██▊       | 327/1153 [07:57<26:52,  1.95s/it] 28%|██▊       | 328/1153 [07:59<24:17,  1.77s/it] 29%|██▊       | 329/1153 [08:00<22:29,  1.64s/it] 29%|██▊       | 330/1153 [08:01<21:15,  1.55s/it] 29%|██▊       | 331/1153 [08:03<20:27,  1.49s/it] 29%|██▉       | 332/1153 [08:04<19:43,  1.44s/it] 29%|██▉       | 333/1153 [08:05<19:08,  1.40s/it] 29%|██▉       | 334/1153 [08:07<19:00,  1.39s/it] 29%|██▉       | 335/1153 [08:08<19:01,  1.40s/it] 29%|██▉       | 336/1153 [08:09<18:49,  1.38s/it] 29%|██▉       | 337/1153 [08:11<18:38,  1.37s/it] 29%|██▉       | 338/1153 [08:12<18:20,  1.35s/it] 29%|██▉       | 339/1153 [08:13<18:08,  1.34s/it] 29%|██▉       | 340/1153 [08:15<18:04,  1.33s/it] 30%|██▉       | 341/1153 [08:16<18:03,  1.33s/it] 30%|██▉       | 342/1153 [08:21<31:26,  2.33s/it] 30%|██▉       | 343/1153 [08:22<27:38,  2.05s/it] 30%|██▉       | 344/1153 [08:23<24:39,  1.83s/it] 30%|██▉       | 345/1153 [08:25<22:49,  1.70s/it] 30%|███       | 346/1153 [08:26<21:18,  1.58s/it] 30%|███       | 347/1153 [08:28<20:17,  1.51s/it] 30%|███       | 348/1153 [08:29<19:35,  1.46s/it] 30%|███       | 349/1153 [08:30<19:06,  1.43s/it] 30%|███       | 350/1153 [08:32<18:38,  1.39s/it] 30%|███       | 351/1153 [08:33<18:20,  1.37s/it] 31%|███       | 352/1153 [08:34<18:07,  1.36s/it] 31%|███       | 353/1153 [08:35<17:59,  1.35s/it] 31%|███       | 354/1153 [08:37<17:21,  1.30s/it] 31%|███       | 355/1153 [08:38<17:23,  1.31s/it] 31%|███       | 356/1153 [08:39<17:25,  1.31s/it] 31%|███       | 357/1153 [08:41<17:28,  1.32s/it] 31%|███       | 358/1153 [08:45<30:44,  2.32s/it] 31%|███       | 359/1153 [08:47<26:43,  2.02s/it] 31%|███       | 360/1153 [08:48<23:52,  1.81s/it] 31%|███▏      | 361/1153 [08:49<22:00,  1.67s/it] 31%|███▏      | 362/1153 [08:51<20:46,  1.58s/it] 31%|███▏      | 363/1153 [08:52<19:56,  1.52s/it] 32%|███▏      | 364/1153 [08:53<19:18,  1.47s/it] 32%|███▏      | 365/1153 [08:55<18:22,  1.40s/it] 32%|███▏      | 366/1153 [08:56<18:04,  1.38s/it] 32%|███▏      | 367/1153 [08:57<18:06,  1.38s/it] 32%|███▏      | 368/1153 [08:59<17:54,  1.37s/it] 32%|███▏      | 369/1153 [08:59<15:19,  1.17s/it] 32%|███▏      | 370/1153 [09:01<15:57,  1.22s/it] 32%|███▏      | 371/1153 [09:02<16:16,  1.25s/it] 32%|███▏      | 372/1153 [09:03<16:30,  1.27s/it] 32%|███▏      | 373/1153 [09:05<16:35,  1.28s/it] 32%|███▏      | 374/1153 [09:06<16:50,  1.30s/it] 33%|███▎      | 375/1153 [09:10<28:20,  2.19s/it] 33%|███▎      | 376/1153 [09:12<25:00,  1.93s/it] 33%|███▎      | 377/1153 [09:13<22:35,  1.75s/it] 33%|███▎      | 378/1153 [09:14<20:56,  1.62s/it] 33%|███▎      | 379/1153 [09:16<19:49,  1.54s/it] 33%|███▎      | 380/1153 [09:17<19:03,  1.48s/it] 33%|███▎      | 381/1153 [09:18<18:32,  1.44s/it] 33%|███▎      | 382/1153 [09:20<18:02,  1.40s/it] 33%|███▎      | 383/1153 [09:21<17:41,  1.38s/it] 33%|███▎      | 384/1153 [09:22<17:28,  1.36s/it] 33%|███▎      | 385/1153 [09:24<17:20,  1.35s/it] 33%|███▎      | 386/1153 [09:25<17:16,  1.35s/it] 34%|███▎      | 387/1153 [09:26<17:13,  1.35s/it] 34%|███▎      | 388/1153 [09:28<17:02,  1.34s/it] 34%|███▎      | 389/1153 [09:29<16:57,  1.33s/it] 34%|███▍      | 390/1153 [09:30<16:57,  1.33s/it] 34%|███▍      | 391/1153 [09:32<16:58,  1.34s/it] 34%|███▍      | 392/1153 [09:36<28:02,  2.21s/it] 34%|███▍      | 393/1153 [09:37<24:48,  1.96s/it] 34%|███▍      | 394/1153 [09:39<22:19,  1.77s/it] 34%|███▍      | 395/1153 [09:40<19:53,  1.57s/it] 34%|███▍      | 396/1153 [09:41<19:30,  1.55s/it] 34%|███▍      | 397/1153 [09:42<18:22,  1.46s/it] 35%|███▍      | 398/1153 [09:44<18:20,  1.46s/it] 35%|███▍      | 399/1153 [09:45<18:10,  1.45s/it] 35%|███▍      | 400/1153 [09:47<17:41,  1.41s/it] 35%|███▍      | 401/1153 [09:48<17:22,  1.39s/it] 35%|███▍      | 402/1153 [09:49<17:10,  1.37s/it] 35%|███▍      | 403/1153 [09:50<16:24,  1.31s/it] 35%|███▌      | 404/1153 [09:52<15:57,  1.28s/it] 35%|███▌      | 405/1153 [09:53<15:32,  1.25s/it] 35%|███▌      | 406/1153 [09:54<16:14,  1.30s/it] 35%|███▌      | 407/1153 [09:56<16:38,  1.34s/it] 35%|███▌      | 408/1153 [09:57<15:58,  1.29s/it] 35%|███▌      | 409/1153 [09:58<15:34,  1.26s/it] 36%|███▌      | 410/1153 [10:03<28:27,  2.30s/it] 36%|███▌      | 411/1153 [10:04<25:51,  2.09s/it] 36%|███▌      | 412/1153 [10:06<22:34,  1.83s/it] 36%|███▌      | 413/1153 [10:07<20:15,  1.64s/it] 36%|███▌      | 414/1153 [10:08<18:39,  1.51s/it] 36%|███▌      | 415/1153 [10:09<17:37,  1.43s/it] 36%|███▌      | 416/1153 [10:10<16:37,  1.35s/it] 36%|███▌      | 417/1153 [10:12<15:57,  1.30s/it] 36%|███▋      | 418/1153 [10:13<15:29,  1.26s/it] 36%|███▋      | 419/1153 [10:14<15:46,  1.29s/it] 36%|███▋      | 420/1153 [10:15<15:07,  1.24s/it] 37%|███▋      | 421/1153 [10:16<15:07,  1.24s/it] 37%|███▋      | 422/1153 [10:18<15:01,  1.23s/it] 37%|███▋      | 423/1153 [10:19<15:53,  1.31s/it] 37%|███▋      | 424/1153 [10:21<16:05,  1.32s/it] 37%|███▋      | 425/1153 [10:22<16:24,  1.35s/it] 37%|███▋      | 426/1153 [10:23<16:28,  1.36s/it] 37%|███▋      | 427/1153 [10:25<17:17,  1.43s/it] 37%|███▋      | 428/1153 [10:30<30:24,  2.52s/it] 37%|███▋      | 429/1153 [10:31<25:29,  2.11s/it] 37%|███▋      | 430/1153 [10:32<22:05,  1.83s/it] 37%|███▋      | 431/1153 [10:34<20:25,  1.70s/it] 37%|███▋      | 432/1153 [10:35<19:12,  1.60s/it] 38%|███▊      | 433/1153 [10:37<18:40,  1.56s/it] 38%|███▊      | 434/1153 [10:38<17:34,  1.47s/it] 38%|███▊      | 435/1153 [10:39<17:12,  1.44s/it] 38%|███▊      | 436/1153 [10:40<16:13,  1.36s/it] 38%|███▊      | 437/1153 [10:41<15:32,  1.30s/it] 38%|███▊      | 438/1153 [10:43<15:06,  1.27s/it] 38%|███▊      | 439/1153 [10:44<15:22,  1.29s/it] 38%|███▊      | 440/1153 [10:45<14:59,  1.26s/it] 38%|███▊      | 441/1153 [10:47<15:35,  1.31s/it] 38%|███▊      | 442/1153 [10:48<15:15,  1.29s/it] 38%|███▊      | 443/1153 [10:49<14:43,  1.24s/it] 39%|███▊      | 444/1153 [10:50<14:57,  1.27s/it] 39%|███▊      | 445/1153 [10:52<15:12,  1.29s/it] 39%|███▊      | 446/1153 [10:56<26:50,  2.28s/it] 39%|███▉      | 447/1153 [10:57<22:55,  1.95s/it] 39%|███▉      | 448/1153 [10:59<20:12,  1.72s/it] 39%|███▉      | 449/1153 [11:00<18:16,  1.56s/it] 39%|███▉      | 450/1153 [11:01<17:12,  1.47s/it] 39%|███▉      | 451/1153 [11:02<16:16,  1.39s/it] 39%|███▉      | 452/1153 [11:03<15:41,  1.34s/it] 39%|███▉      | 453/1153 [11:05<15:45,  1.35s/it] 39%|███▉      | 454/1153 [11:06<15:39,  1.34s/it] 39%|███▉      | 455/1153 [11:07<15:20,  1.32s/it] 40%|███▉      | 456/1153 [11:09<14:48,  1.27s/it] 40%|███▉      | 457/1153 [11:10<14:29,  1.25s/it] 40%|███▉      | 458/1153 [11:11<14:15,  1.23s/it] 40%|███▉      | 459/1153 [11:12<14:11,  1.23s/it] 40%|███▉      | 460/1153 [11:13<14:05,  1.22s/it] 40%|███▉      | 461/1153 [11:15<13:54,  1.21s/it] 40%|████      | 462/1153 [11:16<13:52,  1.20s/it] 40%|████      | 463/1153 [11:17<14:13,  1.24s/it] 40%|████      | 464/1153 [11:18<14:16,  1.24s/it] 40%|████      | 465/1153 [11:23<27:03,  2.36s/it] 40%|████      | 466/1153 [11:25<23:29,  2.05s/it] 41%|████      | 467/1153 [11:26<20:01,  1.75s/it] 41%|████      | 468/1153 [11:27<18:33,  1.63s/it] 41%|████      | 469/1153 [11:28<17:33,  1.54s/it] 41%|████      | 470/1153 [11:30<16:53,  1.48s/it] 41%|████      | 471/1153 [11:31<16:23,  1.44s/it] 41%|████      | 472/1153 [11:32<15:44,  1.39s/it] 41%|████      | 473/1153 [11:34<15:25,  1.36s/it] 41%|████      | 474/1153 [11:35<15:20,  1.36s/it] 41%|████      | 475/1153 [11:36<15:12,  1.35s/it] 41%|████▏     | 476/1153 [11:38<15:07,  1.34s/it] 41%|████▏     | 477/1153 [11:39<15:06,  1.34s/it] 41%|████▏     | 478/1153 [11:40<15:05,  1.34s/it] 42%|████▏     | 479/1153 [11:42<15:06,  1.35s/it] 42%|████▏     | 480/1153 [11:43<14:59,  1.34s/it] 42%|████▏     | 481/1153 [11:44<14:39,  1.31s/it] 42%|████▏     | 482/1153 [11:46<14:44,  1.32s/it] 42%|████▏     | 483/1153 [11:50<26:18,  2.36s/it] 42%|████▏     | 484/1153 [11:52<23:03,  2.07s/it] 42%|████▏     | 485/1153 [11:53<19:24,  1.74s/it] 42%|████▏     | 486/1153 [11:54<17:19,  1.56s/it] 42%|████▏     | 487/1153 [11:55<16:30,  1.49s/it] 42%|████▏     | 488/1153 [11:57<15:56,  1.44s/it] 42%|████▏     | 489/1153 [11:58<15:34,  1.41s/it] 42%|████▏     | 490/1153 [11:59<15:20,  1.39s/it] 43%|████▎     | 491/1153 [12:01<15:12,  1.38s/it] 43%|████▎     | 492/1153 [12:02<15:14,  1.38s/it] 43%|████▎     | 493/1153 [12:03<15:01,  1.37s/it] 43%|████▎     | 494/1153 [12:05<14:41,  1.34s/it] 43%|████▎     | 495/1153 [12:06<14:42,  1.34s/it] 43%|████▎     | 496/1153 [12:07<14:42,  1.34s/it] 43%|████▎     | 497/1153 [12:09<14:44,  1.35s/it] 43%|████▎     | 498/1153 [12:10<14:39,  1.34s/it] 43%|████▎     | 499/1153 [12:11<14:35,  1.34s/it] 43%|████▎     | 500/1153 [12:16<25:08,  2.31s/it] 43%|████▎     | 501/1153 [12:17<22:06,  2.03s/it] 44%|████▎     | 502/1153 [12:18<19:31,  1.80s/it] 44%|████▎     | 503/1153 [12:20<18:04,  1.67s/it] 44%|████▎     | 504/1153 [12:21<15:12,  1.41s/it] 44%|████▍     | 505/1153 [12:22<15:04,  1.40s/it] 44%|████▍     | 506/1153 [12:23<14:54,  1.38s/it] 44%|████▍     | 507/1153 [12:25<14:48,  1.37s/it] 44%|████▍     | 508/1153 [12:26<13:53,  1.29s/it] 44%|████▍     | 509/1153 [12:26<11:38,  1.08s/it] 44%|████▍     | 510/1153 [12:28<12:42,  1.19s/it] 44%|████▍     | 511/1153 [12:29<11:20,  1.06s/it] 44%|████▍     | 512/1153 [12:30<12:32,  1.17s/it] 44%|████▍     | 513/1153 [12:31<11:22,  1.07s/it] 45%|████▍     | 514/1153 [12:32<10:30,  1.01it/s] 45%|████▍     | 515/1153 [12:33<11:36,  1.09s/it] 45%|████▍     | 516/1153 [12:34<12:13,  1.15s/it] 45%|████▍     | 517/1153 [12:35<12:23,  1.17s/it] 45%|████▍     | 518/1153 [12:37<12:57,  1.22s/it] 45%|████▌     | 519/1153 [12:41<22:48,  2.16s/it] 45%|████▌     | 520/1153 [12:43<20:15,  1.92s/it] 45%|████▌     | 521/1153 [12:44<18:27,  1.75s/it] 45%|████▌     | 522/1153 [12:45<17:04,  1.62s/it] 45%|████▌     | 523/1153 [12:47<16:07,  1.54s/it] 45%|████▌     | 524/1153 [12:48<15:29,  1.48s/it] 46%|████▌     | 525/1153 [12:49<15:02,  1.44s/it] 46%|████▌     | 526/1153 [12:51<14:49,  1.42s/it] 46%|████▌     | 527/1153 [12:52<14:32,  1.39s/it] 46%|████▌     | 528/1153 [12:53<14:18,  1.37s/it] 46%|████▌     | 529/1153 [12:55<14:04,  1.35s/it] 46%|████▌     | 530/1153 [12:56<14:01,  1.35s/it] 46%|████▌     | 531/1153 [12:57<13:56,  1.34s/it] 46%|████▌     | 532/1153 [12:59<13:57,  1.35s/it] 46%|████▌     | 533/1153 [13:00<14:10,  1.37s/it] 46%|████▋     | 534/1153 [13:01<13:25,  1.30s/it] 46%|████▋     | 535/1153 [13:03<13:31,  1.31s/it] 46%|████▋     | 536/1153 [13:04<12:57,  1.26s/it] 47%|████▋     | 537/1153 [13:08<22:55,  2.23s/it] 47%|████▋     | 538/1153 [13:09<19:40,  1.92s/it] 47%|████▋     | 539/1153 [13:11<17:15,  1.69s/it] 47%|████▋     | 540/1153 [13:12<16:11,  1.59s/it] 47%|████▋     | 541/1153 [13:13<15:23,  1.51s/it] 47%|████▋     | 542/1153 [13:14<14:45,  1.45s/it] 47%|████▋     | 543/1153 [13:16<13:55,  1.37s/it] 47%|████▋     | 544/1153 [13:17<13:28,  1.33s/it] 47%|████▋     | 545/1153 [13:18<13:09,  1.30s/it] 47%|████▋     | 546/1153 [13:19<12:45,  1.26s/it] 47%|████▋     | 547/1153 [13:21<12:34,  1.25s/it] 48%|████▊     | 548/1153 [13:22<12:24,  1.23s/it] 48%|████▊     | 549/1153 [13:23<12:12,  1.21s/it] 48%|████▊     | 550/1153 [13:24<12:31,  1.25s/it] 48%|████▊     | 551/1153 [13:25<12:23,  1.24s/it] 48%|████▊     | 552/1153 [13:27<12:11,  1.22s/it] 48%|████▊     | 553/1153 [13:28<11:59,  1.20s/it] 48%|████▊     | 554/1153 [13:29<11:53,  1.19s/it] 48%|████▊     | 555/1153 [13:30<11:52,  1.19s/it] 48%|████▊     | 556/1153 [13:35<22:24,  2.25s/it] 48%|████▊     | 557/1153 [13:36<19:24,  1.95s/it] 48%|████▊     | 558/1153 [13:37<17:01,  1.72s/it] 48%|████▊     | 559/1153 [13:38<15:23,  1.55s/it] 49%|████▊     | 560/1153 [13:40<14:16,  1.44s/it] 49%|████▊     | 561/1153 [13:41<13:28,  1.37s/it] 49%|████▊     | 562/1153 [13:42<12:49,  1.30s/it] 49%|████▉     | 563/1153 [13:43<12:27,  1.27s/it] 49%|████▉     | 564/1153 [13:44<12:12,  1.24s/it] 49%|████▉     | 565/1153 [13:45<11:56,  1.22s/it] 49%|████▉     | 566/1153 [13:47<12:18,  1.26s/it] 49%|████▉     | 567/1153 [13:48<12:06,  1.24s/it] 49%|████▉     | 568/1153 [13:49<11:57,  1.23s/it] 49%|████▉     | 569/1153 [13:50<11:46,  1.21s/it] 49%|████▉     | 570/1153 [13:52<11:47,  1.21s/it] 50%|████▉     | 571/1153 [13:53<11:37,  1.20s/it] 50%|████▉     | 572/1153 [13:54<11:30,  1.19s/it] 50%|████▉     | 573/1153 [13:55<11:27,  1.19s/it] 50%|████▉     | 574/1153 [13:56<11:27,  1.19s/it] 50%|████▉     | 575/1153 [13:57<11:21,  1.18s/it] 50%|████▉     | 576/1153 [14:02<20:52,  2.17s/it] 50%|█████     | 577/1153 [14:03<18:05,  1.88s/it] 50%|█████     | 578/1153 [14:04<16:03,  1.68s/it] 50%|█████     | 579/1153 [14:06<14:44,  1.54s/it] 50%|█████     | 580/1153 [14:07<13:44,  1.44s/it] 50%|█████     | 581/1153 [14:08<13:03,  1.37s/it] 50%|█████     | 582/1153 [14:09<12:31,  1.32s/it] 51%|█████     | 583/1153 [14:10<12:08,  1.28s/it] 51%|█████     | 584/1153 [14:12<12:02,  1.27s/it] 51%|█████     | 585/1153 [14:13<11:45,  1.24s/it] 51%|█████     | 586/1153 [14:14<11:33,  1.22s/it] 51%|█████     | 587/1153 [14:15<11:26,  1.21s/it] 51%|█████     | 588/1153 [14:16<11:22,  1.21s/it] 51%|█████     | 589/1153 [14:18<11:46,  1.25s/it] 51%|█████     | 590/1153 [14:19<11:27,  1.22s/it] 51%|█████▏    | 591/1153 [14:20<11:47,  1.26s/it] 51%|█████▏    | 592/1153 [14:21<11:36,  1.24s/it] 51%|█████▏    | 593/1153 [14:23<11:48,  1.27s/it] 52%|█████▏    | 594/1153 [14:24<12:02,  1.29s/it] 52%|█████▏    | 595/1153 [14:25<12:08,  1.31s/it] 52%|█████▏    | 596/1153 [14:30<20:29,  2.21s/it] 52%|█████▏    | 597/1153 [14:31<18:01,  1.94s/it] 52%|█████▏    | 598/1153 [14:32<16:18,  1.76s/it] 52%|█████▏    | 599/1153 [14:33<14:14,  1.54s/it] 52%|█████▏    | 600/1153 [14:35<13:41,  1.49s/it] 52%|█████▏    | 601/1153 [14:36<13:15,  1.44s/it] 52%|█████▏    | 602/1153 [14:38<13:01,  1.42s/it] 52%|█████▏    | 603/1153 [14:39<12:48,  1.40s/it] 52%|█████▏    | 604/1153 [14:40<12:36,  1.38s/it] 52%|█████▏    | 605/1153 [14:42<12:26,  1.36s/it] 53%|█████▎    | 606/1153 [14:43<12:28,  1.37s/it] 53%|█████▎    | 607/1153 [14:44<12:22,  1.36s/it] 53%|█████▎    | 608/1153 [14:45<11:22,  1.25s/it] 53%|█████▎    | 609/1153 [14:46<11:06,  1.23s/it] 53%|█████▎    | 610/1153 [14:48<11:26,  1.26s/it] 53%|█████▎    | 611/1153 [14:49<11:36,  1.29s/it] 53%|█████▎    | 612/1153 [14:50<11:48,  1.31s/it] 53%|█████▎    | 613/1153 [14:52<11:47,  1.31s/it] 53%|█████▎    | 614/1153 [14:56<19:28,  2.17s/it] 53%|█████▎    | 615/1153 [14:57<17:15,  1.93s/it] 53%|█████▎    | 616/1153 [14:59<15:45,  1.76s/it] 54%|█████▎    | 617/1153 [15:00<14:43,  1.65s/it] 54%|█████▎    | 618/1153 [15:01<14:01,  1.57s/it] 54%|█████▎    | 619/1153 [15:03<13:24,  1.51s/it] 54%|█████▍    | 620/1153 [15:04<13:00,  1.46s/it] 54%|█████▍    | 621/1153 [15:06<12:38,  1.43s/it] 54%|█████▍    | 622/1153 [15:07<11:51,  1.34s/it] 54%|█████▍    | 623/1153 [15:08<11:50,  1.34s/it] 54%|█████▍    | 624/1153 [15:09<11:44,  1.33s/it] 54%|█████▍    | 625/1153 [15:11<11:47,  1.34s/it] 54%|█████▍    | 626/1153 [15:12<11:48,  1.34s/it] 54%|█████▍    | 627/1153 [15:13<11:20,  1.29s/it] 54%|█████▍    | 628/1153 [15:15<11:40,  1.33s/it] 55%|█████▍    | 629/1153 [15:16<11:37,  1.33s/it] 55%|█████▍    | 630/1153 [15:17<11:37,  1.33s/it] 55%|█████▍    | 631/1153 [15:19<11:36,  1.33s/it] 55%|█████▍    | 632/1153 [15:23<19:05,  2.20s/it] 55%|█████▍    | 633/1153 [15:24<16:22,  1.89s/it] 55%|█████▍    | 634/1153 [15:25<14:23,  1.66s/it] 55%|█████▌    | 635/1153 [15:26<13:29,  1.56s/it] 55%|█████▌    | 636/1153 [15:28<12:56,  1.50s/it] 55%|█████▌    | 637/1153 [15:29<12:31,  1.46s/it] 55%|█████▌    | 638/1153 [15:31<12:17,  1.43s/it] 55%|█████▌    | 639/1153 [15:32<12:04,  1.41s/it] 56%|█████▌    | 640/1153 [15:33<11:56,  1.40s/it] 56%|█████▌    | 641/1153 [15:35<11:45,  1.38s/it] 56%|█████▌    | 642/1153 [15:36<11:41,  1.37s/it] 56%|█████▌    | 643/1153 [15:37<11:35,  1.36s/it] 56%|█████▌    | 644/1153 [15:39<11:36,  1.37s/it] 56%|█████▌    | 645/1153 [15:40<11:33,  1.37s/it] 56%|█████▌    | 646/1153 [15:41<11:28,  1.36s/it] 56%|█████▌    | 647/1153 [15:42<09:29,  1.13s/it] 56%|█████▌    | 648/1153 [15:43<10:00,  1.19s/it] 56%|█████▋    | 649/1153 [15:45<10:26,  1.24s/it] 56%|█████▋    | 650/1153 [15:49<18:07,  2.16s/it] 56%|█████▋    | 651/1153 [15:50<16:05,  1.92s/it] 57%|█████▋    | 652/1153 [15:51<12:45,  1.53s/it] 57%|█████▋    | 653/1153 [15:52<11:57,  1.43s/it] 57%|█████▋    | 654/1153 [15:54<11:39,  1.40s/it] 57%|█████▋    | 655/1153 [15:54<09:44,  1.17s/it] 57%|█████▋    | 656/1153 [15:55<08:25,  1.02s/it] 57%|█████▋    | 657/1153 [15:56<09:14,  1.12s/it] 57%|█████▋    | 658/1153 [15:58<09:51,  1.19s/it] 57%|█████▋    | 659/1153 [15:59<10:19,  1.25s/it] 57%|█████▋    | 660/1153 [16:00<10:40,  1.30s/it] 57%|█████▋    | 661/1153 [16:02<10:50,  1.32s/it] 57%|█████▋    | 662/1153 [16:03<10:55,  1.34s/it] 58%|█████▊    | 663/1153 [16:04<10:59,  1.35s/it] 58%|█████▊    | 664/1153 [16:06<11:01,  1.35s/it] 58%|█████▊    | 665/1153 [16:07<11:03,  1.36s/it] 58%|█████▊    | 666/1153 [16:09<11:06,  1.37s/it] 58%|█████▊    | 667/1153 [16:10<11:02,  1.36s/it] 58%|█████▊    | 668/1153 [16:11<10:59,  1.36s/it] 58%|█████▊    | 669/1153 [16:16<18:02,  2.24s/it] 58%|█████▊    | 670/1153 [16:17<15:58,  1.98s/it] 58%|█████▊    | 671/1153 [16:18<14:29,  1.80s/it] 58%|█████▊    | 672/1153 [16:20<13:27,  1.68s/it] 58%|█████▊    | 673/1153 [16:21<12:38,  1.58s/it] 58%|█████▊    | 674/1153 [16:22<12:05,  1.51s/it] 59%|█████▊    | 675/1153 [16:24<11:43,  1.47s/it] 59%|█████▊    | 676/1153 [16:25<11:16,  1.42s/it] 59%|█████▊    | 677/1153 [16:26<11:06,  1.40s/it] 59%|█████▉    | 678/1153 [16:28<10:31,  1.33s/it] 59%|█████▉    | 679/1153 [16:29<10:38,  1.35s/it] 59%|█████▉    | 680/1153 [16:30<10:24,  1.32s/it] 59%|█████▉    | 681/1153 [16:31<10:04,  1.28s/it] 59%|█████▉    | 682/1153 [16:33<10:15,  1.31s/it] 59%|█████▉    | 683/1153 [16:34<10:22,  1.32s/it] 59%|█████▉    | 684/1153 [16:35<10:01,  1.28s/it] 59%|█████▉    | 685/1153 [16:37<10:12,  1.31s/it] 59%|█████▉    | 686/1153 [16:38<10:20,  1.33s/it] 60%|█████▉    | 687/1153 [16:39<10:01,  1.29s/it] 60%|█████▉    | 688/1153 [16:44<16:43,  2.16s/it] 60%|█████▉    | 689/1153 [16:45<14:57,  1.94s/it] 60%|█████▉    | 690/1153 [16:46<13:10,  1.71s/it] 60%|█████▉    | 691/1153 [16:47<12:00,  1.56s/it] 60%|██████    | 692/1153 [16:49<11:15,  1.46s/it] 60%|██████    | 693/1153 [16:50<10:35,  1.38s/it] 60%|██████    | 694/1153 [16:51<10:34,  1.38s/it] 60%|██████    | 695/1153 [16:53<10:33,  1.38s/it] 60%|██████    | 696/1153 [16:54<10:28,  1.38s/it] 60%|██████    | 697/1153 [16:55<10:24,  1.37s/it] 61%|██████    | 698/1153 [16:57<10:22,  1.37s/it] 61%|██████    | 699/1153 [16:58<10:22,  1.37s/it] 61%|██████    | 700/1153 [16:59<10:21,  1.37s/it] 61%|██████    | 701/1153 [17:01<10:23,  1.38s/it] 61%|██████    | 702/1153 [17:02<10:23,  1.38s/it] 61%|██████    | 703/1153 [17:04<10:24,  1.39s/it] 61%|██████    | 704/1153 [17:05<10:23,  1.39s/it] 61%|██████    | 705/1153 [17:06<10:22,  1.39s/it] 61%|██████    | 706/1153 [17:11<16:42,  2.24s/it] 61%|██████▏   | 707/1153 [17:12<14:50,  2.00s/it] 61%|██████▏   | 708/1153 [17:13<13:17,  1.79s/it] 61%|██████▏   | 709/1153 [17:15<12:19,  1.67s/it] 62%|██████▏   | 710/1153 [17:16<11:43,  1.59s/it] 62%|██████▏   | 711/1153 [17:17<11:21,  1.54s/it] 62%|██████▏   | 712/1153 [17:19<11:00,  1.50s/it] 62%|██████▏   | 713/1153 [17:20<10:40,  1.46s/it] 62%|██████▏   | 714/1153 [17:22<10:27,  1.43s/it] 62%|██████▏   | 715/1153 [17:23<10:18,  1.41s/it] 62%|██████▏   | 716/1153 [17:24<10:13,  1.40s/it] 62%|██████▏   | 717/1153 [17:26<10:10,  1.40s/it] 62%|██████▏   | 718/1153 [17:27<10:07,  1.40s/it] 62%|██████▏   | 719/1153 [17:29<10:00,  1.38s/it] 62%|██████▏   | 720/1153 [17:30<09:56,  1.38s/it] 63%|██████▎   | 721/1153 [17:31<09:54,  1.38s/it] 63%|██████▎   | 722/1153 [17:33<09:53,  1.38s/it] 63%|██████▎   | 723/1153 [17:37<16:31,  2.31s/it] 63%|██████▎   | 724/1153 [17:38<14:28,  2.02s/it] 63%|██████▎   | 725/1153 [17:40<13:00,  1.82s/it] 63%|██████▎   | 726/1153 [17:41<11:59,  1.69s/it] 63%|██████▎   | 727/1153 [17:43<11:18,  1.59s/it] 63%|██████▎   | 728/1153 [17:44<10:50,  1.53s/it] 63%|██████▎   | 729/1153 [17:45<10:32,  1.49s/it] 63%|██████▎   | 730/1153 [17:47<10:15,  1.45s/it] 63%|██████▎   | 731/1153 [17:48<10:03,  1.43s/it] 63%|██████▎   | 732/1153 [17:49<09:55,  1.42s/it] 64%|██████▎   | 733/1153 [17:51<09:50,  1.41s/it] 64%|██████▎   | 734/1153 [17:52<09:47,  1.40s/it] 64%|██████▎   | 735/1153 [17:54<09:46,  1.40s/it] 64%|██████▍   | 736/1153 [17:55<09:39,  1.39s/it] 64%|██████▍   | 737/1153 [17:56<09:35,  1.38s/it] 64%|██████▍   | 738/1153 [17:58<09:34,  1.38s/it] 64%|██████▍   | 739/1153 [17:59<09:33,  1.38s/it] 64%|██████▍   | 740/1153 [18:01<09:33,  1.39s/it] 64%|██████▍   | 741/1153 [18:05<15:48,  2.30s/it] 64%|██████▍   | 742/1153 [18:06<13:58,  2.04s/it] 64%|██████▍   | 743/1153 [18:08<12:36,  1.85s/it] 65%|██████▍   | 744/1153 [18:09<11:03,  1.62s/it] 65%|██████▍   | 745/1153 [18:10<10:33,  1.55s/it] 65%|██████▍   | 746/1153 [18:12<10:02,  1.48s/it] 65%|██████▍   | 747/1153 [18:13<09:48,  1.45s/it] 65%|██████▍   | 748/1153 [18:14<09:38,  1.43s/it] 65%|██████▍   | 749/1153 [18:16<09:31,  1.42s/it] 65%|██████▌   | 750/1153 [18:17<09:28,  1.41s/it] 65%|██████▌   | 751/1153 [18:19<09:33,  1.43s/it] 65%|██████▌   | 752/1153 [18:20<09:29,  1.42s/it] 65%|██████▌   | 753/1153 [18:21<09:16,  1.39s/it] 65%|██████▌   | 754/1153 [18:23<09:14,  1.39s/it] 65%|██████▌   | 755/1153 [18:24<09:14,  1.39s/it] 66%|██████▌   | 756/1153 [18:25<08:52,  1.34s/it] 66%|██████▌   | 757/1153 [18:27<08:40,  1.32s/it] 66%|██████▌   | 758/1153 [18:28<08:29,  1.29s/it] 66%|██████▌   | 759/1153 [18:29<08:21,  1.27s/it] 66%|██████▌   | 760/1153 [18:33<14:20,  2.19s/it] 66%|██████▌   | 761/1153 [18:35<12:46,  1.96s/it] 66%|██████▌   | 762/1153 [18:36<11:22,  1.74s/it] 66%|██████▌   | 763/1153 [18:37<10:18,  1.59s/it] 66%|██████▋   | 764/1153 [18:39<09:39,  1.49s/it] 66%|██████▋   | 765/1153 [18:40<09:11,  1.42s/it] 66%|██████▋   | 766/1153 [18:41<08:45,  1.36s/it] 67%|██████▋   | 767/1153 [18:42<08:32,  1.33s/it] 67%|██████▋   | 768/1153 [18:43<08:18,  1.30s/it] 67%|██████▋   | 769/1153 [18:45<08:06,  1.27s/it] 67%|██████▋   | 770/1153 [18:46<08:05,  1.27s/it] 67%|██████▋   | 771/1153 [18:47<08:00,  1.26s/it] 67%|██████▋   | 772/1153 [18:48<07:59,  1.26s/it] 67%|██████▋   | 773/1153 [18:50<07:55,  1.25s/it] 67%|██████▋   | 774/1153 [18:51<07:51,  1.24s/it] 67%|██████▋   | 775/1153 [18:52<07:51,  1.25s/it] 67%|██████▋   | 776/1153 [18:53<07:48,  1.24s/it] 67%|██████▋   | 777/1153 [18:55<07:48,  1.24s/it] 67%|██████▋   | 778/1153 [18:56<07:46,  1.24s/it] 68%|██████▊   | 779/1153 [18:57<07:42,  1.24s/it] 68%|██████▊   | 780/1153 [18:58<07:45,  1.25s/it] 68%|██████▊   | 781/1153 [19:03<13:18,  2.15s/it] 68%|██████▊   | 782/1153 [19:04<11:35,  1.87s/it] 68%|██████▊   | 783/1153 [19:05<10:27,  1.70s/it] 68%|██████▊   | 784/1153 [19:06<09:38,  1.57s/it] 68%|██████▊   | 785/1153 [19:08<09:22,  1.53s/it] 68%|██████▊   | 786/1153 [19:09<09:12,  1.51s/it] 68%|██████▊   | 787/1153 [19:11<09:03,  1.48s/it] 68%|██████▊   | 788/1153 [19:12<08:51,  1.46s/it] 68%|██████▊   | 789/1153 [19:14<08:42,  1.44s/it] 69%|██████▊   | 790/1153 [19:15<08:36,  1.42s/it] 69%|██████▊   | 791/1153 [19:16<08:04,  1.34s/it] 69%|██████▊   | 792/1153 [19:17<07:42,  1.28s/it] 69%|██████▉   | 793/1153 [19:19<07:53,  1.31s/it] 69%|██████▉   | 794/1153 [19:20<08:00,  1.34s/it] 69%|██████▉   | 795/1153 [19:21<08:11,  1.37s/it] 69%|██████▉   | 796/1153 [19:23<08:15,  1.39s/it] 69%|██████▉   | 797/1153 [19:24<08:17,  1.40s/it] 69%|██████▉   | 798/1153 [19:26<08:14,  1.39s/it] 69%|██████▉   | 799/1153 [19:27<08:12,  1.39s/it] 69%|██████▉   | 800/1153 [19:31<13:33,  2.31s/it] 69%|██████▉   | 801/1153 [19:33<11:58,  2.04s/it] 70%|██████▉   | 802/1153 [19:34<10:50,  1.85s/it] 70%|██████▉   | 803/1153 [19:36<10:03,  1.72s/it] 70%|██████▉   | 804/1153 [19:37<09:26,  1.62s/it] 70%|██████▉   | 805/1153 [19:39<09:00,  1.55s/it] 70%|██████▉   | 806/1153 [19:40<08:43,  1.51s/it] 70%|██████▉   | 807/1153 [19:41<08:31,  1.48s/it] 70%|███████   | 808/1153 [19:43<08:22,  1.46s/it] 70%|███████   | 809/1153 [19:44<08:17,  1.45s/it] 70%|███████   | 810/1153 [19:46<08:10,  1.43s/it] 70%|███████   | 811/1153 [19:47<08:05,  1.42s/it] 70%|███████   | 812/1153 [19:48<08:02,  1.41s/it] 71%|███████   | 813/1153 [19:50<08:00,  1.41s/it] 71%|███████   | 814/1153 [19:51<07:59,  1.41s/it] 71%|███████   | 815/1153 [19:53<07:58,  1.42s/it] 71%|███████   | 816/1153 [19:53<06:34,  1.17s/it] 71%|███████   | 817/1153 [19:55<06:56,  1.24s/it] 71%|███████   | 818/1153 [19:59<12:08,  2.17s/it] 71%|███████   | 819/1153 [20:00<10:50,  1.95s/it] 71%|███████   | 820/1153 [20:02<09:54,  1.78s/it] 71%|███████   | 821/1153 [20:03<08:57,  1.62s/it] 71%|███████▏  | 822/1153 [20:04<07:33,  1.37s/it] 71%|███████▏  | 823/1153 [20:05<07:29,  1.36s/it] 71%|███████▏  | 824/1153 [20:07<07:33,  1.38s/it] 72%|███████▏  | 825/1153 [20:08<07:05,  1.30s/it] 72%|███████▏  | 826/1153 [20:09<07:18,  1.34s/it] 72%|███████▏  | 827/1153 [20:10<07:06,  1.31s/it] 72%|███████▏  | 828/1153 [20:12<07:18,  1.35s/it] 72%|███████▏  | 829/1153 [20:13<06:46,  1.25s/it] 72%|███████▏  | 830/1153 [20:14<07:02,  1.31s/it] 72%|███████▏  | 831/1153 [20:16<07:09,  1.33s/it] 72%|███████▏  | 832/1153 [20:17<07:13,  1.35s/it] 72%|███████▏  | 833/1153 [20:18<07:17,  1.37s/it] 72%|███████▏  | 834/1153 [20:20<07:20,  1.38s/it] 72%|███████▏  | 835/1153 [20:21<07:23,  1.39s/it] 73%|███████▎  | 836/1153 [20:23<07:26,  1.41s/it] 73%|███████▎  | 837/1153 [20:24<07:26,  1.41s/it] 73%|███████▎  | 838/1153 [20:28<12:00,  2.29s/it] 73%|███████▎  | 839/1153 [20:30<10:37,  2.03s/it] 73%|███████▎  | 840/1153 [20:31<09:35,  1.84s/it] 73%|███████▎  | 841/1153 [20:33<08:54,  1.71s/it] 73%|███████▎  | 842/1153 [20:34<08:22,  1.62s/it] 73%|███████▎  | 843/1153 [20:36<08:00,  1.55s/it] 73%|███████▎  | 844/1153 [20:37<07:45,  1.51s/it] 73%|███████▎  | 845/1153 [20:38<07:35,  1.48s/it] 73%|███████▎  | 846/1153 [20:40<07:10,  1.40s/it] 73%|███████▎  | 847/1153 [20:41<07:09,  1.40s/it] 74%|███████▎  | 848/1153 [20:42<06:48,  1.34s/it] 74%|███████▎  | 849/1153 [20:43<06:42,  1.32s/it] 74%|███████▎  | 850/1153 [20:45<06:50,  1.36s/it] 74%|███████▍  | 851/1153 [20:46<06:52,  1.37s/it] 74%|███████▍  | 852/1153 [20:48<06:54,  1.38s/it] 74%|███████▍  | 853/1153 [20:49<06:34,  1.32s/it] 74%|███████▍  | 854/1153 [20:50<06:40,  1.34s/it] 74%|███████▍  | 855/1153 [20:52<06:45,  1.36s/it] 74%|███████▍  | 856/1153 [20:53<06:30,  1.31s/it] 74%|███████▍  | 857/1153 [20:57<11:24,  2.31s/it] 74%|███████▍  | 858/1153 [20:59<10:05,  2.05s/it] 75%|███████▍  | 859/1153 [21:00<08:50,  1.80s/it] 75%|███████▍  | 860/1153 [21:01<08:01,  1.64s/it] 75%|███████▍  | 861/1153 [21:03<07:24,  1.52s/it] 75%|███████▍  | 862/1153 [21:04<07:14,  1.49s/it] 75%|███████▍  | 863/1153 [21:05<07:03,  1.46s/it] 75%|███████▍  | 864/1153 [21:07<06:43,  1.40s/it] 75%|███████▌  | 865/1153 [21:08<06:30,  1.35s/it] 75%|███████▌  | 866/1153 [21:09<06:21,  1.33s/it] 75%|███████▌  | 867/1153 [21:11<06:28,  1.36s/it] 75%|███████▌  | 868/1153 [21:12<06:19,  1.33s/it] 75%|███████▌  | 869/1153 [21:13<06:14,  1.32s/it] 75%|███████▌  | 870/1153 [21:14<06:07,  1.30s/it] 76%|███████▌  | 871/1153 [21:16<06:17,  1.34s/it] 76%|███████▌  | 872/1153 [21:17<06:24,  1.37s/it] 76%|███████▌  | 873/1153 [21:19<06:10,  1.32s/it] 76%|███████▌  | 874/1153 [21:20<06:04,  1.31s/it] 76%|███████▌  | 875/1153 [21:21<05:59,  1.29s/it] 76%|███████▌  | 876/1153 [21:22<05:52,  1.27s/it] 76%|███████▌  | 877/1153 [21:27<10:11,  2.21s/it] 76%|███████▌  | 878/1153 [21:28<08:52,  1.94s/it] 76%|███████▌  | 879/1153 [21:29<07:52,  1.72s/it] 76%|███████▋  | 880/1153 [21:31<07:13,  1.59s/it] 76%|███████▋  | 881/1153 [21:32<06:44,  1.49s/it] 76%|███████▋  | 882/1153 [21:33<06:22,  1.41s/it] 77%|███████▋  | 883/1153 [21:34<06:10,  1.37s/it] 77%|███████▋  | 884/1153 [21:36<06:14,  1.39s/it] 77%|███████▋  | 885/1153 [21:37<06:13,  1.39s/it] 77%|███████▋  | 886/1153 [21:39<06:13,  1.40s/it] 77%|███████▋  | 887/1153 [21:40<06:13,  1.40s/it] 77%|███████▋  | 888/1153 [21:41<06:13,  1.41s/it] 77%|███████▋  | 889/1153 [21:43<05:57,  1.36s/it] 77%|███████▋  | 890/1153 [21:44<05:51,  1.34s/it] 77%|███████▋  | 891/1153 [21:45<05:59,  1.37s/it] 77%|███████▋  | 892/1153 [21:47<05:49,  1.34s/it] 77%|███████▋  | 893/1153 [21:48<05:42,  1.32s/it] 78%|███████▊  | 894/1153 [21:49<05:40,  1.31s/it] 78%|███████▊  | 895/1153 [21:50<05:34,  1.30s/it] 78%|███████▊  | 896/1153 [21:52<05:27,  1.28s/it] 78%|███████▊  | 897/1153 [21:53<05:27,  1.28s/it] 78%|███████▊  | 898/1153 [21:57<09:25,  2.22s/it] 78%|███████▊  | 899/1153 [21:59<08:17,  1.96s/it] 78%|███████▊  | 900/1153 [22:00<07:42,  1.83s/it] 78%|███████▊  | 901/1153 [22:02<07:01,  1.67s/it] 78%|███████▊  | 902/1153 [22:03<06:26,  1.54s/it] 78%|███████▊  | 903/1153 [22:04<06:07,  1.47s/it] 78%|███████▊  | 904/1153 [22:05<05:52,  1.42s/it] 78%|███████▊  | 905/1153 [22:07<05:42,  1.38s/it] 79%|███████▊  | 906/1153 [22:08<05:37,  1.37s/it] 79%|███████▊  | 907/1153 [22:09<05:29,  1.34s/it] 79%|███████▉  | 908/1153 [22:11<05:23,  1.32s/it] 79%|███████▉  | 909/1153 [22:12<05:17,  1.30s/it] 79%|███████▉  | 910/1153 [22:13<05:13,  1.29s/it] 79%|███████▉  | 911/1153 [22:14<05:14,  1.30s/it] 79%|███████▉  | 912/1153 [22:16<05:11,  1.29s/it] 79%|███████▉  | 913/1153 [22:17<05:06,  1.28s/it] 79%|███████▉  | 914/1153 [22:18<05:06,  1.28s/it] 79%|███████▉  | 915/1153 [22:19<05:04,  1.28s/it] 79%|███████▉  | 916/1153 [22:21<05:03,  1.28s/it] 80%|███████▉  | 917/1153 [22:22<05:01,  1.28s/it] 80%|███████▉  | 918/1153 [22:23<04:58,  1.27s/it] 80%|███████▉  | 919/1153 [22:28<08:39,  2.22s/it] 80%|███████▉  | 920/1153 [22:29<07:46,  2.00s/it] 80%|███████▉  | 921/1153 [22:31<07:05,  1.83s/it] 80%|███████▉  | 922/1153 [22:32<06:36,  1.72s/it] 80%|████████  | 923/1153 [22:33<06:12,  1.62s/it] 80%|████████  | 924/1153 [22:35<05:56,  1.55s/it] 80%|████████  | 925/1153 [22:36<05:44,  1.51s/it] 80%|████████  | 926/1153 [22:38<05:36,  1.48s/it] 80%|████████  | 927/1153 [22:39<05:15,  1.40s/it] 80%|████████  | 928/1153 [22:40<05:15,  1.40s/it] 81%|████████  | 929/1153 [22:42<05:15,  1.41s/it] 81%|████████  | 930/1153 [22:43<05:12,  1.40s/it] 81%|████████  | 931/1153 [22:45<05:11,  1.40s/it] 81%|████████  | 932/1153 [22:46<05:10,  1.40s/it] 81%|████████  | 933/1153 [22:47<05:10,  1.41s/it] 81%|████████  | 934/1153 [22:49<05:09,  1.41s/it] 81%|████████  | 935/1153 [22:50<05:09,  1.42s/it] 81%|████████  | 936/1153 [22:52<05:09,  1.43s/it] 81%|████████▏ | 937/1153 [22:56<08:42,  2.42s/it] 81%|████████▏ | 938/1153 [22:58<07:37,  2.13s/it] 81%|████████▏ | 939/1153 [22:59<06:50,  1.92s/it] 82%|████████▏ | 940/1153 [23:01<06:17,  1.77s/it] 82%|████████▏ | 941/1153 [23:02<05:53,  1.67s/it] 82%|████████▏ | 942/1153 [23:04<05:35,  1.59s/it] 82%|████████▏ | 943/1153 [23:05<05:21,  1.53s/it] 82%|████████▏ | 944/1153 [23:06<05:13,  1.50s/it] 82%|████████▏ | 945/1153 [23:08<05:00,  1.44s/it] 82%|████████▏ | 946/1153 [23:09<04:51,  1.41s/it] 82%|████████▏ | 947/1153 [23:10<04:47,  1.40s/it] 82%|████████▏ | 948/1153 [23:12<04:48,  1.41s/it] 82%|████████▏ | 949/1153 [23:13<04:50,  1.42s/it] 82%|████████▏ | 950/1153 [23:15<04:48,  1.42s/it] 82%|████████▏ | 951/1153 [23:16<04:46,  1.42s/it] 83%|████████▎ | 952/1153 [23:18<04:48,  1.43s/it] 83%|████████▎ | 953/1153 [23:19<04:47,  1.44s/it] 83%|████████▎ | 954/1153 [23:20<04:40,  1.41s/it] 83%|████████▎ | 955/1153 [23:22<04:39,  1.41s/it] 83%|████████▎ | 956/1153 [23:26<07:39,  2.33s/it] 83%|████████▎ | 957/1153 [23:28<06:45,  2.07s/it] 83%|████████▎ | 958/1153 [23:29<06:08,  1.89s/it] 83%|████████▎ | 959/1153 [23:31<05:40,  1.75s/it] 83%|████████▎ | 960/1153 [23:32<05:18,  1.65s/it] 83%|████████▎ | 961/1153 [23:33<04:16,  1.34s/it] 83%|████████▎ | 962/1153 [23:34<04:08,  1.30s/it] 84%|████████▎ | 963/1153 [23:35<04:13,  1.33s/it] 84%|████████▎ | 964/1153 [23:37<04:17,  1.36s/it] 84%|████████▎ | 965/1153 [23:38<04:15,  1.36s/it] 84%|████████▍ | 966/1153 [23:39<04:16,  1.37s/it] 84%|████████▍ | 967/1153 [23:41<04:10,  1.34s/it] 84%|████████▍ | 968/1153 [23:42<04:14,  1.37s/it] 84%|████████▍ | 969/1153 [23:44<04:14,  1.38s/it] 84%|████████▍ | 970/1153 [23:45<03:51,  1.27s/it] 84%|████████▍ | 971/1153 [23:45<03:04,  1.01s/it] 84%|████████▍ | 972/1153 [23:45<02:35,  1.16it/s] 84%|████████▍ | 973/1153 [23:46<02:40,  1.12it/s] 84%|████████▍ | 974/1153 [23:48<03:06,  1.04s/it] 85%|████████▍ | 975/1153 [23:49<03:25,  1.15s/it] 85%|████████▍ | 976/1153 [23:51<03:38,  1.23s/it] 85%|████████▍ | 977/1153 [23:52<03:47,  1.29s/it] 85%|████████▍ | 978/1153 [23:57<06:45,  2.32s/it] 85%|████████▍ | 979/1153 [23:58<05:56,  2.05s/it] 85%|████████▍ | 980/1153 [24:00<05:21,  1.86s/it] 85%|████████▌ | 981/1153 [24:01<04:56,  1.72s/it] 85%|████████▌ | 982/1153 [24:02<04:37,  1.63s/it] 85%|████████▌ | 983/1153 [24:04<04:26,  1.57s/it] 85%|████████▌ | 984/1153 [24:05<04:11,  1.49s/it] 85%|████████▌ | 985/1153 [24:07<04:06,  1.47s/it] 86%|████████▌ | 986/1153 [24:08<04:03,  1.46s/it] 86%|████████▌ | 987/1153 [24:09<04:00,  1.45s/it] 86%|████████▌ | 988/1153 [24:11<03:55,  1.43s/it] 86%|████████▌ | 989/1153 [24:12<03:53,  1.42s/it] 86%|████████▌ | 990/1153 [24:14<03:51,  1.42s/it] 86%|████████▌ | 991/1153 [24:15<03:40,  1.36s/it] 86%|████████▌ | 992/1153 [24:16<03:42,  1.38s/it] 86%|████████▌ | 993/1153 [24:18<03:44,  1.41s/it] 86%|████████▌ | 994/1153 [24:19<03:46,  1.42s/it] 86%|████████▋ | 995/1153 [24:21<03:45,  1.43s/it] 86%|████████▋ | 996/1153 [24:22<03:36,  1.38s/it] 86%|████████▋ | 997/1153 [24:27<06:04,  2.34s/it] 87%|████████▋ | 998/1153 [24:28<05:12,  2.02s/it] 87%|████████▋ | 999/1153 [24:29<04:36,  1.79s/it] 87%|████████▋ | 1000/1153 [24:30<04:16,  1.68s/it] 87%|████████▋ | 1001/1153 [24:32<04:03,  1.60s/it] 87%|████████▋ | 1002/1153 [24:33<03:48,  1.52s/it] 87%|████████▋ | 1003/1153 [24:35<03:38,  1.46s/it] 87%|████████▋ | 1004/1153 [24:36<03:27,  1.40s/it] 87%|████████▋ | 1005/1153 [24:37<03:27,  1.40s/it] 87%|████████▋ | 1006/1153 [24:39<03:26,  1.40s/it] 87%|████████▋ | 1007/1153 [24:40<03:18,  1.36s/it] 87%|████████▋ | 1008/1153 [24:41<03:12,  1.33s/it] 88%|████████▊ | 1009/1153 [24:42<03:09,  1.31s/it] 88%|████████▊ | 1010/1153 [24:44<03:05,  1.30s/it] 88%|████████▊ | 1011/1153 [24:45<03:03,  1.30s/it] 88%|████████▊ | 1012/1153 [24:46<03:02,  1.29s/it] 88%|████████▊ | 1013/1153 [24:48<03:07,  1.34s/it] 88%|████████▊ | 1014/1153 [24:49<03:03,  1.32s/it] 88%|████████▊ | 1015/1153 [24:50<02:59,  1.30s/it] 88%|████████▊ | 1016/1153 [24:52<02:58,  1.30s/it] 88%|████████▊ | 1017/1153 [24:53<02:55,  1.29s/it] 88%|████████▊ | 1018/1153 [24:54<02:54,  1.30s/it] 88%|████████▊ | 1019/1153 [24:59<05:06,  2.29s/it] 88%|████████▊ | 1020/1153 [25:00<04:24,  1.99s/it] 89%|████████▊ | 1021/1153 [25:01<03:54,  1.78s/it] 89%|████████▊ | 1022/1153 [25:03<03:32,  1.62s/it] 89%|████████▊ | 1023/1153 [25:04<03:16,  1.51s/it] 89%|████████▉ | 1024/1153 [25:05<03:06,  1.44s/it] 89%|████████▉ | 1025/1153 [25:06<02:56,  1.38s/it] 89%|████████▉ | 1026/1153 [25:08<02:58,  1.40s/it] 89%|████████▉ | 1027/1153 [25:09<02:59,  1.42s/it] 89%|████████▉ | 1028/1153 [25:11<02:57,  1.42s/it] 89%|████████▉ | 1029/1153 [25:12<02:56,  1.42s/it] 89%|████████▉ | 1030/1153 [25:14<02:55,  1.43s/it] 89%|████████▉ | 1031/1153 [25:15<02:54,  1.43s/it] 90%|████████▉ | 1032/1153 [25:16<02:54,  1.44s/it] 90%|████████▉ | 1033/1153 [25:18<02:47,  1.39s/it] 90%|████████▉ | 1034/1153 [25:19<02:49,  1.42s/it] 90%|████████▉ | 1035/1153 [25:21<02:44,  1.39s/it] 90%|████████▉ | 1036/1153 [25:22<02:39,  1.37s/it] 90%|████████▉ | 1037/1153 [25:23<02:35,  1.34s/it] 90%|█████████ | 1038/1153 [25:27<04:18,  2.25s/it] 90%|█████████ | 1039/1153 [25:29<03:44,  1.97s/it] 90%|█████████ | 1040/1153 [25:30<03:19,  1.76s/it] 90%|█████████ | 1041/1153 [25:31<03:00,  1.61s/it] 90%|█████████ | 1042/1153 [25:33<02:48,  1.52s/it] 90%|█████████ | 1043/1153 [25:34<02:40,  1.46s/it] 91%|█████████ | 1044/1153 [25:35<02:32,  1.40s/it] 91%|█████████ | 1045/1153 [25:37<02:27,  1.37s/it] 91%|█████████ | 1046/1153 [25:38<02:23,  1.34s/it] 91%|█████████ | 1047/1153 [25:39<02:21,  1.33s/it] 91%|█████████ | 1048/1153 [25:40<02:18,  1.32s/it] 91%|█████████ | 1049/1153 [25:42<02:14,  1.29s/it] 91%|█████████ | 1050/1153 [25:43<02:12,  1.29s/it] 91%|█████████ | 1051/1153 [25:44<02:10,  1.28s/it] 91%|█████████ | 1052/1153 [25:45<02:08,  1.27s/it] 91%|█████████▏| 1053/1153 [25:47<02:08,  1.28s/it] 91%|█████████▏| 1054/1153 [25:48<02:06,  1.28s/it] 92%|█████████▏| 1055/1153 [25:49<02:05,  1.28s/it] 92%|█████████▏| 1056/1153 [25:51<02:04,  1.28s/it] 92%|█████████▏| 1057/1153 [25:52<02:02,  1.28s/it] 92%|█████████▏| 1058/1153 [25:53<02:02,  1.29s/it] 92%|█████████▏| 1059/1153 [25:54<02:00,  1.28s/it] 92%|█████████▏| 1060/1153 [25:59<03:35,  2.32s/it] 92%|█████████▏| 1061/1153 [26:01<03:09,  2.06s/it] 92%|█████████▏| 1062/1153 [26:02<02:51,  1.88s/it] 92%|█████████▏| 1063/1153 [26:04<02:38,  1.76s/it] 92%|█████████▏| 1064/1153 [26:05<02:29,  1.68s/it] 92%|█████████▏| 1065/1153 [26:06<02:21,  1.61s/it] 92%|█████████▏| 1066/1153 [26:08<02:15,  1.56s/it] 93%|█████████▎| 1067/1153 [26:09<02:11,  1.52s/it] 93%|█████████▎| 1068/1153 [26:11<02:07,  1.50s/it] 93%|█████████▎| 1069/1153 [26:12<02:05,  1.49s/it] 93%|█████████▎| 1070/1153 [26:14<02:03,  1.48s/it] 93%|█████████▎| 1071/1153 [26:15<02:00,  1.47s/it] 93%|█████████▎| 1072/1153 [26:17<01:58,  1.46s/it] 93%|█████████▎| 1073/1153 [26:18<01:56,  1.45s/it] 93%|█████████▎| 1074/1153 [26:20<01:55,  1.46s/it] 93%|█████████▎| 1075/1153 [26:21<01:54,  1.47s/it] 93%|█████████▎| 1076/1153 [26:22<01:53,  1.47s/it] 93%|█████████▎| 1077/1153 [26:24<01:51,  1.47s/it] 93%|█████████▎| 1078/1153 [26:25<01:50,  1.47s/it] 94%|█████████▎| 1079/1153 [26:27<01:42,  1.38s/it] 94%|█████████▎| 1080/1153 [26:31<02:50,  2.33s/it] 94%|█████████▍| 1081/1153 [26:33<02:29,  2.07s/it] 94%|█████████▍| 1082/1153 [26:34<02:13,  1.89s/it] 94%|█████████▍| 1083/1153 [26:36<02:03,  1.76s/it] 94%|█████████▍| 1084/1153 [26:37<01:56,  1.69s/it] 94%|█████████▍| 1085/1153 [26:39<01:51,  1.64s/it] 94%|█████████▍| 1086/1153 [26:40<01:45,  1.58s/it] 94%|█████████▍| 1087/1153 [26:41<01:38,  1.49s/it] 94%|█████████▍| 1088/1153 [26:43<01:35,  1.47s/it] 94%|█████████▍| 1089/1153 [26:44<01:33,  1.46s/it] 95%|█████████▍| 1090/1153 [26:46<01:31,  1.46s/it] 95%|█████████▍| 1091/1153 [26:47<01:30,  1.46s/it] 95%|█████████▍| 1092/1153 [26:49<01:28,  1.46s/it] 95%|█████████▍| 1093/1153 [26:50<01:27,  1.46s/it] 95%|█████████▍| 1094/1153 [26:51<01:25,  1.45s/it] 95%|█████████▍| 1095/1153 [26:53<01:21,  1.41s/it] 95%|█████████▌| 1096/1153 [26:54<01:21,  1.43s/it] 95%|█████████▌| 1097/1153 [26:56<01:19,  1.43s/it] 95%|█████████▌| 1098/1153 [26:57<01:18,  1.43s/it] 95%|█████████▌| 1099/1153 [27:02<02:12,  2.45s/it] 95%|█████████▌| 1100/1153 [27:03<01:43,  1.94s/it] 95%|█████████▌| 1101/1153 [27:04<01:33,  1.80s/it] 96%|█████████▌| 1102/1153 [27:06<01:26,  1.70s/it] 96%|█████████▌| 1103/1153 [27:07<01:21,  1.63s/it] 96%|█████████▌| 1104/1153 [27:09<01:16,  1.57s/it] 96%|█████████▌| 1105/1153 [27:10<01:07,  1.42s/it] 96%|█████████▌| 1106/1153 [27:11<01:07,  1.43s/it] 96%|█████████▌| 1107/1153 [27:12<00:56,  1.23s/it] 96%|█████████▌| 1108/1153 [27:13<00:58,  1.30s/it] 96%|█████████▌| 1109/1153 [27:14<00:47,  1.08s/it] 96%|█████████▋| 1110/1153 [27:15<00:48,  1.12s/it] 96%|█████████▋| 1111/1153 [27:16<00:51,  1.22s/it] 96%|█████████▋| 1112/1153 [27:18<00:53,  1.30s/it] 97%|█████████▋| 1113/1153 [27:19<00:53,  1.34s/it] 97%|█████████▋| 1114/1153 [27:21<00:53,  1.37s/it] 97%|█████████▋| 1115/1153 [27:22<00:53,  1.40s/it] 97%|█████████▋| 1116/1153 [27:24<00:52,  1.42s/it] 97%|█████████▋| 1117/1153 [27:25<00:51,  1.44s/it] 97%|█████████▋| 1118/1153 [27:27<00:51,  1.47s/it] 97%|█████████▋| 1119/1153 [27:28<00:50,  1.47s/it] 97%|█████████▋| 1120/1153 [27:33<01:19,  2.40s/it] 97%|█████████▋| 1121/1153 [27:34<01:07,  2.10s/it] 97%|█████████▋| 1122/1153 [27:36<00:57,  1.87s/it] 97%|█████████▋| 1123/1153 [27:37<00:52,  1.74s/it] 97%|█████████▋| 1124/1153 [27:39<00:48,  1.67s/it] 98%|█████████▊| 1125/1153 [27:40<00:45,  1.64s/it] 98%|█████████▊| 1126/1153 [27:41<00:41,  1.55s/it] 98%|█████████▊| 1127/1153 [27:43<00:39,  1.52s/it] 98%|█████████▊| 1128/1153 [27:44<00:37,  1.50s/it] 98%|█████████▊| 1129/1153 [27:46<00:35,  1.49s/it] 98%|█████████▊| 1130/1153 [27:47<00:34,  1.49s/it] 98%|█████████▊| 1131/1153 [27:49<00:31,  1.44s/it] 98%|█████████▊| 1132/1153 [27:50<00:30,  1.44s/it] 98%|█████████▊| 1133/1153 [27:52<00:28,  1.45s/it] 98%|█████████▊| 1134/1153 [27:53<00:27,  1.46s/it] 98%|█████████▊| 1135/1153 [27:54<00:26,  1.45s/it] 99%|█████████▊| 1136/1153 [27:56<00:24,  1.45s/it] 99%|█████████▊| 1137/1153 [27:57<00:23,  1.45s/it] 99%|█████████▊| 1138/1153 [27:59<00:20,  1.39s/it] 99%|█████████▉| 1139/1153 [28:00<00:19,  1.36s/it] 99%|█████████▉| 1140/1153 [28:05<00:31,  2.42s/it] 99%|█████████▉| 1141/1153 [28:06<00:25,  2.09s/it] 99%|█████████▉| 1142/1153 [28:07<00:20,  1.85s/it] 99%|█████████▉| 1143/1153 [28:09<00:16,  1.68s/it] 99%|█████████▉| 1144/1153 [28:10<00:14,  1.60s/it] 99%|█████████▉| 1145/1153 [28:12<00:12,  1.55s/it] 99%|█████████▉| 1146/1153 [28:13<00:10,  1.48s/it] 99%|█████████▉| 1147/1153 [28:14<00:08,  1.48s/it]100%|█████████▉| 1148/1153 [28:16<00:07,  1.43s/it]100%|█████████▉| 1149/1153 [28:17<00:05,  1.38s/it]100%|█████████▉| 1150/1153 [28:18<00:04,  1.36s/it]100%|█████████▉| 1151/1153 [28:20<00:02,  1.35s/it]100%|█████████▉| 1152/1153 [28:21<00:01,  1.39s/it]100%|██████████| 1153/1153 [28:22<00:00,  1.26s/it]100%|██████████| 1153/1153 [28:22<00:00,  1.48s/it]
147540
147540
saving data 147540 to ./output/DuEE1.0/role/test_result.json
trigger predict 147540 load from ./output/DuEE1.0/role/test_result.json
role predict 147540 load from ./output/DuEE1.0/role/test_result.json
schema 66 load from ./conf/DuEE1.0/event_schema.json
submit data 147540 save to ./output/DuEE1.0/surbot_news_together.json
********** Done ***************
