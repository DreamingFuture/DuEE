nohup: ignoring input
*********** data_prepare *************
开始数据预处理
id: 499  done!
id: 999  done!
id: 1499  done!
id: 1999  done!
id: 2499  done!
id: 2999  done!
id: 3499  done!
id: 3999  done!
id: 4499  done!
id: 4999  done!
id: 5499  done!
id: 5999  done!
id: 6499  done!
id: 6999  done!
id: 7499  done!
id: 7999  done!
id: 8499  done!
id: 8999  done!
id: 9499  done!
id: 9999  done!
id: 10499  done!
id: 10999  done!
id: 11499  done!
id: 11999  done!
id: 12499  done!
id: 12999  done!
id: 13499  done!
id: 13999  done!
id: 14499  done!
id: 14999  done!
id: 15499  done!
id: 15999  done!
id: 16499  done!
id: 16999  done!
id: 17499  done!
id: 17999  done!
id: 18499  done!
id: 18999  done!
id: 19499  done!
id: 19999  done!
id: 20499  done!
id: 20999  done!
id: 21499  done!
id: 21999  done!
id: 22499  done!
id: 22999  done!
id: 23499  done!
id: 23999  done!
id: 24499  done!
id: 24999  done!
id: 25499  done!
id: 25999  done!
id: 26499  done!
id: 26999  done!
id: 27499  done!
id: 27999  done!
id: 28499  done!
id: 28999  done!
id: 29499  done!
id: 29999  done!
id: 30499  done!
id: 30999  done!
id: 31499  done!
id: 31999  done!
id: 32499  done!
id: 32999  done!
id: 33499  done!
id: 33999  done!
id: 34499  done!
id: 34999  done!
id: 35499  done!
id: 35999  done!
id: 36499  done!
id: 36999  done!
id: 37499  done!
id: 37999  done!
id: 38499  done!
id: 38999  done!
id: 39499  done!
id: 39999  done!
id: 40499  done!
id: 40999  done!
id: 41499  done!
id: 41999  done!
id: 42499  done!
id: 42999  done!
id: 43499  done!
id: 43999  done!
id: 44499  done!
id: 44999  done!
id: 45499  done!
id: 45999  done!
id: 46499  done!
id: 46999  done!
id: 47499  done!
id: 47999  done!
id: 48499  done!
id: 48999  done!
id: 49499  done!
id: 49999  done!
id: 50000  done!
开始处理tag.dict 和 tsv文件

=================DUEE 1.0 DATASET==============

=================start schema process==============
input path ./conf/DuEE1.0/event_schema.json
save trigger tag 393 at ./conf/DuEE1.0/role_tag.dict
=================end schema process===============

=================start sentence process==============

----trigger------for dir ./data/DuEE1.0 to ./data/DuEE1.0/trigger
train 6379 dev 1633 test 147541

----role------for dir ./data/DuEE1.0 to ./data/DuEE1.0/role
train 7502 dev 1925 test 147541
=================end schema process==============
********** train start *************
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 5, 
    "event_type": "role", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 1e-05, 
    "max_len": 150, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 30.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 48, 
    "per_gpu_train_batch_size": 64, 
    "seed": 66, 
    "stride": 100, 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/7501 [00:00<?, ?it/s]tokenizing...:   4%|▎         | 278/7501 [00:00<00:02, 2775.35it/s]tokenizing...:   8%|▊         | 586/7501 [00:00<00:02, 2952.18it/s]tokenizing...:  12%|█▏        | 927/7501 [00:00<00:02, 3157.32it/s]tokenizing...:  17%|█▋        | 1243/7501 [00:00<00:02, 2895.72it/s]tokenizing...:  21%|██        | 1541/7501 [00:00<00:02, 2920.00it/s]tokenizing...:  25%|██▍       | 1858/7501 [00:00<00:01, 2999.63it/s]tokenizing...:  29%|██▉       | 2160/7501 [00:00<00:01, 2924.36it/s]tokenizing...:  33%|███▎      | 2454/7501 [00:00<00:01, 2791.06it/s]tokenizing...:  37%|███▋      | 2745/7501 [00:00<00:01, 2825.87it/s]tokenizing...:  40%|████      | 3031/7501 [00:01<00:01, 2834.92it/s]tokenizing...:  44%|████▍     | 3316/7501 [00:01<00:01, 2811.12it/s]tokenizing...:  48%|████▊     | 3606/7501 [00:01<00:01, 2834.41it/s]tokenizing...:  52%|█████▏    | 3890/7501 [00:01<00:01, 2775.72it/s]tokenizing...:  56%|█████▌    | 4177/7501 [00:01<00:01, 2802.72it/s]tokenizing...:  59%|█████▉    | 4458/7501 [00:01<00:01, 2714.84it/s]tokenizing...:  63%|██████▎   | 4751/7501 [00:01<00:00, 2777.18it/s]tokenizing...:  67%|██████▋   | 5030/7501 [00:01<00:00, 2762.20it/s]tokenizing...:  71%|███████   | 5335/7501 [00:01<00:00, 2846.54it/s]tokenizing...:  75%|███████▍  | 5621/7501 [00:02<00:00, 2109.51it/s]tokenizing...:  78%|███████▊  | 5860/7501 [00:02<00:00, 2120.54it/s]tokenizing...:  81%|████████▏ | 6111/7501 [00:02<00:00, 2216.29it/s]tokenizing...:  85%|████████▌ | 6410/7501 [00:02<00:00, 2418.81it/s]tokenizing...:  89%|████████▉ | 6688/7501 [00:02<00:00, 2515.81it/s]tokenizing...:  93%|█████████▎| 6951/7501 [00:02<00:00, 2399.94it/s]tokenizing...:  96%|█████████▌| 7201/7501 [00:02<00:00, 2425.61it/s]tokenizing...: 100%|█████████▉| 7490/7501 [00:02<00:00, 2555.14it/s]tokenizing...: 100%|██████████| 7501/7501 [00:02<00:00, 2649.70it/s]
tokenizing...:   0%|          | 0/1924 [00:00<?, ?it/s]tokenizing...:  17%|█▋        | 319/1924 [00:00<00:00, 3184.29it/s]tokenizing...:  33%|███▎      | 638/1924 [00:00<00:00, 3041.39it/s]tokenizing...:  49%|████▉     | 943/1924 [00:00<00:00, 2869.99it/s]tokenizing...:  64%|██████▍   | 1231/1924 [00:00<00:00, 2827.03it/s]tokenizing...:  79%|███████▊  | 1515/1924 [00:00<00:00, 2748.68it/s]tokenizing...:  93%|█████████▎| 1791/1924 [00:00<00:00, 2558.48it/s]tokenizing...: 100%|██████████| 1924/1924 [00:00<00:00, 2723.56it/s]
09/18/2022 13:26:42 - INFO - root -   The nums of the train_dataset features is 7501
09/18/2022 13:26:42 - INFO - root -   The nums of the eval_dataset features is 1924
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
09/18/2022 13:26:47 - INFO - root -   ***** Running train *****
********************************************************************************
/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:31  batch_loss: 6.3595 [Training] 2/118 [..............................] - ETA: 58s  batch_loss: 6.3496 [Training] 3/118 [..............................] - ETA: 47s  batch_loss: 6.3427 [Training] 4/118 [>.............................] - ETA: 42s  batch_loss: 6.3387 [Training] 5/118 [>.............................] - ETA: 39s  batch_loss: 6.3086 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 6.2622 [Training] 7/118 [>.............................] - ETA: 35s  batch_loss: 6.2080 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 6.1466 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 6.0844 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 6.0195 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 5.9613 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 5.8884 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 5.8213 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 5.7505 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 5.6768 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 5.5993 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 5.5196 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 5.4566 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 5.3849 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 5.2905 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 5.2091 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 5.1184 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 5.0338 [Training] 24/118 [=====>........................] - ETA: 24s  batch_loss: 4.9491 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 4.8649 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 4.7728 [Training] 27/118 [=====>........................] - ETA: 23s  batch_loss: 4.6899 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 4.5983 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 4.5237 [Training] 30/118 [======>.......................] - ETA: 22s  batch_loss: 4.4407 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 4.3816 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 4.3260 [Training] 33/118 [=======>......................] - ETA: 21s  batch_loss: 4.2589 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 4.2070 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 4.1559 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 4.1051 [Training] 37/118 [========>.....................] - ETA: 20s  batch_loss: 4.0590 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 4.0108 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 3.9647 [Training] 40/118 [=========>....................] - ETA: 19s  batch_loss: 3.9184 [Training] 41/118 [=========>....................] - ETA: 19s  batch_loss: 3.8821 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 3.8436 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 3.8104 [Training] 44/118 [==========>...................] - ETA: 18s  batch_loss: 3.7735 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 3.7416 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 3.7174 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 3.6870 [Training] 48/118 [===========>..................] - ETA: 17s  batch_loss: 3.6569 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 3.6304 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 3.6026 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 3.5751 [Training] 52/118 [============>.................] - ETA: 16s  batch_loss: 3.5456 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 3.5200 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 3.4930 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 3.4756 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 3.4590 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 3.4382 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 3.4174 [Training] 59/118 [==============>...............] - ETA: 14s  batch_loss: 3.3958 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 3.3820 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 3.3637 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 3.3396 [Training] 63/118 [===============>..............] - ETA: 13s  batch_loss: 3.3221 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 3.3059 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 3.2921 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 3.2771 [Training] 67/118 [================>.............] - ETA: 12s  batch_loss: 3.2598 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 3.2463 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 3.2349 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 3.2183 [Training] 71/118 [=================>............] - ETA: 11s  batch_loss: 3.2063 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 3.1904 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 3.1790 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 3.1652 [Training] 75/118 [==================>...........] - ETA: 10s  batch_loss: 3.1537 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 3.1418 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 3.1287 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 3.1151 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 3.1026 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 3.0882 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 3.0734 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 3.0633 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 3.0517 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 3.0402 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 3.0281 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 3.0170 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 3.0036 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 2.9985 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 2.9840 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 2.9719 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 2.9650 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 2.9562 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 2.9448 [Training] 94/118 [======================>.......] - ETA: 5s  batch_loss: 2.9360 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 2.9267 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 2.9179 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 2.9088 [Training] 98/118 [=======================>......] - ETA: 4s  batch_loss: 2.9006 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 2.8922 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 2.8821 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 2.8731 [Training] 102/118 [========================>.....] - ETA: 3s  batch_loss: 2.8647 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 2.8564 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 2.8506 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 2.8428 [Training] 106/118 [=========================>....] - ETA: 2s  batch_loss: 2.8343 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 2.8278 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 2.8195 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 2.8134 [Training] 110/118 [==========================>...] - ETA: 1s  batch_loss: 2.8053 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 2.7985 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 2.7924 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 2.7866 [Training] 114/118 [===========================>..] - ETA: 0s  batch_loss: 2.7792 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 2.7698 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 2.7622 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 2.7541 [Training] 118/118 [==============================] 246.6ms/step  batch_loss: 2.7494 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:27:22 - INFO - root -   The F1-score is 0.036049568156214795
09/18/2022 13:27:22 - INFO - root -   the best eval f1 is 0.0360, saving model !!
09/18/2022 13:27:24 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:14  batch_loss: 1.7935 [Training] 2/118 [..............................] - ETA: 51s  batch_loss: 1.9375 [Training] 3/118 [..............................] - ETA: 43s  batch_loss: 1.9127 [Training] 4/118 [>.............................] - ETA: 39s  batch_loss: 1.8895 [Training] 5/118 [>.............................] - ETA: 36s  batch_loss: 1.9074 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 1.9107 [Training] 7/118 [>.............................] - ETA: 33s  batch_loss: 1.8735 [Training] 8/118 [=>............................] - ETA: 32s  batch_loss: 1.8611 [Training] 9/118 [=>............................] - ETA: 32s  batch_loss: 1.8464 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 1.8572 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 1.8460 [Training] 12/118 [==>...........................] - ETA: 29s  batch_loss: 1.8789 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 1.8732 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 1.8748 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 1.8517 [Training] 16/118 [===>..........................] - ETA: 27s  batch_loss: 1.8534 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 1.8530 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 1.8465 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 1.8361 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 1.8411 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 1.8538 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 1.8472 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 1.8615 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 1.8582 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 1.8588 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 1.8488 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 1.8516 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 1.8613 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 1.8528 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 1.8555 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 1.8454 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 1.8408 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 1.8339 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 1.8369 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 1.8307 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 1.8232 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 1.8208 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 1.8216 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 1.8253 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 1.8263 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 1.8244 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 1.8242 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 1.8183 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 1.8181 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 1.8211 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 1.8198 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 1.8226 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 1.8210 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 1.8193 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 1.8163 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 1.8147 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 1.8150 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 1.8133 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 1.8053 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 1.8014 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 1.8029 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 1.8017 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 1.7967 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 1.7915 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 1.7879 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 1.7861 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 1.7853 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 1.7858 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 1.7867 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 1.7879 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 1.7810 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 1.7765 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 1.7726 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 1.7694 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 1.7687 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 1.7667 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 1.7644 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 1.7614 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 1.7614 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 1.7607 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 1.7593 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 1.7559 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 1.7576 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 1.7547 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 1.7477 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 1.7458 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 1.7449 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 1.7403 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 1.7376 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 1.7363 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 1.7354 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 1.7339 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 1.7336 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 1.7321 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 1.7301 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 1.7276 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 1.7247 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 1.7244 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 1.7203 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 1.7182 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 1.7151 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 1.7142 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 1.7111 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 1.7086 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 1.7049 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 1.7012 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 1.6997 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 1.6976 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 1.6953 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 1.6931 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 1.6925 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 1.6900 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 1.6880 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 1.6850 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 1.6818 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 1.6802 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 1.6768 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 1.6744 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 1.6728 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 1.6707 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 1.6694 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 1.6708 [Training] 118/118 [==============================] 252.7ms/step  batch_loss: 1.6675 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:28:00 - INFO - root -   The F1-score is 0.13505831618151526
09/18/2022 13:28:00 - INFO - root -   the best eval f1 is 0.1351, saving model !!
09/18/2022 13:28:03 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:15  batch_loss: 1.3971 [Training] 2/118 [..............................] - ETA: 51s  batch_loss: 1.4176 [Training] 3/118 [..............................] - ETA: 43s  batch_loss: 1.4002 [Training] 4/118 [>.............................] - ETA: 39s  batch_loss: 1.4052 [Training] 5/118 [>.............................] - ETA: 37s  batch_loss: 1.4443 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 1.4370 [Training] 7/118 [>.............................] - ETA: 33s  batch_loss: 1.4051 [Training] 8/118 [=>............................] - ETA: 32s  batch_loss: 1.4026 [Training] 9/118 [=>............................] - ETA: 32s  batch_loss: 1.3902 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 1.3990 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 1.3954 [Training] 12/118 [==>...........................] - ETA: 29s  batch_loss: 1.3850 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 1.3823 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 1.3743 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 1.3746 [Training] 16/118 [===>..........................] - ETA: 27s  batch_loss: 1.3831 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 1.3863 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 1.3782 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 1.3819 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 1.3911 [Training] 21/118 [====>.........................] - ETA: 25s  batch_loss: 1.3926 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 1.3860 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 1.3832 [Training] 24/118 [=====>........................] - ETA: 24s  batch_loss: 1.3742 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 1.3730 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 1.3727 [Training] 27/118 [=====>........................] - ETA: 23s  batch_loss: 1.3680 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 1.3771 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 1.3786 [Training] 30/118 [======>.......................] - ETA: 22s  batch_loss: 1.3792 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 1.3839 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 1.3844 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 1.3822 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 1.3802 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 1.3834 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 1.3812 [Training] 37/118 [========>.....................] - ETA: 20s  batch_loss: 1.3763 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 1.3746 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 1.3741 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 1.3752 [Training] 41/118 [=========>....................] - ETA: 19s  batch_loss: 1.3725 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 1.3669 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 1.3674 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 1.3674 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 1.3634 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 1.3622 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 1.3565 [Training] 48/118 [===========>..................] - ETA: 17s  batch_loss: 1.3559 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 1.3569 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 1.3563 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 1.3550 [Training] 52/118 [============>.................] - ETA: 16s  batch_loss: 1.3547 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 1.3530 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 1.3518 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 1.3497 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 1.3540 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 1.3518 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 1.3491 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 1.3473 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 1.3436 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 1.3421 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 1.3406 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 1.3400 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 1.3419 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 1.3385 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 1.3372 [Training] 67/118 [================>.............] - ETA: 12s  batch_loss: 1.3357 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 1.3338 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 1.3352 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 1.3342 [Training] 71/118 [=================>............] - ETA: 11s  batch_loss: 1.3335 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 1.3308 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 1.3284 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 1.3272 [Training] 75/118 [==================>...........] - ETA: 10s  batch_loss: 1.3255 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 1.3256 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 1.3237 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 1.3242 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 1.3222 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 1.3231 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 1.3229 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 1.3207 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 1.3207 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 1.3195 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 1.3192 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 1.3192 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 1.3187 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 1.3190 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 1.3196 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 1.3186 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 1.3165 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 1.3165 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 1.3165 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 1.3162 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 1.3154 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 1.3137 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 1.3122 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 1.3113 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 1.3096 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 1.3068 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 1.3037 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 1.3032 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 1.3033 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 1.3028 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 1.3009 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 1.2998 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 1.2990 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 1.2996 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 1.2992 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 1.2970 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 1.2961 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 1.2956 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 1.2952 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 1.2938 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 1.2914 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 1.2905 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 1.2901 [Training] 118/118 [==============================] 251.6ms/step  batch_loss: 1.2884 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:28:39 - INFO - root -   The F1-score is 0.21400386847195357
09/18/2022 13:28:39 - INFO - root -   the best eval f1 is 0.2140, saving model !!
09/18/2022 13:28:41 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:11  batch_loss: 1.2597 [Training] 2/118 [..............................] - ETA: 49s  batch_loss: 1.2567 [Training] 3/118 [..............................] - ETA: 42s  batch_loss: 1.2213 [Training] 4/118 [>.............................] - ETA: 38s  batch_loss: 1.2094 [Training] 5/118 [>.............................] - ETA: 36s  batch_loss: 1.1823 [Training] 6/118 [>.............................] - ETA: 34s  batch_loss: 1.1452 [Training] 7/118 [>.............................] - ETA: 33s  batch_loss: 1.1300 [Training] 8/118 [=>............................] - ETA: 32s  batch_loss: 1.1261 [Training] 9/118 [=>............................] - ETA: 31s  batch_loss: 1.1375 [Training] 10/118 [=>............................] - ETA: 30s  batch_loss: 1.1547 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 1.1452 [Training] 12/118 [==>...........................] - ETA: 29s  batch_loss: 1.1465 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 1.1425 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 1.1459 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 1.1397 [Training] 16/118 [===>..........................] - ETA: 27s  batch_loss: 1.1316 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 1.1401 [Training] 18/118 [===>..........................] - ETA: 26s  batch_loss: 1.1417 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 1.1313 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 1.1333 [Training] 21/118 [====>.........................] - ETA: 25s  batch_loss: 1.1259 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 1.1274 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 1.1274 [Training] 24/118 [=====>........................] - ETA: 24s  batch_loss: 1.1252 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 1.1260 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 1.1293 [Training] 27/118 [=====>........................] - ETA: 23s  batch_loss: 1.1245 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 1.1234 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 1.1210 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 1.1215 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 1.1158 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 1.1135 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 1.1115 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 1.1130 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 1.1114 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 1.1112 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 1.1139 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 1.1150 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 1.1127 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 1.1121 [Training] 41/118 [=========>....................] - ETA: 19s  batch_loss: 1.1091 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 1.1077 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 1.1083 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 1.1074 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 1.1066 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 1.1079 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 1.1078 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 1.1103 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 1.1078 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 1.1073 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 1.1081 [Training] 52/118 [============>.................] - ETA: 16s  batch_loss: 1.1107 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 1.1112 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 1.1082 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 1.1066 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 1.1073 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 1.1073 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 1.1037 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 1.1038 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 1.1046 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 1.1024 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 1.1032 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 1.1027 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 1.1029 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 1.1023 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 1.1016 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 1.1016 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 1.1009 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 1.0990 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 1.0984 [Training] 71/118 [=================>............] - ETA: 11s  batch_loss: 1.0969 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 1.0954 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 1.0945 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 1.0955 [Training] 75/118 [==================>...........] - ETA: 10s  batch_loss: 1.0968 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 1.0958 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 1.0958 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 1.0967 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 1.0952 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 1.0941 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 1.0910 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 1.0890 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 1.0877 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 1.0876 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 1.0857 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 1.0843 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 1.0852 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 1.0843 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 1.0836 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 1.0832 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 1.0810 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 1.0801 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 1.0779 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 1.0752 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 1.0740 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 1.0722 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 1.0713 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 1.0705 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 1.0696 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 1.0693 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 1.0687 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 1.0687 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 1.0676 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 1.0667 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 1.0651 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 1.0652 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 1.0635 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 1.0627 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 1.0607 [Training] 110/118 [==========================>...] - ETA: 1s  batch_loss: 1.0604 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 1.0592 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 1.0581 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 1.0562 [Training] 114/118 [===========================>..] - ETA: 0s  batch_loss: 1.0542 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 1.0530 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 1.0517 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 1.0507 [Training] 118/118 [==============================] 248.2ms/step  batch_loss: 1.0502 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:29:16 - INFO - root -   The F1-score is 0.33532934131736525
09/18/2022 13:29:16 - INFO - root -   the best eval f1 is 0.3353, saving model !!
09/18/2022 13:29:19 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:14  batch_loss: 1.0136 [Training] 2/118 [..............................] - ETA: 52s  batch_loss: 0.9571 [Training] 3/118 [..............................] - ETA: 43s  batch_loss: 0.9464 [Training] 4/118 [>.............................] - ETA: 39s  batch_loss: 0.9659 [Training] 5/118 [>.............................] - ETA: 36s  batch_loss: 0.9763 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 0.9308 [Training] 7/118 [>.............................] - ETA: 33s  batch_loss: 0.9497 [Training] 8/118 [=>............................] - ETA: 32s  batch_loss: 0.9390 [Training] 9/118 [=>............................] - ETA: 31s  batch_loss: 0.9346 [Training] 10/118 [=>............................] - ETA: 30s  batch_loss: 0.9376 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.9425 [Training] 12/118 [==>...........................] - ETA: 29s  batch_loss: 0.9444 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.9423 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.9375 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.9365 [Training] 16/118 [===>..........................] - ETA: 27s  batch_loss: 0.9411 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.9385 [Training] 18/118 [===>..........................] - ETA: 26s  batch_loss: 0.9351 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.9271 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.9194 [Training] 21/118 [====>.........................] - ETA: 25s  batch_loss: 0.9262 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.9311 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.9315 [Training] 24/118 [=====>........................] - ETA: 24s  batch_loss: 0.9303 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.9301 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.9323 [Training] 27/118 [=====>........................] - ETA: 23s  batch_loss: 0.9352 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.9300 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.9311 [Training] 30/118 [======>.......................] - ETA: 22s  batch_loss: 0.9284 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.9231 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.9252 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.9249 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 0.9237 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.9232 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.9184 [Training] 37/118 [========>.....................] - ETA: 20s  batch_loss: 0.9220 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.9190 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.9189 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.9216 [Training] 41/118 [=========>....................] - ETA: 19s  batch_loss: 0.9243 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.9197 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.9177 [Training] 44/118 [==========>...................] - ETA: 18s  batch_loss: 0.9153 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.9177 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.9182 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.9181 [Training] 48/118 [===========>..................] - ETA: 17s  batch_loss: 0.9206 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.9181 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.9149 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.9131 [Training] 52/118 [============>.................] - ETA: 16s  batch_loss: 0.9118 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.9121 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.9092 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.9068 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.9051 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.9045 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.9036 [Training] 59/118 [==============>...............] - ETA: 14s  batch_loss: 0.9022 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.9028 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.9011 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.9020 [Training] 63/118 [===============>..............] - ETA: 13s  batch_loss: 0.9034 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.9026 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.9018 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.9026 [Training] 67/118 [================>.............] - ETA: 12s  batch_loss: 0.8996 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.8983 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.8973 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.8964 [Training] 71/118 [=================>............] - ETA: 11s  batch_loss: 0.8952 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.8948 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.8943 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.8935 [Training] 75/118 [==================>...........] - ETA: 10s  batch_loss: 0.8929 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.8914 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.8900 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.8905 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.8896 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.8887 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.8895 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.8887 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.8870 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.8859 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.8863 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.8868 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.8875 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.8860 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.8861 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.8861 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.8875 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.8875 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.8875 [Training] 94/118 [======================>.......] - ETA: 5s  batch_loss: 0.8855 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.8857 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.8843 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.8831 [Training] 98/118 [=======================>......] - ETA: 4s  batch_loss: 0.8822 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.8805 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.8799 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.8790 [Training] 102/118 [========================>.....] - ETA: 3s  batch_loss: 0.8796 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.8792 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.8779 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.8774 [Training] 106/118 [=========================>....] - ETA: 2s  batch_loss: 0.8768 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.8770 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.8765 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.8765 [Training] 110/118 [==========================>...] - ETA: 1s  batch_loss: 0.8758 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.8762 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.8782 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.8774 [Training] 114/118 [===========================>..] - ETA: 0s  batch_loss: 0.8773 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.8769 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.8771 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.8766 [Training] 118/118 [==============================] 247.4ms/step  batch_loss: 0.8765 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:29:54 - INFO - root -   The F1-score is 0.43169398907103823
09/18/2022 13:29:54 - INFO - root -   the best eval f1 is 0.4317, saving model !!
09/18/2022 13:29:57 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:17  batch_loss: 0.9120 [Training] 2/118 [..............................] - ETA: 53s  batch_loss: 0.8390 [Training] 3/118 [..............................] - ETA: 45s  batch_loss: 0.8618 [Training] 4/118 [>.............................] - ETA: 41s  batch_loss: 0.8452 [Training] 5/118 [>.............................] - ETA: 38s  batch_loss: 0.8192 [Training] 6/118 [>.............................] - ETA: 36s  batch_loss: 0.8352 [Training] 7/118 [>.............................] - ETA: 35s  batch_loss: 0.8353 [Training] 8/118 [=>............................] - ETA: 33s  batch_loss: 0.8269 [Training] 9/118 [=>............................] - ETA: 32s  batch_loss: 0.8220 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.8165 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.8108 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.8063 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.8113 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.8097 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.8082 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.8057 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.8081 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.8001 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.7999 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.8034 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.8051 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.8085 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.8097 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.8018 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.7963 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.7947 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.7919 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.7901 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.7880 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.7859 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.7849 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.7827 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.7810 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.7797 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.7774 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.7754 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.7699 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.7706 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.7706 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.7685 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.7697 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.7705 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.7713 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.7710 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.7733 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.7724 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.7736 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.7764 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.7757 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.7737 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.7696 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.7675 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.7667 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.7655 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.7645 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.7637 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.7608 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.7598 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.7590 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.7628 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.7627 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.7631 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.7630 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.7614 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.7607 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.7621 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.7622 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.7613 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.7613 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.7616 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.7612 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.7599 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.7596 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.7581 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.7571 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.7559 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.7560 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.7557 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.7568 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.7551 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.7553 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.7544 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.7532 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.7522 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.7509 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.7500 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.7499 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.7495 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.7488 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.7478 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.7469 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.7462 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.7467 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.7448 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.7449 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.7455 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.7441 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.7458 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.7453 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.7442 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.7451 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.7453 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.7443 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.7441 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.7450 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.7448 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.7444 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.7442 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.7445 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.7446 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.7448 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.7448 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.7445 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.7434 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.7431 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.7426 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.7433 [Training] 118/118 [==============================] 255.3ms/step  batch_loss: 0.7465 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:30:33 - INFO - root -   The F1-score is 0.4890764309535153
09/18/2022 13:30:33 - INFO - root -   the best eval f1 is 0.4891, saving model !!
09/18/2022 13:30:35 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:18  batch_loss: 0.6653 [Training] 2/118 [..............................] - ETA: 53s  batch_loss: 0.6620 [Training] 3/118 [..............................] - ETA: 44s  batch_loss: 0.6278 [Training] 4/118 [>.............................] - ETA: 40s  batch_loss: 0.6341 [Training] 5/118 [>.............................] - ETA: 37s  batch_loss: 0.6452 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 0.6544 [Training] 7/118 [>.............................] - ETA: 34s  batch_loss: 0.6674 [Training] 8/118 [=>............................] - ETA: 33s  batch_loss: 0.6829 [Training] 9/118 [=>............................] - ETA: 32s  batch_loss: 0.6738 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 0.6739 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.6822 [Training] 12/118 [==>...........................] - ETA: 29s  batch_loss: 0.6799 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.6857 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.6798 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.6814 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.6849 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.6818 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.6848 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.6842 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.6825 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.6809 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.6781 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.6770 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.6753 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.6719 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.6731 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.6723 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.6719 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.6676 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.6681 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.6701 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.6694 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.6694 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.6717 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.6725 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.6732 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.6727 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.6729 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.6725 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.6698 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.6698 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.6669 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.6653 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.6658 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.6649 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.6634 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.6625 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.6636 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.6614 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.6585 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.6610 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.6623 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.6603 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.6591 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.6595 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.6571 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.6576 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.6564 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.6544 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.6553 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.6552 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.6517 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.6506 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.6514 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.6496 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.6495 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.6506 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.6518 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.6517 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.6510 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.6495 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.6488 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.6486 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.6487 [Training] 75/118 [==================>...........] - ETA: 10s  batch_loss: 0.6481 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.6476 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.6467 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.6449 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.6449 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.6446 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.6448 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.6442 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.6438 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.6452 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.6444 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.6454 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.6447 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.6442 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.6443 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.6442 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.6432 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.6423 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.6444 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.6445 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.6448 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.6449 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.6449 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.6437 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.6443 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.6447 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.6443 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.6446 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.6440 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.6432 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.6430 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.6433 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.6431 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.6422 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.6418 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.6408 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.6404 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.6407 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.6402 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.6398 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.6392 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.6390 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.6385 [Training] 118/118 [==============================] 252.7ms/step  batch_loss: 0.6439 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:31:11 - INFO - root -   The F1-score is 0.5095890410958903
09/18/2022 13:31:11 - INFO - root -   the best eval f1 is 0.5096, saving model !!
09/18/2022 13:31:14 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:15  batch_loss: 0.5906 [Training] 2/118 [..............................] - ETA: 52s  batch_loss: 0.6605 [Training] 3/118 [..............................] - ETA: 44s  batch_loss: 0.6085 [Training] 4/118 [>.............................] - ETA: 40s  batch_loss: 0.5937 [Training] 5/118 [>.............................] - ETA: 37s  batch_loss: 0.5842 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 0.6213 [Training] 7/118 [>.............................] - ETA: 33s  batch_loss: 0.6166 [Training] 8/118 [=>............................] - ETA: 32s  batch_loss: 0.6116 [Training] 9/118 [=>............................] - ETA: 32s  batch_loss: 0.6085 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 0.6093 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.6030 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.5959 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.5989 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.5994 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.5991 [Training] 16/118 [===>..........................] - ETA: 27s  batch_loss: 0.6005 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.5951 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.5893 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.5890 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.5893 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.5872 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.5843 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.5841 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.5810 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.5785 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.5762 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.5739 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.5778 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.5772 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.5767 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.5763 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.5805 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.5777 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 0.5774 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.5764 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.5765 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.5762 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.5778 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.5784 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.5800 [Training] 41/118 [=========>....................] - ETA: 19s  batch_loss: 0.5797 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.5787 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.5790 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.5789 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.5785 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.5795 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.5783 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.5769 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.5764 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.5767 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.5756 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.5765 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.5757 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.5765 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.5751 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.5740 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.5727 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.5724 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.5719 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.5716 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.5709 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.5710 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.5721 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.5729 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.5723 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.5705 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.5696 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.5701 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.5695 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.5686 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.5685 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.5673 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.5667 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.5662 [Training] 75/118 [==================>...........] - ETA: 10s  batch_loss: 0.5659 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.5657 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.5650 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.5644 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.5639 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.5632 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.5633 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.5626 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.5617 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.5616 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.5614 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.5604 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.5602 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.5592 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.5589 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.5590 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.5595 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.5592 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.5584 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.5585 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.5584 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.5582 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.5578 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.5574 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.5575 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.5569 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.5565 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.5560 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.5565 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.5572 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.5568 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.5555 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.5558 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.5561 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.5561 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.5561 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.5575 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.5569 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.5573 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.5563 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.5560 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.5560 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.5555 [Training] 118/118 [==============================] 252.3ms/step  batch_loss: 0.5560 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:31:50 - INFO - root -   The F1-score is 0.5394233884029802
09/18/2022 13:31:50 - INFO - root -   the best eval f1 is 0.5394, saving model !!
09/18/2022 13:31:52 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:12  batch_loss: 0.5553 [Training] 2/118 [..............................] - ETA: 50s  batch_loss: 0.5460 [Training] 3/118 [..............................] - ETA: 43s  batch_loss: 0.5384 [Training] 4/118 [>.............................] - ETA: 39s  batch_loss: 0.5129 [Training] 5/118 [>.............................] - ETA: 36s  batch_loss: 0.5165 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 0.5248 [Training] 7/118 [>.............................] - ETA: 33s  batch_loss: 0.5309 [Training] 8/118 [=>............................] - ETA: 32s  batch_loss: 0.5205 [Training] 9/118 [=>............................] - ETA: 31s  batch_loss: 0.5234 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 0.5158 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.5244 [Training] 12/118 [==>...........................] - ETA: 29s  batch_loss: 0.5359 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.5273 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.5214 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.5220 [Training] 16/118 [===>..........................] - ETA: 27s  batch_loss: 0.5192 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.5205 [Training] 18/118 [===>..........................] - ETA: 26s  batch_loss: 0.5238 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.5197 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.5214 [Training] 21/118 [====>.........................] - ETA: 25s  batch_loss: 0.5169 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.5216 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.5198 [Training] 24/118 [=====>........................] - ETA: 24s  batch_loss: 0.5207 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.5173 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.5171 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.5146 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.5148 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.5141 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.5154 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.5143 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.5124 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.5123 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 0.5131 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.5124 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.5125 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.5134 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.5142 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.5121 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.5110 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.5108 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.5118 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.5105 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.5105 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.5095 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.5097 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.5073 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.5047 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.5042 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.5031 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.5022 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.5023 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.5034 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.5035 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.5051 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.5049 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.5043 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.5033 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.5022 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.5021 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.4999 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.4992 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.4987 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.4984 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.4977 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.4971 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.4975 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.4967 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.4971 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.4964 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.4976 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.4969 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.4966 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.4960 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.4966 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.4965 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.4960 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.4956 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.4953 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.4942 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.4944 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.4938 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.4933 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.4924 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.4922 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.4909 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.4906 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.4897 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.4900 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.4893 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.4892 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.4900 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.4889 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.4890 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.4888 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.4883 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.4886 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.4881 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.4871 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.4868 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.4867 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.4868 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.4869 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.4872 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.4878 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.4873 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.4880 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.4885 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.4878 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.4871 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.4862 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.4856 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.4858 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.4854 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.4858 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.4856 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.4860 [Training] 118/118 [==============================] 253.6ms/step  batch_loss: 0.4852 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:32:29 - INFO - root -   The F1-score is 0.5735947712418301
09/18/2022 13:32:29 - INFO - root -   the best eval f1 is 0.5736, saving model !!
09/18/2022 13:32:31 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:11  batch_loss: 0.3490 [Training] 2/118 [..............................] - ETA: 50s  batch_loss: 0.4007 [Training] 3/118 [..............................] - ETA: 43s  batch_loss: 0.3899 [Training] 4/118 [>.............................] - ETA: 39s  batch_loss: 0.3861 [Training] 5/118 [>.............................] - ETA: 36s  batch_loss: 0.3908 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 0.4059 [Training] 7/118 [>.............................] - ETA: 33s  batch_loss: 0.4063 [Training] 8/118 [=>............................] - ETA: 32s  batch_loss: 0.4045 [Training] 9/118 [=>............................] - ETA: 31s  batch_loss: 0.4006 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 0.4083 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.4121 [Training] 12/118 [==>...........................] - ETA: 29s  batch_loss: 0.4087 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.4098 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.4103 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.4142 [Training] 16/118 [===>..........................] - ETA: 27s  batch_loss: 0.4198 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.4213 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.4250 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.4271 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.4247 [Training] 21/118 [====>.........................] - ETA: 25s  batch_loss: 0.4240 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.4228 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.4217 [Training] 24/118 [=====>........................] - ETA: 24s  batch_loss: 0.4251 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.4277 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.4295 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.4295 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.4281 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.4321 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.4353 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.4386 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.4376 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.4378 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 0.4355 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.4361 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.4362 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.4380 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.4366 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.4367 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.4365 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.4345 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.4339 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.4354 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.4367 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.4348 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.4356 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.4357 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.4352 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.4366 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.4370 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.4365 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.4385 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.4375 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.4357 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.4362 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.4359 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.4355 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.4337 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.4350 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.4347 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.4350 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.4360 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.4359 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.4358 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.4358 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.4362 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.4379 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.4378 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.4367 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.4374 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.4371 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.4371 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.4370 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.4364 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.4364 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.4362 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.4362 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.4360 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.4357 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.4350 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.4361 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.4350 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.4351 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.4363 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.4366 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.4360 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.4356 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.4354 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.4352 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.4357 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.4353 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.4355 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.4349 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.4345 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.4344 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.4340 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.4335 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.4336 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.4331 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.4326 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.4325 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.4319 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.4323 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.4320 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.4320 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.4310 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.4309 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.4304 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.4303 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.4296 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.4293 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.4301 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.4303 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.4309 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.4316 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.4309 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.4305 [Training] 118/118 [==============================] 253.1ms/step  batch_loss: 0.4309 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:33:07 - INFO - root -   The F1-score is 0.5788753502313155
09/18/2022 13:33:07 - INFO - root -   the best eval f1 is 0.5789, saving model !!
09/18/2022 13:33:09 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:13  batch_loss: 0.4387 [Training] 2/118 [..............................] - ETA: 51s  batch_loss: 0.4019 [Training] 3/118 [..............................] - ETA: 43s  batch_loss: 0.4077 [Training] 4/118 [>.............................] - ETA: 39s  batch_loss: 0.3996 [Training] 5/118 [>.............................] - ETA: 36s  batch_loss: 0.4073 [Training] 6/118 [>.............................] - ETA: 34s  batch_loss: 0.4220 [Training] 7/118 [>.............................] - ETA: 33s  batch_loss: 0.4245 [Training] 8/118 [=>............................] - ETA: 32s  batch_loss: 0.4099 [Training] 9/118 [=>............................] - ETA: 31s  batch_loss: 0.4065 [Training] 10/118 [=>............................] - ETA: 30s  batch_loss: 0.4014 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.4049 [Training] 12/118 [==>...........................] - ETA: 29s  batch_loss: 0.4010 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.4041 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.4040 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.4008 [Training] 16/118 [===>..........................] - ETA: 27s  batch_loss: 0.4079 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.4091 [Training] 18/118 [===>..........................] - ETA: 26s  batch_loss: 0.4074 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.4050 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.4026 [Training] 21/118 [====>.........................] - ETA: 25s  batch_loss: 0.4019 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.4030 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.4008 [Training] 24/118 [=====>........................] - ETA: 24s  batch_loss: 0.3993 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.4001 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.4000 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.3980 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.3969 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.3973 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.3956 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.3947 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.3952 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.3936 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 0.3918 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.3911 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.3923 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.3931 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.3925 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.3924 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.3916 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.3920 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.3931 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.3925 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.3916 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.3910 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.3915 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.3909 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.3912 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.3915 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.3934 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.3938 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.3918 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.3901 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.3904 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.3900 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.3886 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.3897 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.3893 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.3920 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.3932 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.3930 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.3927 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.3935 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.3926 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.3931 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.3929 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.3919 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.3911 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.3906 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.3905 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.3909 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.3908 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.3899 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.3906 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.3908 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.3916 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.3914 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.3922 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.3922 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.3928 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.3928 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.3924 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.3920 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.3918 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.3911 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.3915 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.3917 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.3907 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.3907 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.3909 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.3904 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.3892 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.3894 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.3891 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.3889 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.3882 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.3879 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.3877 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.3877 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.3875 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.3869 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.3865 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.3862 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.3855 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.3859 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.3855 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.3851 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.3855 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.3852 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.3848 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.3849 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.3854 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.3853 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.3846 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.3847 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.3851 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.3847 [Training] 118/118 [==============================] 253.2ms/step  batch_loss: 0.3843 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:33:45 - INFO - root -   The F1-score is 0.5916918627579861
09/18/2022 13:33:45 - INFO - root -   the best eval f1 is 0.5917, saving model !!
09/18/2022 13:33:48 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:15  batch_loss: 0.3520 [Training] 2/118 [..............................] - ETA: 52s  batch_loss: 0.3643 [Training] 3/118 [..............................] - ETA: 44s  batch_loss: 0.3518 [Training] 4/118 [>.............................] - ETA: 40s  batch_loss: 0.3398 [Training] 5/118 [>.............................] - ETA: 37s  batch_loss: 0.3383 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 0.3453 [Training] 7/118 [>.............................] - ETA: 34s  batch_loss: 0.3437 [Training] 8/118 [=>............................] - ETA: 33s  batch_loss: 0.3400 [Training] 9/118 [=>............................] - ETA: 32s  batch_loss: 0.3407 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 0.3425 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.3407 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.3404 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.3424 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.3399 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.3392 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.3428 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.3435 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.3432 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.3417 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.3426 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.3412 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.3437 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.3439 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.3418 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.3432 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.3437 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.3440 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.3446 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.3452 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.3430 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.3433 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.3426 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.3451 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 0.3474 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.3469 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.3473 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.3479 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.3496 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.3492 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.3479 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.3470 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.3476 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.3483 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.3471 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.3475 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.3482 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.3484 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.3486 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.3484 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.3480 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.3481 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.3484 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.3470 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.3455 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.3474 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.3483 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.3470 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.3470 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.3469 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.3470 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.3466 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.3465 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.3464 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.3458 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.3466 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.3466 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.3470 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.3464 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.3453 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.3458 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.3452 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.3448 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.3444 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.3441 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.3436 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.3435 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.3435 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.3436 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.3433 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.3433 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.3426 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.3432 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.3444 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.3442 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.3437 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.3436 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.3440 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.3436 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.3440 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.3448 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.3447 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.3447 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.3445 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.3441 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.3439 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.3451 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.3449 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.3457 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.3458 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.3455 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.3450 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.3451 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.3454 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.3452 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.3445 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.3451 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.3450 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.3450 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.3445 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.3448 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.3445 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.3443 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.3446 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.3437 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.3439 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.3438 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.3438 [Training] 118/118 [==============================] 255.3ms/step  batch_loss: 0.3458 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:34:24 - INFO - root -   The F1-score is 0.6031363088057902
09/18/2022 13:34:24 - INFO - root -   the best eval f1 is 0.6031, saving model !!
09/18/2022 13:34:26 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:14  batch_loss: 0.3013 [Training] 2/118 [..............................] - ETA: 51s  batch_loss: 0.2906 [Training] 3/118 [..............................] - ETA: 44s  batch_loss: 0.3215 [Training] 4/118 [>.............................] - ETA: 39s  batch_loss: 0.3115 [Training] 5/118 [>.............................] - ETA: 37s  batch_loss: 0.3131 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 0.3202 [Training] 7/118 [>.............................] - ETA: 34s  batch_loss: 0.3169 [Training] 8/118 [=>............................] - ETA: 32s  batch_loss: 0.3158 [Training] 9/118 [=>............................] - ETA: 31s  batch_loss: 0.3185 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 0.3215 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.3162 [Training] 12/118 [==>...........................] - ETA: 29s  batch_loss: 0.3148 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.3198 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.3202 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.3209 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.3224 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.3262 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.3274 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.3274 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.3259 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.3258 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.3259 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.3256 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.3243 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.3239 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.3217 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.3220 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.3220 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.3231 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.3211 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.3220 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.3201 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.3185 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.3192 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.3191 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.3184 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.3186 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.3185 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.3179 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.3188 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.3187 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.3192 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.3173 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.3162 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.3168 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.3167 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.3167 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.3159 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.3153 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.3158 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.3140 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.3133 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.3132 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.3135 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.3129 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.3119 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.3123 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.3119 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.3124 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.3116 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.3105 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.3104 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.3118 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.3115 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.3116 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.3115 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.3111 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.3115 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.3114 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.3116 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.3115 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.3122 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.3123 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.3122 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.3135 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.3143 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.3137 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.3137 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.3142 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.3142 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.3134 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.3141 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.3140 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.3144 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.3140 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.3147 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.3145 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.3141 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.3134 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.3135 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.3132 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.3140 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.3139 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.3136 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.3136 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.3132 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.3133 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.3132 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.3134 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.3131 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.3126 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.3126 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.3134 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.3134 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.3131 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.3132 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.3131 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.3130 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.3129 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.3130 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.3127 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.3125 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.3124 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.3121 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.3123 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.3128 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.3123 [Training] 118/118 [==============================] 252.7ms/step  batch_loss: 0.3122 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:35:02 - INFO - root -   The F1-score is 0.6069182389937108
09/18/2022 13:35:02 - INFO - root -   the best eval f1 is 0.6069, saving model !!
09/18/2022 13:35:05 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:11  batch_loss: 0.2225 [Training] 2/118 [..............................] - ETA: 50s  batch_loss: 0.2573 [Training] 3/118 [..............................] - ETA: 43s  batch_loss: 0.2593 [Training] 4/118 [>.............................] - ETA: 39s  batch_loss: 0.2574 [Training] 5/118 [>.............................] - ETA: 36s  batch_loss: 0.2496 [Training] 6/118 [>.............................] - ETA: 34s  batch_loss: 0.2653 [Training] 7/118 [>.............................] - ETA: 33s  batch_loss: 0.2612 [Training] 8/118 [=>............................] - ETA: 32s  batch_loss: 0.2595 [Training] 9/118 [=>............................] - ETA: 31s  batch_loss: 0.2588 [Training] 10/118 [=>............................] - ETA: 30s  batch_loss: 0.2558 [Training] 11/118 [=>............................] - ETA: 29s  batch_loss: 0.2561 [Training] 12/118 [==>...........................] - ETA: 29s  batch_loss: 0.2618 [Training] 13/118 [==>...........................] - ETA: 28s  batch_loss: 0.2660 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.2643 [Training] 15/118 [==>...........................] - ETA: 27s  batch_loss: 0.2641 [Training] 16/118 [===>..........................] - ETA: 27s  batch_loss: 0.2628 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.2630 [Training] 18/118 [===>..........................] - ETA: 26s  batch_loss: 0.2710 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.2721 [Training] 20/118 [====>.........................] - ETA: 25s  batch_loss: 0.2737 [Training] 21/118 [====>.........................] - ETA: 25s  batch_loss: 0.2737 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.2750 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.2767 [Training] 24/118 [=====>........................] - ETA: 24s  batch_loss: 0.2814 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.2827 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.2833 [Training] 27/118 [=====>........................] - ETA: 23s  batch_loss: 0.2838 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.2855 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.2861 [Training] 30/118 [======>.......................] - ETA: 22s  batch_loss: 0.2855 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.2858 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.2849 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.2848 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 0.2839 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.2833 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.2844 [Training] 37/118 [========>.....................] - ETA: 20s  batch_loss: 0.2847 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.2838 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.2868 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2864 [Training] 41/118 [=========>....................] - ETA: 19s  batch_loss: 0.2866 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.2869 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.2879 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2883 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.2894 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.2890 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2891 [Training] 48/118 [===========>..................] - ETA: 17s  batch_loss: 0.2894 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.2886 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.2885 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2871 [Training] 52/118 [============>.................] - ETA: 16s  batch_loss: 0.2882 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.2878 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2867 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2874 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.2875 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.2888 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2886 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2892 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.2882 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2887 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2886 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2881 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.2878 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.2877 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2874 [Training] 67/118 [================>.............] - ETA: 12s  batch_loss: 0.2873 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.2868 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2867 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2869 [Training] 71/118 [=================>............] - ETA: 11s  batch_loss: 0.2875 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.2878 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2879 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2887 [Training] 75/118 [==================>...........] - ETA: 10s  batch_loss: 0.2881 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.2884 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2888 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2886 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.2883 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2879 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2874 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2881 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.2877 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2876 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2869 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2879 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.2880 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2871 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2867 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2862 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.2874 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2878 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2877 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2878 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.2880 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2883 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2885 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2890 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2889 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2891 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2886 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2889 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2887 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2885 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2881 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2882 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2881 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2880 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2882 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2880 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2882 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2884 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2882 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2879 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2877 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2879 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2878 [Training] 118/118 [==============================] 252.1ms/step  batch_loss: 0.2878 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:35:41 - INFO - root -   The F1-score is 0.6174436475409836
09/18/2022 13:35:41 - INFO - root -   the best eval f1 is 0.6174, saving model !!
09/18/2022 13:35:43 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:16  batch_loss: 0.2066 [Training] 2/118 [..............................] - ETA: 53s  batch_loss: 0.2330 [Training] 3/118 [..............................] - ETA: 45s  batch_loss: 0.2509 [Training] 4/118 [>.............................] - ETA: 41s  batch_loss: 0.2688 [Training] 5/118 [>.............................] - ETA: 39s  batch_loss: 0.2608 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.2508 [Training] 7/118 [>.............................] - ETA: 35s  batch_loss: 0.2497 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.2517 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.2440 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.2457 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.2440 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.2435 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.2449 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.2487 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.2491 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.2496 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.2482 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.2518 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.2502 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.2499 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.2540 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.2543 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.2586 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.2585 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.2593 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.2579 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.2587 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.2616 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.2623 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.2609 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.2611 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.2603 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.2607 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.2591 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.2578 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.2570 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2568 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2573 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.2583 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.2575 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2598 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.2600 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.2604 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2611 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2616 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.2607 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.2608 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2613 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.2606 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.2615 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2622 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2618 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.2613 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.2627 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2616 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.2615 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.2612 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2616 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2611 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.2622 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.2634 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2637 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2638 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.2628 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.2620 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2627 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2620 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.2627 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2637 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2637 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.2630 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.2637 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2635 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2628 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.2631 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.2648 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2662 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2656 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.2656 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2650 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2648 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2649 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.2646 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2640 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2645 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2639 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.2643 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2654 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2651 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2657 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.2663 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2665 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2659 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2665 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.2663 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2664 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2666 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2666 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2665 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2663 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2664 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2668 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2667 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2673 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2672 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2675 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2669 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2665 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2663 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2660 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2667 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2663 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2668 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2671 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2669 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2667 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2667 [Training] 118/118 [==============================] 256.5ms/step  batch_loss: 0.2670 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:36:19 - INFO - root -   The F1-score is 0.6266942042080805
09/18/2022 13:36:19 - INFO - root -   the best eval f1 is 0.6267, saving model !!
09/18/2022 13:36:22 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:19  batch_loss: 0.2111 [Training] 2/118 [..............................] - ETA: 53s  batch_loss: 0.2209 [Training] 3/118 [..............................] - ETA: 45s  batch_loss: 0.2248 [Training] 4/118 [>.............................] - ETA: 40s  batch_loss: 0.2287 [Training] 5/118 [>.............................] - ETA: 37s  batch_loss: 0.2296 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 0.2330 [Training] 7/118 [>.............................] - ETA: 34s  batch_loss: 0.2408 [Training] 8/118 [=>............................] - ETA: 33s  batch_loss: 0.2380 [Training] 9/118 [=>............................] - ETA: 32s  batch_loss: 0.2444 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 0.2419 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.2405 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.2401 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.2443 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.2440 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.2457 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.2490 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.2506 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.2500 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.2501 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.2496 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.2484 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.2493 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.2498 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.2509 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.2510 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.2504 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.2508 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.2491 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.2498 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.2515 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.2502 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.2476 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.2478 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 0.2483 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.2480 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.2474 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2471 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.2476 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.2470 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2485 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2487 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.2482 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.2474 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2490 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.2487 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.2480 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2475 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2483 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.2472 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.2471 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2476 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2473 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.2465 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2456 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2456 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.2453 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.2458 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2453 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2462 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.2456 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2465 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2460 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2459 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.2466 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.2455 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2449 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2449 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.2450 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2448 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2444 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.2437 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.2434 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2437 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2441 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.2438 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.2436 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2437 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2433 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.2432 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2436 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2431 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2428 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.2429 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2438 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2437 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2434 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.2436 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2443 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2448 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2443 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.2442 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2446 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2442 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2444 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.2446 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2441 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2441 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2447 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2448 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2447 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2450 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2457 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2455 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2453 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2450 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2447 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2446 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2444 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2449 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2443 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2439 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2451 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2448 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2453 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2449 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2441 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2444 [Training] 118/118 [==============================] 255.3ms/step  batch_loss: 0.2458 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:36:58 - INFO - root -   The F1-score is 0.6290322580645161
09/18/2022 13:36:58 - INFO - root -   the best eval f1 is 0.6290, saving model !!
09/18/2022 13:37:01 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:14  batch_loss: 0.1806 [Training] 2/118 [..............................] - ETA: 51s  batch_loss: 0.2138 [Training] 3/118 [..............................] - ETA: 43s  batch_loss: 0.2062 [Training] 4/118 [>.............................] - ETA: 40s  batch_loss: 0.1941 [Training] 5/118 [>.............................] - ETA: 37s  batch_loss: 0.2016 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 0.2082 [Training] 7/118 [>.............................] - ETA: 34s  batch_loss: 0.2181 [Training] 8/118 [=>............................] - ETA: 33s  batch_loss: 0.2184 [Training] 9/118 [=>............................] - ETA: 32s  batch_loss: 0.2206 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 0.2156 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.2207 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.2221 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.2240 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.2233 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.2236 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.2235 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.2259 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.2248 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.2238 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.2254 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.2235 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.2229 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.2229 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.2236 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.2247 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.2230 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.2248 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.2235 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.2249 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.2268 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.2256 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.2245 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.2240 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.2247 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.2245 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.2244 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2251 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2256 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.2258 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2255 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2253 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.2242 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.2251 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2244 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2246 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.2230 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2224 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2217 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.2215 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.2211 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2212 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2219 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.2225 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2218 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2220 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.2218 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.2216 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2216 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2213 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.2214 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2219 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2219 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2226 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.2225 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.2225 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2221 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2224 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.2231 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2227 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2221 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.2220 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.2219 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2224 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2229 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.2230 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.2223 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2226 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2234 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.2229 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2234 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2232 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2234 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.2239 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2240 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2239 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2241 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.2241 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2248 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2252 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2250 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.2251 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2249 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2250 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2253 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.2254 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2254 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2256 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2264 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2261 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2261 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2262 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2257 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2259 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2258 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2262 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2263 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2265 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2264 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2266 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2264 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2269 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2268 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2263 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2261 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2260 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2264 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2262 [Training] 118/118 [==============================] 253.3ms/step  batch_loss: 0.2264 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:37:37 - INFO - root -   The F1-score is 0.6323557237464522
09/18/2022 13:37:37 - INFO - root -   the best eval f1 is 0.6324, saving model !!
09/18/2022 13:37:39 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:17  batch_loss: 0.1895 [Training] 2/118 [..............................] - ETA: 53s  batch_loss: 0.1800 [Training] 3/118 [..............................] - ETA: 43s  batch_loss: 0.1825 [Training] 4/118 [>.............................] - ETA: 40s  batch_loss: 0.1797 [Training] 5/118 [>.............................] - ETA: 37s  batch_loss: 0.1899 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 0.1971 [Training] 7/118 [>.............................] - ETA: 34s  batch_loss: 0.1998 [Training] 8/118 [=>............................] - ETA: 33s  batch_loss: 0.2030 [Training] 9/118 [=>............................] - ETA: 32s  batch_loss: 0.2025 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 0.2012 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.1982 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.1989 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.1993 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.2011 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.2050 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.2029 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.2019 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.2020 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.2017 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.2032 [Training] 21/118 [====>.........................] - ETA: 25s  batch_loss: 0.2030 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.2039 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.2055 [Training] 24/118 [=====>........................] - ETA: 24s  batch_loss: 0.2068 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.2078 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.2071 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.2074 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.2083 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.2086 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.2081 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.2070 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.2062 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.2060 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 0.2059 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.2057 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.2057 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2056 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.2044 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.2041 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2051 [Training] 41/118 [=========>....................] - ETA: 19s  batch_loss: 0.2046 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.2045 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.2049 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2049 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.2052 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.2049 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2047 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2057 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.2059 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.2062 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2057 [Training] 52/118 [============>.................] - ETA: 16s  batch_loss: 0.2053 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.2058 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2064 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2065 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.2070 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.2060 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2056 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2061 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.2058 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2062 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2055 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2055 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.2059 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.2059 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2057 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2061 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.2072 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2068 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2071 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.2072 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.2088 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2087 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2085 [Training] 75/118 [==================>...........] - ETA: 10s  batch_loss: 0.2081 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.2086 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2083 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2083 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.2087 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2086 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2093 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2095 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.2097 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2091 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2090 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2086 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.2091 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2095 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2095 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2102 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.2098 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2105 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2106 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2113 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.2114 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2113 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2111 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2110 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2108 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2104 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2106 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2104 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2101 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2102 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2102 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2104 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2100 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2100 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2105 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2101 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2102 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2099 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2104 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2112 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2113 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2113 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2111 [Training] 118/118 [==============================] 251.9ms/step  batch_loss: 0.2110 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:38:15 - INFO - root -   The F1-score is 0.6332846158734194
09/18/2022 13:38:15 - INFO - root -   the best eval f1 is 0.6333, saving model !!
09/18/2022 13:38:17 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:17  batch_loss: 0.2449 [Training] 2/118 [..............................] - ETA: 52s  batch_loss: 0.2234 [Training] 3/118 [..............................] - ETA: 44s  batch_loss: 0.2179 [Training] 4/118 [>.............................] - ETA: 40s  batch_loss: 0.2102 [Training] 5/118 [>.............................] - ETA: 37s  batch_loss: 0.2053 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 0.2095 [Training] 7/118 [>.............................] - ETA: 34s  batch_loss: 0.2169 [Training] 8/118 [=>............................] - ETA: 33s  batch_loss: 0.2163 [Training] 9/118 [=>............................] - ETA: 32s  batch_loss: 0.2150 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 0.2130 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.2113 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.2094 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.2071 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.2054 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.2031 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.2044 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.2032 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.2051 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.2032 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.2034 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.2015 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.2015 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.2026 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.2016 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.2023 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.2030 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.2028 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.2041 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.2023 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.2034 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.2023 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.2016 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1998 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1996 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.1999 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.2001 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2011 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.2005 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.2007 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2005 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2005 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.2006 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1997 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2007 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.2003 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.2007 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2006 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2005 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.1998 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1999 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2000 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2002 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.2010 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2000 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1992 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.1995 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.2000 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1998 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1998 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.2001 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2004 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1998 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1998 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.1999 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1997 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1993 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1989 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1988 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1983 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1979 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1979 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1983 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1980 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1977 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1987 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1988 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1989 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1995 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.1992 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1988 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1988 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1988 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.1991 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1991 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1996 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1991 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1985 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1996 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2004 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2003 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.2001 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2000 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2004 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2002 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1995 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1997 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1992 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1989 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1988 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1996 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1995 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1997 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1997 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1992 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1992 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1987 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1985 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1984 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1987 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1988 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1991 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1991 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1990 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1993 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1994 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1992 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1994 [Training] 118/118 [==============================] 254.4ms/step  batch_loss: 0.1987 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:38:54 - INFO - root -   The F1-score is 0.6310195087173505
09/18/2022 13:38:54 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:14  batch_loss: 0.1329 [Training] 2/118 [..............................] - ETA: 52s  batch_loss: 0.1635 [Training] 3/118 [..............................] - ETA: 44s  batch_loss: 0.1734 [Training] 4/118 [>.............................] - ETA: 40s  batch_loss: 0.1703 [Training] 5/118 [>.............................] - ETA: 37s  batch_loss: 0.1675 [Training] 6/118 [>.............................] - ETA: 36s  batch_loss: 0.1671 [Training] 7/118 [>.............................] - ETA: 34s  batch_loss: 0.1696 [Training] 8/118 [=>............................] - ETA: 33s  batch_loss: 0.1695 [Training] 9/118 [=>............................] - ETA: 32s  batch_loss: 0.1708 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.1690 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.1713 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.1691 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.1690 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.1689 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.1707 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.1716 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.1715 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.1723 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.1722 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.1716 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1726 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.1721 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.1740 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1738 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.1758 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1757 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1759 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.1752 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1774 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1768 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1773 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1765 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1759 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1763 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1772 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.1770 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1784 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1806 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1796 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1794 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1792 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1797 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.1813 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1810 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1811 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.1822 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.1816 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1825 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1828 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.1829 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1830 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1842 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1841 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.1843 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1841 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1847 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.1842 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1843 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1853 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1848 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.1851 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1855 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1854 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1853 [Training] 65/118 [===============>..............] - ETA: 14s  batch_loss: 0.1856 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1863 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1860 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1861 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1857 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1857 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1865 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.1863 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1865 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1862 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1858 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.1860 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1862 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1863 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1862 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1862 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1863 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1862 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1869 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1867 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1868 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1869 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1864 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1873 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1871 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1872 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.1868 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1866 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1867 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1869 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1872 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1876 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1880 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1880 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1877 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1881 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1883 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1885 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1881 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1878 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1877 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1878 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1878 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1879 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1879 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1880 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1876 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1877 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1878 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1879 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1880 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1878 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1880 [Training] 118/118 [==============================] 256.9ms/step  batch_loss: 0.1871 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:39:30 - INFO - root -   The F1-score is 0.6455952458695227
09/18/2022 13:39:30 - INFO - root -   the best eval f1 is 0.6456, saving model !!
09/18/2022 13:39:32 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:17  batch_loss: 0.1548 [Training] 2/118 [..............................] - ETA: 53s  batch_loss: 0.1608 [Training] 3/118 [..............................] - ETA: 44s  batch_loss: 0.1839 [Training] 4/118 [>.............................] - ETA: 40s  batch_loss: 0.1837 [Training] 5/118 [>.............................] - ETA: 37s  batch_loss: 0.1877 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 0.1864 [Training] 7/118 [>.............................] - ETA: 33s  batch_loss: 0.1805 [Training] 8/118 [=>............................] - ETA: 32s  batch_loss: 0.1871 [Training] 9/118 [=>............................] - ETA: 31s  batch_loss: 0.1855 [Training] 10/118 [=>............................] - ETA: 30s  batch_loss: 0.1815 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.1797 [Training] 12/118 [==>...........................] - ETA: 29s  batch_loss: 0.1841 [Training] 13/118 [==>...........................] - ETA: 28s  batch_loss: 0.1847 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.1842 [Training] 15/118 [==>...........................] - ETA: 27s  batch_loss: 0.1825 [Training] 16/118 [===>..........................] - ETA: 27s  batch_loss: 0.1812 [Training] 17/118 [===>..........................] - ETA: 26s  batch_loss: 0.1802 [Training] 18/118 [===>..........................] - ETA: 26s  batch_loss: 0.1788 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.1775 [Training] 20/118 [====>.........................] - ETA: 25s  batch_loss: 0.1757 [Training] 21/118 [====>.........................] - ETA: 25s  batch_loss: 0.1753 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.1749 [Training] 23/118 [====>.........................] - ETA: 24s  batch_loss: 0.1748 [Training] 24/118 [=====>........................] - ETA: 24s  batch_loss: 0.1732 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.1729 [Training] 26/118 [=====>........................] - ETA: 23s  batch_loss: 0.1722 [Training] 27/118 [=====>........................] - ETA: 23s  batch_loss: 0.1724 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.1734 [Training] 29/118 [======>.......................] - ETA: 22s  batch_loss: 0.1726 [Training] 30/118 [======>.......................] - ETA: 22s  batch_loss: 0.1709 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.1699 [Training] 32/118 [=======>......................] - ETA: 21s  batch_loss: 0.1701 [Training] 33/118 [=======>......................] - ETA: 21s  batch_loss: 0.1700 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 0.1709 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.1704 [Training] 36/118 [========>.....................] - ETA: 20s  batch_loss: 0.1697 [Training] 37/118 [========>.....................] - ETA: 20s  batch_loss: 0.1707 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.1708 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1713 [Training] 40/118 [=========>....................] - ETA: 19s  batch_loss: 0.1717 [Training] 41/118 [=========>....................] - ETA: 19s  batch_loss: 0.1713 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.1708 [Training] 43/118 [=========>....................] - ETA: 18s  batch_loss: 0.1709 [Training] 44/118 [==========>...................] - ETA: 18s  batch_loss: 0.1714 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.1720 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1720 [Training] 47/118 [==========>...................] - ETA: 17s  batch_loss: 0.1720 [Training] 48/118 [===========>..................] - ETA: 17s  batch_loss: 0.1719 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.1721 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1722 [Training] 51/118 [===========>..................] - ETA: 16s  batch_loss: 0.1726 [Training] 52/118 [============>.................] - ETA: 16s  batch_loss: 0.1733 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.1736 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1745 [Training] 55/118 [============>.................] - ETA: 15s  batch_loss: 0.1747 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.1741 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1742 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1741 [Training] 59/118 [==============>...............] - ETA: 14s  batch_loss: 0.1735 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.1735 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1740 [Training] 62/118 [==============>...............] - ETA: 13s  batch_loss: 0.1745 [Training] 63/118 [===============>..............] - ETA: 13s  batch_loss: 0.1745 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.1746 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1749 [Training] 66/118 [===============>..............] - ETA: 12s  batch_loss: 0.1748 [Training] 67/118 [================>.............] - ETA: 12s  batch_loss: 0.1752 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1753 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1754 [Training] 70/118 [================>.............] - ETA: 11s  batch_loss: 0.1759 [Training] 71/118 [=================>............] - ETA: 11s  batch_loss: 0.1756 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1762 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1760 [Training] 74/118 [=================>............] - ETA: 10s  batch_loss: 0.1756 [Training] 75/118 [==================>...........] - ETA: 10s  batch_loss: 0.1756 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1758 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1760 [Training] 78/118 [==================>...........] - ETA: 9s  batch_loss: 0.1757 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.1756 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1754 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1759 [Training] 82/118 [===================>..........] - ETA: 8s  batch_loss: 0.1762 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.1762 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1766 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1768 [Training] 86/118 [====================>.........] - ETA: 7s  batch_loss: 0.1765 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1762 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1760 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1765 [Training] 90/118 [=====================>........] - ETA: 6s  batch_loss: 0.1760 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1762 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1759 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1761 [Training] 94/118 [======================>.......] - ETA: 5s  batch_loss: 0.1772 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1772 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1775 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1777 [Training] 98/118 [=======================>......] - ETA: 4s  batch_loss: 0.1779 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1777 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1781 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1781 [Training] 102/118 [========================>.....] - ETA: 3s  batch_loss: 0.1784 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1778 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1779 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1784 [Training] 106/118 [=========================>....] - ETA: 2s  batch_loss: 0.1784 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1782 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1782 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1784 [Training] 110/118 [==========================>...] - ETA: 1s  batch_loss: 0.1783 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1783 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1784 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1787 [Training] 114/118 [===========================>..] - ETA: 0s  batch_loss: 0.1787 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1788 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1786 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1790 [Training] 118/118 [==============================] 245.1ms/step  batch_loss: 0.1785 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:40:08 - INFO - root -   The F1-score is 0.6419350632416222
09/18/2022 13:40:08 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:11  batch_loss: 0.1462 [Training] 2/118 [..............................] - ETA: 49s  batch_loss: 0.1677 [Training] 3/118 [..............................] - ETA: 42s  batch_loss: 0.1811 [Training] 4/118 [>.............................] - ETA: 38s  batch_loss: 0.1725 [Training] 5/118 [>.............................] - ETA: 36s  batch_loss: 0.1670 [Training] 6/118 [>.............................] - ETA: 34s  batch_loss: 0.1711 [Training] 7/118 [>.............................] - ETA: 33s  batch_loss: 0.1670 [Training] 8/118 [=>............................] - ETA: 32s  batch_loss: 0.1670 [Training] 9/118 [=>............................] - ETA: 31s  batch_loss: 0.1650 [Training] 10/118 [=>............................] - ETA: 30s  batch_loss: 0.1638 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.1614 [Training] 12/118 [==>...........................] - ETA: 29s  batch_loss: 0.1634 [Training] 13/118 [==>...........................] - ETA: 28s  batch_loss: 0.1626 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.1655 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.1668 [Training] 16/118 [===>..........................] - ETA: 27s  batch_loss: 0.1667 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.1664 [Training] 18/118 [===>..........................] - ETA: 26s  batch_loss: 0.1656 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.1655 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.1645 [Training] 21/118 [====>.........................] - ETA: 25s  batch_loss: 0.1630 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.1651 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.1649 [Training] 24/118 [=====>........................] - ETA: 24s  batch_loss: 0.1646 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.1645 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.1646 [Training] 27/118 [=====>........................] - ETA: 23s  batch_loss: 0.1659 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.1663 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.1668 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1670 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.1652 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.1638 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1647 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 0.1655 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.1662 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1655 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1649 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.1641 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1639 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1640 [Training] 41/118 [=========>....................] - ETA: 19s  batch_loss: 0.1635 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.1632 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1625 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1628 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.1630 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1627 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1625 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1640 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.1630 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1629 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1631 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1628 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.1627 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1619 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1628 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.1632 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1632 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1634 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1638 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.1641 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1647 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1644 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1640 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.1641 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1645 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1649 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1657 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1654 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1652 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1645 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1643 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1639 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1645 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1652 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1656 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1652 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1654 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1656 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.1662 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1667 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1669 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1672 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.1671 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1672 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1681 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1679 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1677 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1678 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1676 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1672 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1674 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1677 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1673 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1677 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1677 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1677 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1675 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1678 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1675 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1673 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1675 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1674 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1675 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1673 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1673 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1675 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1681 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1681 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1679 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1684 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1682 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1683 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1683 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1684 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1683 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1681 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1683 [Training] 118/118 [==============================] 253.6ms/step  batch_loss: 0.1688 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:40:44 - INFO - root -   The F1-score is 0.6447223482148642
09/18/2022 13:40:44 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:16  batch_loss: 0.1338 [Training] 2/118 [..............................] - ETA: 53s  batch_loss: 0.1217 [Training] 3/118 [..............................] - ETA: 45s  batch_loss: 0.1306 [Training] 4/118 [>.............................] - ETA: 40s  batch_loss: 0.1309 [Training] 5/118 [>.............................] - ETA: 37s  batch_loss: 0.1328 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 0.1405 [Training] 7/118 [>.............................] - ETA: 34s  batch_loss: 0.1421 [Training] 8/118 [=>............................] - ETA: 33s  batch_loss: 0.1500 [Training] 9/118 [=>............................] - ETA: 32s  batch_loss: 0.1533 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 0.1546 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.1597 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.1575 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.1568 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.1567 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.1562 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.1541 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.1539 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.1542 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.1538 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.1546 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1549 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.1540 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.1534 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1528 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.1535 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.1521 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1517 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.1531 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.1523 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1528 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1521 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.1521 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1524 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1547 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.1562 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1570 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1583 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.1579 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1570 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1576 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1575 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.1579 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1581 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1590 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.1581 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1583 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1588 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1583 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.1577 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1581 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1573 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1584 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.1580 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1580 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1584 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1581 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1577 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1579 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1581 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.1575 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1578 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1575 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1582 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.1574 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1582 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1595 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1617 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1616 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1615 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1616 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1614 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1619 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1617 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1623 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1619 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1614 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1618 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1616 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1623 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1623 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1637 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1641 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.1636 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1633 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1638 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1635 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1634 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1629 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1628 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1626 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1633 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1632 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1634 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1639 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1633 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1632 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1633 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1631 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1634 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1633 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1628 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1628 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1628 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1625 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1624 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1624 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1625 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1622 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1627 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1626 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1626 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1625 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1625 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1629 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1629 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1627 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1624 [Training] 118/118 [==============================] 253.1ms/step  batch_loss: 0.1631 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:41:20 - INFO - root -   The F1-score is 0.6407079646017698
09/18/2022 13:41:20 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:10  batch_loss: 0.1059 [Training] 2/118 [..............................] - ETA: 49s  batch_loss: 0.1194 [Training] 3/118 [..............................] - ETA: 42s  batch_loss: 0.1200 [Training] 4/118 [>.............................] - ETA: 38s  batch_loss: 0.1242 [Training] 5/118 [>.............................] - ETA: 36s  batch_loss: 0.1310 [Training] 6/118 [>.............................] - ETA: 34s  batch_loss: 0.1331 [Training] 7/118 [>.............................] - ETA: 33s  batch_loss: 0.1326 [Training] 8/118 [=>............................] - ETA: 32s  batch_loss: 0.1335 [Training] 9/118 [=>............................] - ETA: 31s  batch_loss: 0.1377 [Training] 10/118 [=>............................] - ETA: 30s  batch_loss: 0.1373 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.1413 [Training] 12/118 [==>...........................] - ETA: 29s  batch_loss: 0.1420 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.1398 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.1380 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.1420 [Training] 16/118 [===>..........................] - ETA: 27s  batch_loss: 0.1415 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.1400 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.1407 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.1411 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.1414 [Training] 21/118 [====>.........................] - ETA: 25s  batch_loss: 0.1404 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.1412 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.1404 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1407 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.1418 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.1432 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1434 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.1442 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.1447 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1448 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.1439 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.1448 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1450 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 0.1463 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.1463 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1464 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1471 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.1490 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1482 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1490 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1488 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.1482 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1477 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1470 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1479 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1489 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1486 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1495 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.1499 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1492 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1495 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1490 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.1490 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1493 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1493 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.1497 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1498 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1504 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1501 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.1501 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1497 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1496 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1501 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.1495 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1492 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1499 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1499 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1501 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1504 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1504 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1501 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1500 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1496 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1495 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1491 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1489 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1487 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1494 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.1496 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1494 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1496 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1498 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.1499 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1496 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1497 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1495 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1495 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1493 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1490 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1490 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1491 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1492 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1492 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1489 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1487 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1495 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1497 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1499 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1496 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1506 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1508 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1518 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1524 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1529 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1527 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1526 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1525 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1528 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1530 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1533 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1533 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1533 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1530 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1526 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1525 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1525 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1524 [Training] 118/118 [==============================] 253.6ms/step  batch_loss: 0.1527 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:41:56 - INFO - root -   The F1-score is 0.645869890167024
09/18/2022 13:41:56 - INFO - root -   the best eval f1 is 0.6459, saving model !!
09/18/2022 13:41:58 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:14  batch_loss: 0.0959 [Training] 2/118 [..............................] - ETA: 51s  batch_loss: 0.1086 [Training] 3/118 [..............................] - ETA: 44s  batch_loss: 0.1171 [Training] 4/118 [>.............................] - ETA: 39s  batch_loss: 0.1255 [Training] 5/118 [>.............................] - ETA: 37s  batch_loss: 0.1290 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 0.1268 [Training] 7/118 [>.............................] - ETA: 34s  batch_loss: 0.1350 [Training] 8/118 [=>............................] - ETA: 33s  batch_loss: 0.1355 [Training] 9/118 [=>............................] - ETA: 32s  batch_loss: 0.1393 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 0.1363 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.1361 [Training] 12/118 [==>...........................] - ETA: 29s  batch_loss: 0.1348 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.1390 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.1369 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.1358 [Training] 16/118 [===>..........................] - ETA: 27s  batch_loss: 0.1337 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.1321 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.1329 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.1326 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.1311 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1310 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.1356 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.1363 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1349 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.1399 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.1390 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1384 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.1383 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.1394 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1389 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.1386 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.1380 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1393 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 0.1403 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.1404 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1415 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1419 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.1413 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1423 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1418 [Training] 41/118 [=========>....................] - ETA: 19s  batch_loss: 0.1424 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.1426 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1426 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1433 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.1441 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1437 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1437 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1440 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.1447 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1445 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1447 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1439 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.1445 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1452 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1446 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.1453 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1450 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1451 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1450 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.1454 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1453 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1450 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1459 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.1455 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1459 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1459 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1454 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1448 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1451 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1451 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1457 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1456 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1459 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1461 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1462 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1457 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1455 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1452 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1455 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1453 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1454 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1449 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.1444 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1446 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1445 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1448 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1443 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1443 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1443 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1442 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1438 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1439 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1436 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1438 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1444 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1448 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1450 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1450 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1449 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1449 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1460 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1459 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1456 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1454 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1455 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1459 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1458 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1460 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1462 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1464 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1462 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1463 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1461 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1461 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1462 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1463 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1461 [Training] 118/118 [==============================] 253.2ms/step  batch_loss: 0.1460 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:42:35 - INFO - root -   The F1-score is 0.6478418195916258
09/18/2022 13:42:35 - INFO - root -   the best eval f1 is 0.6478, saving model !!
09/18/2022 13:42:37 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:18  batch_loss: 0.1649 [Training] 2/118 [..............................] - ETA: 53s  batch_loss: 0.1503 [Training] 3/118 [..............................] - ETA: 44s  batch_loss: 0.1392 [Training] 4/118 [>.............................] - ETA: 40s  batch_loss: 0.1293 [Training] 5/118 [>.............................] - ETA: 37s  batch_loss: 0.1364 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 0.1396 [Training] 7/118 [>.............................] - ETA: 34s  batch_loss: 0.1402 [Training] 8/118 [=>............................] - ETA: 33s  batch_loss: 0.1409 [Training] 9/118 [=>............................] - ETA: 32s  batch_loss: 0.1408 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 0.1356 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.1374 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.1375 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.1383 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.1379 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.1374 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.1356 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.1353 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.1358 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.1363 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.1361 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1380 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.1378 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.1361 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1355 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.1355 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.1343 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1341 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.1348 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.1353 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1361 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.1356 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.1381 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1388 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1377 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.1372 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1376 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1398 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.1398 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1398 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1396 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1393 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.1387 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1386 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1386 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.1390 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1387 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1388 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1382 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.1379 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1380 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1377 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1376 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.1374 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1377 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1370 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.1375 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1377 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1375 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1378 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.1381 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1384 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1383 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1381 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.1376 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1379 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1379 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1389 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1392 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1390 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1387 [Training] 71/118 [=================>............] - ETA: 11s  batch_loss: 0.1383 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1380 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1380 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1380 [Training] 75/118 [==================>...........] - ETA: 10s  batch_loss: 0.1379 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1384 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1379 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1377 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.1383 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1382 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1380 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1381 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.1383 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1384 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1379 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1384 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1391 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1398 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1404 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1404 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1405 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1406 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1407 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1408 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1414 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1409 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1409 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1405 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1410 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1408 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1406 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1409 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1408 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1404 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1408 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1408 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1416 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1421 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1421 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1417 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1417 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1417 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1421 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1422 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1422 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1424 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1418 [Training] 118/118 [==============================] 252.0ms/step  batch_loss: 0.1424 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:43:13 - INFO - root -   The F1-score is 0.6563856670770427
09/18/2022 13:43:13 - INFO - root -   the best eval f1 is 0.6564, saving model !!
09/18/2022 13:43:15 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:19  batch_loss: 0.1255 [Training] 2/118 [..............................] - ETA: 53s  batch_loss: 0.1225 [Training] 3/118 [..............................] - ETA: 45s  batch_loss: 0.1234 [Training] 4/118 [>.............................] - ETA: 40s  batch_loss: 0.1259 [Training] 5/118 [>.............................] - ETA: 37s  batch_loss: 0.1271 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 0.1298 [Training] 7/118 [>.............................] - ETA: 34s  batch_loss: 0.1283 [Training] 8/118 [=>............................] - ETA: 33s  batch_loss: 0.1275 [Training] 9/118 [=>............................] - ETA: 32s  batch_loss: 0.1289 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 0.1325 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.1343 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.1313 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.1315 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.1297 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.1344 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.1378 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.1347 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.1313 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.1321 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.1306 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1295 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.1298 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.1309 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1337 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.1332 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.1334 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1329 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.1318 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.1328 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1329 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.1328 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.1322 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1313 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 0.1316 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.1308 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1320 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1328 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.1328 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1330 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1320 [Training] 41/118 [=========>....................] - ETA: 19s  batch_loss: 0.1315 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.1321 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1319 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1317 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.1320 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1312 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1316 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1316 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.1319 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1325 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1322 [Training] 52/118 [============>.................] - ETA: 16s  batch_loss: 0.1322 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.1316 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1314 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1313 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.1314 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1313 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1313 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1313 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.1314 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1316 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1323 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1318 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.1314 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1325 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1330 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1334 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1335 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1333 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1335 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1335 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1334 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1337 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1336 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1340 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1342 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1340 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1343 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.1344 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1342 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1348 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1346 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.1341 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1340 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1341 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1346 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1347 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1349 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1346 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1344 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1341 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1341 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1337 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1339 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1338 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1339 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1338 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1339 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1337 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1334 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1332 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1332 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1335 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1336 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1336 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1335 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1339 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1345 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1346 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1344 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1348 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1347 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1346 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1345 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1346 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1345 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1344 [Training] 118/118 [==============================] 253.6ms/step  batch_loss: 0.1342 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:43:52 - INFO - root -   The F1-score is 0.6497263487099296
09/18/2022 13:43:52 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:13  batch_loss: 0.1650 [Training] 2/118 [..............................] - ETA: 51s  batch_loss: 0.1448 [Training] 3/118 [..............................] - ETA: 43s  batch_loss: 0.1422 [Training] 4/118 [>.............................] - ETA: 39s  batch_loss: 0.1293 [Training] 5/118 [>.............................] - ETA: 37s  batch_loss: 0.1254 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 0.1259 [Training] 7/118 [>.............................] - ETA: 34s  batch_loss: 0.1267 [Training] 8/118 [=>............................] - ETA: 33s  batch_loss: 0.1244 [Training] 9/118 [=>............................] - ETA: 32s  batch_loss: 0.1249 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 0.1271 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.1303 [Training] 12/118 [==>...........................] - ETA: 29s  batch_loss: 0.1275 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.1282 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.1274 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.1267 [Training] 16/118 [===>..........................] - ETA: 27s  batch_loss: 0.1262 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.1265 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.1259 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.1270 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.1278 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1291 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.1294 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.1289 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1291 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.1274 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.1278 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1285 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.1293 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.1282 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1295 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.1298 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.1305 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1309 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1309 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.1311 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1316 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1312 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.1308 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1304 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1303 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1294 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.1281 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1276 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1277 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.1281 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1274 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1264 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1261 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.1256 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1257 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1263 [Training] 52/118 [============>.................] - ETA: 16s  batch_loss: 0.1272 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.1286 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1280 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1278 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.1276 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1272 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1267 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1262 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.1265 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1264 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1261 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1260 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.1261 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1267 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1269 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1271 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1268 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1269 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1264 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1266 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1266 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1270 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1263 [Training] 75/118 [==================>...........] - ETA: 10s  batch_loss: 0.1265 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1271 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1268 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1279 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.1279 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1282 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1282 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1282 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.1285 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1280 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1283 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1292 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1298 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1296 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1292 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1289 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1288 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1290 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1287 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1289 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1285 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1288 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1288 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1293 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1292 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1296 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1301 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1303 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1302 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1304 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1303 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1302 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1306 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1306 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1309 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1308 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1313 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1309 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1311 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1309 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1310 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1312 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1316 [Training] 118/118 [==============================] 252.4ms/step  batch_loss: 0.1320 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:44:28 - INFO - root -   The F1-score is 0.6418150266726654
09/18/2022 13:44:28 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:14  batch_loss: 0.1550 [Training] 2/118 [..............................] - ETA: 51s  batch_loss: 0.1357 [Training] 3/118 [..............................] - ETA: 43s  batch_loss: 0.1337 [Training] 4/118 [>.............................] - ETA: 39s  batch_loss: 0.1300 [Training] 5/118 [>.............................] - ETA: 36s  batch_loss: 0.1344 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 0.1363 [Training] 7/118 [>.............................] - ETA: 33s  batch_loss: 0.1389 [Training] 8/118 [=>............................] - ETA: 32s  batch_loss: 0.1367 [Training] 9/118 [=>............................] - ETA: 32s  batch_loss: 0.1374 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 0.1350 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.1326 [Training] 12/118 [==>...........................] - ETA: 29s  batch_loss: 0.1314 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.1298 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.1262 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.1302 [Training] 16/118 [===>..........................] - ETA: 27s  batch_loss: 0.1281 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.1286 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.1283 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.1276 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.1268 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1257 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.1253 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.1274 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1270 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.1255 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.1251 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1245 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.1249 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.1241 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1235 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.1243 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.1254 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1259 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 0.1255 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.1259 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1258 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1250 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.1247 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1246 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1237 [Training] 41/118 [=========>....................] - ETA: 19s  batch_loss: 0.1239 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.1243 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1244 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1246 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.1242 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1239 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1233 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1233 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.1229 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1233 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1229 [Training] 52/118 [============>.................] - ETA: 16s  batch_loss: 0.1236 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.1237 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1242 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1248 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.1246 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1244 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1239 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1246 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.1245 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1244 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1238 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1239 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.1238 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1238 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1235 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1235 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1240 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1233 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1233 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1231 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1230 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1227 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1227 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1230 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1232 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1229 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1229 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1229 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1228 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1226 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1228 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.1234 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1234 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1238 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1237 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1238 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1239 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1237 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1235 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1246 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1248 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1250 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1249 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1247 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1254 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1254 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1253 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1254 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1252 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1252 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1252 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1253 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1257 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1256 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1256 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1257 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1257 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1259 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1262 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1259 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1258 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1258 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1261 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1263 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1261 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1261 [Training] 118/118 [==============================] 253.7ms/step  batch_loss: 0.1260 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:45:04 - INFO - root -   The F1-score is 0.6505408062930187
09/18/2022 13:45:04 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:13  batch_loss: 0.1671 [Training] 2/118 [..............................] - ETA: 50s  batch_loss: 0.1381 [Training] 3/118 [..............................] - ETA: 43s  batch_loss: 0.1236 [Training] 4/118 [>.............................] - ETA: 39s  batch_loss: 0.1176 [Training] 5/118 [>.............................] - ETA: 36s  batch_loss: 0.1162 [Training] 6/118 [>.............................] - ETA: 35s  batch_loss: 0.1139 [Training] 7/118 [>.............................] - ETA: 33s  batch_loss: 0.1080 [Training] 8/118 [=>............................] - ETA: 32s  batch_loss: 0.1064 [Training] 9/118 [=>............................] - ETA: 31s  batch_loss: 0.1058 [Training] 10/118 [=>............................] - ETA: 30s  batch_loss: 0.1123 [Training] 11/118 [=>............................] - ETA: 29s  batch_loss: 0.1105 [Training] 12/118 [==>...........................] - ETA: 29s  batch_loss: 0.1141 [Training] 13/118 [==>...........................] - ETA: 28s  batch_loss: 0.1138 [Training] 14/118 [==>...........................] - ETA: 28s  batch_loss: 0.1171 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.1184 [Training] 16/118 [===>..........................] - ETA: 27s  batch_loss: 0.1152 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.1175 [Training] 18/118 [===>..........................] - ETA: 26s  batch_loss: 0.1168 [Training] 19/118 [===>..........................] - ETA: 26s  batch_loss: 0.1159 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.1151 [Training] 21/118 [====>.........................] - ETA: 25s  batch_loss: 0.1140 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.1176 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.1176 [Training] 24/118 [=====>........................] - ETA: 24s  batch_loss: 0.1183 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.1187 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.1184 [Training] 27/118 [=====>........................] - ETA: 23s  batch_loss: 0.1181 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.1173 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.1177 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1181 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.1180 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.1171 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1165 [Training] 34/118 [=======>......................] - ETA: 21s  batch_loss: 0.1159 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.1147 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1155 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1150 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.1150 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1149 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1147 [Training] 41/118 [=========>....................] - ETA: 19s  batch_loss: 0.1146 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.1142 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1147 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1144 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.1138 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1131 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1135 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1135 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.1133 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1142 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1141 [Training] 52/118 [============>.................] - ETA: 16s  batch_loss: 0.1145 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.1147 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1144 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1139 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.1141 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1146 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1148 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1153 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.1159 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1155 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1155 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1159 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.1157 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1156 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1149 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1149 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1147 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1145 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1145 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1147 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1147 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1154 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1164 [Training] 75/118 [==================>...........] - ETA: 10s  batch_loss: 0.1160 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1161 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1161 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1156 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.1160 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1160 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1162 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1161 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.1161 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1162 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1162 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1159 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1164 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1168 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1170 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1173 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1172 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1173 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1175 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1176 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1176 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1177 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1179 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1183 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1184 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1182 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1181 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1180 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1182 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1188 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1191 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1190 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1190 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1189 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1194 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1195 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1198 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1200 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1201 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1203 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1206 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1209 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1210 [Training] 118/118 [==============================] 252.3ms/step  batch_loss: 0.1211 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/18/2022 13:45:40 - INFO - root -   The F1-score is 0.6540272135164764
********** predict start ***********
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 8, 
    "event_type": "role", 
    "fine_tunning_model_path": "./output/DuEE1.0/role/best_model.pkl", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 0.001, 
    "max_len": 250, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 50.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 128, 
    "per_gpu_train_batch_size": 8, 
    "predict_save_path": "./output/DuEE1.0/role/test_result.json", 
    "seed": 66, 
    "stride": 100, 
    "test_json": "./data/DuEE1.0/subor_news.json", 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/147540 [00:00<?, ?it/s]tokenizing...:   0%|          | 161/147540 [00:00<01:31, 1603.95it/s]tokenizing...:   0%|          | 326/147540 [00:00<01:30, 1626.21it/s]tokenizing...:   0%|          | 551/147540 [00:00<01:17, 1907.56it/s]tokenizing...:   1%|          | 804/147540 [00:00<01:08, 2151.91it/s]tokenizing...:   1%|          | 1020/147540 [00:00<01:08, 2147.01it/s]tokenizing...:   1%|          | 1235/147540 [00:00<01:17, 1880.89it/s]tokenizing...:   1%|          | 1429/147540 [00:00<01:19, 1842.97it/s]tokenizing...:   1%|          | 1677/147540 [00:00<01:11, 2026.78it/s]tokenizing...:   1%|▏         | 1884/147540 [00:00<01:12, 1999.06it/s]tokenizing...:   1%|▏         | 2087/147540 [00:01<02:44, 886.02it/s] tokenizing...:   2%|▏         | 2312/147540 [00:01<02:12, 1097.07it/s]tokenizing...:   2%|▏         | 2530/147540 [00:01<01:52, 1292.62it/s]tokenizing...:   2%|▏         | 2736/147540 [00:01<01:39, 1449.31it/s]tokenizing...:   2%|▏         | 2959/147540 [00:01<01:28, 1626.38it/s]tokenizing...:   2%|▏         | 3169/147540 [00:01<01:22, 1742.17it/s]tokenizing...:   2%|▏         | 3388/147540 [00:02<01:17, 1857.44it/s]tokenizing...:   2%|▏         | 3623/147540 [00:02<01:12, 1990.13it/s]tokenizing...:   3%|▎         | 3858/147540 [00:02<01:08, 2089.77it/s]tokenizing...:   3%|▎         | 4080/147540 [00:02<01:08, 2083.27it/s]tokenizing...:   3%|▎         | 4301/147540 [00:02<01:07, 2113.77it/s]tokenizing...:   3%|▎         | 4526/147540 [00:02<01:06, 2146.79it/s]tokenizing...:   3%|▎         | 4746/147540 [00:02<01:08, 2073.15it/s]tokenizing...:   3%|▎         | 4964/147540 [00:02<01:07, 2101.70it/s]tokenizing...:   4%|▎         | 5177/147540 [00:02<01:13, 1938.86it/s]tokenizing...:   4%|▎         | 5376/147540 [00:03<01:15, 1890.82it/s]tokenizing...:   4%|▍         | 5568/147540 [00:03<01:17, 1841.57it/s]tokenizing...:   4%|▍         | 5755/147540 [00:03<01:17, 1822.54it/s]tokenizing...:   4%|▍         | 5939/147540 [00:03<01:18, 1803.13it/s]tokenizing...:   4%|▍         | 6121/147540 [00:03<01:20, 1749.78it/s]tokenizing...:   4%|▍         | 6299/147540 [00:03<01:20, 1756.87it/s]tokenizing...:   4%|▍         | 6476/147540 [00:03<01:20, 1757.25it/s]tokenizing...:   5%|▍         | 6654/147540 [00:03<01:20, 1759.69it/s]tokenizing...:   5%|▍         | 6832/147540 [00:03<01:19, 1764.28it/s]tokenizing...:   5%|▍         | 7009/147540 [00:04<01:20, 1751.53it/s]tokenizing...:   5%|▍         | 7185/147540 [00:04<01:21, 1712.35it/s]tokenizing...:   5%|▍         | 7364/147540 [00:04<01:20, 1732.85it/s]tokenizing...:   5%|▌         | 7540/147540 [00:04<01:20, 1739.46it/s]tokenizing...:   5%|▌         | 7717/147540 [00:04<01:20, 1747.62it/s]tokenizing...:   5%|▌         | 7898/147540 [00:04<01:19, 1763.48it/s]tokenizing...:   5%|▌         | 8075/147540 [00:04<01:19, 1759.67it/s]tokenizing...:   6%|▌         | 8252/147540 [00:04<01:21, 1715.90it/s]tokenizing...:   6%|▌         | 8428/147540 [00:04<01:20, 1728.78it/s]tokenizing...:   6%|▌         | 8603/147540 [00:04<01:20, 1734.70it/s]tokenizing...:   6%|▌         | 8792/147540 [00:05<01:18, 1776.75it/s]tokenizing...:   6%|▌         | 9028/147540 [00:05<01:11, 1947.20it/s]tokenizing...:   6%|▋         | 9223/147540 [00:05<01:12, 1913.04it/s]tokenizing...:   6%|▋         | 9420/147540 [00:05<01:11, 1929.34it/s]tokenizing...:   7%|▋         | 9674/147540 [00:05<01:05, 2107.02it/s]tokenizing...:   7%|▋         | 9886/147540 [00:05<01:07, 2053.33it/s]tokenizing...:   7%|▋         | 10131/147540 [00:05<01:03, 2166.12it/s]tokenizing...:   7%|▋         | 10354/147540 [00:05<01:02, 2183.00it/s]tokenizing...:   7%|▋         | 10596/147540 [00:05<01:00, 2252.32it/s]tokenizing...:   7%|▋         | 10827/147540 [00:05<01:00, 2269.17it/s]tokenizing...:   7%|▋         | 11055/147540 [00:06<01:00, 2242.48it/s]tokenizing...:   8%|▊         | 11302/147540 [00:06<00:59, 2307.09it/s]tokenizing...:   8%|▊         | 11533/147540 [00:06<01:00, 2255.88it/s]tokenizing...:   8%|▊         | 11765/147540 [00:06<00:59, 2273.47it/s]tokenizing...:   8%|▊         | 11993/147540 [00:06<00:59, 2273.60it/s]tokenizing...:   8%|▊         | 12221/147540 [00:06<01:00, 2229.78it/s]tokenizing...:   8%|▊         | 12445/147540 [00:06<01:03, 2140.45it/s]tokenizing...:   9%|▊         | 12671/147540 [00:06<01:02, 2170.64it/s]tokenizing...:   9%|▊         | 12889/147540 [00:06<01:02, 2168.31it/s]tokenizing...:   9%|▉         | 13114/147540 [00:06<01:01, 2188.75it/s]tokenizing...:   9%|▉         | 13334/147540 [00:07<01:02, 2162.84it/s]tokenizing...:   9%|▉         | 13555/147540 [00:07<01:01, 2176.36it/s]tokenizing...:   9%|▉         | 13779/147540 [00:07<01:01, 2192.17it/s]tokenizing...:   9%|▉         | 14008/147540 [00:07<01:00, 2212.54it/s]tokenizing...:  10%|▉         | 14230/147540 [00:07<01:07, 1985.12it/s]tokenizing...:  10%|▉         | 14457/147540 [00:07<01:04, 2063.35it/s]tokenizing...:  10%|▉         | 14668/147540 [00:07<01:05, 2036.29it/s]tokenizing...:  10%|█         | 14881/147540 [00:07<01:04, 2059.63it/s]tokenizing...:  10%|█         | 15113/147540 [00:07<01:02, 2131.82it/s]tokenizing...:  10%|█         | 15331/147540 [00:08<01:01, 2140.88it/s]tokenizing...:  11%|█         | 15547/147540 [00:08<01:04, 2039.61it/s]tokenizing...:  11%|█         | 15753/147540 [00:08<01:04, 2030.63it/s]tokenizing...:  11%|█         | 15964/147540 [00:08<01:04, 2053.30it/s]tokenizing...:  11%|█         | 16171/147540 [00:08<01:04, 2046.05it/s]tokenizing...:  11%|█         | 16383/147540 [00:08<01:03, 2067.05it/s]tokenizing...:  11%|█         | 16591/147540 [00:08<01:09, 1878.24it/s]tokenizing...:  11%|█▏        | 16792/147540 [00:08<01:08, 1913.02it/s]tokenizing...:  12%|█▏        | 16987/147540 [00:08<01:09, 1865.70it/s]tokenizing...:  12%|█▏        | 17176/147540 [00:09<01:10, 1840.66it/s]tokenizing...:  12%|█▏        | 17386/147540 [00:09<01:08, 1911.76it/s]tokenizing...:  12%|█▏        | 17579/147540 [00:09<01:09, 1869.69it/s]tokenizing...:  12%|█▏        | 17767/147540 [00:09<01:12, 1793.08it/s]tokenizing...:  12%|█▏        | 17950/147540 [00:09<01:11, 1801.12it/s]tokenizing...:  12%|█▏        | 18131/147540 [00:09<01:12, 1785.94it/s]tokenizing...:  12%|█▏        | 18314/147540 [00:09<01:11, 1798.03it/s]tokenizing...:  13%|█▎        | 18515/147540 [00:09<01:09, 1858.55it/s]tokenizing...:  13%|█▎        | 18702/147540 [00:09<01:12, 1786.02it/s]tokenizing...:  13%|█▎        | 18882/147540 [00:09<01:12, 1777.74it/s]tokenizing...:  13%|█▎        | 19062/147540 [00:10<01:12, 1783.83it/s]tokenizing...:  13%|█▎        | 19241/147540 [00:10<01:12, 1778.34it/s]tokenizing...:  13%|█▎        | 19420/147540 [00:10<01:12, 1769.29it/s]tokenizing...:  13%|█▎        | 19598/147540 [00:10<01:12, 1763.55it/s]tokenizing...:  13%|█▎        | 19775/147540 [00:10<01:14, 1716.88it/s]tokenizing...:  14%|█▎        | 19960/147540 [00:10<01:12, 1754.92it/s]tokenizing...:  14%|█▎        | 20176/147540 [00:10<01:08, 1871.80it/s]tokenizing...:  14%|█▍        | 20397/147540 [00:10<01:04, 1969.44it/s]tokenizing...:  14%|█▍        | 20602/147540 [00:10<01:03, 1990.60it/s]tokenizing...:  14%|█▍        | 20802/147540 [00:11<01:10, 1803.29it/s]tokenizing...:  14%|█▍        | 20986/147540 [00:11<03:49, 550.68it/s] tokenizing...:  14%|█▍        | 21160/147540 [00:12<03:06, 678.07it/s]tokenizing...:  14%|█▍        | 21334/147540 [00:12<02:34, 818.37it/s]tokenizing...:  15%|█▍        | 21511/147540 [00:12<02:09, 969.48it/s]tokenizing...:  15%|█▍        | 21687/147540 [00:12<01:52, 1115.91it/s]tokenizing...:  15%|█▍        | 21855/147540 [00:12<01:42, 1224.83it/s]tokenizing...:  15%|█▍        | 22031/147540 [00:12<01:33, 1347.05it/s]tokenizing...:  15%|█▌        | 22207/147540 [00:12<01:26, 1447.67it/s]tokenizing...:  15%|█▌        | 22385/147540 [00:12<01:21, 1533.72it/s]tokenizing...:  15%|█▌        | 22561/147540 [00:12<01:18, 1594.32it/s]tokenizing...:  15%|█▌        | 22738/147540 [00:12<01:16, 1639.31it/s]tokenizing...:  16%|█▌        | 22913/147540 [00:13<01:15, 1644.07it/s]tokenizing...:  16%|█▌        | 23085/147540 [00:13<01:15, 1657.37it/s]tokenizing...:  16%|█▌        | 23260/147540 [00:13<01:13, 1682.35it/s]tokenizing...:  16%|█▌        | 23437/147540 [00:13<01:12, 1704.92it/s]tokenizing...:  16%|█▌        | 23614/147540 [00:13<01:12, 1720.43it/s]tokenizing...:  16%|█▌        | 23789/147540 [00:13<01:11, 1729.01it/s]tokenizing...:  16%|█▌        | 23964/147540 [00:13<01:12, 1695.75it/s]tokenizing...:  16%|█▋        | 24139/147540 [00:13<01:12, 1711.11it/s]tokenizing...:  16%|█▋        | 24316/147540 [00:13<01:11, 1726.03it/s]tokenizing...:  17%|█▋        | 24511/147540 [00:13<01:08, 1792.00it/s]tokenizing...:  17%|█▋        | 24736/147540 [00:14<01:03, 1926.93it/s]tokenizing...:  17%|█▋        | 24962/147540 [00:14<01:00, 2025.97it/s]tokenizing...:  17%|█▋        | 25165/147540 [00:14<01:04, 1908.39it/s]tokenizing...:  17%|█▋        | 25358/147540 [00:14<01:04, 1888.27it/s]tokenizing...:  17%|█▋        | 25561/147540 [00:14<01:03, 1927.93it/s]tokenizing...:  17%|█▋        | 25755/147540 [00:14<01:05, 1848.59it/s]tokenizing...:  18%|█▊        | 25942/147540 [00:14<01:08, 1787.40it/s]tokenizing...:  18%|█▊        | 26125/147540 [00:14<01:07, 1797.54it/s]tokenizing...:  18%|█▊        | 26344/147540 [00:14<01:03, 1909.48it/s]tokenizing...:  18%|█▊        | 26577/147540 [00:15<00:59, 2030.68it/s]tokenizing...:  18%|█▊        | 26792/147540 [00:15<00:58, 2064.87it/s]tokenizing...:  18%|█▊        | 27024/147540 [00:15<00:56, 2139.73it/s]tokenizing...:  18%|█▊        | 27241/147540 [00:15<00:56, 2146.59it/s]tokenizing...:  19%|█▊        | 27473/147540 [00:15<00:54, 2196.39it/s]tokenizing...:  19%|█▉        | 27694/147540 [00:15<00:54, 2200.25it/s]tokenizing...:  19%|█▉        | 27943/147540 [00:15<00:52, 2285.86it/s]tokenizing...:  19%|█▉        | 28172/147540 [00:15<00:59, 1991.57it/s]tokenizing...:  19%|█▉        | 28396/147540 [00:15<00:57, 2057.66it/s]tokenizing...:  19%|█▉        | 28631/147540 [00:15<00:55, 2137.28it/s]tokenizing...:  20%|█▉        | 28861/147540 [00:16<00:54, 2180.14it/s]tokenizing...:  20%|█▉        | 29092/147540 [00:16<00:53, 2217.16it/s]tokenizing...:  20%|█▉        | 29317/147540 [00:16<00:53, 2199.31it/s]tokenizing...:  20%|██        | 29539/147540 [00:16<00:53, 2202.95it/s]tokenizing...:  20%|██        | 29761/147540 [00:16<00:53, 2188.18it/s]tokenizing...:  20%|██        | 29988/147540 [00:16<00:53, 2207.35it/s]tokenizing...:  20%|██        | 30210/147540 [00:16<00:53, 2183.05it/s]tokenizing...:  21%|██        | 30440/147540 [00:16<00:52, 2216.69it/s]tokenizing...:  21%|██        | 30665/147540 [00:16<00:52, 2226.22it/s]tokenizing...:  21%|██        | 30900/147540 [00:16<00:51, 2261.64it/s]tokenizing...:  21%|██        | 31127/147540 [00:17<00:51, 2252.68it/s]tokenizing...:  21%|██▏       | 31353/147540 [00:17<00:52, 2226.98it/s]tokenizing...:  21%|██▏       | 31591/147540 [00:17<00:51, 2269.27it/s]tokenizing...:  22%|██▏       | 31819/147540 [00:17<00:54, 2141.26it/s]tokenizing...:  22%|██▏       | 32044/147540 [00:17<00:53, 2165.36it/s]tokenizing...:  22%|██▏       | 32262/147540 [00:17<00:53, 2148.11it/s]tokenizing...:  22%|██▏       | 32478/147540 [00:17<00:55, 2056.00it/s]tokenizing...:  22%|██▏       | 32685/147540 [00:17<00:57, 2012.35it/s]tokenizing...:  22%|██▏       | 32899/147540 [00:17<00:55, 2047.31it/s]tokenizing...:  22%|██▏       | 33105/147540 [00:18<00:58, 1955.53it/s]tokenizing...:  23%|██▎       | 33302/147540 [00:18<01:02, 1827.14it/s]tokenizing...:  23%|██▎       | 33487/147540 [00:18<01:08, 1675.79it/s]tokenizing...:  23%|██▎       | 33664/147540 [00:18<01:07, 1698.89it/s]tokenizing...:  23%|██▎       | 33882/147540 [00:18<01:02, 1827.57it/s]tokenizing...:  23%|██▎       | 34068/147540 [00:18<01:02, 1823.61it/s]tokenizing...:  23%|██▎       | 34253/147540 [00:18<01:02, 1813.99it/s]tokenizing...:  23%|██▎       | 34436/147540 [00:18<01:04, 1757.06it/s]tokenizing...:  23%|██▎       | 34614/147540 [00:18<01:04, 1759.97it/s]tokenizing...:  24%|██▎       | 34810/147540 [00:19<01:02, 1813.27it/s]tokenizing...:  24%|██▎       | 34993/147540 [00:19<01:02, 1802.76it/s]tokenizing...:  24%|██▍       | 35174/147540 [00:19<01:02, 1794.68it/s]tokenizing...:  24%|██▍       | 35358/147540 [00:19<01:02, 1805.97it/s]tokenizing...:  24%|██▍       | 35539/147540 [00:19<01:04, 1749.02it/s]tokenizing...:  24%|██▍       | 35716/147540 [00:19<01:03, 1752.64it/s]tokenizing...:  24%|██▍       | 35895/147540 [00:19<01:03, 1763.29it/s]tokenizing...:  24%|██▍       | 36073/147540 [00:19<01:03, 1765.57it/s]tokenizing...:  25%|██▍       | 36250/147540 [00:19<01:03, 1764.71it/s]tokenizing...:  25%|██▍       | 36457/147540 [00:19<00:59, 1853.80it/s]tokenizing...:  25%|██▍       | 36661/147540 [00:20<00:58, 1907.67it/s]tokenizing...:  25%|██▍       | 36872/147540 [00:20<00:56, 1967.94it/s]tokenizing...:  25%|██▌       | 37069/147540 [00:20<01:01, 1799.62it/s]tokenizing...:  25%|██▌       | 37252/147540 [00:20<01:06, 1663.04it/s]tokenizing...:  25%|██▌       | 37427/147540 [00:20<01:05, 1683.04it/s]tokenizing...:  25%|██▌       | 37599/147540 [00:20<01:06, 1664.45it/s]tokenizing...:  26%|██▌       | 37777/147540 [00:20<01:04, 1694.36it/s]tokenizing...:  26%|██▌       | 37954/147540 [00:20<01:03, 1713.14it/s]tokenizing...:  26%|██▌       | 38128/147540 [00:20<01:03, 1718.53it/s]tokenizing...:  26%|██▌       | 38305/147540 [00:21<01:03, 1730.71it/s]tokenizing...:  26%|██▌       | 38482/147540 [00:21<01:02, 1738.50it/s]tokenizing...:  26%|██▌       | 38657/147540 [00:21<01:03, 1701.31it/s]tokenizing...:  26%|██▋       | 38835/147540 [00:21<01:03, 1722.50it/s]tokenizing...:  26%|██▋       | 39012/147540 [00:21<01:02, 1734.36it/s]tokenizing...:  27%|██▋       | 39188/147540 [00:21<01:02, 1741.88it/s]tokenizing...:  27%|██▋       | 39365/147540 [00:21<01:01, 1749.57it/s]tokenizing...:  27%|██▋       | 39541/147540 [00:21<01:01, 1751.68it/s]tokenizing...:  27%|██▋       | 39717/147540 [00:21<01:02, 1714.93it/s]tokenizing...:  27%|██▋       | 39894/147540 [00:21<01:02, 1728.98it/s]tokenizing...:  27%|██▋       | 40070/147540 [00:22<01:01, 1737.28it/s]tokenizing...:  27%|██▋       | 40244/147540 [00:22<01:01, 1730.90it/s]tokenizing...:  27%|██▋       | 40427/147540 [00:22<01:00, 1759.02it/s]tokenizing...:  28%|██▊       | 40606/147540 [00:22<01:00, 1763.76it/s]tokenizing...:  28%|██▊       | 40783/147540 [00:22<01:00, 1753.94it/s]tokenizing...:  28%|██▊       | 40997/147540 [00:22<00:57, 1868.19it/s]tokenizing...:  28%|██▊       | 41245/147540 [00:22<00:51, 2050.33it/s]tokenizing...:  28%|██▊       | 41454/147540 [00:22<00:51, 2060.66it/s]tokenizing...:  28%|██▊       | 41661/147540 [00:22<00:53, 1997.44it/s]tokenizing...:  28%|██▊       | 41864/147540 [00:22<00:52, 2004.93it/s]tokenizing...:  29%|██▊       | 42091/147540 [00:23<00:50, 2080.45it/s]tokenizing...:  29%|██▊       | 42300/147540 [00:23<00:51, 2061.95it/s]tokenizing...:  29%|██▉       | 42515/147540 [00:23<00:50, 2086.40it/s]tokenizing...:  29%|██▉       | 42724/147540 [00:23<00:51, 2040.75it/s]tokenizing...:  29%|██▉       | 42929/147540 [00:23<00:52, 1991.71it/s]tokenizing...:  29%|██▉       | 43135/147540 [00:23<00:51, 2010.40it/s]tokenizing...:  29%|██▉       | 43357/147540 [00:23<00:50, 2071.52it/s]tokenizing...:  30%|██▉       | 43595/147540 [00:23<00:48, 2162.00it/s]tokenizing...:  30%|██▉       | 43812/147540 [00:23<00:55, 1882.70it/s]tokenizing...:  30%|██▉       | 44007/147540 [00:24<03:13, 534.24it/s] tokenizing...:  30%|███       | 44262/147540 [00:25<02:21, 730.66it/s]tokenizing...:  30%|███       | 44493/147540 [00:25<01:51, 923.49it/s]tokenizing...:  30%|███       | 44724/147540 [00:25<01:31, 1128.18it/s]tokenizing...:  30%|███       | 44928/147540 [00:25<01:24, 1213.61it/s]tokenizing...:  31%|███       | 45168/147540 [00:25<01:11, 1439.25it/s]tokenizing...:  31%|███       | 45411/147540 [00:25<01:01, 1650.49it/s]tokenizing...:  31%|███       | 45657/147540 [00:25<00:55, 1839.08it/s]tokenizing...:  31%|███       | 45882/147540 [00:25<00:53, 1899.11it/s]tokenizing...:  31%|███       | 46103/147540 [00:25<00:51, 1978.95it/s]tokenizing...:  31%|███▏      | 46323/147540 [00:26<00:50, 2007.44it/s]tokenizing...:  32%|███▏      | 46539/147540 [00:26<00:51, 1978.00it/s]tokenizing...:  32%|███▏      | 46799/147540 [00:26<00:47, 2141.44it/s]tokenizing...:  32%|███▏      | 47045/147540 [00:26<00:45, 2230.18it/s]tokenizing...:  32%|███▏      | 47319/147540 [00:26<00:42, 2374.18it/s]tokenizing...:  32%|███▏      | 47562/147540 [00:26<00:42, 2334.54it/s]tokenizing...:  32%|███▏      | 47800/147540 [00:26<00:49, 2035.30it/s]tokenizing...:  33%|███▎      | 48013/147540 [00:26<00:51, 1950.81it/s]tokenizing...:  33%|███▎      | 48215/147540 [00:26<00:50, 1948.61it/s]tokenizing...:  33%|███▎      | 48436/147540 [00:27<00:49, 2018.82it/s]tokenizing...:  33%|███▎      | 48654/147540 [00:27<00:47, 2061.52it/s]tokenizing...:  33%|███▎      | 48864/147540 [00:27<00:47, 2060.13it/s]tokenizing...:  33%|███▎      | 49082/147540 [00:27<00:47, 2057.62it/s]tokenizing...:  33%|███▎      | 49312/147540 [00:27<00:46, 2126.24it/s]tokenizing...:  34%|███▎      | 49526/147540 [00:27<00:53, 1840.08it/s]tokenizing...:  34%|███▎      | 49718/147540 [00:27<00:53, 1822.54it/s]tokenizing...:  34%|███▍      | 49939/147540 [00:27<00:50, 1925.71it/s]tokenizing...:  34%|███▍      | 50137/147540 [00:27<00:50, 1928.12it/s]tokenizing...:  34%|███▍      | 50334/147540 [00:28<00:56, 1733.19it/s]tokenizing...:  34%|███▍      | 50513/147540 [00:28<00:56, 1714.98it/s]tokenizing...:  34%|███▍      | 50699/147540 [00:28<00:55, 1753.63it/s]tokenizing...:  34%|███▍      | 50897/147540 [00:28<00:53, 1816.28it/s]tokenizing...:  35%|███▍      | 51101/147540 [00:28<00:51, 1878.05it/s]tokenizing...:  35%|███▍      | 51291/147540 [00:28<00:51, 1866.87it/s]tokenizing...:  35%|███▍      | 51496/147540 [00:28<00:50, 1919.51it/s]tokenizing...:  35%|███▌      | 51690/147540 [00:28<00:51, 1871.41it/s]tokenizing...:  35%|███▌      | 51879/147540 [00:28<00:51, 1868.53it/s]tokenizing...:  35%|███▌      | 52082/147540 [00:28<00:49, 1914.20it/s]tokenizing...:  35%|███▌      | 52275/147540 [00:29<00:52, 1819.36it/s]tokenizing...:  36%|███▌      | 52459/147540 [00:29<00:52, 1802.46it/s]tokenizing...:  36%|███▌      | 52677/147540 [00:29<00:49, 1909.59it/s]tokenizing...:  36%|███▌      | 52870/147540 [00:29<00:50, 1864.65it/s]tokenizing...:  36%|███▌      | 53058/147540 [00:29<00:51, 1836.09it/s]tokenizing...:  36%|███▌      | 53243/147540 [00:29<00:51, 1822.75it/s]tokenizing...:  36%|███▌      | 53426/147540 [00:29<00:53, 1765.16it/s]tokenizing...:  36%|███▋      | 53625/147540 [00:29<00:51, 1828.71it/s]tokenizing...:  36%|███▋      | 53809/147540 [00:29<00:51, 1806.50it/s]tokenizing...:  37%|███▋      | 53991/147540 [00:30<00:52, 1794.64it/s]tokenizing...:  37%|███▋      | 54171/147540 [00:30<00:57, 1617.22it/s]tokenizing...:  37%|███▋      | 54336/147540 [00:30<01:00, 1541.49it/s]tokenizing...:  37%|███▋      | 54512/147540 [00:30<00:58, 1600.04it/s]tokenizing...:  37%|███▋      | 54688/147540 [00:30<00:56, 1641.42it/s]tokenizing...:  37%|███▋      | 54862/147540 [00:30<00:55, 1667.74it/s]tokenizing...:  37%|███▋      | 55039/147540 [00:30<00:54, 1694.65it/s]tokenizing...:  37%|███▋      | 55251/147540 [00:30<00:50, 1816.75it/s]tokenizing...:  38%|███▊      | 55443/147540 [00:30<00:49, 1843.69it/s]tokenizing...:  38%|███▊      | 55654/147540 [00:30<00:47, 1921.08it/s]tokenizing...:  38%|███▊      | 55847/147540 [00:31<00:50, 1821.25it/s]tokenizing...:  38%|███▊      | 56031/147540 [00:31<00:50, 1800.84it/s]tokenizing...:  38%|███▊      | 56213/147540 [00:31<00:55, 1647.86it/s]tokenizing...:  38%|███▊      | 56381/147540 [00:31<00:56, 1611.60it/s]tokenizing...:  38%|███▊      | 56545/147540 [00:31<00:56, 1603.13it/s]tokenizing...:  38%|███▊      | 56719/147540 [00:31<00:55, 1641.35it/s]tokenizing...:  39%|███▊      | 56894/147540 [00:31<00:54, 1670.68it/s]tokenizing...:  39%|███▊      | 57063/147540 [00:31<00:54, 1671.84it/s]tokenizing...:  39%|███▉      | 57237/147540 [00:31<00:53, 1691.22it/s]tokenizing...:  39%|███▉      | 57409/147540 [00:32<00:53, 1698.85it/s]tokenizing...:  39%|███▉      | 57580/147540 [00:32<00:53, 1666.03it/s]tokenizing...:  39%|███▉      | 57753/147540 [00:32<00:53, 1684.73it/s]tokenizing...:  39%|███▉      | 57927/147540 [00:32<00:52, 1699.48it/s]tokenizing...:  39%|███▉      | 58104/147540 [00:32<00:52, 1718.33it/s]tokenizing...:  40%|███▉      | 58281/147540 [00:32<00:51, 1730.80it/s]tokenizing...:  40%|███▉      | 58458/147540 [00:32<00:51, 1739.03it/s]tokenizing...:  40%|███▉      | 58633/147540 [00:32<00:52, 1700.55it/s]tokenizing...:  40%|███▉      | 58809/147540 [00:32<00:51, 1714.29it/s]tokenizing...:  40%|███▉      | 58983/147540 [00:32<00:51, 1720.08it/s]tokenizing...:  40%|████      | 59157/147540 [00:33<00:51, 1723.88it/s]tokenizing...:  40%|████      | 59331/147540 [00:33<00:51, 1727.49it/s]tokenizing...:  40%|████      | 59521/147540 [00:33<00:50, 1736.17it/s]tokenizing...:  40%|████      | 59737/147540 [00:33<00:47, 1859.50it/s]tokenizing...:  41%|████      | 59924/147540 [00:33<00:49, 1757.48it/s]tokenizing...:  41%|████      | 60103/147540 [00:33<00:49, 1765.06it/s]tokenizing...:  41%|████      | 60306/147540 [00:33<00:47, 1841.40it/s]tokenizing...:  41%|████      | 60528/147540 [00:33<00:44, 1949.74it/s]tokenizing...:  41%|████      | 60724/147540 [00:33<00:51, 1699.53it/s]tokenizing...:  41%|████▏     | 60901/147540 [00:34<00:55, 1547.85it/s]tokenizing...:  41%|████▏     | 61080/147540 [00:34<00:53, 1608.52it/s]tokenizing...:  42%|████▏     | 61247/147540 [00:34<00:53, 1601.91it/s]tokenizing...:  42%|████▏     | 61411/147540 [00:34<00:57, 1488.46it/s]tokenizing...:  42%|████▏     | 61615/147540 [00:34<00:52, 1632.80it/s]tokenizing...:  42%|████▏     | 61802/147540 [00:34<00:50, 1693.84it/s]tokenizing...:  42%|████▏     | 62032/147540 [00:34<00:45, 1861.48it/s]tokenizing...:  42%|████▏     | 62266/147540 [00:34<00:42, 1997.13it/s]tokenizing...:  42%|████▏     | 62511/147540 [00:34<00:39, 2127.44it/s]tokenizing...:  43%|████▎     | 62727/147540 [00:35<00:41, 2036.32it/s]tokenizing...:  43%|████▎     | 62934/147540 [00:35<00:42, 2009.12it/s]tokenizing...:  43%|████▎     | 63165/147540 [00:35<00:40, 2093.33it/s]tokenizing...:  43%|████▎     | 63376/147540 [00:35<00:40, 2086.66it/s]tokenizing...:  43%|████▎     | 63586/147540 [00:35<00:41, 2046.49it/s]tokenizing...:  43%|████▎     | 63792/147540 [00:35<00:42, 1958.10it/s]tokenizing...:  43%|████▎     | 64012/147540 [00:35<00:41, 2025.04it/s]tokenizing...:  44%|████▎     | 64247/147540 [00:35<00:39, 2117.77it/s]tokenizing...:  44%|████▍     | 64556/147540 [00:35<00:34, 2399.99it/s]tokenizing...:  44%|████▍     | 64798/147540 [00:36<00:35, 2300.90it/s]tokenizing...:  44%|████▍     | 65051/147540 [00:36<00:34, 2365.74it/s]tokenizing...:  44%|████▍     | 65302/147540 [00:36<00:34, 2407.17it/s]tokenizing...:  44%|████▍     | 65611/147540 [00:36<00:31, 2603.95it/s]tokenizing...:  45%|████▍     | 65873/147540 [00:36<00:31, 2594.15it/s]tokenizing...:  45%|████▍     | 66134/147540 [00:36<00:32, 2540.02it/s]tokenizing...:  45%|████▍     | 66389/147540 [00:36<00:32, 2477.08it/s]tokenizing...:  45%|████▌     | 66638/147540 [00:36<00:33, 2390.34it/s]tokenizing...:  45%|████▌     | 66878/147540 [00:36<00:35, 2288.99it/s]tokenizing...:  45%|████▌     | 67109/147540 [00:36<00:35, 2285.28it/s]tokenizing...:  46%|████▌     | 67339/147540 [00:37<00:35, 2272.40it/s]tokenizing...:  46%|████▌     | 67568/147540 [00:37<00:35, 2272.23it/s]tokenizing...:  46%|████▌     | 67796/147540 [00:37<00:35, 2227.62it/s]tokenizing...:  46%|████▌     | 68020/147540 [00:37<00:39, 2004.91it/s]tokenizing...:  46%|████▌     | 68225/147540 [00:37<00:40, 1962.75it/s]tokenizing...:  46%|████▋     | 68424/147540 [00:37<00:41, 1917.03it/s]tokenizing...:  47%|████▋     | 68618/147540 [00:37<00:41, 1893.41it/s]tokenizing...:  47%|████▋     | 68809/147540 [00:37<00:42, 1860.39it/s]tokenizing...:  47%|████▋     | 68996/147540 [00:37<00:43, 1802.44it/s]tokenizing...:  47%|████▋     | 69211/147540 [00:38<00:41, 1899.24it/s]tokenizing...:  47%|████▋     | 69403/147540 [00:38<00:49, 1576.56it/s]tokenizing...:  47%|████▋     | 69571/147540 [00:38<00:51, 1525.18it/s]tokenizing...:  47%|████▋     | 69748/147540 [00:38<00:49, 1587.38it/s]tokenizing...:  47%|████▋     | 69929/147540 [00:38<00:47, 1643.61it/s]tokenizing...:  48%|████▊     | 70098/147540 [00:38<00:47, 1636.20it/s]tokenizing...:  48%|████▊     | 70279/147540 [00:38<00:45, 1684.21it/s]tokenizing...:  48%|████▊     | 70492/147540 [00:38<00:42, 1811.13it/s]tokenizing...:  48%|████▊     | 70676/147540 [00:38<00:42, 1794.12it/s]tokenizing...:  48%|████▊     | 70858/147540 [00:39<00:42, 1787.06it/s]tokenizing...:  48%|████▊     | 71038/147540 [00:39<00:44, 1734.03it/s]tokenizing...:  48%|████▊     | 71213/147540 [00:39<00:43, 1737.33it/s]tokenizing...:  48%|████▊     | 71390/147540 [00:39<00:43, 1746.44it/s]tokenizing...:  49%|████▊     | 71567/147540 [00:39<00:43, 1752.23it/s]tokenizing...:  49%|████▊     | 71744/147540 [00:39<00:43, 1756.87it/s]tokenizing...:  49%|████▊     | 71925/147540 [00:39<00:42, 1768.47it/s]tokenizing...:  49%|████▉     | 72103/147540 [00:39<00:43, 1723.34it/s]tokenizing...:  49%|████▉     | 72292/147540 [00:39<00:42, 1771.88it/s]tokenizing...:  49%|████▉     | 72470/147540 [00:39<00:42, 1770.74it/s]tokenizing...:  49%|████▉     | 72648/147540 [00:40<00:42, 1768.82it/s]tokenizing...:  49%|████▉     | 72826/147540 [00:40<00:42, 1766.24it/s]tokenizing...:  49%|████▉     | 73003/147540 [00:40<00:42, 1759.22it/s]tokenizing...:  50%|████▉     | 73180/147540 [00:41<03:27, 358.24it/s] tokenizing...:  50%|████▉     | 73354/147540 [00:41<02:38, 467.72it/s]tokenizing...:  50%|████▉     | 73529/147540 [00:41<02:03, 597.85it/s]tokenizing...:  50%|████▉     | 73706/147540 [00:42<01:38, 745.86it/s]tokenizing...:  50%|█████     | 73882/147540 [00:42<01:21, 901.07it/s]tokenizing...:  50%|█████     | 74059/147540 [00:42<01:09, 1056.80it/s]tokenizing...:  50%|█████     | 74227/147540 [00:42<01:02, 1173.29it/s]tokenizing...:  50%|█████     | 74408/147540 [00:42<00:55, 1315.83it/s]tokenizing...:  51%|█████     | 74585/147540 [00:42<00:51, 1425.14it/s]tokenizing...:  51%|█████     | 74765/147540 [00:42<00:47, 1520.10it/s]tokenizing...:  51%|█████     | 74942/147540 [00:42<00:45, 1584.56it/s]tokenizing...:  51%|█████     | 75119/147540 [00:42<00:44, 1632.57it/s]tokenizing...:  51%|█████     | 75295/147540 [00:42<00:44, 1641.21it/s]tokenizing...:  51%|█████     | 75498/147540 [00:43<00:41, 1749.02it/s]tokenizing...:  51%|█████▏    | 75680/147540 [00:43<00:41, 1731.40it/s]tokenizing...:  51%|█████▏    | 75858/147540 [00:43<00:42, 1705.62it/s]tokenizing...:  52%|█████▏    | 76084/147540 [00:43<00:38, 1863.63it/s]tokenizing...:  52%|█████▏    | 76289/147540 [00:43<00:37, 1916.75it/s]tokenizing...:  52%|█████▏    | 76483/147540 [00:43<00:40, 1761.27it/s]tokenizing...:  52%|█████▏    | 76693/147540 [00:43<00:38, 1854.86it/s]tokenizing...:  52%|█████▏    | 76919/147540 [00:43<00:35, 1964.63it/s]tokenizing...:  52%|█████▏    | 77145/147540 [00:43<00:34, 2049.21it/s]tokenizing...:  52%|█████▏    | 77363/147540 [00:43<00:33, 2084.73it/s]tokenizing...:  53%|█████▎    | 77574/147540 [00:44<00:39, 1772.74it/s]tokenizing...:  53%|█████▎    | 77761/147540 [00:44<00:40, 1718.78it/s]tokenizing...:  53%|█████▎    | 77976/147540 [00:44<00:37, 1831.99it/s]tokenizing...:  53%|█████▎    | 78166/147540 [00:44<00:39, 1767.24it/s]tokenizing...:  53%|█████▎    | 78348/147540 [00:44<00:39, 1731.32it/s]tokenizing...:  53%|█████▎    | 78547/147540 [00:44<00:38, 1801.42it/s]tokenizing...:  53%|█████▎    | 78740/147540 [00:44<00:37, 1837.13it/s]tokenizing...:  54%|█████▎    | 78956/147540 [00:44<00:35, 1928.65it/s]tokenizing...:  54%|█████▎    | 79167/147540 [00:44<00:34, 1981.09it/s]tokenizing...:  54%|█████▍    | 79367/147540 [00:45<00:35, 1929.63it/s]tokenizing...:  54%|█████▍    | 79562/147540 [00:45<00:35, 1911.06it/s]tokenizing...:  54%|█████▍    | 79754/147540 [00:45<00:37, 1830.22it/s]tokenizing...:  54%|█████▍    | 79981/147540 [00:45<00:34, 1954.74it/s]tokenizing...:  54%|█████▍    | 80178/147540 [00:45<00:35, 1911.03it/s]tokenizing...:  54%|█████▍    | 80371/147540 [00:45<00:36, 1825.20it/s]tokenizing...:  55%|█████▍    | 80555/147540 [00:45<00:42, 1582.69it/s]tokenizing...:  55%|█████▍    | 80761/147540 [00:45<00:39, 1703.76it/s]tokenizing...:  55%|█████▍    | 81012/147540 [00:45<00:34, 1917.49it/s]tokenizing...:  55%|█████▌    | 81251/147540 [00:46<00:32, 2048.06it/s]tokenizing...:  55%|█████▌    | 81462/147540 [00:46<00:37, 1754.96it/s]tokenizing...:  55%|█████▌    | 81649/147540 [00:46<00:40, 1624.25it/s]tokenizing...:  55%|█████▌    | 81875/147540 [00:46<00:36, 1782.59it/s]tokenizing...:  56%|█████▌    | 82071/147540 [00:46<00:35, 1826.63it/s]tokenizing...:  56%|█████▌    | 82301/147540 [00:46<00:33, 1955.14it/s]tokenizing...:  56%|█████▌    | 82503/147540 [00:46<00:36, 1782.54it/s]tokenizing...:  56%|█████▌    | 82707/147540 [00:46<00:35, 1849.14it/s]tokenizing...:  56%|█████▌    | 82991/147540 [00:47<00:30, 2120.11it/s]tokenizing...:  56%|█████▋    | 83212/147540 [00:47<00:29, 2145.27it/s]tokenizing...:  57%|█████▋    | 83476/147540 [00:47<00:28, 2286.98it/s]tokenizing...:  57%|█████▋    | 83718/147540 [00:47<00:27, 2325.53it/s]tokenizing...:  57%|█████▋    | 84047/147540 [00:47<00:24, 2604.70it/s]tokenizing...:  57%|█████▋    | 84310/147540 [00:47<00:25, 2503.13it/s]tokenizing...:  57%|█████▋    | 84563/147540 [00:47<00:25, 2491.75it/s]tokenizing...:  57%|█████▋    | 84814/147540 [00:47<00:26, 2366.12it/s]tokenizing...:  58%|█████▊    | 85053/147540 [00:47<00:27, 2312.26it/s]tokenizing...:  58%|█████▊    | 85286/147540 [00:47<00:27, 2262.00it/s]tokenizing...:  58%|█████▊    | 85514/147540 [00:48<00:27, 2239.88it/s]tokenizing...:  58%|█████▊    | 85739/147540 [00:48<00:28, 2197.77it/s]tokenizing...:  58%|█████▊    | 85976/147540 [00:48<00:27, 2245.79it/s]tokenizing...:  58%|█████▊    | 86204/147540 [00:48<00:27, 2255.23it/s]tokenizing...:  59%|█████▊    | 86430/147540 [00:48<00:27, 2212.80it/s]tokenizing...:  59%|█████▊    | 86652/147540 [00:48<00:27, 2187.30it/s]tokenizing...:  59%|█████▉    | 86871/147540 [00:48<00:28, 2140.29it/s]tokenizing...:  59%|█████▉    | 87096/147540 [00:48<00:27, 2171.26it/s]tokenizing...:  59%|█████▉    | 87322/147540 [00:48<00:27, 2196.13it/s]tokenizing...:  59%|█████▉    | 87542/147540 [00:49<00:28, 2085.20it/s]tokenizing...:  59%|█████▉    | 87754/147540 [00:49<00:28, 2094.35it/s]tokenizing...:  60%|█████▉    | 87965/147540 [00:49<00:30, 1983.95it/s]tokenizing...:  60%|█████▉    | 88171/147540 [00:49<00:29, 2005.07it/s]tokenizing...:  60%|█████▉    | 88377/147540 [00:49<00:29, 2019.18it/s]tokenizing...:  60%|██████    | 88580/147540 [00:49<00:30, 1940.26it/s]tokenizing...:  60%|██████    | 88776/147540 [00:49<00:31, 1886.61it/s]tokenizing...:  60%|██████    | 88974/147540 [00:49<00:30, 1911.85it/s]tokenizing...:  60%|██████    | 89172/147540 [00:49<00:30, 1928.84it/s]tokenizing...:  61%|██████    | 89382/147540 [00:49<00:29, 1976.26it/s]tokenizing...:  61%|██████    | 89602/147540 [00:50<00:28, 2041.41it/s]tokenizing...:  61%|██████    | 89825/147540 [00:50<00:27, 2094.97it/s]tokenizing...:  61%|██████    | 90035/147540 [00:50<00:27, 2072.09it/s]tokenizing...:  61%|██████    | 90243/147540 [00:50<00:27, 2070.30it/s]tokenizing...:  61%|██████▏   | 90451/147540 [00:50<00:29, 1945.89it/s]tokenizing...:  61%|██████▏   | 90660/147540 [00:50<00:28, 1986.69it/s]tokenizing...:  62%|██████▏   | 90861/147540 [00:50<00:35, 1594.91it/s]tokenizing...:  62%|██████▏   | 91034/147540 [00:50<00:35, 1590.51it/s]tokenizing...:  62%|██████▏   | 91258/147540 [00:51<00:32, 1757.27it/s]tokenizing...:  62%|██████▏   | 91490/147540 [00:51<00:29, 1907.36it/s]tokenizing...:  62%|██████▏   | 91689/147540 [00:51<00:28, 1928.55it/s]tokenizing...:  62%|██████▏   | 91904/147540 [00:51<00:27, 1989.91it/s]tokenizing...:  62%|██████▏   | 92118/147540 [00:51<00:27, 2031.65it/s]tokenizing...:  63%|██████▎   | 92357/147540 [00:51<00:25, 2134.92it/s]tokenizing...:  63%|██████▎   | 92574/147540 [00:51<00:26, 2068.43it/s]tokenizing...:  63%|██████▎   | 92792/147540 [00:51<00:26, 2098.51it/s]tokenizing...:  63%|██████▎   | 93014/147540 [00:51<00:25, 2130.70it/s]tokenizing...:  63%|██████▎   | 93252/147540 [00:51<00:24, 2199.84it/s]tokenizing...:  63%|██████▎   | 93478/147540 [00:52<00:24, 2216.29it/s]tokenizing...:  64%|██████▎   | 93701/147540 [00:52<00:25, 2122.13it/s]tokenizing...:  64%|██████▎   | 93915/147540 [00:52<00:27, 1942.80it/s]tokenizing...:  64%|██████▍   | 94113/147540 [00:52<00:27, 1931.71it/s]tokenizing...:  64%|██████▍   | 94330/147540 [00:52<00:26, 1997.80it/s]tokenizing...:  64%|██████▍   | 94557/147540 [00:52<00:25, 2073.18it/s]tokenizing...:  64%|██████▍   | 94791/147540 [00:52<00:24, 2148.10it/s]tokenizing...:  64%|██████▍   | 95008/147540 [00:52<00:24, 2140.57it/s]tokenizing...:  65%|██████▍   | 95232/147540 [00:52<00:24, 2169.52it/s]tokenizing...:  65%|██████▍   | 95469/147540 [00:52<00:23, 2225.02it/s]tokenizing...:  65%|██████▍   | 95693/147540 [00:53<00:23, 2195.27it/s]tokenizing...:  65%|██████▌   | 95914/147540 [00:53<00:23, 2180.77it/s]tokenizing...:  65%|██████▌   | 96137/147540 [00:53<00:23, 2192.46it/s]tokenizing...:  65%|██████▌   | 96357/147540 [00:53<00:23, 2139.13it/s]tokenizing...:  65%|██████▌   | 96572/147540 [00:53<00:24, 2085.44it/s]tokenizing...:  66%|██████▌   | 96782/147540 [00:53<00:24, 2048.06it/s]tokenizing...:  66%|██████▌   | 96988/147540 [00:53<00:26, 1941.09it/s]tokenizing...:  66%|██████▌   | 97184/147540 [00:53<00:26, 1871.39it/s]tokenizing...:  66%|██████▌   | 97373/147540 [00:53<00:28, 1774.31it/s]tokenizing...:  66%|██████▌   | 97552/147540 [00:54<00:28, 1758.93it/s]tokenizing...:  66%|██████▌   | 97729/147540 [00:54<00:28, 1748.79it/s]tokenizing...:  66%|██████▋   | 97905/147540 [00:54<00:28, 1735.38it/s]tokenizing...:  66%|██████▋   | 98079/147540 [00:54<00:28, 1733.40it/s]tokenizing...:  67%|██████▋   | 98253/147540 [00:54<00:29, 1675.28it/s]tokenizing...:  67%|██████▋   | 98427/147540 [00:54<00:29, 1691.25it/s]tokenizing...:  67%|██████▋   | 98599/147540 [00:54<00:28, 1698.07it/s]tokenizing...:  67%|██████▋   | 98770/147540 [00:54<00:28, 1700.85it/s]tokenizing...:  67%|██████▋   | 98941/147540 [00:54<00:28, 1690.79it/s]tokenizing...:  67%|██████▋   | 99115/147540 [00:54<00:28, 1705.03it/s]tokenizing...:  67%|██████▋   | 99286/147540 [00:55<00:29, 1655.83it/s]tokenizing...:  67%|██████▋   | 99458/147540 [00:55<00:28, 1672.13it/s]tokenizing...:  68%|██████▊   | 99630/147540 [00:55<00:28, 1684.42it/s]tokenizing...:  68%|██████▊   | 99804/147540 [00:55<00:28, 1699.11it/s]tokenizing...:  68%|██████▊   | 99978/147540 [00:55<00:27, 1708.26it/s]tokenizing...:  68%|██████▊   | 100151/147540 [00:55<00:27, 1712.07it/s]tokenizing...:  68%|██████▊   | 100323/147540 [00:55<00:28, 1658.30it/s]tokenizing...:  68%|██████▊   | 100537/147540 [00:55<00:26, 1798.09it/s]tokenizing...:  68%|██████▊   | 100756/147540 [00:55<00:24, 1910.61it/s]tokenizing...:  68%|██████▊   | 100948/147540 [00:56<00:33, 1409.22it/s]tokenizing...:  69%|██████▊   | 101137/147540 [00:56<00:30, 1522.71it/s]tokenizing...:  69%|██████▊   | 101365/147540 [00:56<00:27, 1690.08it/s]tokenizing...:  69%|██████▉   | 101575/147540 [00:56<00:25, 1790.29it/s]tokenizing...:  69%|██████▉   | 101795/147540 [00:56<00:24, 1900.36it/s]tokenizing...:  69%|██████▉   | 102000/147540 [00:56<00:23, 1938.58it/s]tokenizing...:  69%|██████▉   | 102222/147540 [00:56<00:22, 2010.76it/s]tokenizing...:  69%|██████▉   | 102428/147540 [00:56<00:22, 1999.40it/s]tokenizing...:  70%|██████▉   | 102638/147540 [00:56<00:22, 2028.36it/s]tokenizing...:  70%|██████▉   | 102869/147540 [00:57<00:21, 2109.74it/s]tokenizing...:  70%|██████▉   | 103082/147540 [00:57<00:21, 2112.30it/s]tokenizing...:  70%|███████   | 103295/147540 [00:57<00:21, 2106.79it/s]tokenizing...:  70%|███████   | 103510/147540 [00:57<00:20, 2118.04it/s]tokenizing...:  70%|███████   | 103723/147540 [00:57<00:20, 2106.26it/s]tokenizing...:  70%|███████   | 103944/147540 [00:57<00:20, 2136.56it/s]tokenizing...:  71%|███████   | 104170/147540 [00:57<00:19, 2169.08it/s]tokenizing...:  71%|███████   | 104458/147540 [00:57<00:18, 2380.37it/s]tokenizing...:  71%|███████   | 104697/147540 [00:57<00:19, 2247.10it/s]tokenizing...:  71%|███████   | 104924/147540 [00:57<00:19, 2188.06it/s]tokenizing...:  71%|███████▏  | 105166/147540 [00:58<00:18, 2251.08it/s]tokenizing...:  71%|███████▏  | 105421/147540 [00:58<00:18, 2337.31it/s]tokenizing...:  72%|███████▏  | 105668/147540 [00:58<00:17, 2375.27it/s]tokenizing...:  72%|███████▏  | 105970/147540 [00:58<00:16, 2560.91it/s]tokenizing...:  72%|███████▏  | 106228/147540 [00:58<00:16, 2522.48it/s]tokenizing...:  72%|███████▏  | 106482/147540 [00:58<00:17, 2337.35it/s]tokenizing...:  72%|███████▏  | 106719/147540 [00:58<00:18, 2197.53it/s]tokenizing...:  72%|███████▏  | 106943/147540 [00:58<00:18, 2207.50it/s]tokenizing...:  73%|███████▎  | 107167/147540 [00:58<00:18, 2211.54it/s]tokenizing...:  73%|███████▎  | 107394/147540 [00:59<00:18, 2228.01it/s]tokenizing...:  73%|███████▎  | 107619/147540 [00:59<00:17, 2224.81it/s]tokenizing...:  73%|███████▎  | 107843/147540 [00:59<00:18, 2161.75it/s]tokenizing...:  73%|███████▎  | 108066/147540 [00:59<00:18, 2178.05it/s]tokenizing...:  73%|███████▎  | 108285/147540 [00:59<00:20, 1892.42it/s]tokenizing...:  74%|███████▎  | 108499/147540 [00:59<00:19, 1956.29it/s]tokenizing...:  74%|███████▎  | 108701/147540 [00:59<00:20, 1917.58it/s]tokenizing...:  74%|███████▍  | 108897/147540 [00:59<00:20, 1894.13it/s]tokenizing...:  74%|███████▍  | 109091/147540 [00:59<00:20, 1901.94it/s]tokenizing...:  74%|███████▍  | 109284/147540 [01:00<00:20, 1865.81it/s]tokenizing...:  74%|███████▍  | 109472/147540 [01:00<00:22, 1669.01it/s]tokenizing...:  74%|███████▍  | 109644/147540 [01:00<00:22, 1648.06it/s]tokenizing...:  74%|███████▍  | 109812/147540 [01:02<02:15, 277.97it/s] tokenizing...:  75%|███████▍  | 109986/147540 [01:02<01:42, 367.01it/s]tokenizing...:  75%|███████▍  | 110160/147540 [01:02<01:18, 476.88it/s]tokenizing...:  75%|███████▍  | 110368/147540 [01:02<00:57, 641.31it/s]tokenizing...:  75%|███████▍  | 110570/147540 [01:02<00:45, 817.38it/s]tokenizing...:  75%|███████▌  | 110746/147540 [01:02<00:38, 961.91it/s]tokenizing...:  75%|███████▌  | 110922/147540 [01:02<00:33, 1080.95it/s]tokenizing...:  75%|███████▌  | 111096/147540 [01:02<00:30, 1213.13it/s]tokenizing...:  75%|███████▌  | 111270/147540 [01:03<00:27, 1329.79it/s]tokenizing...:  76%|███████▌  | 111464/147540 [01:03<00:24, 1476.57it/s]tokenizing...:  76%|███████▌  | 111648/147540 [01:03<00:22, 1569.20it/s]tokenizing...:  76%|███████▌  | 111828/147540 [01:03<00:22, 1618.41it/s]tokenizing...:  76%|███████▌  | 112007/147540 [01:03<00:22, 1602.82it/s]tokenizing...:  76%|███████▌  | 112181/147540 [01:03<00:21, 1639.37it/s]tokenizing...:  76%|███████▌  | 112355/147540 [01:03<00:21, 1665.22it/s]tokenizing...:  76%|███████▋  | 112529/147540 [01:03<00:20, 1683.48it/s]tokenizing...:  76%|███████▋  | 112708/147540 [01:03<00:20, 1714.36it/s]tokenizing...:  77%|███████▋  | 112883/147540 [01:03<00:20, 1711.68it/s]tokenizing...:  77%|███████▋  | 113057/147540 [01:04<00:20, 1693.25it/s]tokenizing...:  77%|███████▋  | 113286/147540 [01:04<00:18, 1865.82it/s]tokenizing...:  77%|███████▋  | 113489/147540 [01:04<00:17, 1910.85it/s]tokenizing...:  77%|███████▋  | 113692/147540 [01:04<00:17, 1944.84it/s]tokenizing...:  77%|███████▋  | 113888/147540 [01:04<00:17, 1879.75it/s]tokenizing...:  77%|███████▋  | 114078/147540 [01:04<00:18, 1841.39it/s]tokenizing...:  77%|███████▋  | 114263/147540 [01:04<00:18, 1812.12it/s]tokenizing...:  78%|███████▊  | 114445/147540 [01:04<00:18, 1791.06it/s]tokenizing...:  78%|███████▊  | 114625/147540 [01:04<00:18, 1774.91it/s]tokenizing...:  78%|███████▊  | 114803/147540 [01:05<00:18, 1772.05it/s]tokenizing...:  78%|███████▊  | 114981/147540 [01:05<00:18, 1757.44it/s]tokenizing...:  78%|███████▊  | 115157/147540 [01:05<00:18, 1729.47it/s]tokenizing...:  78%|███████▊  | 115331/147540 [01:05<00:18, 1726.86it/s]tokenizing...:  78%|███████▊  | 115507/147540 [01:05<00:18, 1733.99it/s]tokenizing...:  78%|███████▊  | 115681/147540 [01:05<00:18, 1731.93it/s]tokenizing...:  79%|███████▊  | 115856/147540 [01:05<00:18, 1737.18it/s]tokenizing...:  79%|███████▊  | 116032/147540 [01:05<00:18, 1739.33it/s]tokenizing...:  79%|███████▉  | 116206/147540 [01:05<00:18, 1688.56it/s]tokenizing...:  79%|███████▉  | 116377/147540 [01:05<00:18, 1693.34it/s]tokenizing...:  79%|███████▉  | 116553/147540 [01:06<00:18, 1711.65it/s]tokenizing...:  79%|███████▉  | 116727/147540 [01:06<00:17, 1717.64it/s]tokenizing...:  79%|███████▉  | 116901/147540 [01:06<00:17, 1721.51it/s]tokenizing...:  79%|███████▉  | 117074/147540 [01:06<00:17, 1722.82it/s]tokenizing...:  79%|███████▉  | 117247/147540 [01:06<00:18, 1670.13it/s]tokenizing...:  80%|███████▉  | 117420/147540 [01:06<00:17, 1686.41it/s]tokenizing...:  80%|███████▉  | 117600/147540 [01:06<00:17, 1718.41it/s]tokenizing...:  80%|███████▉  | 117777/147540 [01:06<00:17, 1714.05it/s]tokenizing...:  80%|███████▉  | 117996/147540 [01:06<00:15, 1851.30it/s]tokenizing...:  80%|████████  | 118182/147540 [01:07<00:18, 1583.59it/s]tokenizing...:  80%|████████  | 118348/147540 [01:07<00:20, 1434.08it/s]tokenizing...:  80%|████████  | 118499/147540 [01:07<00:20, 1448.15it/s]tokenizing...:  80%|████████  | 118659/147540 [01:07<00:19, 1487.95it/s]tokenizing...:  81%|████████  | 118853/147540 [01:07<00:17, 1611.91it/s]tokenizing...:  81%|████████  | 119069/147540 [01:07<00:16, 1759.58it/s]tokenizing...:  81%|████████  | 119249/147540 [01:07<00:18, 1546.64it/s]tokenizing...:  81%|████████  | 119411/147540 [01:07<00:20, 1363.72it/s]tokenizing...:  81%|████████  | 119589/147540 [01:07<00:19, 1465.76it/s]tokenizing...:  81%|████████  | 119812/147540 [01:08<00:16, 1664.28it/s]tokenizing...:  81%|████████▏ | 119987/147540 [01:08<00:16, 1669.79it/s]tokenizing...:  81%|████████▏ | 120204/147540 [01:08<00:15, 1802.76it/s]tokenizing...:  82%|████████▏ | 120390/147540 [01:08<00:15, 1801.01it/s]tokenizing...:  82%|████████▏ | 120610/147540 [01:08<00:14, 1914.73it/s]tokenizing...:  82%|████████▏ | 120840/147540 [01:08<00:13, 2024.84it/s]tokenizing...:  82%|████████▏ | 121081/147540 [01:08<00:12, 2136.83it/s]tokenizing...:  82%|████████▏ | 121297/147540 [01:08<00:12, 2103.11it/s]tokenizing...:  82%|████████▏ | 121534/147540 [01:08<00:11, 2179.27it/s]tokenizing...:  83%|████████▎ | 121778/147540 [01:08<00:11, 2255.48it/s]tokenizing...:  83%|████████▎ | 122005/147540 [01:09<00:11, 2227.67it/s]tokenizing...:  83%|████████▎ | 122229/147540 [01:09<00:11, 2227.98it/s]tokenizing...:  83%|████████▎ | 122453/147540 [01:09<00:11, 2167.15it/s]tokenizing...:  83%|████████▎ | 122671/147540 [01:09<00:11, 2148.93it/s]tokenizing...:  83%|████████▎ | 122960/147540 [01:09<00:10, 2363.40it/s]tokenizing...:  84%|████████▎ | 123213/147540 [01:09<00:10, 2411.90it/s]tokenizing...:  84%|████████▎ | 123455/147540 [01:09<00:10, 2325.72it/s]tokenizing...:  84%|████████▍ | 123712/147540 [01:09<00:09, 2394.80it/s]tokenizing...:  84%|████████▍ | 123967/147540 [01:09<00:09, 2437.38it/s]tokenizing...:  84%|████████▍ | 124245/147540 [01:10<00:09, 2536.57it/s]tokenizing...:  84%|████████▍ | 124528/147540 [01:10<00:08, 2623.30it/s]tokenizing...:  85%|████████▍ | 124791/147540 [01:10<00:09, 2508.15it/s]tokenizing...:  85%|████████▍ | 125044/147540 [01:10<00:10, 2227.50it/s]tokenizing...:  85%|████████▍ | 125274/147540 [01:10<00:10, 2174.79it/s]tokenizing...:  85%|████████▌ | 125496/147540 [01:10<00:10, 2125.58it/s]tokenizing...:  85%|████████▌ | 125722/147540 [01:10<00:10, 2159.31it/s]tokenizing...:  85%|████████▌ | 125951/147540 [01:10<00:09, 2193.65it/s]tokenizing...:  86%|████████▌ | 126173/147540 [01:10<00:09, 2199.60it/s]tokenizing...:  86%|████████▌ | 126415/147540 [01:10<00:09, 2261.16it/s]tokenizing...:  86%|████████▌ | 126643/147540 [01:11<00:09, 2147.52it/s]tokenizing...:  86%|████████▌ | 126860/147540 [01:11<00:09, 2148.91it/s]tokenizing...:  86%|████████▌ | 127077/147540 [01:11<00:09, 2139.22it/s]tokenizing...:  86%|████████▋ | 127292/147540 [01:11<00:09, 2106.31it/s]tokenizing...:  86%|████████▋ | 127504/147540 [01:11<00:09, 2016.22it/s]tokenizing...:  87%|████████▋ | 127707/147540 [01:11<00:10, 1980.15it/s]tokenizing...:  87%|████████▋ | 127906/147540 [01:11<00:10, 1894.35it/s]tokenizing...:  87%|████████▋ | 128113/147540 [01:11<00:09, 1942.78it/s]tokenizing...:  87%|████████▋ | 128309/147540 [01:11<00:10, 1883.72it/s]tokenizing...:  87%|████████▋ | 128499/147540 [01:12<00:10, 1835.73it/s]tokenizing...:  87%|████████▋ | 128700/147540 [01:12<00:10, 1883.84it/s]tokenizing...:  87%|████████▋ | 128890/147540 [01:12<00:10, 1839.39it/s]tokenizing...:  87%|████████▋ | 129075/147540 [01:12<00:10, 1809.03it/s]tokenizing...:  88%|████████▊ | 129257/147540 [01:12<00:10, 1793.99it/s]tokenizing...:  88%|████████▊ | 129437/147540 [01:12<00:10, 1775.14it/s]tokenizing...:  88%|████████▊ | 129619/147540 [01:12<00:10, 1750.37it/s]tokenizing...:  88%|████████▊ | 129795/147540 [01:12<00:11, 1564.43it/s]tokenizing...:  88%|████████▊ | 129960/147540 [01:12<00:11, 1584.87it/s]tokenizing...:  88%|████████▊ | 130123/147540 [01:13<00:10, 1593.56it/s]tokenizing...:  88%|████████▊ | 130285/147540 [01:13<00:10, 1594.42it/s]tokenizing...:  88%|████████▊ | 130446/147540 [01:13<00:10, 1595.82it/s]tokenizing...:  89%|████████▊ | 130609/147540 [01:13<00:10, 1602.62it/s]tokenizing...:  89%|████████▊ | 130770/147540 [01:13<00:10, 1562.39it/s]tokenizing...:  89%|████████▊ | 130935/147540 [01:13<00:10, 1585.25it/s]tokenizing...:  89%|████████▉ | 131100/147540 [01:13<00:10, 1602.05it/s]tokenizing...:  89%|████████▉ | 131284/147540 [01:13<00:09, 1671.07it/s]tokenizing...:  89%|████████▉ | 131489/147540 [01:13<00:09, 1782.53it/s]tokenizing...:  89%|████████▉ | 131668/147540 [01:13<00:09, 1670.85it/s]tokenizing...:  89%|████████▉ | 131837/147540 [01:14<00:09, 1618.58it/s]tokenizing...:  90%|████████▉ | 132053/147540 [01:14<00:08, 1770.59it/s]tokenizing...:  90%|████████▉ | 132232/147540 [01:14<00:09, 1688.41it/s]tokenizing...:  90%|████████▉ | 132406/147540 [01:14<00:08, 1702.84it/s]tokenizing...:  90%|████████▉ | 132578/147540 [01:14<00:08, 1702.90it/s]tokenizing...:  90%|████████▉ | 132759/147540 [01:14<00:08, 1733.51it/s]tokenizing...:  90%|█████████ | 132934/147540 [01:14<00:08, 1680.03it/s]tokenizing...:  90%|█████████ | 133105/147540 [01:14<00:08, 1687.91it/s]tokenizing...:  90%|█████████ | 133275/147540 [01:14<00:08, 1691.04it/s]tokenizing...:  90%|█████████ | 133448/147540 [01:15<00:08, 1700.34it/s]tokenizing...:  91%|█████████ | 133621/147540 [01:15<00:08, 1708.36it/s]tokenizing...:  91%|█████████ | 133794/147540 [01:15<00:08, 1713.69it/s]tokenizing...:  91%|█████████ | 133966/147540 [01:15<00:08, 1660.38it/s]tokenizing...:  91%|█████████ | 134138/147540 [01:15<00:07, 1676.65it/s]tokenizing...:  91%|█████████ | 134311/147540 [01:15<00:07, 1691.24it/s]tokenizing...:  91%|█████████ | 134485/147540 [01:15<00:07, 1704.11it/s]tokenizing...:  91%|█████████▏| 134657/147540 [01:15<00:07, 1708.17it/s]tokenizing...:  91%|█████████▏| 134831/147540 [01:15<00:07, 1714.71it/s]tokenizing...:  92%|█████████▏| 135003/147540 [01:15<00:07, 1654.16it/s]tokenizing...:  92%|█████████▏| 135176/147540 [01:16<00:07, 1674.03it/s]tokenizing...:  92%|█████████▏| 135356/147540 [01:16<00:07, 1708.29it/s]tokenizing...:  92%|█████████▏| 135529/147540 [01:16<00:07, 1712.56it/s]tokenizing...:  92%|█████████▏| 135703/147540 [01:16<00:06, 1718.88it/s]tokenizing...:  92%|█████████▏| 135901/147540 [01:16<00:06, 1747.38it/s]tokenizing...:  92%|█████████▏| 136117/147540 [01:16<00:06, 1867.80it/s]tokenizing...:  92%|█████████▏| 136347/147540 [01:16<00:05, 1994.19it/s]tokenizing...:  93%|█████████▎| 136547/147540 [01:16<00:06, 1821.64it/s]tokenizing...:  93%|█████████▎| 136733/147540 [01:16<00:06, 1779.25it/s]tokenizing...:  93%|█████████▎| 136940/147540 [01:17<00:05, 1859.92it/s]tokenizing...:  93%|█████████▎| 137129/147540 [01:17<00:05, 1835.19it/s]tokenizing...:  93%|█████████▎| 137325/147540 [01:17<00:05, 1858.47it/s]tokenizing...:  93%|█████████▎| 137512/147540 [01:17<00:07, 1428.78it/s]tokenizing...:  93%|█████████▎| 137671/147540 [01:17<00:06, 1417.19it/s]tokenizing...:  93%|█████████▎| 137896/147540 [01:17<00:05, 1623.90it/s]tokenizing...:  94%|█████████▎| 138091/147540 [01:17<00:05, 1708.19it/s]tokenizing...:  94%|█████████▍| 138331/147540 [01:17<00:04, 1896.54it/s]tokenizing...:  94%|█████████▍| 138555/147540 [01:17<00:04, 1992.39it/s]tokenizing...:  94%|█████████▍| 138769/147540 [01:18<00:04, 2034.15it/s]tokenizing...:  94%|█████████▍| 139005/147540 [01:18<00:04, 2125.65it/s]tokenizing...:  94%|█████████▍| 139222/147540 [01:18<00:03, 2112.09it/s]tokenizing...:  95%|█████████▍| 139437/147540 [01:18<00:03, 2121.45it/s]tokenizing...:  95%|█████████▍| 139651/147540 [01:18<00:03, 2124.48it/s]tokenizing...:  95%|█████████▍| 139879/147540 [01:18<00:03, 2170.17it/s]tokenizing...:  95%|█████████▍| 140097/147540 [01:18<00:03, 2118.13it/s]tokenizing...:  95%|█████████▌| 140331/147540 [01:18<00:03, 2180.47it/s]tokenizing...:  95%|█████████▌| 140552/147540 [01:18<00:03, 2187.05it/s]tokenizing...:  95%|█████████▌| 140840/147540 [01:18<00:02, 2391.65it/s]tokenizing...:  96%|█████████▌| 141098/147540 [01:19<00:02, 2446.80it/s]tokenizing...:  96%|█████████▌| 141344/147540 [01:19<00:02, 2350.23it/s]tokenizing...:  96%|█████████▌| 141612/147540 [01:19<00:02, 2444.09it/s]tokenizing...:  96%|█████████▌| 141882/147540 [01:19<00:02, 2517.59it/s]tokenizing...:  96%|█████████▋| 142165/147540 [01:19<00:02, 2606.22it/s]tokenizing...:  97%|█████████▋| 142427/147540 [01:19<00:02, 2529.48it/s]tokenizing...:  97%|█████████▋| 142681/147540 [01:19<00:01, 2482.01it/s]tokenizing...:  97%|█████████▋| 142930/147540 [01:19<00:01, 2365.28it/s]tokenizing...:  97%|█████████▋| 143168/147540 [01:19<00:01, 2319.93it/s]tokenizing...:  97%|█████████▋| 143401/147540 [01:20<00:01, 2218.29it/s]tokenizing...:  97%|█████████▋| 143637/147540 [01:20<00:01, 2257.24it/s]tokenizing...:  98%|█████████▊| 143864/147540 [01:20<00:01, 1957.20it/s]tokenizing...:  98%|█████████▊| 144067/147540 [01:20<00:01, 1871.29it/s]tokenizing...:  98%|█████████▊| 144260/147540 [01:20<00:01, 1822.64it/s]tokenizing...:  98%|█████████▊| 144446/147540 [01:20<00:01, 1829.84it/s]tokenizing...:  98%|█████████▊| 144665/147540 [01:20<00:01, 1925.87it/s]tokenizing...:  98%|█████████▊| 144861/147540 [01:20<00:01, 1872.54it/s]tokenizing...:  98%|█████████▊| 145051/147540 [01:20<00:01, 1849.71it/s]tokenizing...:  98%|█████████▊| 145247/147540 [01:21<00:01, 1880.79it/s]tokenizing...:  99%|█████████▊| 145438/147540 [01:21<00:01, 1885.66it/s]tokenizing...:  99%|█████████▊| 145628/147540 [01:21<00:01, 1813.74it/s]tokenizing...:  99%|█████████▉| 145813/147540 [01:21<00:00, 1806.15it/s]tokenizing...:  99%|█████████▉| 145995/147540 [01:21<00:00, 1782.46it/s]tokenizing...:  99%|█████████▉| 146174/147540 [01:21<00:00, 1729.33it/s]tokenizing...:  99%|█████████▉| 146348/147540 [01:21<00:00, 1647.12it/s]tokenizing...:  99%|█████████▉| 146555/147540 [01:21<00:00, 1763.59it/s]tokenizing...:  99%|█████████▉| 146733/147540 [01:21<00:00, 1724.67it/s]tokenizing...: 100%|█████████▉| 146907/147540 [01:22<00:00, 1683.93it/s]tokenizing...: 100%|█████████▉| 147077/147540 [01:22<00:00, 1658.08it/s]tokenizing...: 100%|█████████▉| 147244/147540 [01:22<00:00, 1653.85it/s]tokenizing...: 100%|█████████▉| 147410/147540 [01:22<00:00, 1590.96it/s]tokenizing...: 100%|██████████| 147540/147540 [01:22<00:00, 1790.48it/s]
09/18/2022 13:47:07 - INFO - root -   The nums of the test_dataset features is 147540
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 0/1153 [00:00<?, ?it/s]  0%|          | 1/1153 [00:01<38:04,  1.98s/it]  0%|          | 2/1153 [00:06<1:11:32,  3.73s/it]  0%|          | 3/1153 [00:08<51:46,  2.70s/it]    0%|          | 4/1153 [00:09<41:55,  2.19s/it]  0%|          | 5/1153 [00:11<36:16,  1.90s/it]  1%|          | 6/1153 [00:12<32:14,  1.69s/it]  1%|          | 7/1153 [00:13<30:44,  1.61s/it]  1%|          | 8/1153 [00:15<29:50,  1.56s/it]  1%|          | 9/1153 [00:16<28:33,  1.50s/it]  1%|          | 10/1153 [00:18<27:42,  1.45s/it]  1%|          | 11/1153 [00:19<27:11,  1.43s/it]  1%|          | 12/1153 [00:20<24:56,  1.31s/it]  1%|          | 13/1153 [00:21<25:18,  1.33s/it]  1%|          | 14/1153 [00:23<25:35,  1.35s/it]  1%|▏         | 15/1153 [00:24<25:50,  1.36s/it]  1%|▏         | 16/1153 [00:26<25:45,  1.36s/it]  1%|▏         | 17/1153 [00:27<25:53,  1.37s/it]  2%|▏         | 18/1153 [00:32<45:01,  2.38s/it]  2%|▏         | 19/1153 [00:33<39:27,  2.09s/it]  2%|▏         | 20/1153 [00:34<35:29,  1.88s/it]  2%|▏         | 21/1153 [00:36<32:29,  1.72s/it]  2%|▏         | 22/1153 [00:37<30:24,  1.61s/it]  2%|▏         | 23/1153 [00:39<29:01,  1.54s/it]  2%|▏         | 24/1153 [00:40<28:05,  1.49s/it]  2%|▏         | 25/1153 [00:41<27:28,  1.46s/it]  2%|▏         | 26/1153 [00:43<27:07,  1.44s/it]  2%|▏         | 27/1153 [00:44<25:01,  1.33s/it]  2%|▏         | 28/1153 [00:45<25:09,  1.34s/it]  3%|▎         | 29/1153 [00:47<25:17,  1.35s/it]  3%|▎         | 30/1153 [00:48<25:25,  1.36s/it]  3%|▎         | 31/1153 [00:49<25:33,  1.37s/it]  3%|▎         | 32/1153 [00:51<25:41,  1.38s/it]  3%|▎         | 33/1153 [00:52<25:50,  1.38s/it]  3%|▎         | 34/1153 [00:56<42:08,  2.26s/it]  3%|▎         | 35/1153 [00:58<37:24,  2.01s/it]  3%|▎         | 36/1153 [00:59<34:00,  1.83s/it]  3%|▎         | 37/1153 [01:01<31:40,  1.70s/it]  3%|▎         | 38/1153 [01:02<29:59,  1.61s/it]  3%|▎         | 39/1153 [01:03<28:56,  1.56s/it]  3%|▎         | 40/1153 [01:05<26:56,  1.45s/it]  4%|▎         | 41/1153 [01:06<25:49,  1.39s/it]  4%|▎         | 42/1153 [01:07<24:53,  1.34s/it]  4%|▎         | 43/1153 [01:08<24:10,  1.31s/it]  4%|▍         | 44/1153 [01:10<23:23,  1.27s/it]  4%|▍         | 45/1153 [01:11<23:07,  1.25s/it]  4%|▍         | 46/1153 [01:12<22:44,  1.23s/it]  4%|▍         | 47/1153 [01:13<22:28,  1.22s/it]  4%|▍         | 48/1153 [01:14<22:16,  1.21s/it]  4%|▍         | 49/1153 [01:16<22:20,  1.21s/it]  4%|▍         | 50/1153 [01:20<39:13,  2.13s/it]  4%|▍         | 51/1153 [01:21<34:13,  1.86s/it]  5%|▍         | 52/1153 [01:22<30:45,  1.68s/it]  5%|▍         | 53/1153 [01:24<28:18,  1.54s/it]  5%|▍         | 54/1153 [01:25<26:45,  1.46s/it]  5%|▍         | 55/1153 [01:26<25:26,  1.39s/it]  5%|▍         | 56/1153 [01:27<24:09,  1.32s/it]  5%|▍         | 57/1153 [01:28<23:45,  1.30s/it]  5%|▌         | 58/1153 [01:30<23:25,  1.28s/it]  5%|▌         | 59/1153 [01:31<23:17,  1.28s/it]  5%|▌         | 60/1153 [01:32<23:04,  1.27s/it]  5%|▌         | 61/1153 [01:33<22:48,  1.25s/it]  5%|▌         | 62/1153 [01:35<23:00,  1.27s/it]  5%|▌         | 63/1153 [01:36<22:28,  1.24s/it]  6%|▌         | 64/1153 [01:37<22:30,  1.24s/it]  6%|▌         | 65/1153 [01:38<22:33,  1.24s/it]  6%|▌         | 66/1153 [01:40<22:27,  1.24s/it]  6%|▌         | 67/1153 [01:41<22:30,  1.24s/it]  6%|▌         | 68/1153 [01:45<37:11,  2.06s/it]  6%|▌         | 69/1153 [01:46<33:47,  1.87s/it]  6%|▌         | 70/1153 [01:47<28:21,  1.57s/it]  6%|▌         | 71/1153 [01:49<27:19,  1.51s/it]  6%|▌         | 72/1153 [01:50<26:39,  1.48s/it]  6%|▋         | 73/1153 [01:51<26:22,  1.47s/it]  6%|▋         | 74/1153 [01:53<25:22,  1.41s/it]  7%|▋         | 75/1153 [01:54<25:22,  1.41s/it]  7%|▋         | 76/1153 [01:55<25:15,  1.41s/it]  7%|▋         | 77/1153 [01:57<25:13,  1.41s/it]  7%|▋         | 78/1153 [01:58<25:15,  1.41s/it]  7%|▋         | 79/1153 [02:00<25:07,  1.40s/it]  7%|▋         | 80/1153 [02:01<25:01,  1.40s/it]  7%|▋         | 81/1153 [02:02<24:17,  1.36s/it]  7%|▋         | 82/1153 [02:04<24:32,  1.37s/it]  7%|▋         | 83/1153 [02:05<24:34,  1.38s/it]  7%|▋         | 84/1153 [02:06<24:39,  1.38s/it]  7%|▋         | 85/1153 [02:11<38:53,  2.18s/it]  7%|▋         | 86/1153 [02:12<34:56,  1.96s/it]  8%|▊         | 87/1153 [02:13<32:03,  1.80s/it]  8%|▊         | 88/1153 [02:15<29:50,  1.68s/it]  8%|▊         | 89/1153 [02:16<28:17,  1.60s/it]  8%|▊         | 90/1153 [02:18<27:15,  1.54s/it]  8%|▊         | 91/1153 [02:19<26:36,  1.50s/it]  8%|▊         | 92/1153 [02:20<26:11,  1.48s/it]  8%|▊         | 93/1153 [02:22<25:53,  1.47s/it]  8%|▊         | 94/1153 [02:23<25:27,  1.44s/it]  8%|▊         | 95/1153 [02:25<25:11,  1.43s/it]  8%|▊         | 96/1153 [02:26<25:02,  1.42s/it]  8%|▊         | 97/1153 [02:28<24:58,  1.42s/it]  8%|▊         | 98/1153 [02:29<24:59,  1.42s/it]  9%|▊         | 99/1153 [02:30<25:01,  1.42s/it]  9%|▊         | 100/1153 [02:35<39:52,  2.27s/it]  9%|▉         | 101/1153 [02:36<35:21,  2.02s/it]  9%|▉         | 102/1153 [02:37<32:09,  1.84s/it]  9%|▉         | 103/1153 [02:39<29:57,  1.71s/it]  9%|▉         | 104/1153 [02:40<28:26,  1.63s/it]  9%|▉         | 105/1153 [02:42<27:09,  1.56s/it]  9%|▉         | 106/1153 [02:43<26:17,  1.51s/it]  9%|▉         | 107/1153 [02:44<25:42,  1.48s/it]  9%|▉         | 108/1153 [02:46<25:21,  1.46s/it]  9%|▉         | 109/1153 [02:47<24:03,  1.38s/it] 10%|▉         | 110/1153 [02:49<24:17,  1.40s/it] 10%|▉         | 111/1153 [02:50<24:21,  1.40s/it] 10%|▉         | 112/1153 [02:51<24:37,  1.42s/it] 10%|▉         | 113/1153 [02:53<24:42,  1.43s/it] 10%|▉         | 114/1153 [02:54<24:30,  1.42s/it] 10%|▉         | 115/1153 [02:56<24:24,  1.41s/it] 10%|█         | 116/1153 [03:00<38:12,  2.21s/it] 10%|█         | 117/1153 [03:01<33:58,  1.97s/it] 10%|█         | 118/1153 [03:03<31:05,  1.80s/it] 10%|█         | 119/1153 [03:04<28:59,  1.68s/it] 10%|█         | 120/1153 [03:05<27:33,  1.60s/it] 10%|█         | 121/1153 [03:07<26:36,  1.55s/it] 11%|█         | 122/1153 [03:08<24:56,  1.45s/it] 11%|█         | 123/1153 [03:09<24:45,  1.44s/it] 11%|█         | 124/1153 [03:11<24:37,  1.44s/it] 11%|█         | 125/1153 [03:12<24:21,  1.42s/it] 11%|█         | 126/1153 [03:14<23:44,  1.39s/it] 11%|█         | 127/1153 [03:15<22:38,  1.32s/it] 11%|█         | 128/1153 [03:16<22:19,  1.31s/it] 11%|█         | 129/1153 [03:17<23:01,  1.35s/it] 11%|█▏        | 130/1153 [03:19<23:16,  1.36s/it] 11%|█▏        | 131/1153 [03:20<23:27,  1.38s/it] 11%|█▏        | 132/1153 [03:25<38:21,  2.25s/it] 12%|█▏        | 133/1153 [03:26<33:12,  1.95s/it] 12%|█▏        | 134/1153 [03:27<29:40,  1.75s/it] 12%|█▏        | 135/1153 [03:28<27:13,  1.60s/it] 12%|█▏        | 136/1153 [03:30<25:05,  1.48s/it] 12%|█▏        | 137/1153 [03:31<24:09,  1.43s/it] 12%|█▏        | 138/1153 [03:32<23:13,  1.37s/it] 12%|█▏        | 139/1153 [03:33<22:33,  1.33s/it] 12%|█▏        | 140/1153 [03:35<22:06,  1.31s/it] 12%|█▏        | 141/1153 [03:36<21:43,  1.29s/it] 12%|█▏        | 142/1153 [03:37<21:22,  1.27s/it] 12%|█▏        | 143/1153 [03:38<22:04,  1.31s/it] 12%|█▏        | 144/1153 [03:40<22:35,  1.34s/it] 13%|█▎        | 145/1153 [03:41<23:00,  1.37s/it] 13%|█▎        | 146/1153 [03:43<22:21,  1.33s/it] 13%|█▎        | 147/1153 [03:44<21:53,  1.31s/it] 13%|█▎        | 148/1153 [03:45<21:35,  1.29s/it] 13%|█▎        | 149/1153 [03:46<21:20,  1.27s/it] 13%|█▎        | 150/1153 [03:48<21:17,  1.27s/it] 13%|█▎        | 151/1153 [03:51<34:32,  2.07s/it] 13%|█▎        | 152/1153 [03:53<30:42,  1.84s/it] 13%|█▎        | 153/1153 [03:54<27:44,  1.66s/it] 13%|█▎        | 154/1153 [03:55<25:37,  1.54s/it] 13%|█▎        | 155/1153 [03:57<24:11,  1.45s/it] 14%|█▎        | 156/1153 [03:58<23:58,  1.44s/it] 14%|█▎        | 157/1153 [03:59<23:48,  1.43s/it] 14%|█▎        | 158/1153 [04:01<23:55,  1.44s/it] 14%|█▍        | 159/1153 [04:02<23:59,  1.45s/it] 14%|█▍        | 160/1153 [04:04<23:56,  1.45s/it] 14%|█▍        | 161/1153 [04:05<23:57,  1.45s/it] 14%|█▍        | 162/1153 [04:06<22:58,  1.39s/it] 14%|█▍        | 163/1153 [04:08<23:13,  1.41s/it] 14%|█▍        | 164/1153 [04:09<22:23,  1.36s/it] 14%|█▍        | 165/1153 [04:10<21:41,  1.32s/it] 14%|█▍        | 166/1153 [04:12<21:23,  1.30s/it] 14%|█▍        | 167/1153 [04:13<21:06,  1.28s/it] 15%|█▍        | 168/1153 [04:17<35:49,  2.18s/it] 15%|█▍        | 169/1153 [04:18<31:20,  1.91s/it] 15%|█▍        | 170/1153 [04:20<28:11,  1.72s/it] 15%|█▍        | 171/1153 [04:21<26:08,  1.60s/it] 15%|█▍        | 172/1153 [04:22<24:27,  1.50s/it] 15%|█▌        | 173/1153 [04:24<23:24,  1.43s/it] 15%|█▌        | 174/1153 [04:25<22:31,  1.38s/it] 15%|█▌        | 175/1153 [04:26<21:46,  1.34s/it] 15%|█▌        | 176/1153 [04:27<21:23,  1.31s/it] 15%|█▌        | 177/1153 [04:29<21:08,  1.30s/it] 15%|█▌        | 178/1153 [04:30<21:06,  1.30s/it] 16%|█▌        | 179/1153 [04:31<21:08,  1.30s/it] 16%|█▌        | 180/1153 [04:32<20:47,  1.28s/it] 16%|█▌        | 181/1153 [04:34<20:49,  1.29s/it] 16%|█▌        | 182/1153 [04:35<20:39,  1.28s/it] 16%|█▌        | 183/1153 [04:36<20:44,  1.28s/it] 16%|█▌        | 184/1153 [04:38<20:40,  1.28s/it] 16%|█▌        | 185/1153 [04:39<20:31,  1.27s/it] 16%|█▌        | 186/1153 [04:43<34:19,  2.13s/it] 16%|█▌        | 187/1153 [04:44<30:11,  1.88s/it] 16%|█▋        | 188/1153 [04:45<27:10,  1.69s/it] 16%|█▋        | 189/1153 [04:47<25:10,  1.57s/it] 16%|█▋        | 190/1153 [04:48<23:35,  1.47s/it] 17%|█▋        | 191/1153 [04:49<23:28,  1.46s/it] 17%|█▋        | 192/1153 [04:51<23:32,  1.47s/it] 17%|█▋        | 193/1153 [04:52<23:27,  1.47s/it] 17%|█▋        | 194/1153 [04:54<23:32,  1.47s/it] 17%|█▋        | 195/1153 [04:55<23:23,  1.47s/it] 17%|█▋        | 196/1153 [04:57<23:12,  1.46s/it] 17%|█▋        | 197/1153 [04:58<23:09,  1.45s/it] 17%|█▋        | 198/1153 [05:00<23:08,  1.45s/it] 17%|█▋        | 199/1153 [05:01<23:09,  1.46s/it] 17%|█▋        | 200/1153 [05:03<23:12,  1.46s/it] 17%|█▋        | 201/1153 [05:04<23:00,  1.45s/it] 18%|█▊        | 202/1153 [05:05<23:04,  1.46s/it] 18%|█▊        | 203/1153 [05:10<35:47,  2.26s/it] 18%|█▊        | 204/1153 [05:11<32:04,  2.03s/it] 18%|█▊        | 205/1153 [05:13<29:22,  1.86s/it] 18%|█▊        | 206/1153 [05:14<27:16,  1.73s/it] 18%|█▊        | 207/1153 [05:15<25:49,  1.64s/it] 18%|█▊        | 208/1153 [05:17<24:50,  1.58s/it] 18%|█▊        | 209/1153 [05:18<24:12,  1.54s/it] 18%|█▊        | 210/1153 [05:19<22:19,  1.42s/it] 18%|█▊        | 211/1153 [05:21<22:27,  1.43s/it] 18%|█▊        | 212/1153 [05:22<20:01,  1.28s/it] 18%|█▊        | 213/1153 [05:23<20:45,  1.32s/it] 19%|█▊        | 214/1153 [05:25<21:18,  1.36s/it] 19%|█▊        | 215/1153 [05:26<21:44,  1.39s/it] 19%|█▊        | 216/1153 [05:28<22:04,  1.41s/it] 19%|█▉        | 217/1153 [05:29<22:05,  1.42s/it] 19%|█▉        | 218/1153 [05:30<21:44,  1.40s/it] 19%|█▉        | 219/1153 [05:35<35:47,  2.30s/it] 19%|█▉        | 220/1153 [05:36<31:57,  2.06s/it] 19%|█▉        | 221/1153 [05:38<28:58,  1.87s/it] 19%|█▉        | 222/1153 [05:39<26:53,  1.73s/it] 19%|█▉        | 223/1153 [05:41<25:29,  1.64s/it] 19%|█▉        | 224/1153 [05:42<24:31,  1.58s/it] 20%|█▉        | 225/1153 [05:43<23:55,  1.55s/it] 20%|█▉        | 226/1153 [05:45<23:31,  1.52s/it] 20%|█▉        | 227/1153 [05:46<23:02,  1.49s/it] 20%|█▉        | 228/1153 [05:48<22:42,  1.47s/it] 20%|█▉        | 229/1153 [05:49<21:44,  1.41s/it] 20%|█▉        | 230/1153 [05:50<21:48,  1.42s/it] 20%|██        | 231/1153 [05:52<21:16,  1.38s/it] 20%|██        | 232/1153 [05:53<21:40,  1.41s/it] 20%|██        | 233/1153 [05:55<21:50,  1.42s/it] 20%|██        | 234/1153 [05:56<21:52,  1.43s/it] 20%|██        | 235/1153 [05:58<22:03,  1.44s/it] 20%|██        | 236/1153 [06:02<34:02,  2.23s/it] 21%|██        | 237/1153 [06:03<30:38,  2.01s/it] 21%|██        | 238/1153 [06:05<27:58,  1.83s/it] 21%|██        | 239/1153 [06:06<26:07,  1.71s/it] 21%|██        | 240/1153 [06:08<25:02,  1.65s/it] 21%|██        | 241/1153 [06:09<23:38,  1.55s/it] 21%|██        | 242/1153 [06:10<23:03,  1.52s/it] 21%|██        | 243/1153 [06:12<22:41,  1.50s/it] 21%|██        | 244/1153 [06:13<22:27,  1.48s/it] 21%|██        | 245/1153 [06:14<20:40,  1.37s/it] 21%|██▏       | 246/1153 [06:15<19:57,  1.32s/it] 21%|██▏       | 247/1153 [06:17<20:32,  1.36s/it] 22%|██▏       | 248/1153 [06:18<20:59,  1.39s/it] 22%|██▏       | 249/1153 [06:20<21:20,  1.42s/it] 22%|██▏       | 250/1153 [06:21<21:36,  1.44s/it] 22%|██▏       | 251/1153 [06:23<21:35,  1.44s/it] 22%|██▏       | 252/1153 [06:27<34:10,  2.28s/it] 22%|██▏       | 253/1153 [06:29<30:28,  2.03s/it] 22%|██▏       | 254/1153 [06:30<27:47,  1.85s/it] 22%|██▏       | 255/1153 [06:31<25:17,  1.69s/it] 22%|██▏       | 256/1153 [06:33<24:08,  1.61s/it] 22%|██▏       | 257/1153 [06:34<23:21,  1.56s/it] 22%|██▏       | 258/1153 [06:35<22:16,  1.49s/it] 22%|██▏       | 259/1153 [06:37<21:48,  1.46s/it] 23%|██▎       | 260/1153 [06:38<21:43,  1.46s/it] 23%|██▎       | 261/1153 [06:40<21:41,  1.46s/it] 23%|██▎       | 262/1153 [06:41<20:36,  1.39s/it] 23%|██▎       | 263/1153 [06:42<20:11,  1.36s/it] 23%|██▎       | 264/1153 [06:44<20:10,  1.36s/it] 23%|██▎       | 265/1153 [06:45<20:37,  1.39s/it] 23%|██▎       | 266/1153 [06:46<20:02,  1.36s/it] 23%|██▎       | 267/1153 [06:48<19:46,  1.34s/it] 23%|██▎       | 268/1153 [06:49<20:12,  1.37s/it] 23%|██▎       | 269/1153 [06:50<19:48,  1.34s/it] 23%|██▎       | 270/1153 [06:55<32:10,  2.19s/it] 24%|██▎       | 271/1153 [06:56<28:06,  1.91s/it] 24%|██▎       | 272/1153 [06:57<26:07,  1.78s/it] 24%|██▎       | 273/1153 [06:59<24:03,  1.64s/it] 24%|██▍       | 274/1153 [07:00<22:47,  1.56s/it] 24%|██▍       | 275/1153 [07:01<21:31,  1.47s/it] 24%|██▍       | 276/1153 [07:03<20:45,  1.42s/it] 24%|██▍       | 277/1153 [07:04<20:11,  1.38s/it] 24%|██▍       | 278/1153 [07:05<19:55,  1.37s/it] 24%|██▍       | 279/1153 [07:06<19:36,  1.35s/it] 24%|██▍       | 280/1153 [07:08<19:09,  1.32s/it] 24%|██▍       | 281/1153 [07:09<18:59,  1.31s/it] 24%|██▍       | 282/1153 [07:10<19:06,  1.32s/it] 25%|██▍       | 283/1153 [07:12<19:03,  1.31s/it] 25%|██▍       | 284/1153 [07:13<19:43,  1.36s/it] 25%|██▍       | 285/1153 [07:15<20:10,  1.39s/it] 25%|██▍       | 286/1153 [07:16<20:30,  1.42s/it] 25%|██▍       | 287/1153 [07:18<20:47,  1.44s/it] 25%|██▍       | 288/1153 [07:22<33:18,  2.31s/it] 25%|██▌       | 289/1153 [07:23<29:40,  2.06s/it] 25%|██▌       | 290/1153 [07:25<27:05,  1.88s/it] 25%|██▌       | 291/1153 [07:26<24:25,  1.70s/it] 25%|██▌       | 292/1153 [07:27<22:40,  1.58s/it] 25%|██▌       | 293/1153 [07:29<21:27,  1.50s/it] 25%|██▌       | 294/1153 [07:30<20:26,  1.43s/it] 26%|██▌       | 295/1153 [07:31<19:56,  1.39s/it] 26%|██▌       | 296/1153 [07:33<19:26,  1.36s/it] 26%|██▌       | 297/1153 [07:34<19:14,  1.35s/it] 26%|██▌       | 298/1153 [07:35<19:15,  1.35s/it] 26%|██▌       | 299/1153 [07:37<18:57,  1.33s/it] 26%|██▌       | 300/1153 [07:38<18:48,  1.32s/it] 26%|██▌       | 301/1153 [07:39<18:38,  1.31s/it] 26%|██▌       | 302/1153 [07:40<18:26,  1.30s/it] 26%|██▋       | 303/1153 [07:42<18:26,  1.30s/it] 26%|██▋       | 304/1153 [07:43<18:28,  1.31s/it] 26%|██▋       | 305/1153 [07:44<18:36,  1.32s/it] 27%|██▋       | 306/1153 [07:46<18:38,  1.32s/it] 27%|██▋       | 307/1153 [07:50<31:07,  2.21s/it] 27%|██▋       | 308/1153 [07:51<27:25,  1.95s/it] 27%|██▋       | 309/1153 [07:53<24:40,  1.75s/it] 27%|██▋       | 310/1153 [07:54<22:51,  1.63s/it] 27%|██▋       | 311/1153 [07:55<22:05,  1.57s/it] 27%|██▋       | 312/1153 [07:57<20:58,  1.50s/it] 27%|██▋       | 313/1153 [07:58<20:03,  1.43s/it] 27%|██▋       | 314/1153 [07:59<19:45,  1.41s/it] 27%|██▋       | 315/1153 [08:01<19:18,  1.38s/it] 27%|██▋       | 316/1153 [08:02<18:57,  1.36s/it] 27%|██▋       | 317/1153 [08:03<18:45,  1.35s/it] 28%|██▊       | 318/1153 [08:05<18:28,  1.33s/it] 28%|██▊       | 319/1153 [08:06<19:15,  1.39s/it] 28%|██▊       | 320/1153 [08:08<19:38,  1.41s/it] 28%|██▊       | 321/1153 [08:09<19:17,  1.39s/it] 28%|██▊       | 322/1153 [08:10<19:34,  1.41s/it] 28%|██▊       | 323/1153 [08:12<19:48,  1.43s/it] 28%|██▊       | 324/1153 [08:13<20:01,  1.45s/it] 28%|██▊       | 325/1153 [08:18<32:08,  2.33s/it] 28%|██▊       | 326/1153 [08:19<28:35,  2.07s/it] 28%|██▊       | 327/1153 [08:21<26:01,  1.89s/it] 28%|██▊       | 328/1153 [08:22<24:17,  1.77s/it] 29%|██▊       | 329/1153 [08:24<23:05,  1.68s/it] 29%|██▊       | 330/1153 [08:25<22:15,  1.62s/it] 29%|██▊       | 331/1153 [08:27<21:43,  1.59s/it] 29%|██▉       | 332/1153 [08:28<21:09,  1.55s/it] 29%|██▉       | 333/1153 [08:30<20:46,  1.52s/it] 29%|██▉       | 334/1153 [08:31<20:33,  1.51s/it] 29%|██▉       | 335/1153 [08:33<20:27,  1.50s/it] 29%|██▉       | 336/1153 [08:34<20:26,  1.50s/it] 29%|██▉       | 337/1153 [08:36<20:26,  1.50s/it] 29%|██▉       | 338/1153 [08:37<20:14,  1.49s/it] 29%|██▉       | 339/1153 [08:38<20:08,  1.49s/it] 29%|██▉       | 340/1153 [08:40<20:05,  1.48s/it] 30%|██▉       | 341/1153 [08:41<20:05,  1.48s/it] 30%|██▉       | 342/1153 [08:46<32:07,  2.38s/it] 30%|██▉       | 343/1153 [08:47<28:36,  2.12s/it] 30%|██▉       | 344/1153 [08:49<25:54,  1.92s/it] 30%|██▉       | 345/1153 [08:50<24:04,  1.79s/it] 30%|███       | 346/1153 [08:52<22:48,  1.70s/it] 30%|███       | 347/1153 [08:53<21:57,  1.63s/it] 30%|███       | 348/1153 [08:55<21:24,  1.60s/it] 30%|███       | 349/1153 [08:56<21:02,  1.57s/it] 30%|███       | 350/1153 [08:58<20:36,  1.54s/it] 30%|███       | 351/1153 [08:59<20:19,  1.52s/it] 31%|███       | 352/1153 [09:01<20:09,  1.51s/it] 31%|███       | 353/1153 [09:02<20:03,  1.50s/it] 31%|███       | 354/1153 [09:04<19:29,  1.46s/it] 31%|███       | 355/1153 [09:05<19:31,  1.47s/it] 31%|███       | 356/1153 [09:07<19:41,  1.48s/it] 31%|███       | 357/1153 [09:08<19:47,  1.49s/it] 31%|███       | 358/1153 [09:13<31:20,  2.36s/it] 31%|███       | 359/1153 [09:14<27:49,  2.10s/it] 31%|███       | 360/1153 [09:15<25:15,  1.91s/it] 31%|███▏      | 361/1153 [09:17<23:33,  1.78s/it] 31%|███▏      | 362/1153 [09:19<22:38,  1.72s/it] 31%|███▏      | 363/1153 [09:20<21:46,  1.65s/it] 32%|███▏      | 364/1153 [09:22<21:10,  1.61s/it] 32%|███▏      | 365/1153 [09:23<20:17,  1.55s/it] 32%|███▏      | 366/1153 [09:24<20:03,  1.53s/it] 32%|███▏      | 367/1153 [09:26<19:55,  1.52s/it] 32%|███▏      | 368/1153 [09:27<19:51,  1.52s/it] 32%|███▏      | 369/1153 [09:28<17:00,  1.30s/it] 32%|███▏      | 370/1153 [09:30<17:48,  1.36s/it] 32%|███▏      | 371/1153 [09:31<18:11,  1.40s/it] 32%|███▏      | 372/1153 [09:33<18:27,  1.42s/it] 32%|███▏      | 373/1153 [09:34<18:36,  1.43s/it] 32%|███▏      | 374/1153 [09:36<18:51,  1.45s/it] 33%|███▎      | 375/1153 [09:40<31:17,  2.41s/it] 33%|███▎      | 376/1153 [09:42<27:39,  2.14s/it] 33%|███▎      | 377/1153 [09:43<25:02,  1.94s/it] 33%|███▎      | 378/1153 [09:45<23:15,  1.80s/it] 33%|███▎      | 379/1153 [09:46<22:02,  1.71s/it] 33%|███▎      | 380/1153 [09:48<21:12,  1.65s/it] 33%|███▎      | 381/1153 [09:49<20:40,  1.61s/it] 33%|███▎      | 382/1153 [09:51<20:06,  1.56s/it] 33%|███▎      | 383/1153 [09:52<19:43,  1.54s/it] 33%|███▎      | 384/1153 [09:54<19:31,  1.52s/it] 33%|███▎      | 385/1153 [09:55<19:23,  1.52s/it] 33%|███▎      | 386/1153 [09:57<19:19,  1.51s/it] 34%|███▎      | 387/1153 [09:58<19:18,  1.51s/it] 34%|███▎      | 388/1153 [10:00<19:07,  1.50s/it] 34%|███▎      | 389/1153 [10:01<19:03,  1.50s/it] 34%|███▍      | 390/1153 [10:03<19:01,  1.50s/it] 34%|███▍      | 391/1153 [10:04<19:01,  1.50s/it] 34%|███▍      | 392/1153 [10:09<30:36,  2.41s/it] 34%|███▍      | 393/1153 [10:10<27:21,  2.16s/it] 34%|███▍      | 394/1153 [10:12<24:50,  1.96s/it] 34%|███▍      | 395/1153 [10:13<22:09,  1.75s/it] 34%|███▍      | 396/1153 [10:15<21:07,  1.67s/it] 34%|███▍      | 397/1153 [10:16<20:01,  1.59s/it] 35%|███▍      | 398/1153 [10:17<19:44,  1.57s/it] 35%|███▍      | 399/1153 [10:19<19:36,  1.56s/it] 35%|███▍      | 400/1153 [10:20<19:19,  1.54s/it] 35%|███▍      | 401/1153 [10:22<19:19,  1.54s/it] 35%|███▍      | 402/1153 [10:24<19:13,  1.54s/it] 35%|███▍      | 403/1153 [10:25<18:22,  1.47s/it] 35%|███▌      | 404/1153 [10:26<17:54,  1.43s/it] 35%|███▌      | 405/1153 [10:28<17:27,  1.40s/it] 35%|███▌      | 406/1153 [10:29<17:50,  1.43s/it] 35%|███▌      | 407/1153 [10:31<18:08,  1.46s/it] 35%|███▌      | 408/1153 [10:32<17:34,  1.42s/it] 35%|███▌      | 409/1153 [10:33<17:17,  1.39s/it] 36%|███▌      | 410/1153 [10:38<28:18,  2.29s/it] 36%|███▌      | 411/1153 [10:39<25:34,  2.07s/it] 36%|███▌      | 412/1153 [10:41<22:53,  1.85s/it] 36%|███▌      | 413/1153 [10:42<20:47,  1.69s/it] 36%|███▌      | 414/1153 [10:43<19:26,  1.58s/it] 36%|███▌      | 415/1153 [10:44<18:30,  1.51s/it] 36%|███▌      | 416/1153 [10:46<17:49,  1.45s/it] 36%|███▌      | 417/1153 [10:47<17:24,  1.42s/it] 36%|███▋      | 418/1153 [10:48<17:06,  1.40s/it] 36%|███▋      | 419/1153 [10:50<17:32,  1.43s/it] 36%|███▋      | 420/1153 [10:51<16:56,  1.39s/it] 37%|███▋      | 421/1153 [10:53<16:51,  1.38s/it] 37%|███▋      | 422/1153 [10:54<16:43,  1.37s/it] 37%|███▋      | 423/1153 [10:56<17:13,  1.42s/it] 37%|███▋      | 424/1153 [10:57<16:49,  1.38s/it] 37%|███▋      | 425/1153 [10:58<16:40,  1.37s/it] 37%|███▋      | 426/1153 [11:00<16:27,  1.36s/it] 37%|███▋      | 427/1153 [11:01<17:02,  1.41s/it] 37%|███▋      | 428/1153 [11:05<27:48,  2.30s/it] 37%|███▋      | 429/1153 [11:07<24:14,  2.01s/it] 37%|███▋      | 430/1153 [11:08<21:46,  1.81s/it] 37%|███▋      | 431/1153 [11:10<20:45,  1.72s/it] 37%|███▋      | 432/1153 [11:11<19:56,  1.66s/it] 38%|███▊      | 433/1153 [11:13<19:18,  1.61s/it] 38%|███▊      | 434/1153 [11:14<18:40,  1.56s/it] 38%|███▊      | 435/1153 [11:16<18:37,  1.56s/it] 38%|███▊      | 436/1153 [11:17<17:47,  1.49s/it] 38%|███▊      | 437/1153 [11:18<17:16,  1.45s/it] 38%|███▊      | 438/1153 [11:20<16:54,  1.42s/it] 38%|███▊      | 439/1153 [11:21<17:21,  1.46s/it] 38%|███▊      | 440/1153 [11:23<16:59,  1.43s/it] 38%|███▊      | 441/1153 [11:24<17:22,  1.46s/it] 38%|███▊      | 442/1153 [11:25<16:52,  1.42s/it] 38%|███▊      | 443/1153 [11:27<16:24,  1.39s/it] 39%|███▊      | 444/1153 [11:28<16:27,  1.39s/it] 39%|███▊      | 445/1153 [11:30<17:03,  1.45s/it] 39%|███▊      | 446/1153 [11:34<27:06,  2.30s/it] 39%|███▉      | 447/1153 [11:35<23:35,  2.01s/it] 39%|███▉      | 448/1153 [11:37<21:17,  1.81s/it] 39%|███▉      | 449/1153 [11:38<19:36,  1.67s/it] 39%|███▉      | 450/1153 [11:39<18:24,  1.57s/it] 39%|███▉      | 451/1153 [11:41<17:37,  1.51s/it] 39%|███▉      | 452/1153 [11:42<17:00,  1.46s/it] 39%|███▉      | 453/1153 [11:44<17:16,  1.48s/it] 39%|███▉      | 454/1153 [11:45<17:20,  1.49s/it] 39%|███▉      | 455/1153 [11:46<16:53,  1.45s/it] 40%|███▉      | 456/1153 [11:48<16:25,  1.41s/it] 40%|███▉      | 457/1153 [11:49<16:11,  1.40s/it] 40%|███▉      | 458/1153 [11:50<16:00,  1.38s/it] 40%|███▉      | 459/1153 [11:52<15:57,  1.38s/it] 40%|███▉      | 460/1153 [11:53<15:52,  1.37s/it] 40%|███▉      | 461/1153 [11:55<15:42,  1.36s/it] 40%|████      | 462/1153 [11:56<15:42,  1.36s/it] 40%|████      | 463/1153 [11:57<15:36,  1.36s/it] 40%|████      | 464/1153 [11:59<15:48,  1.38s/it] 40%|████      | 465/1153 [12:03<27:22,  2.39s/it] 40%|████      | 466/1153 [12:05<24:20,  2.13s/it] 41%|████      | 467/1153 [12:06<21:06,  1.85s/it] 41%|████      | 468/1153 [12:08<19:53,  1.74s/it] 41%|████      | 469/1153 [12:09<19:05,  1.67s/it] 41%|████      | 470/1153 [12:11<18:33,  1.63s/it] 41%|████      | 471/1153 [12:12<18:12,  1.60s/it] 41%|████      | 472/1153 [12:14<17:15,  1.52s/it] 41%|████      | 473/1153 [12:15<16:53,  1.49s/it] 41%|████      | 474/1153 [12:16<16:53,  1.49s/it] 41%|████      | 475/1153 [12:18<16:54,  1.50s/it] 41%|████▏     | 476/1153 [12:20<17:02,  1.51s/it] 41%|████▏     | 477/1153 [12:21<17:04,  1.52s/it] 41%|████▏     | 478/1153 [12:23<17:13,  1.53s/it] 42%|████▏     | 479/1153 [12:24<17:19,  1.54s/it] 42%|████▏     | 480/1153 [12:26<17:13,  1.54s/it] 42%|████▏     | 481/1153 [12:27<16:49,  1.50s/it] 42%|████▏     | 482/1153 [12:29<16:54,  1.51s/it] 42%|████▏     | 483/1153 [12:33<26:42,  2.39s/it] 42%|████▏     | 484/1153 [12:35<23:55,  2.15s/it] 42%|████▏     | 485/1153 [12:36<20:32,  1.84s/it] 42%|████▏     | 486/1153 [12:37<18:40,  1.68s/it] 42%|████▏     | 487/1153 [12:39<18:03,  1.63s/it] 42%|████▏     | 488/1153 [12:40<17:38,  1.59s/it] 42%|████▏     | 489/1153 [12:42<17:23,  1.57s/it] 42%|████▏     | 490/1153 [12:43<17:14,  1.56s/it] 43%|████▎     | 491/1153 [12:45<17:10,  1.56s/it] 43%|████▎     | 492/1153 [12:46<17:07,  1.55s/it] 43%|████▎     | 493/1153 [12:48<16:55,  1.54s/it] 43%|████▎     | 494/1153 [12:49<16:37,  1.51s/it] 43%|████▎     | 495/1153 [12:51<16:39,  1.52s/it] 43%|████▎     | 496/1153 [12:52<16:43,  1.53s/it] 43%|████▎     | 497/1153 [12:54<16:47,  1.54s/it] 43%|████▎     | 498/1153 [12:55<16:40,  1.53s/it] 43%|████▎     | 499/1153 [12:57<16:35,  1.52s/it] 43%|████▎     | 500/1153 [13:01<26:37,  2.45s/it] 43%|████▎     | 501/1153 [13:03<23:39,  2.18s/it] 44%|████▎     | 502/1153 [13:04<20:59,  1.93s/it] 44%|████▎     | 503/1153 [13:06<19:38,  1.81s/it] 44%|████▎     | 504/1153 [13:07<16:38,  1.54s/it] 44%|████▍     | 505/1153 [13:08<16:32,  1.53s/it] 44%|████▍     | 506/1153 [13:10<16:30,  1.53s/it] 44%|████▍     | 507/1153 [13:11<16:30,  1.53s/it] 44%|████▍     | 508/1153 [13:13<15:35,  1.45s/it] 44%|████▍     | 509/1153 [13:13<13:03,  1.22s/it] 44%|████▍     | 510/1153 [13:15<14:08,  1.32s/it] 44%|████▍     | 511/1153 [13:16<12:34,  1.18s/it] 44%|████▍     | 512/1153 [13:17<13:45,  1.29s/it] 44%|████▍     | 513/1153 [13:18<12:29,  1.17s/it] 45%|████▍     | 514/1153 [13:19<11:40,  1.10s/it] 45%|████▍     | 515/1153 [13:21<13:01,  1.22s/it] 45%|████▍     | 516/1153 [13:22<13:53,  1.31s/it] 45%|████▍     | 517/1153 [13:24<14:01,  1.32s/it] 45%|████▍     | 518/1153 [13:25<14:44,  1.39s/it] 45%|████▌     | 519/1153 [13:30<25:14,  2.39s/it] 45%|████▌     | 520/1153 [13:31<22:40,  2.15s/it] 45%|████▌     | 521/1153 [13:33<20:46,  1.97s/it] 45%|████▌     | 522/1153 [13:34<19:17,  1.83s/it] 45%|████▌     | 523/1153 [13:36<18:10,  1.73s/it] 45%|████▌     | 524/1153 [13:37<17:31,  1.67s/it] 46%|████▌     | 525/1153 [13:39<17:02,  1.63s/it] 46%|████▌     | 526/1153 [13:41<16:53,  1.62s/it] 46%|████▌     | 527/1153 [13:42<16:40,  1.60s/it] 46%|████▌     | 528/1153 [13:44<16:22,  1.57s/it] 46%|████▌     | 529/1153 [13:45<16:11,  1.56s/it] 46%|████▌     | 530/1153 [13:47<16:04,  1.55s/it] 46%|████▌     | 531/1153 [13:48<16:01,  1.55s/it] 46%|████▌     | 532/1153 [13:50<16:00,  1.55s/it] 46%|████▌     | 533/1153 [13:51<15:16,  1.48s/it] 46%|████▋     | 534/1153 [13:52<14:45,  1.43s/it] 46%|████▋     | 535/1153 [13:54<14:54,  1.45s/it] 46%|████▋     | 536/1153 [13:55<14:25,  1.40s/it] 47%|████▋     | 537/1153 [14:00<24:02,  2.34s/it] 47%|████▋     | 538/1153 [14:01<20:57,  2.05s/it] 47%|████▋     | 539/1153 [14:02<18:34,  1.82s/it] 47%|████▋     | 540/1153 [14:04<17:39,  1.73s/it] 47%|████▋     | 541/1153 [14:05<17:03,  1.67s/it] 47%|████▋     | 542/1153 [14:07<16:30,  1.62s/it] 47%|████▋     | 543/1153 [14:08<15:41,  1.54s/it] 47%|████▋     | 544/1153 [14:10<14:53,  1.47s/it] 47%|████▋     | 545/1153 [14:11<14:37,  1.44s/it] 47%|████▋     | 546/1153 [14:12<14:10,  1.40s/it] 47%|████▋     | 547/1153 [14:14<14:12,  1.41s/it] 48%|████▊     | 548/1153 [14:15<14:08,  1.40s/it] 48%|████▊     | 549/1153 [14:16<13:58,  1.39s/it] 48%|████▊     | 550/1153 [14:18<14:23,  1.43s/it] 48%|████▊     | 551/1153 [14:19<14:14,  1.42s/it] 48%|████▊     | 552/1153 [14:21<13:58,  1.40s/it] 48%|████▊     | 553/1153 [14:22<13:49,  1.38s/it] 48%|████▊     | 554/1153 [14:23<13:41,  1.37s/it] 48%|████▊     | 555/1153 [14:25<13:44,  1.38s/it] 48%|████▊     | 556/1153 [14:29<22:58,  2.31s/it] 48%|████▊     | 557/1153 [14:31<20:08,  2.03s/it] 48%|████▊     | 558/1153 [14:32<18:09,  1.83s/it] 48%|████▊     | 559/1153 [14:33<16:47,  1.70s/it] 49%|████▊     | 560/1153 [14:35<15:51,  1.60s/it] 49%|████▊     | 561/1153 [14:36<15:09,  1.54s/it] 49%|████▊     | 562/1153 [14:38<14:33,  1.48s/it] 49%|████▉     | 563/1153 [14:39<14:17,  1.45s/it] 49%|████▉     | 564/1153 [14:40<14:05,  1.44s/it] 49%|████▉     | 565/1153 [14:42<13:51,  1.41s/it] 49%|████▉     | 566/1153 [14:43<14:15,  1.46s/it] 49%|████▉     | 567/1153 [14:45<14:05,  1.44s/it] 49%|████▉     | 568/1153 [14:46<13:59,  1.43s/it] 49%|████▉     | 569/1153 [14:47<13:53,  1.43s/it] 49%|████▉     | 570/1153 [14:49<13:50,  1.42s/it] 50%|████▉     | 571/1153 [14:50<13:39,  1.41s/it] 50%|████▉     | 572/1153 [14:52<13:25,  1.39s/it] 50%|████▉     | 573/1153 [14:53<13:25,  1.39s/it] 50%|████▉     | 574/1153 [14:54<13:20,  1.38s/it] 50%|████▉     | 575/1153 [14:56<13:15,  1.38s/it] 50%|████▉     | 576/1153 [15:00<22:36,  2.35s/it] 50%|█████     | 577/1153 [15:02<19:50,  2.07s/it] 50%|█████     | 578/1153 [15:03<17:38,  1.84s/it] 50%|█████     | 579/1153 [15:04<16:20,  1.71s/it] 50%|█████     | 580/1153 [15:06<15:22,  1.61s/it] 50%|█████     | 581/1153 [15:07<14:45,  1.55s/it] 50%|█████     | 582/1153 [15:09<14:14,  1.50s/it] 51%|█████     | 583/1153 [15:10<13:47,  1.45s/it] 51%|█████     | 584/1153 [15:11<13:37,  1.44s/it] 51%|█████     | 585/1153 [15:13<13:25,  1.42s/it] 51%|█████     | 586/1153 [15:14<13:14,  1.40s/it] 51%|█████     | 587/1153 [15:15<13:13,  1.40s/it] 51%|█████     | 588/1153 [15:17<13:08,  1.40s/it] 51%|█████     | 589/1153 [15:18<13:38,  1.45s/it] 51%|█████     | 590/1153 [15:20<13:16,  1.41s/it] 51%|█████▏    | 591/1153 [15:21<13:40,  1.46s/it] 51%|█████▏    | 592/1153 [15:23<13:25,  1.44s/it] 51%|█████▏    | 593/1153 [15:24<13:43,  1.47s/it] 52%|█████▏    | 594/1153 [15:26<13:56,  1.50s/it] 52%|█████▏    | 595/1153 [15:27<14:06,  1.52s/it] 52%|█████▏    | 596/1153 [15:32<22:57,  2.47s/it] 52%|█████▏    | 597/1153 [15:34<20:23,  2.20s/it] 52%|█████▏    | 598/1153 [15:35<18:34,  2.01s/it] 52%|█████▏    | 599/1153 [15:36<16:19,  1.77s/it] 52%|█████▏    | 600/1153 [15:38<15:46,  1.71s/it] 52%|█████▏    | 601/1153 [15:40<15:25,  1.68s/it] 52%|█████▏    | 602/1153 [15:41<15:09,  1.65s/it] 52%|█████▏    | 603/1153 [15:43<14:58,  1.63s/it] 52%|█████▏    | 604/1153 [15:44<14:40,  1.60s/it] 52%|█████▏    | 605/1153 [15:46<14:28,  1.59s/it] 53%|█████▎    | 606/1153 [15:47<14:22,  1.58s/it] 53%|█████▎    | 607/1153 [15:49<14:20,  1.58s/it] 53%|█████▎    | 608/1153 [15:50<13:11,  1.45s/it] 53%|█████▎    | 609/1153 [15:52<12:58,  1.43s/it] 53%|█████▎    | 610/1153 [15:53<13:26,  1.49s/it] 53%|█████▎    | 611/1153 [15:55<13:39,  1.51s/it] 53%|█████▎    | 612/1153 [15:56<13:50,  1.53s/it] 53%|█████▎    | 613/1153 [15:58<13:48,  1.53s/it] 53%|█████▎    | 614/1153 [16:03<22:12,  2.47s/it] 53%|█████▎    | 615/1153 [16:04<19:46,  2.21s/it] 53%|█████▎    | 616/1153 [16:06<18:02,  2.02s/it] 54%|█████▎    | 617/1153 [16:07<16:50,  1.88s/it] 54%|█████▎    | 618/1153 [16:09<16:01,  1.80s/it] 54%|█████▎    | 619/1153 [16:10<15:19,  1.72s/it] 54%|█████▍    | 620/1153 [16:12<14:50,  1.67s/it] 54%|█████▍    | 621/1153 [16:14<14:30,  1.64s/it] 54%|█████▍    | 622/1153 [16:15<13:37,  1.54s/it] 54%|█████▍    | 623/1153 [16:16<13:39,  1.55s/it] 54%|█████▍    | 624/1153 [16:18<13:32,  1.54s/it] 54%|█████▍    | 625/1153 [16:19<13:39,  1.55s/it] 54%|█████▍    | 626/1153 [16:21<13:36,  1.55s/it] 54%|█████▍    | 627/1153 [16:22<13:09,  1.50s/it] 54%|█████▍    | 628/1153 [16:24<13:22,  1.53s/it] 55%|█████▍    | 629/1153 [16:26<13:23,  1.53s/it] 55%|█████▍    | 630/1153 [16:27<13:24,  1.54s/it] 55%|█████▍    | 631/1153 [16:29<13:27,  1.55s/it] 55%|█████▍    | 632/1153 [16:33<21:50,  2.51s/it] 55%|█████▍    | 633/1153 [16:35<18:49,  2.17s/it] 55%|█████▍    | 634/1153 [16:36<16:34,  1.92s/it] 55%|█████▌    | 635/1153 [16:38<15:36,  1.81s/it] 55%|█████▌    | 636/1153 [16:39<14:58,  1.74s/it] 55%|█████▌    | 637/1153 [16:41<14:35,  1.70s/it] 55%|█████▌    | 638/1153 [16:42<14:20,  1.67s/it] 55%|█████▌    | 639/1153 [16:44<14:11,  1.66s/it] 56%|█████▌    | 640/1153 [16:46<13:56,  1.63s/it] 56%|█████▌    | 641/1153 [16:47<13:44,  1.61s/it] 56%|█████▌    | 642/1153 [16:49<13:37,  1.60s/it] 56%|█████▌    | 643/1153 [16:50<13:31,  1.59s/it] 56%|█████▌    | 644/1153 [16:52<13:29,  1.59s/it] 56%|█████▌    | 645/1153 [16:54<13:28,  1.59s/it] 56%|█████▌    | 646/1153 [16:55<13:22,  1.58s/it] 56%|█████▌    | 647/1153 [16:56<11:04,  1.31s/it] 56%|█████▌    | 648/1153 [16:57<11:46,  1.40s/it] 56%|█████▋    | 649/1153 [16:59<12:14,  1.46s/it] 56%|█████▋    | 650/1153 [17:04<20:21,  2.43s/it] 56%|█████▋    | 651/1153 [17:05<18:09,  2.17s/it] 57%|█████▋    | 652/1153 [17:06<14:26,  1.73s/it] 57%|█████▋    | 653/1153 [17:07<13:37,  1.64s/it] 57%|█████▋    | 654/1153 [17:09<13:24,  1.61s/it] 57%|█████▋    | 655/1153 [17:10<11:09,  1.34s/it] 57%|█████▋    | 656/1153 [17:10<09:41,  1.17s/it] 57%|█████▋    | 657/1153 [17:12<10:39,  1.29s/it] 57%|█████▋    | 658/1153 [17:14<11:20,  1.37s/it] 57%|█████▋    | 659/1153 [17:15<11:49,  1.44s/it] 57%|█████▋    | 660/1153 [17:17<12:10,  1.48s/it] 57%|█████▋    | 661/1153 [17:18<12:18,  1.50s/it] 57%|█████▋    | 662/1153 [17:20<12:17,  1.50s/it] 58%|█████▊    | 663/1153 [17:21<12:25,  1.52s/it] 58%|█████▊    | 664/1153 [17:23<12:25,  1.52s/it] 58%|█████▊    | 665/1153 [17:24<12:32,  1.54s/it] 58%|█████▊    | 666/1153 [17:26<12:38,  1.56s/it] 58%|█████▊    | 667/1153 [17:28<12:34,  1.55s/it] 58%|█████▊    | 668/1153 [17:29<12:33,  1.55s/it] 58%|█████▊    | 669/1153 [17:34<20:42,  2.57s/it] 58%|█████▊    | 670/1153 [17:36<18:20,  2.28s/it] 58%|█████▊    | 671/1153 [17:37<16:38,  2.07s/it] 58%|█████▊    | 672/1153 [17:39<15:22,  1.92s/it] 58%|█████▊    | 673/1153 [17:40<14:27,  1.81s/it] 58%|█████▊    | 674/1153 [17:42<13:43,  1.72s/it] 59%|█████▊    | 675/1153 [17:44<13:24,  1.68s/it] 59%|█████▊    | 676/1153 [17:45<12:54,  1.62s/it] 59%|█████▊    | 677/1153 [17:47<12:43,  1.60s/it] 59%|█████▉    | 678/1153 [17:48<12:05,  1.53s/it] 59%|█████▉    | 679/1153 [17:50<12:15,  1.55s/it] 59%|█████▉    | 680/1153 [17:51<11:57,  1.52s/it] 59%|█████▉    | 681/1153 [17:52<11:35,  1.47s/it] 59%|█████▉    | 682/1153 [17:54<11:47,  1.50s/it] 59%|█████▉    | 683/1153 [17:55<11:56,  1.52s/it] 59%|█████▉    | 684/1153 [17:57<11:26,  1.46s/it] 59%|█████▉    | 685/1153 [17:58<11:39,  1.49s/it] 59%|█████▉    | 686/1153 [18:00<11:53,  1.53s/it] 60%|█████▉    | 687/1153 [18:01<11:38,  1.50s/it] 60%|█████▉    | 688/1153 [18:06<18:35,  2.40s/it] 60%|█████▉    | 689/1153 [18:08<16:47,  2.17s/it] 60%|█████▉    | 690/1153 [18:09<14:47,  1.92s/it] 60%|█████▉    | 691/1153 [18:10<13:34,  1.76s/it] 60%|██████    | 692/1153 [18:12<12:46,  1.66s/it] 60%|██████    | 693/1153 [18:13<11:58,  1.56s/it] 60%|██████    | 694/1153 [18:15<12:01,  1.57s/it] 60%|██████    | 695/1153 [18:16<12:04,  1.58s/it] 60%|██████    | 696/1153 [18:18<11:59,  1.57s/it] 60%|██████    | 697/1153 [18:19<11:56,  1.57s/it] 61%|██████    | 698/1153 [18:21<11:56,  1.57s/it] 61%|██████    | 699/1153 [18:22<11:55,  1.58s/it] 61%|██████    | 700/1153 [18:24<11:56,  1.58s/it] 61%|██████    | 701/1153 [18:26<11:57,  1.59s/it] 61%|██████    | 702/1153 [18:27<11:50,  1.58s/it] 61%|██████    | 703/1153 [18:29<11:47,  1.57s/it] 61%|██████    | 704/1153 [18:30<11:45,  1.57s/it] 61%|██████    | 705/1153 [18:32<11:45,  1.58s/it] 61%|██████    | 706/1153 [18:37<19:24,  2.61s/it] 61%|██████▏   | 707/1153 [18:39<17:12,  2.31s/it] 61%|██████▏   | 708/1153 [18:40<15:23,  2.08s/it] 61%|██████▏   | 709/1153 [18:42<14:15,  1.93s/it] 62%|██████▏   | 710/1153 [18:43<13:28,  1.83s/it] 62%|██████▏   | 711/1153 [18:45<12:56,  1.76s/it] 62%|██████▏   | 712/1153 [18:46<12:35,  1.71s/it] 62%|██████▏   | 713/1153 [18:48<12:12,  1.67s/it] 62%|██████▏   | 714/1153 [18:50<12:01,  1.64s/it] 62%|██████▏   | 715/1153 [18:51<11:54,  1.63s/it] 62%|██████▏   | 716/1153 [18:53<11:53,  1.63s/it] 62%|██████▏   | 717/1153 [18:55<11:51,  1.63s/it] 62%|██████▏   | 718/1153 [18:56<11:48,  1.63s/it] 62%|██████▏   | 719/1153 [18:58<11:39,  1.61s/it] 62%|██████▏   | 720/1153 [18:59<11:33,  1.60s/it] 63%|██████▎   | 721/1153 [19:01<11:30,  1.60s/it] 63%|██████▎   | 722/1153 [19:02<11:27,  1.60s/it] 63%|██████▎   | 723/1153 [19:07<18:31,  2.58s/it] 63%|██████▎   | 724/1153 [19:09<16:20,  2.29s/it] 63%|██████▎   | 725/1153 [19:11<14:45,  2.07s/it] 63%|██████▎   | 726/1153 [19:12<13:40,  1.92s/it] 63%|██████▎   | 727/1153 [19:14<12:56,  1.82s/it] 63%|██████▎   | 728/1153 [19:15<12:25,  1.76s/it] 63%|██████▎   | 729/1153 [19:17<12:05,  1.71s/it] 63%|██████▎   | 730/1153 [19:18<11:44,  1.67s/it] 63%|██████▎   | 731/1153 [19:20<11:31,  1.64s/it] 63%|██████▎   | 732/1153 [19:22<11:22,  1.62s/it] 64%|██████▎   | 733/1153 [19:23<11:16,  1.61s/it] 64%|██████▎   | 734/1153 [19:25<11:13,  1.61s/it] 64%|██████▎   | 735/1153 [19:26<11:12,  1.61s/it] 64%|██████▍   | 736/1153 [19:28<11:04,  1.59s/it] 64%|██████▍   | 737/1153 [19:30<11:01,  1.59s/it] 64%|██████▍   | 738/1153 [19:31<10:59,  1.59s/it] 64%|██████▍   | 739/1153 [19:33<10:58,  1.59s/it] 64%|██████▍   | 740/1153 [19:34<10:58,  1.59s/it] 64%|██████▍   | 741/1153 [19:39<18:01,  2.63s/it] 64%|██████▍   | 742/1153 [19:41<15:50,  2.31s/it] 64%|██████▍   | 743/1153 [19:43<14:17,  2.09s/it] 65%|██████▍   | 744/1153 [19:44<12:29,  1.83s/it] 65%|██████▍   | 745/1153 [19:45<11:57,  1.76s/it] 65%|██████▍   | 746/1153 [19:47<11:24,  1.68s/it] 65%|██████▍   | 747/1153 [19:48<11:09,  1.65s/it] 65%|██████▍   | 748/1153 [19:50<10:59,  1.63s/it] 65%|██████▍   | 749/1153 [19:52<10:53,  1.62s/it] 65%|██████▌   | 750/1153 [19:53<10:49,  1.61s/it] 65%|██████▌   | 751/1153 [19:55<10:48,  1.61s/it] 65%|██████▌   | 752/1153 [19:56<10:53,  1.63s/it] 65%|██████▌   | 753/1153 [19:58<10:41,  1.60s/it] 65%|██████▌   | 754/1153 [20:00<10:38,  1.60s/it] 65%|██████▌   | 755/1153 [20:01<10:40,  1.61s/it] 66%|██████▌   | 756/1153 [20:03<10:15,  1.55s/it] 66%|██████▌   | 757/1153 [20:04<10:03,  1.52s/it] 66%|██████▌   | 758/1153 [20:06<09:49,  1.49s/it] 66%|██████▌   | 759/1153 [20:07<09:39,  1.47s/it] 66%|██████▌   | 760/1153 [20:12<16:12,  2.47s/it] 66%|██████▌   | 761/1153 [20:13<14:27,  2.21s/it] 66%|██████▌   | 762/1153 [20:15<12:54,  1.98s/it] 66%|██████▌   | 763/1153 [20:16<11:44,  1.81s/it] 66%|██████▋   | 764/1153 [20:18<11:02,  1.70s/it] 66%|██████▋   | 765/1153 [20:19<10:31,  1.63s/it] 66%|██████▋   | 766/1153 [20:20<10:03,  1.56s/it] 67%|██████▋   | 767/1153 [20:22<09:50,  1.53s/it] 67%|██████▋   | 768/1153 [20:23<09:33,  1.49s/it] 67%|██████▋   | 769/1153 [20:25<09:19,  1.46s/it] 67%|██████▋   | 770/1153 [20:26<09:18,  1.46s/it] 67%|██████▋   | 771/1153 [20:28<09:11,  1.44s/it] 67%|██████▋   | 772/1153 [20:29<09:13,  1.45s/it] 67%|██████▋   | 773/1153 [20:30<09:08,  1.44s/it] 67%|██████▋   | 774/1153 [20:32<09:03,  1.43s/it] 67%|██████▋   | 775/1153 [20:33<09:03,  1.44s/it] 67%|██████▋   | 776/1153 [20:35<09:00,  1.43s/it] 67%|██████▋   | 777/1153 [20:36<09:00,  1.44s/it] 67%|██████▋   | 778/1153 [20:38<08:58,  1.44s/it] 68%|██████▊   | 779/1153 [20:39<08:54,  1.43s/it] 68%|██████▊   | 780/1153 [20:41<08:57,  1.44s/it] 68%|██████▊   | 781/1153 [20:45<15:09,  2.45s/it] 68%|██████▊   | 782/1153 [20:47<13:12,  2.14s/it] 68%|██████▊   | 783/1153 [20:48<11:55,  1.93s/it] 68%|██████▊   | 784/1153 [20:50<10:58,  1.78s/it] 68%|██████▊   | 785/1153 [20:51<10:38,  1.73s/it] 68%|██████▊   | 786/1153 [20:53<10:23,  1.70s/it] 68%|██████▊   | 787/1153 [20:55<10:16,  1.68s/it] 68%|██████▊   | 788/1153 [20:56<10:04,  1.66s/it] 68%|██████▊   | 789/1153 [20:58<09:56,  1.64s/it] 69%|██████▊   | 790/1153 [20:59<09:52,  1.63s/it] 69%|██████▊   | 791/1153 [21:01<09:14,  1.53s/it] 69%|██████▊   | 792/1153 [21:02<08:52,  1.48s/it] 69%|██████▉   | 793/1153 [21:04<09:09,  1.53s/it] 69%|██████▉   | 794/1153 [21:05<09:16,  1.55s/it] 69%|██████▉   | 795/1153 [21:07<09:26,  1.58s/it] 69%|██████▉   | 796/1153 [21:09<09:29,  1.60s/it] 69%|██████▉   | 797/1153 [21:10<09:35,  1.62s/it] 69%|██████▉   | 798/1153 [21:12<09:32,  1.61s/it] 69%|██████▉   | 799/1153 [21:13<09:29,  1.61s/it] 69%|██████▉   | 800/1153 [21:18<15:10,  2.58s/it] 69%|██████▉   | 801/1153 [21:20<13:30,  2.30s/it] 70%|██████▉   | 802/1153 [21:22<12:24,  2.12s/it] 70%|██████▉   | 803/1153 [21:23<11:34,  1.98s/it] 70%|██████▉   | 804/1153 [21:25<10:51,  1.87s/it] 70%|██████▉   | 805/1153 [21:26<10:23,  1.79s/it] 70%|██████▉   | 806/1153 [21:28<10:04,  1.74s/it] 70%|██████▉   | 807/1153 [21:30<09:50,  1.71s/it] 70%|███████   | 808/1153 [21:31<09:42,  1.69s/it] 70%|███████   | 809/1153 [21:33<09:36,  1.68s/it] 70%|███████   | 810/1153 [21:35<09:27,  1.66s/it] 70%|███████   | 811/1153 [21:36<09:22,  1.64s/it] 70%|███████   | 812/1153 [21:38<09:18,  1.64s/it] 71%|███████   | 813/1153 [21:39<09:15,  1.64s/it] 71%|███████   | 814/1153 [21:41<09:14,  1.64s/it] 71%|███████   | 815/1153 [21:43<09:14,  1.64s/it] 71%|███████   | 816/1153 [21:43<07:34,  1.35s/it] 71%|███████   | 817/1153 [21:45<07:59,  1.43s/it] 71%|███████   | 818/1153 [21:50<13:45,  2.46s/it] 71%|███████   | 819/1153 [21:52<12:20,  2.22s/it] 71%|███████   | 820/1153 [21:53<11:19,  2.04s/it] 71%|███████   | 821/1153 [21:55<10:16,  1.86s/it] 71%|███████▏  | 822/1153 [21:56<08:39,  1.57s/it] 71%|███████▏  | 823/1153 [21:57<08:37,  1.57s/it] 71%|███████▏  | 824/1153 [21:59<08:41,  1.58s/it] 72%|███████▏  | 825/1153 [22:00<08:08,  1.49s/it] 72%|███████▏  | 826/1153 [22:02<08:21,  1.53s/it] 72%|███████▏  | 827/1153 [22:03<08:09,  1.50s/it] 72%|███████▏  | 828/1153 [22:05<08:20,  1.54s/it] 72%|███████▏  | 829/1153 [22:06<07:41,  1.42s/it] 72%|███████▏  | 830/1153 [22:07<08:01,  1.49s/it] 72%|███████▏  | 831/1153 [22:09<08:11,  1.53s/it] 72%|███████▏  | 832/1153 [22:11<08:18,  1.55s/it] 72%|███████▏  | 833/1153 [22:12<08:27,  1.59s/it] 72%|███████▏  | 834/1153 [22:14<08:32,  1.61s/it] 72%|███████▏  | 835/1153 [22:16<08:36,  1.62s/it] 73%|███████▎  | 836/1153 [22:17<08:37,  1.63s/it] 73%|███████▎  | 837/1153 [22:19<08:34,  1.63s/it] 73%|███████▎  | 838/1153 [22:24<13:37,  2.60s/it] 73%|███████▎  | 839/1153 [22:25<12:04,  2.31s/it] 73%|███████▎  | 840/1153 [22:27<10:55,  2.09s/it] 73%|███████▎  | 841/1153 [22:29<10:16,  1.98s/it] 73%|███████▎  | 842/1153 [22:30<09:40,  1.87s/it] 73%|███████▎  | 843/1153 [22:32<09:15,  1.79s/it] 73%|███████▎  | 844/1153 [22:34<08:57,  1.74s/it] 73%|███████▎  | 845/1153 [22:35<08:45,  1.71s/it] 73%|███████▎  | 846/1153 [22:37<08:16,  1.62s/it] 73%|███████▎  | 847/1153 [22:38<08:15,  1.62s/it] 74%|███████▎  | 848/1153 [22:40<07:50,  1.54s/it] 74%|███████▎  | 849/1153 [22:41<07:44,  1.53s/it] 74%|███████▎  | 850/1153 [22:43<07:54,  1.57s/it] 74%|███████▍  | 851/1153 [22:44<07:56,  1.58s/it] 74%|███████▍  | 852/1153 [22:46<07:58,  1.59s/it] 74%|███████▍  | 853/1153 [22:47<07:36,  1.52s/it] 74%|███████▍  | 854/1153 [22:49<07:42,  1.55s/it] 74%|███████▍  | 855/1153 [22:51<07:47,  1.57s/it] 74%|███████▍  | 856/1153 [22:52<07:30,  1.52s/it] 74%|███████▍  | 857/1153 [22:57<12:54,  2.62s/it] 74%|███████▍  | 858/1153 [22:59<11:27,  2.33s/it] 75%|███████▍  | 859/1153 [23:00<10:04,  2.06s/it] 75%|███████▍  | 860/1153 [23:02<09:11,  1.88s/it] 75%|███████▍  | 861/1153 [23:03<08:30,  1.75s/it] 75%|███████▍  | 862/1153 [23:05<08:21,  1.72s/it] 75%|███████▍  | 863/1153 [23:06<08:10,  1.69s/it] 75%|███████▍  | 864/1153 [23:08<07:49,  1.62s/it] 75%|███████▌  | 865/1153 [23:09<07:42,  1.60s/it] 75%|███████▌  | 866/1153 [23:11<07:30,  1.57s/it] 75%|███████▌  | 867/1153 [23:13<07:33,  1.59s/it] 75%|███████▌  | 868/1153 [23:14<07:20,  1.55s/it] 75%|███████▌  | 869/1153 [23:15<07:11,  1.52s/it] 75%|███████▌  | 870/1153 [23:17<07:05,  1.50s/it] 76%|███████▌  | 871/1153 [23:19<07:17,  1.55s/it] 76%|███████▌  | 872/1153 [23:20<07:23,  1.58s/it] 76%|███████▌  | 873/1153 [23:22<07:11,  1.54s/it] 76%|███████▌  | 874/1153 [23:23<07:06,  1.53s/it] 76%|███████▌  | 875/1153 [23:25<07:01,  1.52s/it] 76%|███████▌  | 876/1153 [23:26<06:51,  1.49s/it] 76%|███████▌  | 877/1153 [23:31<11:39,  2.53s/it] 76%|███████▌  | 878/1153 [23:33<10:11,  2.22s/it] 76%|███████▌  | 879/1153 [23:34<09:06,  1.99s/it] 76%|███████▋  | 880/1153 [23:36<08:23,  1.84s/it] 76%|███████▋  | 881/1153 [23:37<07:50,  1.73s/it] 76%|███████▋  | 882/1153 [23:38<07:24,  1.64s/it] 77%|███████▋  | 883/1153 [23:40<07:11,  1.60s/it] 77%|███████▋  | 884/1153 [23:42<07:16,  1.62s/it] 77%|███████▋  | 885/1153 [23:43<07:14,  1.62s/it] 77%|███████▋  | 886/1153 [23:45<07:13,  1.63s/it] 77%|███████▋  | 887/1153 [23:46<07:14,  1.63s/it] 77%|███████▋  | 888/1153 [23:48<07:14,  1.64s/it] 77%|███████▋  | 889/1153 [23:50<06:56,  1.58s/it] 77%|███████▋  | 890/1153 [23:51<06:49,  1.56s/it] 77%|███████▋  | 891/1153 [23:53<06:57,  1.60s/it] 77%|███████▋  | 892/1153 [23:54<06:47,  1.56s/it] 77%|███████▋  | 893/1153 [23:56<06:35,  1.52s/it] 78%|███████▊  | 894/1153 [23:57<06:34,  1.52s/it] 78%|███████▊  | 895/1153 [23:59<06:29,  1.51s/it] 78%|███████▊  | 896/1153 [24:00<06:22,  1.49s/it] 78%|███████▊  | 897/1153 [24:02<06:21,  1.49s/it] 78%|███████▊  | 898/1153 [24:07<10:54,  2.57s/it] 78%|███████▊  | 899/1153 [24:08<09:30,  2.24s/it] 78%|███████▊  | 900/1153 [24:10<08:31,  2.02s/it] 78%|███████▊  | 901/1153 [24:11<07:48,  1.86s/it] 78%|███████▊  | 902/1153 [24:13<07:15,  1.74s/it] 78%|███████▊  | 903/1153 [24:14<06:57,  1.67s/it] 78%|███████▊  | 904/1153 [24:16<06:42,  1.62s/it] 78%|███████▊  | 905/1153 [24:17<06:28,  1.57s/it] 79%|███████▊  | 906/1153 [24:19<06:23,  1.55s/it] 79%|███████▊  | 907/1153 [24:20<06:13,  1.52s/it] 79%|███████▉  | 908/1153 [24:22<06:12,  1.52s/it] 79%|███████▉  | 909/1153 [24:23<06:09,  1.51s/it] 79%|███████▉  | 910/1153 [24:25<06:04,  1.50s/it] 79%|███████▉  | 911/1153 [24:26<06:05,  1.51s/it] 79%|███████▉  | 912/1153 [24:28<06:01,  1.50s/it] 79%|███████▉  | 913/1153 [24:29<05:58,  1.49s/it] 79%|███████▉  | 914/1153 [24:31<05:57,  1.49s/it] 79%|███████▉  | 915/1153 [24:32<05:54,  1.49s/it] 79%|███████▉  | 916/1153 [24:34<05:54,  1.50s/it] 80%|███████▉  | 917/1153 [24:35<05:52,  1.49s/it] 80%|███████▉  | 918/1153 [24:36<05:49,  1.49s/it] 80%|███████▉  | 919/1153 [24:42<10:01,  2.57s/it] 80%|███████▉  | 920/1153 [24:43<08:57,  2.31s/it] 80%|███████▉  | 921/1153 [24:45<08:09,  2.11s/it] 80%|███████▉  | 922/1153 [24:47<07:37,  1.98s/it] 80%|████████  | 923/1153 [24:48<07:10,  1.87s/it] 80%|████████  | 924/1153 [24:50<06:51,  1.80s/it] 80%|████████  | 925/1153 [24:51<06:39,  1.75s/it] 80%|████████  | 926/1153 [24:53<06:31,  1.72s/it] 80%|████████  | 927/1153 [24:55<06:07,  1.63s/it] 80%|████████  | 928/1153 [24:56<06:07,  1.63s/it] 81%|████████  | 929/1153 [24:58<06:07,  1.64s/it] 81%|████████  | 930/1153 [24:59<06:03,  1.63s/it] 81%|████████  | 931/1153 [25:01<06:01,  1.63s/it] 81%|████████  | 932/1153 [25:03<06:00,  1.63s/it] 81%|████████  | 933/1153 [25:04<06:00,  1.64s/it] 81%|████████  | 934/1153 [25:06<05:59,  1.64s/it] 81%|████████  | 935/1153 [25:08<05:59,  1.65s/it] 81%|████████  | 936/1153 [25:09<06:00,  1.66s/it] 81%|████████▏ | 937/1153 [25:15<09:55,  2.76s/it] 81%|████████▏ | 938/1153 [25:16<08:42,  2.43s/it] 81%|████████▏ | 939/1153 [25:18<07:50,  2.20s/it] 82%|████████▏ | 940/1153 [25:20<07:13,  2.04s/it] 82%|████████▏ | 941/1153 [25:21<06:48,  1.93s/it] 82%|████████▏ | 942/1153 [25:23<06:27,  1.84s/it] 82%|████████▏ | 943/1153 [25:25<06:12,  1.78s/it] 82%|████████▏ | 944/1153 [25:26<06:02,  1.74s/it] 82%|████████▏ | 945/1153 [25:28<05:49,  1.68s/it] 82%|████████▏ | 946/1153 [25:29<05:38,  1.64s/it] 82%|████████▏ | 947/1153 [25:31<05:37,  1.64s/it] 82%|████████▏ | 948/1153 [25:33<05:41,  1.67s/it] 82%|████████▏ | 949/1153 [25:34<05:43,  1.68s/it] 82%|████████▏ | 950/1153 [25:36<05:39,  1.67s/it] 82%|████████▏ | 951/1153 [25:38<05:38,  1.68s/it] 83%|████████▎ | 952/1153 [25:39<05:36,  1.67s/it] 83%|████████▎ | 953/1153 [25:41<05:35,  1.68s/it] 83%|████████▎ | 954/1153 [25:43<05:25,  1.64s/it] 83%|████████▎ | 955/1153 [25:44<05:25,  1.64s/it] 83%|████████▎ | 956/1153 [25:50<08:56,  2.72s/it] 83%|████████▎ | 957/1153 [25:51<07:52,  2.41s/it] 83%|████████▎ | 958/1153 [25:53<07:08,  2.20s/it] 83%|████████▎ | 959/1153 [25:55<06:33,  2.03s/it] 83%|████████▎ | 960/1153 [25:56<06:09,  1.91s/it] 83%|████████▎ | 961/1153 [25:57<04:58,  1.55s/it] 83%|████████▎ | 962/1153 [25:58<04:49,  1.52s/it] 84%|████████▎ | 963/1153 [26:00<04:55,  1.56s/it] 84%|████████▎ | 964/1153 [26:02<05:00,  1.59s/it] 84%|████████▎ | 965/1153 [26:03<04:58,  1.59s/it] 84%|████████▍ | 966/1153 [26:05<04:59,  1.60s/it] 84%|████████▍ | 967/1153 [26:06<04:52,  1.57s/it] 84%|████████▍ | 968/1153 [26:08<04:57,  1.61s/it] 84%|████████▍ | 969/1153 [26:10<04:56,  1.61s/it] 84%|████████▍ | 970/1153 [26:11<04:30,  1.48s/it] 84%|████████▍ | 971/1153 [26:11<03:34,  1.18s/it] 84%|████████▍ | 972/1153 [26:12<03:00,  1.00it/s] 84%|████████▍ | 973/1153 [26:13<03:07,  1.04s/it] 84%|████████▍ | 974/1153 [26:15<03:38,  1.22s/it] 85%|████████▍ | 975/1153 [26:16<04:00,  1.35s/it] 85%|████████▍ | 976/1153 [26:18<04:15,  1.44s/it] 85%|████████▍ | 977/1153 [26:20<04:26,  1.51s/it] 85%|████████▍ | 978/1153 [26:25<07:45,  2.66s/it] 85%|████████▍ | 979/1153 [26:27<06:49,  2.36s/it] 85%|████████▍ | 980/1153 [26:28<06:10,  2.14s/it] 85%|████████▌ | 981/1153 [26:30<05:42,  1.99s/it] 85%|████████▌ | 982/1153 [26:32<05:22,  1.88s/it] 85%|████████▌ | 983/1153 [26:33<05:10,  1.82s/it] 85%|████████▌ | 984/1153 [26:35<04:52,  1.73s/it] 85%|████████▌ | 985/1153 [26:36<04:47,  1.71s/it] 86%|████████▌ | 986/1153 [26:38<04:45,  1.71s/it] 86%|████████▌ | 987/1153 [26:40<04:42,  1.70s/it] 86%|████████▌ | 988/1153 [26:41<04:37,  1.68s/it] 86%|████████▌ | 989/1153 [26:43<04:33,  1.67s/it] 86%|████████▌ | 990/1153 [26:45<04:32,  1.67s/it] 86%|████████▌ | 991/1153 [26:46<04:20,  1.61s/it] 86%|████████▌ | 992/1153 [26:48<04:20,  1.62s/it] 86%|████████▌ | 993/1153 [26:50<04:20,  1.63s/it] 86%|████████▌ | 994/1153 [26:51<04:21,  1.64s/it] 86%|████████▋ | 995/1153 [26:53<04:21,  1.66s/it] 86%|████████▋ | 996/1153 [26:54<04:12,  1.61s/it] 86%|████████▋ | 997/1153 [27:00<07:01,  2.70s/it] 87%|████████▋ | 998/1153 [27:01<06:01,  2.33s/it] 87%|████████▋ | 999/1153 [27:03<05:20,  2.08s/it] 87%|████████▋ | 1000/1153 [27:04<04:58,  1.95s/it] 87%|████████▋ | 1001/1153 [27:06<04:43,  1.86s/it] 87%|████████▋ | 1002/1153 [27:07<04:25,  1.76s/it] 87%|████████▋ | 1003/1153 [27:09<04:12,  1.68s/it] 87%|████████▋ | 1004/1153 [27:10<04:00,  1.62s/it] 87%|████████▋ | 1005/1153 [27:12<04:01,  1.63s/it] 87%|████████▋ | 1006/1153 [27:14<04:00,  1.63s/it] 87%|████████▋ | 1007/1153 [27:15<03:51,  1.59s/it] 87%|████████▋ | 1008/1153 [27:17<03:44,  1.55s/it] 88%|████████▊ | 1009/1153 [27:18<03:41,  1.54s/it] 88%|████████▊ | 1010/1153 [27:20<03:36,  1.51s/it] 88%|████████▊ | 1011/1153 [27:21<03:35,  1.51s/it] 88%|████████▊ | 1012/1153 [27:23<03:33,  1.51s/it] 88%|████████▊ | 1013/1153 [27:24<03:39,  1.57s/it] 88%|████████▊ | 1014/1153 [27:26<03:34,  1.54s/it] 88%|████████▊ | 1015/1153 [27:27<03:29,  1.52s/it] 88%|████████▊ | 1016/1153 [27:29<03:27,  1.52s/it] 88%|████████▊ | 1017/1153 [27:30<03:24,  1.51s/it] 88%|████████▊ | 1018/1153 [27:32<03:24,  1.52s/it] 88%|████████▊ | 1019/1153 [27:37<05:45,  2.58s/it] 88%|████████▊ | 1020/1153 [27:38<05:00,  2.26s/it] 89%|████████▊ | 1021/1153 [27:40<04:28,  2.03s/it] 89%|████████▊ | 1022/1153 [27:41<04:04,  1.87s/it] 89%|████████▊ | 1023/1153 [27:43<03:46,  1.74s/it] 89%|████████▉ | 1024/1153 [27:44<03:35,  1.67s/it] 89%|████████▉ | 1025/1153 [27:46<03:26,  1.61s/it] 89%|████████▉ | 1026/1153 [27:48<03:29,  1.65s/it] 89%|████████▉ | 1027/1153 [27:49<03:31,  1.68s/it] 89%|████████▉ | 1028/1153 [27:51<03:29,  1.67s/it] 89%|████████▉ | 1029/1153 [27:53<03:28,  1.68s/it] 89%|████████▉ | 1030/1153 [27:54<03:26,  1.68s/it] 89%|████████▉ | 1031/1153 [27:56<03:25,  1.68s/it] 90%|████████▉ | 1032/1153 [27:58<03:24,  1.69s/it] 90%|████████▉ | 1033/1153 [27:59<03:15,  1.63s/it] 90%|████████▉ | 1034/1153 [28:01<03:17,  1.66s/it] 90%|████████▉ | 1035/1153 [28:02<03:11,  1.62s/it] 90%|████████▉ | 1036/1153 [28:04<03:06,  1.60s/it] 90%|████████▉ | 1037/1153 [28:06<03:01,  1.57s/it] 90%|█████████ | 1038/1153 [28:11<05:00,  2.61s/it] 90%|█████████ | 1039/1153 [28:12<04:21,  2.29s/it] 90%|█████████ | 1040/1153 [28:14<03:52,  2.05s/it] 90%|█████████ | 1041/1153 [28:15<03:30,  1.88s/it] 90%|█████████ | 1042/1153 [28:17<03:16,  1.77s/it] 90%|█████████ | 1043/1153 [28:18<03:06,  1.69s/it] 91%|█████████ | 1044/1153 [28:20<02:55,  1.61s/it] 91%|█████████ | 1045/1153 [28:21<02:51,  1.59s/it] 91%|█████████ | 1046/1153 [28:23<02:46,  1.56s/it] 91%|█████████ | 1047/1153 [28:24<02:44,  1.55s/it] 91%|█████████ | 1048/1153 [28:26<02:41,  1.54s/it] 91%|█████████ | 1049/1153 [28:27<02:37,  1.52s/it] 91%|█████████ | 1050/1153 [28:29<02:35,  1.51s/it] 91%|█████████ | 1051/1153 [28:30<02:33,  1.50s/it] 91%|█████████ | 1052/1153 [28:32<02:30,  1.49s/it] 91%|█████████▏| 1053/1153 [28:33<02:30,  1.50s/it] 91%|█████████▏| 1054/1153 [28:35<02:28,  1.50s/it] 92%|█████████▏| 1055/1153 [28:36<02:27,  1.51s/it] 92%|█████████▏| 1056/1153 [28:38<02:25,  1.51s/it] 92%|█████████▏| 1057/1153 [28:39<02:24,  1.50s/it] 92%|█████████▏| 1058/1153 [28:41<02:23,  1.51s/it] 92%|█████████▏| 1059/1153 [28:42<02:21,  1.51s/it] 92%|█████████▏| 1060/1153 [28:47<04:02,  2.61s/it] 92%|█████████▏| 1061/1153 [28:49<03:36,  2.35s/it] 92%|█████████▏| 1062/1153 [28:51<03:17,  2.17s/it] 92%|█████████▏| 1063/1153 [28:52<03:02,  2.03s/it] 92%|█████████▏| 1064/1153 [28:54<02:52,  1.94s/it] 92%|█████████▏| 1065/1153 [28:56<02:43,  1.86s/it] 92%|█████████▏| 1066/1153 [28:58<02:36,  1.80s/it] 93%|█████████▎| 1067/1153 [28:59<02:32,  1.77s/it] 93%|█████████▎| 1068/1153 [29:01<02:28,  1.75s/it] 93%|█████████▎| 1069/1153 [29:03<02:25,  1.73s/it] 93%|█████████▎| 1070/1153 [29:04<02:24,  1.74s/it] 93%|█████████▎| 1071/1153 [29:06<02:21,  1.73s/it] 93%|█████████▎| 1072/1153 [29:08<02:18,  1.71s/it] 93%|█████████▎| 1073/1153 [29:09<02:16,  1.70s/it] 93%|█████████▎| 1074/1153 [29:11<02:14,  1.70s/it] 93%|█████████▎| 1075/1153 [29:13<02:13,  1.71s/it] 93%|█████████▎| 1076/1153 [29:15<02:11,  1.71s/it] 93%|█████████▎| 1077/1153 [29:16<02:08,  1.70s/it] 93%|█████████▎| 1078/1153 [29:18<02:06,  1.69s/it] 94%|█████████▎| 1079/1153 [29:19<01:58,  1.60s/it] 94%|█████████▎| 1080/1153 [29:25<03:18,  2.73s/it] 94%|█████████▍| 1081/1153 [29:26<02:54,  2.42s/it] 94%|█████████▍| 1082/1153 [29:28<02:36,  2.21s/it] 94%|█████████▍| 1083/1153 [29:30<02:24,  2.06s/it] 94%|█████████▍| 1084/1153 [29:31<02:14,  1.96s/it] 94%|█████████▍| 1085/1153 [29:33<02:07,  1.87s/it] 94%|█████████▍| 1086/1153 [29:35<02:01,  1.81s/it] 94%|█████████▍| 1087/1153 [29:36<01:54,  1.73s/it] 94%|█████████▍| 1088/1153 [29:38<01:51,  1.71s/it] 94%|█████████▍| 1089/1153 [29:40<01:49,  1.70s/it] 95%|█████████▍| 1090/1153 [29:41<01:47,  1.70s/it] 95%|█████████▍| 1091/1153 [29:43<01:45,  1.70s/it] 95%|█████████▍| 1092/1153 [29:45<01:44,  1.71s/it] 95%|█████████▍| 1093/1153 [29:47<01:42,  1.71s/it] 95%|█████████▍| 1094/1153 [29:48<01:40,  1.70s/it] 95%|█████████▍| 1095/1153 [29:50<01:35,  1.65s/it] 95%|█████████▌| 1096/1153 [29:52<01:35,  1.67s/it] 95%|█████████▌| 1097/1153 [29:53<01:34,  1.68s/it] 95%|█████████▌| 1098/1153 [29:55<01:33,  1.69s/it] 95%|█████████▌| 1099/1153 [30:00<02:30,  2.79s/it] 95%|█████████▌| 1100/1153 [30:01<01:57,  2.22s/it] 95%|█████████▌| 1101/1153 [30:03<01:47,  2.07s/it] 96%|█████████▌| 1102/1153 [30:05<01:39,  1.96s/it] 96%|█████████▌| 1103/1153 [30:06<01:34,  1.89s/it] 96%|█████████▌| 1104/1153 [30:08<01:29,  1.83s/it] 96%|█████████▌| 1105/1153 [30:09<01:19,  1.65s/it] 96%|█████████▌| 1106/1153 [30:11<01:18,  1.68s/it] 96%|█████████▌| 1107/1153 [30:12<01:06,  1.45s/it] 96%|█████████▌| 1108/1153 [30:14<01:08,  1.52s/it] 96%|█████████▌| 1109/1153 [30:14<00:55,  1.27s/it] 96%|█████████▋| 1110/1153 [30:16<00:56,  1.31s/it] 96%|█████████▋| 1111/1153 [30:17<01:00,  1.43s/it] 96%|█████████▋| 1112/1153 [30:19<01:02,  1.52s/it] 97%|█████████▋| 1113/1153 [30:21<01:02,  1.56s/it] 97%|█████████▋| 1114/1153 [30:22<01:02,  1.60s/it] 97%|█████████▋| 1115/1153 [30:24<01:01,  1.63s/it] 97%|█████████▋| 1116/1153 [30:26<01:01,  1.65s/it] 97%|█████████▋| 1117/1153 [30:28<01:00,  1.67s/it] 97%|█████████▋| 1118/1153 [30:29<00:58,  1.68s/it] 97%|█████████▋| 1119/1153 [30:31<00:57,  1.68s/it] 97%|█████████▋| 1120/1153 [30:36<01:32,  2.81s/it] 97%|█████████▋| 1121/1153 [30:38<01:18,  2.46s/it] 97%|█████████▋| 1122/1153 [30:40<01:07,  2.18s/it] 97%|█████████▋| 1123/1153 [30:41<01:01,  2.04s/it] 97%|█████████▋| 1124/1153 [30:43<00:56,  1.94s/it] 98%|█████████▊| 1125/1153 [30:45<00:52,  1.87s/it] 98%|█████████▊| 1126/1153 [30:46<00:48,  1.78s/it] 98%|█████████▊| 1127/1153 [30:48<00:45,  1.76s/it] 98%|█████████▊| 1128/1153 [30:50<00:43,  1.74s/it] 98%|█████████▊| 1129/1153 [30:51<00:41,  1.73s/it] 98%|█████████▊| 1130/1153 [30:53<00:39,  1.73s/it] 98%|█████████▊| 1131/1153 [30:55<00:36,  1.67s/it] 98%|█████████▊| 1132/1153 [30:56<00:35,  1.68s/it] 98%|█████████▊| 1133/1153 [30:58<00:33,  1.69s/it] 98%|█████████▊| 1134/1153 [31:00<00:32,  1.71s/it] 98%|█████████▊| 1135/1153 [31:02<00:30,  1.71s/it] 99%|█████████▊| 1136/1153 [31:03<00:29,  1.72s/it] 99%|█████████▊| 1137/1153 [31:05<00:27,  1.72s/it] 99%|█████████▊| 1138/1153 [31:06<00:24,  1.65s/it] 99%|█████████▉| 1139/1153 [31:08<00:22,  1.62s/it] 99%|█████████▉| 1140/1153 [31:13<00:35,  2.71s/it] 99%|█████████▉| 1141/1153 [31:15<00:28,  2.37s/it] 99%|█████████▉| 1142/1153 [31:16<00:23,  2.12s/it] 99%|█████████▉| 1143/1153 [31:18<00:19,  1.95s/it] 99%|█████████▉| 1144/1153 [31:20<00:16,  1.86s/it] 99%|█████████▉| 1145/1153 [31:21<00:14,  1.81s/it] 99%|█████████▉| 1146/1153 [31:23<00:12,  1.73s/it] 99%|█████████▉| 1147/1153 [31:25<00:10,  1.74s/it]100%|█████████▉| 1148/1153 [31:26<00:08,  1.68s/it]100%|█████████▉| 1149/1153 [31:28<00:06,  1.63s/it]100%|█████████▉| 1150/1153 [31:29<00:04,  1.60s/it]100%|█████████▉| 1151/1153 [31:31<00:03,  1.57s/it]100%|█████████▉| 1152/1153 [31:32<00:01,  1.62s/it]100%|██████████| 1153/1153 [31:34<00:00,  1.46s/it]100%|██████████| 1153/1153 [31:34<00:00,  1.64s/it]
147540
147540
saving data 147540 to ./output/DuEE1.0/role/test_result.json
trigger predict 147540 load from ./output/DuEE1.0/role/test_result.json
role predict 147540 load from ./output/DuEE1.0/role/test_result.json
schema 68 load from ./conf/DuEE1.0/event_schema.json
submit data 147540 save to ./output/DuEE1.0/surbot_news_together.json
********** Done ***************
nohup: ignoring input
*********** data_prepare *************
开始数据预处理
Traceback (most recent call last):
  File "/data/qingyang/event_extration/DuEE_merge/data/DuEE1.0/our_test.py", line 7, in <module>
    df = pd.read_excel('新闻_8k_带摘要.xlsx')
  File "/data/qingyang/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py", line 311, in wrapper
    return func(*args, **kwargs)
  File "/data/qingyang/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py", line 457, in read_excel
    io = ExcelFile(io, storage_options=storage_options, engine=engine)
  File "/data/qingyang/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py", line 1376, in __init__
    ext = inspect_excel_format(
  File "/data/qingyang/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py", line 1250, in inspect_excel_format
    with get_handle(
  File "/data/qingyang/anaconda3/lib/python3.9/site-packages/pandas/io/common.py", line 798, in get_handle
    handle = open(handle, ioargs.mode)
FileNotFoundError: [Errno 2] No such file or directory: '新闻_8k_带摘要.xlsx'
开始处理tag.dict 和 tsv文件
Traceback (most recent call last):
  File "/data/qingyang/event_extration/DuEE_merge/duee_1_data_prepare.py", line 16, in <module>
    from data.data_utils import schema_process, data_process
  File "/data/qingyang/event_extration/DuEE_merge/data/data_utils.py", line 11, in <module>
    from utils.utils import read_by_lines
  File "/data/qingyang/event_extration/DuEE_merge/utils/utils.py", line 17, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
********** train start *************
Traceback (most recent call last):
  File "/data/qingyang/event_extration/DuEE_merge/run_ner.py", line 7, in <module>
    from torch.cuda.amp import autocast, GradScaler
ModuleNotFoundError: No module named 'torch'
********** predict start ***********
Traceback (most recent call last):
  File "/data/qingyang/event_extration/DuEE_merge/predict_ner.py", line 11, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
Traceback (most recent call last):
  File "/data/qingyang/event_extration/DuEE_merge/duee_1_my_postprocess.py", line 19, in <module>
    from utils.utils import read_by_lines, write_by_lines, extract_result
  File "/data/qingyang/event_extration/DuEE_merge/utils/utils.py", line 17, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
********** Done ***************
