nohup: ignoring input
*********** data_prepare *************
开始数据预处理
开始处理tag.dict 和 tsv文件

=================DUEE 1.0 DATASET==============

=================start schema process==============
input path ./conf/DuEE1.0/event_schema.json
save trigger tag 393 at ./conf/DuEE1.0/role_tag.dict
=================end schema process===============

=================start sentence process==============

----trigger------for dir ./data/DuEE1.0 to ./data/DuEE1.0/trigger
train 6161 dev 1622 test 147541

----role------for dir ./data/DuEE1.0 to ./data/DuEE1.0/role
train 7284 dev 1914 test 147541
=================end schema process==============
********** train start *************
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 5, 
    "event_type": "role", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 1e-05, 
    "max_len": 150, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 30.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 48, 
    "per_gpu_train_batch_size": 64, 
    "seed": 66, 
    "stride": 100, 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/7283 [00:00<?, ?it/s]tokenizing...:   4%|▎         | 273/7283 [00:00<00:02, 2728.66it/s]tokenizing...:   8%|▊         | 618/7283 [00:00<00:02, 3152.08it/s]tokenizing...:  13%|█▎        | 934/7283 [00:00<00:02, 2909.12it/s]tokenizing...:  17%|█▋        | 1227/7283 [00:00<00:02, 2790.85it/s]tokenizing...:  21%|██        | 1510/7283 [00:00<00:02, 2801.80it/s]tokenizing...:  25%|██▍       | 1799/7283 [00:00<00:01, 2827.12it/s]tokenizing...:  29%|██▊       | 2083/7283 [00:00<00:01, 2773.14it/s]tokenizing...:  32%|███▏      | 2361/7283 [00:00<00:01, 2689.29it/s]tokenizing...:  36%|███▌      | 2631/7283 [00:00<00:01, 2618.42it/s]tokenizing...:  40%|████      | 2915/7283 [00:01<00:01, 2682.45it/s]tokenizing...:  44%|████▎     | 3184/7283 [00:01<00:01, 2672.04it/s]tokenizing...:  47%|████▋     | 3452/7283 [00:01<00:01, 2668.58it/s]tokenizing...:  51%|█████     | 3720/7283 [00:01<00:01, 2637.24it/s]tokenizing...:  55%|█████▍    | 3984/7283 [00:01<00:01, 2628.81it/s]tokenizing...:  58%|█████▊    | 4248/7283 [00:01<00:01, 2588.30it/s]tokenizing...:  62%|██████▏   | 4511/7283 [00:01<00:01, 2598.02it/s]tokenizing...:  66%|██████▌   | 4771/7283 [00:01<00:00, 2585.77it/s]tokenizing...:  70%|██████▉   | 5068/7283 [00:01<00:00, 2698.96it/s]tokenizing...:  74%|███████▎  | 5358/7283 [00:01<00:00, 2756.79it/s]tokenizing...:  77%|███████▋  | 5634/7283 [00:02<00:00, 1669.67it/s]tokenizing...:  80%|████████  | 5854/7283 [00:02<00:00, 1752.64it/s]tokenizing...:  84%|████████▍ | 6128/7283 [00:02<00:00, 1973.01it/s]tokenizing...:  88%|████████▊ | 6420/7283 [00:02<00:00, 2199.70it/s]tokenizing...:  92%|█████████▏| 6670/7283 [00:02<00:00, 2195.93it/s]tokenizing...:  95%|█████████▍| 6911/7283 [00:02<00:00, 2157.95it/s]tokenizing...:  99%|█████████▉| 7218/7283 [00:02<00:00, 2394.41it/s]tokenizing...: 100%|██████████| 7283/7283 [00:02<00:00, 2465.91it/s]
tokenizing...:   0%|          | 0/1913 [00:00<?, ?it/s]tokenizing...:  17%|█▋        | 317/1913 [00:00<00:00, 3157.06it/s]tokenizing...:  33%|███▎      | 633/1913 [00:00<00:00, 2675.60it/s]tokenizing...:  47%|████▋     | 906/1913 [00:00<00:00, 2624.24it/s]tokenizing...:  61%|██████    | 1171/1913 [00:00<00:00, 2621.75it/s]tokenizing...:  75%|███████▌  | 1435/1913 [00:00<00:00, 2608.52it/s]tokenizing...:  89%|████████▊ | 1697/1913 [00:00<00:00, 2420.64it/s]tokenizing...: 100%|██████████| 1913/1913 [00:00<00:00, 2543.14it/s]
09/25/2022 13:38:39 - INFO - root -   The nums of the train_dataset features is 7283
09/25/2022 13:38:39 - INFO - root -   The nums of the eval_dataset features is 1913
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
09/25/2022 13:38:44 - INFO - root -   ***** Running train *****
********************************************************************************
/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:52  batch_loss: 6.3567 [Training] 2/114 [..............................] - ETA: 1:08  batch_loss: 6.3462 [Training] 3/114 [..............................] - ETA: 54s  batch_loss: 6.3454 [Training] 4/114 [>.............................] - ETA: 47s  batch_loss: 6.3434 [Training] 5/114 [>.............................] - ETA: 42s  batch_loss: 6.3099 [Training] 6/114 [>.............................] - ETA: 39s  batch_loss: 6.2636 [Training] 7/114 [>.............................] - ETA: 37s  batch_loss: 6.2068 [Training] 8/114 [=>............................] - ETA: 35s  batch_loss: 6.1470 [Training] 9/114 [=>............................] - ETA: 34s  batch_loss: 6.0878 [Training] 10/114 [=>............................] - ETA: 33s  batch_loss: 6.0180 [Training] 11/114 [=>............................] - ETA: 32s  batch_loss: 5.9487 [Training] 12/114 [==>...........................] - ETA: 31s  batch_loss: 5.8795 [Training] 13/114 [==>...........................] - ETA: 30s  batch_loss: 5.8085 [Training] 14/114 [==>...........................] - ETA: 29s  batch_loss: 5.7385 [Training] 15/114 [==>...........................] - ETA: 29s  batch_loss: 5.6593 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 5.5853 [Training] 17/114 [===>..........................] - ETA: 27s  batch_loss: 5.5020 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 5.4179 [Training] 19/114 [====>.........................] - ETA: 26s  batch_loss: 5.3343 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 5.2574 [Training] 21/114 [====>.........................] - ETA: 26s  batch_loss: 5.1778 [Training] 22/114 [====>.........................] - ETA: 25s  batch_loss: 5.0968 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 5.0116 [Training] 24/114 [=====>........................] - ETA: 24s  batch_loss: 4.9302 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 4.8373 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 4.7412 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 4.6634 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 4.5936 [Training] 29/114 [======>.......................] - ETA: 22s  batch_loss: 4.5230 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 4.4537 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 4.3878 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 4.3223 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 4.2575 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 4.2075 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 4.1628 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 4.1163 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 4.0742 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 4.0258 [Training] 39/114 [=========>....................] - ETA: 19s  batch_loss: 3.9888 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 3.9448 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 3.9006 [Training] 42/114 [==========>...................] - ETA: 18s  batch_loss: 3.8615 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 3.8301 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 3.8060 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 3.7769 [Training] 46/114 [===========>..................] - ETA: 17s  batch_loss: 3.7441 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 3.7195 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 3.6887 [Training] 49/114 [===========>..................] - ETA: 16s  batch_loss: 3.6626 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 3.6331 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 3.6043 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 3.5747 [Training] 53/114 [============>.................] - ETA: 15s  batch_loss: 3.5468 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 3.5270 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 3.5053 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 3.4824 [Training] 57/114 [==============>...............] - ETA: 14s  batch_loss: 3.4620 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 3.4376 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 3.4171 [Training] 60/114 [==============>...............] - ETA: 13s  batch_loss: 3.3939 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 3.3751 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 3.3573 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 3.3393 [Training] 64/114 [===============>..............] - ETA: 12s  batch_loss: 3.3257 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 3.3069 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 3.2904 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 3.2702 [Training] 68/114 [================>.............] - ETA: 11s  batch_loss: 3.2539 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 3.2390 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 3.2226 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 3.2117 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 3.1994 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 3.1817 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 3.1696 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 3.1546 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 3.1409 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 3.1251 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 3.1104 [Training] 79/114 [===================>..........] - ETA: 8s  batch_loss: 3.1015 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 3.0902 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 3.0785 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 3.0646 [Training] 83/114 [====================>.........] - ETA: 7s  batch_loss: 3.0514 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 3.0416 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 3.0306 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 3.0225 [Training] 87/114 [=====================>........] - ETA: 6s  batch_loss: 3.0100 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 3.0024 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 2.9933 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 2.9842 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 2.9741 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 2.9633 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 2.9545 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 2.9481 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 2.9374 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 2.9294 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 2.9194 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 2.9114 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 2.9006 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 2.8943 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 2.8881 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 2.8804 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 2.8730 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 2.8625 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 2.8520 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 2.8418 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 2.8340 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 2.8255 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 2.8174 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 2.8128 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 2.8055 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 2.7977 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 2.7926 [Training] 114/114 [==============================] 254.8ms/step  batch_loss: 2.7838 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:39:19 - INFO - root -   The F1-score is 0.023728385712482647
09/25/2022 13:39:19 - INFO - root -   the best eval f1 is 0.0237, saving model !!
09/25/2022 13:39:22 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:31  batch_loss: 2.1602 [Training] 2/114 [..............................] - ETA: 59s  batch_loss: 2.0579 [Training] 3/114 [..............................] - ETA: 48s  batch_loss: 2.1258 [Training] 4/114 [>.............................] - ETA: 43s  batch_loss: 2.0667 [Training] 5/114 [>.............................] - ETA: 40s  batch_loss: 2.0519 [Training] 6/114 [>.............................] - ETA: 37s  batch_loss: 2.0597 [Training] 7/114 [>.............................] - ETA: 35s  batch_loss: 2.0438 [Training] 8/114 [=>............................] - ETA: 34s  batch_loss: 2.0051 [Training] 9/114 [=>............................] - ETA: 33s  batch_loss: 1.9868 [Training] 10/114 [=>............................] - ETA: 32s  batch_loss: 1.9556 [Training] 11/114 [=>............................] - ETA: 31s  batch_loss: 1.9391 [Training] 12/114 [==>...........................] - ETA: 31s  batch_loss: 1.9568 [Training] 13/114 [==>...........................] - ETA: 30s  batch_loss: 1.9581 [Training] 14/114 [==>...........................] - ETA: 29s  batch_loss: 1.9416 [Training] 15/114 [==>...........................] - ETA: 29s  batch_loss: 1.9392 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 1.9261 [Training] 17/114 [===>..........................] - ETA: 28s  batch_loss: 1.9282 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 1.9222 [Training] 19/114 [====>.........................] - ETA: 27s  batch_loss: 1.9083 [Training] 20/114 [====>.........................] - ETA: 27s  batch_loss: 1.9100 [Training] 21/114 [====>.........................] - ETA: 26s  batch_loss: 1.9197 [Training] 22/114 [====>.........................] - ETA: 26s  batch_loss: 1.9168 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 1.9042 [Training] 24/114 [=====>........................] - ETA: 25s  batch_loss: 1.9146 [Training] 25/114 [=====>........................] - ETA: 25s  batch_loss: 1.9058 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 1.8961 [Training] 27/114 [======>.......................] - ETA: 24s  batch_loss: 1.8875 [Training] 28/114 [======>.......................] - ETA: 24s  batch_loss: 1.8865 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 1.8868 [Training] 30/114 [======>.......................] - ETA: 23s  batch_loss: 1.8801 [Training] 31/114 [=======>......................] - ETA: 23s  batch_loss: 1.8722 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 1.8732 [Training] 33/114 [=======>......................] - ETA: 22s  batch_loss: 1.8667 [Training] 34/114 [=======>......................] - ETA: 22s  batch_loss: 1.8622 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 1.8645 [Training] 36/114 [========>.....................] - ETA: 21s  batch_loss: 1.8664 [Training] 37/114 [========>.....................] - ETA: 21s  batch_loss: 1.8698 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 1.8610 [Training] 39/114 [=========>....................] - ETA: 20s  batch_loss: 1.8611 [Training] 40/114 [=========>....................] - ETA: 20s  batch_loss: 1.8619 [Training] 41/114 [=========>....................] - ETA: 20s  batch_loss: 1.8564 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 1.8514 [Training] 43/114 [==========>...................] - ETA: 19s  batch_loss: 1.8502 [Training] 44/114 [==========>...................] - ETA: 19s  batch_loss: 1.8539 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 1.8445 [Training] 46/114 [===========>..................] - ETA: 18s  batch_loss: 1.8421 [Training] 47/114 [===========>..................] - ETA: 18s  batch_loss: 1.8379 [Training] 48/114 [===========>..................] - ETA: 18s  batch_loss: 1.8398 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 1.8344 [Training] 50/114 [============>.................] - ETA: 17s  batch_loss: 1.8276 [Training] 51/114 [============>.................] - ETA: 17s  batch_loss: 1.8229 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 1.8215 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 1.8173 [Training] 54/114 [=============>................] - ETA: 16s  batch_loss: 1.8155 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 1.8113 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 1.8052 [Training] 57/114 [==============>...............] - ETA: 15s  batch_loss: 1.8033 [Training] 58/114 [==============>...............] - ETA: 15s  batch_loss: 1.7996 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 1.7975 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 1.7942 [Training] 61/114 [===============>..............] - ETA: 14s  batch_loss: 1.7881 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 1.7868 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 1.7831 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 1.7807 [Training] 65/114 [================>.............] - ETA: 13s  batch_loss: 1.7777 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 1.7750 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 1.7695 [Training] 68/114 [================>.............] - ETA: 12s  batch_loss: 1.7669 [Training] 69/114 [=================>............] - ETA: 12s  batch_loss: 1.7644 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 1.7605 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 1.7605 [Training] 72/114 [=================>............] - ETA: 11s  batch_loss: 1.7580 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 1.7592 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 1.7560 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 1.7536 [Training] 76/114 [===================>..........] - ETA: 10s  batch_loss: 1.7538 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 1.7522 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 1.7478 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 1.7453 [Training] 80/114 [====================>.........] - ETA: 9s  batch_loss: 1.7446 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 1.7438 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 1.7429 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 1.7430 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 1.7413 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 1.7379 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 1.7353 [Training] 87/114 [=====================>........] - ETA: 7s  batch_loss: 1.7328 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 1.7312 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 1.7295 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 1.7293 [Training] 91/114 [======================>.......] - ETA: 6s  batch_loss: 1.7282 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 1.7263 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 1.7241 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 1.7213 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 1.7197 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 1.7194 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 1.7182 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 1.7148 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 1.7129 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 1.7100 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 1.7074 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 1.7057 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 1.7055 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 1.7034 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 1.7028 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 1.7007 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 1.6996 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 1.6985 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 1.6978 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 1.6961 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 1.6937 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 1.6922 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 1.6889 [Training] 114/114 [==============================] 261.2ms/step  batch_loss: 1.6868 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:39:58 - INFO - root -   The F1-score is 0.08937809576224545
09/25/2022 13:39:58 - INFO - root -   the best eval f1 is 0.0894, saving model !!
09/25/2022 13:40:01 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:29  batch_loss: 1.2623 [Training] 2/114 [..............................] - ETA: 58s  batch_loss: 1.3154 [Training] 3/114 [..............................] - ETA: 47s  batch_loss: 1.3988 [Training] 4/114 [>.............................] - ETA: 42s  batch_loss: 1.3638 [Training] 5/114 [>.............................] - ETA: 39s  batch_loss: 1.3837 [Training] 6/114 [>.............................] - ETA: 36s  batch_loss: 1.4117 [Training] 7/114 [>.............................] - ETA: 34s  batch_loss: 1.4372 [Training] 8/114 [=>............................] - ETA: 33s  batch_loss: 1.4394 [Training] 9/114 [=>............................] - ETA: 32s  batch_loss: 1.4523 [Training] 10/114 [=>............................] - ETA: 30s  batch_loss: 1.4536 [Training] 11/114 [=>............................] - ETA: 30s  batch_loss: 1.4582 [Training] 12/114 [==>...........................] - ETA: 29s  batch_loss: 1.4589 [Training] 13/114 [==>...........................] - ETA: 28s  batch_loss: 1.4695 [Training] 14/114 [==>...........................] - ETA: 28s  batch_loss: 1.4640 [Training] 15/114 [==>...........................] - ETA: 27s  batch_loss: 1.4581 [Training] 16/114 [===>..........................] - ETA: 27s  batch_loss: 1.4569 [Training] 17/114 [===>..........................] - ETA: 26s  batch_loss: 1.4454 [Training] 18/114 [===>..........................] - ETA: 26s  batch_loss: 1.4464 [Training] 19/114 [====>.........................] - ETA: 26s  batch_loss: 1.4413 [Training] 20/114 [====>.........................] - ETA: 25s  batch_loss: 1.4268 [Training] 21/114 [====>.........................] - ETA: 25s  batch_loss: 1.4301 [Training] 22/114 [====>.........................] - ETA: 24s  batch_loss: 1.4290 [Training] 23/114 [=====>........................] - ETA: 24s  batch_loss: 1.4195 [Training] 24/114 [=====>........................] - ETA: 24s  batch_loss: 1.4150 [Training] 25/114 [=====>........................] - ETA: 23s  batch_loss: 1.4125 [Training] 26/114 [=====>........................] - ETA: 23s  batch_loss: 1.4067 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 1.4020 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 1.3979 [Training] 29/114 [======>.......................] - ETA: 22s  batch_loss: 1.3937 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 1.3877 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 1.3824 [Training] 32/114 [=======>......................] - ETA: 21s  batch_loss: 1.3847 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 1.3820 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 1.3796 [Training] 35/114 [========>.....................] - ETA: 20s  batch_loss: 1.3765 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 1.3701 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 1.3682 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 1.3673 [Training] 39/114 [=========>....................] - ETA: 19s  batch_loss: 1.3668 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 1.3672 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 1.3647 [Training] 42/114 [==========>...................] - ETA: 18s  batch_loss: 1.3692 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 1.3662 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 1.3655 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 1.3614 [Training] 46/114 [===========>..................] - ETA: 17s  batch_loss: 1.3575 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 1.3576 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 1.3559 [Training] 49/114 [===========>..................] - ETA: 16s  batch_loss: 1.3554 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 1.3550 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 1.3519 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 1.3496 [Training] 53/114 [============>.................] - ETA: 15s  batch_loss: 1.3482 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 1.3523 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 1.3518 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 1.3506 [Training] 57/114 [==============>...............] - ETA: 14s  batch_loss: 1.3465 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 1.3468 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 1.3434 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 1.3423 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 1.3413 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 1.3405 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 1.3381 [Training] 64/114 [===============>..............] - ETA: 12s  batch_loss: 1.3351 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 1.3355 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 1.3327 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 1.3312 [Training] 68/114 [================>.............] - ETA: 11s  batch_loss: 1.3273 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 1.3289 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 1.3281 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 1.3322 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 1.3322 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 1.3306 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 1.3332 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 1.3309 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 1.3321 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 1.3311 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 1.3311 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 1.3315 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 1.3273 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 1.3278 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 1.3284 [Training] 83/114 [====================>.........] - ETA: 7s  batch_loss: 1.3282 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 1.3259 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 1.3246 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 1.3238 [Training] 87/114 [=====================>........] - ETA: 6s  batch_loss: 1.3233 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 1.3202 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 1.3207 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 1.3188 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 1.3176 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 1.3148 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 1.3148 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 1.3135 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 1.3124 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 1.3119 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 1.3109 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 1.3090 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 1.3079 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 1.3070 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 1.3067 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 1.3066 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 1.3059 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 1.3037 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 1.3036 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 1.3022 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 1.3027 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 1.3028 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 1.3032 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 1.3010 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 1.2999 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 1.2981 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 1.2976 [Training] 114/114 [==============================] 258.9ms/step  batch_loss: 1.2971 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:40:37 - INFO - root -   The F1-score is 0.1895649329407916
09/25/2022 13:40:37 - INFO - root -   the best eval f1 is 0.1896, saving model !!
09/25/2022 13:40:40 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:39  batch_loss: 1.1451 [Training] 2/114 [..............................] - ETA: 1:03  batch_loss: 1.1571 [Training] 3/114 [..............................] - ETA: 51s  batch_loss: 1.1855 [Training] 4/114 [>.............................] - ETA: 45s  batch_loss: 1.1940 [Training] 5/114 [>.............................] - ETA: 42s  batch_loss: 1.1914 [Training] 6/114 [>.............................] - ETA: 39s  batch_loss: 1.1594 [Training] 7/114 [>.............................] - ETA: 37s  batch_loss: 1.1590 [Training] 8/114 [=>............................] - ETA: 36s  batch_loss: 1.1755 [Training] 9/114 [=>............................] - ETA: 34s  batch_loss: 1.1732 [Training] 10/114 [=>............................] - ETA: 33s  batch_loss: 1.1732 [Training] 11/114 [=>............................] - ETA: 32s  batch_loss: 1.1635 [Training] 12/114 [==>...........................] - ETA: 31s  batch_loss: 1.1805 [Training] 13/114 [==>...........................] - ETA: 31s  batch_loss: 1.1726 [Training] 14/114 [==>...........................] - ETA: 30s  batch_loss: 1.1703 [Training] 15/114 [==>...........................] - ETA: 29s  batch_loss: 1.1708 [Training] 16/114 [===>..........................] - ETA: 29s  batch_loss: 1.1675 [Training] 17/114 [===>..........................] - ETA: 28s  batch_loss: 1.1650 [Training] 18/114 [===>..........................] - ETA: 28s  batch_loss: 1.1594 [Training] 19/114 [====>.........................] - ETA: 27s  batch_loss: 1.1623 [Training] 20/114 [====>.........................] - ETA: 27s  batch_loss: 1.1549 [Training] 21/114 [====>.........................] - ETA: 27s  batch_loss: 1.1593 [Training] 22/114 [====>.........................] - ETA: 26s  batch_loss: 1.1638 [Training] 23/114 [=====>........................] - ETA: 26s  batch_loss: 1.1556 [Training] 24/114 [=====>........................] - ETA: 25s  batch_loss: 1.1502 [Training] 25/114 [=====>........................] - ETA: 25s  batch_loss: 1.1460 [Training] 26/114 [=====>........................] - ETA: 25s  batch_loss: 1.1359 [Training] 27/114 [======>.......................] - ETA: 24s  batch_loss: 1.1328 [Training] 28/114 [======>.......................] - ETA: 24s  batch_loss: 1.1324 [Training] 29/114 [======>.......................] - ETA: 24s  batch_loss: 1.1367 [Training] 30/114 [======>.......................] - ETA: 23s  batch_loss: 1.1338 [Training] 31/114 [=======>......................] - ETA: 23s  batch_loss: 1.1355 [Training] 32/114 [=======>......................] - ETA: 23s  batch_loss: 1.1320 [Training] 33/114 [=======>......................] - ETA: 22s  batch_loss: 1.1317 [Training] 34/114 [=======>......................] - ETA: 22s  batch_loss: 1.1286 [Training] 35/114 [========>.....................] - ETA: 22s  batch_loss: 1.1307 [Training] 36/114 [========>.....................] - ETA: 21s  batch_loss: 1.1321 [Training] 37/114 [========>.....................] - ETA: 21s  batch_loss: 1.1328 [Training] 38/114 [=========>....................] - ETA: 21s  batch_loss: 1.1327 [Training] 39/114 [=========>....................] - ETA: 20s  batch_loss: 1.1356 [Training] 40/114 [=========>....................] - ETA: 20s  batch_loss: 1.1368 [Training] 41/114 [=========>....................] - ETA: 20s  batch_loss: 1.1360 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 1.1340 [Training] 43/114 [==========>...................] - ETA: 19s  batch_loss: 1.1332 [Training] 44/114 [==========>...................] - ETA: 19s  batch_loss: 1.1288 [Training] 45/114 [==========>...................] - ETA: 19s  batch_loss: 1.1295 [Training] 46/114 [===========>..................] - ETA: 18s  batch_loss: 1.1294 [Training] 47/114 [===========>..................] - ETA: 18s  batch_loss: 1.1289 [Training] 48/114 [===========>..................] - ETA: 18s  batch_loss: 1.1302 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 1.1288 [Training] 50/114 [============>.................] - ETA: 17s  batch_loss: 1.1280 [Training] 51/114 [============>.................] - ETA: 17s  batch_loss: 1.1276 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 1.1253 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 1.1234 [Training] 54/114 [=============>................] - ETA: 16s  batch_loss: 1.1245 [Training] 55/114 [=============>................] - ETA: 16s  batch_loss: 1.1247 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 1.1230 [Training] 57/114 [==============>...............] - ETA: 15s  batch_loss: 1.1219 [Training] 58/114 [==============>...............] - ETA: 15s  batch_loss: 1.1210 [Training] 59/114 [==============>...............] - ETA: 15s  batch_loss: 1.1184 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 1.1168 [Training] 61/114 [===============>..............] - ETA: 14s  batch_loss: 1.1169 [Training] 62/114 [===============>..............] - ETA: 14s  batch_loss: 1.1173 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 1.1162 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 1.1159 [Training] 65/114 [================>.............] - ETA: 13s  batch_loss: 1.1161 [Training] 66/114 [================>.............] - ETA: 13s  batch_loss: 1.1141 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 1.1105 [Training] 68/114 [================>.............] - ETA: 12s  batch_loss: 1.1097 [Training] 69/114 [=================>............] - ETA: 12s  batch_loss: 1.1079 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 1.1063 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 1.1053 [Training] 72/114 [=================>............] - ETA: 11s  batch_loss: 1.1044 [Training] 73/114 [==================>...........] - ETA: 11s  batch_loss: 1.1041 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 1.1028 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 1.1018 [Training] 76/114 [===================>..........] - ETA: 10s  batch_loss: 1.1012 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 1.1001 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 1.1006 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 1.1002 [Training] 80/114 [====================>.........] - ETA: 9s  batch_loss: 1.0984 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 1.0978 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 1.0975 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 1.0971 [Training] 84/114 [=====================>........] - ETA: 8s  batch_loss: 1.0971 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 1.0932 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 1.0927 [Training] 87/114 [=====================>........] - ETA: 7s  batch_loss: 1.0941 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 1.0930 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 1.0928 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 1.0904 [Training] 91/114 [======================>.......] - ETA: 6s  batch_loss: 1.0898 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 1.0887 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 1.0870 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 1.0858 [Training] 95/114 [========================>.....] - ETA: 5s  batch_loss: 1.0860 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 1.0841 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 1.0824 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 1.0809 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 1.0801 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 1.0783 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 1.0765 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 1.0757 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 1.0749 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 1.0729 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 1.0712 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 1.0700 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 1.0691 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 1.0669 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 1.0666 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 1.0651 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 1.0637 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 1.0616 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 1.0597 [Training] 114/114 [==============================] 263.9ms/step  batch_loss: 1.0590 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:41:16 - INFO - root -   The F1-score is 0.32122137404580153
09/25/2022 13:41:16 - INFO - root -   the best eval f1 is 0.3212, saving model !!
09/25/2022 13:41:19 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:35  batch_loss: 1.0505 [Training] 2/114 [..............................] - ETA: 1:00  batch_loss: 1.0495 [Training] 3/114 [..............................] - ETA: 49s  batch_loss: 1.0396 [Training] 4/114 [>.............................] - ETA: 43s  batch_loss: 1.0398 [Training] 5/114 [>.............................] - ETA: 40s  batch_loss: 1.0416 [Training] 6/114 [>.............................] - ETA: 37s  batch_loss: 1.0190 [Training] 7/114 [>.............................] - ETA: 35s  batch_loss: 1.0113 [Training] 8/114 [=>............................] - ETA: 34s  batch_loss: 0.9931 [Training] 9/114 [=>............................] - ETA: 33s  batch_loss: 0.9953 [Training] 10/114 [=>............................] - ETA: 32s  batch_loss: 0.9949 [Training] 11/114 [=>............................] - ETA: 31s  batch_loss: 0.9917 [Training] 12/114 [==>...........................] - ETA: 30s  batch_loss: 0.9754 [Training] 13/114 [==>...........................] - ETA: 29s  batch_loss: 0.9770 [Training] 14/114 [==>...........................] - ETA: 29s  batch_loss: 0.9751 [Training] 15/114 [==>...........................] - ETA: 28s  batch_loss: 0.9832 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 0.9756 [Training] 17/114 [===>..........................] - ETA: 27s  batch_loss: 0.9732 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 0.9672 [Training] 19/114 [====>.........................] - ETA: 26s  batch_loss: 0.9609 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.9554 [Training] 21/114 [====>.........................] - ETA: 25s  batch_loss: 0.9494 [Training] 22/114 [====>.........................] - ETA: 25s  batch_loss: 0.9455 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.9441 [Training] 24/114 [=====>........................] - ETA: 24s  batch_loss: 0.9444 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 0.9458 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.9462 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 0.9490 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 0.9436 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 0.9443 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.9389 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.9373 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.9400 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.9365 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.9362 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.9350 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 0.9353 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.9359 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.9366 [Training] 39/114 [=========>....................] - ETA: 19s  batch_loss: 0.9341 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.9345 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.9306 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.9309 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.9285 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.9285 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.9289 [Training] 46/114 [===========>..................] - ETA: 17s  batch_loss: 0.9254 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.9279 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.9289 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.9256 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.9244 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.9222 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.9202 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.9221 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.9212 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.9189 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.9184 [Training] 57/114 [==============>...............] - ETA: 14s  batch_loss: 0.9171 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.9179 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.9184 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.9160 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.9158 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.9133 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.9131 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.9119 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.9115 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.9107 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.9104 [Training] 68/114 [================>.............] - ETA: 11s  batch_loss: 0.9093 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.9105 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.9107 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.9097 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.9084 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.9076 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.9071 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.9069 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.9057 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.9049 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.9051 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.9046 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.9033 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.9043 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.9032 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.9035 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.9019 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.9014 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.9020 [Training] 87/114 [=====================>........] - ETA: 6s  batch_loss: 0.9031 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.9020 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.9015 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.8998 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.9000 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.8989 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.8982 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.8984 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.8975 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.8976 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.8990 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.8990 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.8972 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.8958 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.8957 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.8939 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.8933 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.8932 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.8932 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.8931 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.8925 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.8920 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.8920 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.8921 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.8908 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.8906 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.8900 [Training] 114/114 [==============================] 257.9ms/step  batch_loss: 0.8900 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:41:55 - INFO - root -   The F1-score is 0.44268558951965065
09/25/2022 13:41:55 - INFO - root -   the best eval f1 is 0.4427, saving model !!
09/25/2022 13:41:59 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:39  batch_loss: 0.8936 [Training] 2/114 [..............................] - ETA: 1:03  batch_loss: 0.8968 [Training] 3/114 [..............................] - ETA: 51s  batch_loss: 0.8594 [Training] 4/114 [>.............................] - ETA: 44s  batch_loss: 0.8471 [Training] 5/114 [>.............................] - ETA: 40s  batch_loss: 0.8329 [Training] 6/114 [>.............................] - ETA: 38s  batch_loss: 0.8390 [Training] 7/114 [>.............................] - ETA: 36s  batch_loss: 0.8465 [Training] 8/114 [=>............................] - ETA: 34s  batch_loss: 0.8316 [Training] 9/114 [=>............................] - ETA: 33s  batch_loss: 0.8250 [Training] 10/114 [=>............................] - ETA: 32s  batch_loss: 0.8356 [Training] 11/114 [=>............................] - ETA: 31s  batch_loss: 0.8316 [Training] 12/114 [==>...........................] - ETA: 30s  batch_loss: 0.8422 [Training] 13/114 [==>...........................] - ETA: 30s  batch_loss: 0.8457 [Training] 14/114 [==>...........................] - ETA: 29s  batch_loss: 0.8432 [Training] 15/114 [==>...........................] - ETA: 28s  batch_loss: 0.8448 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 0.8512 [Training] 17/114 [===>..........................] - ETA: 27s  batch_loss: 0.8505 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 0.8477 [Training] 19/114 [====>.........................] - ETA: 26s  batch_loss: 0.8488 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.8383 [Training] 21/114 [====>.........................] - ETA: 26s  batch_loss: 0.8381 [Training] 22/114 [====>.........................] - ETA: 25s  batch_loss: 0.8434 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.8366 [Training] 24/114 [=====>........................] - ETA: 24s  batch_loss: 0.8311 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 0.8275 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.8220 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 0.8192 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 0.8159 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 0.8141 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.8112 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.8069 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.8073 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.8062 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.8061 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.8060 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 0.8050 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.8017 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.7998 [Training] 39/114 [=========>....................] - ETA: 20s  batch_loss: 0.7994 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.7978 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.7972 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.7963 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.7973 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.7965 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.7929 [Training] 46/114 [===========>..................] - ETA: 18s  batch_loss: 0.7939 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.7931 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.7942 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.7933 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.7936 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.7929 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.7920 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.7918 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.7898 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.7916 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.7922 [Training] 57/114 [==============>...............] - ETA: 14s  batch_loss: 0.7920 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.7905 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.7895 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.7899 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.7895 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.7887 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.7881 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.7863 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.7845 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.7843 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.7830 [Training] 68/114 [================>.............] - ETA: 12s  batch_loss: 0.7846 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.7819 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.7811 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.7793 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.7774 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.7777 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.7753 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.7746 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.7745 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.7732 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.7728 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.7711 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.7707 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.7704 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.7699 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.7688 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.7688 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.7685 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.7689 [Training] 87/114 [=====================>........] - ETA: 7s  batch_loss: 0.7695 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.7701 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.7701 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.7705 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.7688 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.7691 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.7678 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.7673 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.7668 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.7665 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.7661 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.7657 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.7655 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.7650 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.7635 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.7626 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.7625 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.7630 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.7627 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.7640 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.7640 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.7631 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.7628 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.7615 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.7606 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.7606 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.7612 [Training] 114/114 [==============================] 257.9ms/step  batch_loss: 0.7598 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:42:35 - INFO - root -   The F1-score is 0.4820790900674514
09/25/2022 13:42:35 - INFO - root -   the best eval f1 is 0.4821, saving model !!
09/25/2022 13:42:38 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:32  batch_loss: 0.6692 [Training] 2/114 [..............................] - ETA: 1:00  batch_loss: 0.6435 [Training] 3/114 [..............................] - ETA: 49s  batch_loss: 0.6690 [Training] 4/114 [>.............................] - ETA: 43s  batch_loss: 0.6841 [Training] 5/114 [>.............................] - ETA: 39s  batch_loss: 0.6913 [Training] 6/114 [>.............................] - ETA: 37s  batch_loss: 0.6855 [Training] 7/114 [>.............................] - ETA: 35s  batch_loss: 0.6948 [Training] 8/114 [=>............................] - ETA: 34s  batch_loss: 0.6885 [Training] 9/114 [=>............................] - ETA: 32s  batch_loss: 0.6844 [Training] 10/114 [=>............................] - ETA: 31s  batch_loss: 0.6786 [Training] 11/114 [=>............................] - ETA: 31s  batch_loss: 0.6763 [Training] 12/114 [==>...........................] - ETA: 30s  batch_loss: 0.6775 [Training] 13/114 [==>...........................] - ETA: 29s  batch_loss: 0.6800 [Training] 14/114 [==>...........................] - ETA: 28s  batch_loss: 0.6736 [Training] 15/114 [==>...........................] - ETA: 28s  batch_loss: 0.6698 [Training] 16/114 [===>..........................] - ETA: 27s  batch_loss: 0.6711 [Training] 17/114 [===>..........................] - ETA: 27s  batch_loss: 0.6701 [Training] 18/114 [===>..........................] - ETA: 26s  batch_loss: 0.6711 [Training] 19/114 [====>.........................] - ETA: 26s  batch_loss: 0.6749 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.6789 [Training] 21/114 [====>.........................] - ETA: 25s  batch_loss: 0.6805 [Training] 22/114 [====>.........................] - ETA: 25s  batch_loss: 0.6783 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.6777 [Training] 24/114 [=====>........................] - ETA: 24s  batch_loss: 0.6705 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 0.6691 [Training] 26/114 [=====>........................] - ETA: 23s  batch_loss: 0.6701 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 0.6752 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 0.6759 [Training] 29/114 [======>.......................] - ETA: 22s  batch_loss: 0.6725 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.6712 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.6718 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.6706 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.6761 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.6734 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.6719 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 0.6739 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.6742 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.6723 [Training] 39/114 [=========>....................] - ETA: 19s  batch_loss: 0.6727 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.6726 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.6720 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.6728 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.6719 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.6746 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.6745 [Training] 46/114 [===========>..................] - ETA: 17s  batch_loss: 0.6774 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.6764 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.6767 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.6756 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.6750 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.6744 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.6742 [Training] 53/114 [============>.................] - ETA: 15s  batch_loss: 0.6736 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.6726 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.6716 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.6687 [Training] 57/114 [==============>...............] - ETA: 14s  batch_loss: 0.6689 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.6695 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.6689 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.6678 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.6673 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.6673 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.6684 [Training] 64/114 [===============>..............] - ETA: 12s  batch_loss: 0.6683 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.6673 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.6659 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.6653 [Training] 68/114 [================>.............] - ETA: 11s  batch_loss: 0.6655 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.6668 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.6662 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.6658 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.6660 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.6671 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.6669 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.6660 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.6652 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.6641 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.6635 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.6625 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.6622 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.6629 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.6624 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.6613 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.6610 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.6604 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.6594 [Training] 87/114 [=====================>........] - ETA: 6s  batch_loss: 0.6595 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.6584 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.6597 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.6588 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.6583 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.6573 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.6562 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.6552 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.6550 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.6555 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.6558 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.6569 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.6561 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.6555 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.6552 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.6544 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.6541 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.6541 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.6536 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.6533 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.6537 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.6532 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.6535 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.6527 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.6525 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.6522 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.6517 [Training] 114/114 [==============================] 256.9ms/step  batch_loss: 0.6506 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:43:13 - INFO - root -   The F1-score is 0.5103750483309705
09/25/2022 13:43:13 - INFO - root -   the best eval f1 is 0.5104, saving model !!
09/25/2022 13:43:17 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:32  batch_loss: 0.5730 [Training] 2/114 [..............................] - ETA: 59s  batch_loss: 0.5496 [Training] 3/114 [..............................] - ETA: 48s  batch_loss: 0.5467 [Training] 4/114 [>.............................] - ETA: 42s  batch_loss: 0.5553 [Training] 5/114 [>.............................] - ETA: 39s  batch_loss: 0.5552 [Training] 6/114 [>.............................] - ETA: 37s  batch_loss: 0.5733 [Training] 7/114 [>.............................] - ETA: 35s  batch_loss: 0.5799 [Training] 8/114 [=>............................] - ETA: 34s  batch_loss: 0.5781 [Training] 9/114 [=>............................] - ETA: 32s  batch_loss: 0.5894 [Training] 10/114 [=>............................] - ETA: 32s  batch_loss: 0.5833 [Training] 11/114 [=>............................] - ETA: 31s  batch_loss: 0.5864 [Training] 12/114 [==>...........................] - ETA: 30s  batch_loss: 0.5786 [Training] 13/114 [==>...........................] - ETA: 29s  batch_loss: 0.5803 [Training] 14/114 [==>...........................] - ETA: 29s  batch_loss: 0.5872 [Training] 15/114 [==>...........................] - ETA: 28s  batch_loss: 0.5823 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 0.5788 [Training] 17/114 [===>..........................] - ETA: 27s  batch_loss: 0.5795 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 0.5781 [Training] 19/114 [====>.........................] - ETA: 26s  batch_loss: 0.5782 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.5777 [Training] 21/114 [====>.........................] - ETA: 25s  batch_loss: 0.5776 [Training] 22/114 [====>.........................] - ETA: 25s  batch_loss: 0.5825 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.5792 [Training] 24/114 [=====>........................] - ETA: 24s  batch_loss: 0.5824 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 0.5825 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.5788 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 0.5806 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 0.5795 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 0.5787 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.5798 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.5785 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.5787 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.5799 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.5816 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.5831 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 0.5837 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.5848 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.5854 [Training] 39/114 [=========>....................] - ETA: 19s  batch_loss: 0.5853 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.5844 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.5838 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.5835 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.5836 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.5836 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.5830 [Training] 46/114 [===========>..................] - ETA: 17s  batch_loss: 0.5846 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.5842 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.5844 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.5856 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.5848 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.5826 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.5842 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.5843 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.5842 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.5835 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.5841 [Training] 57/114 [==============>...............] - ETA: 14s  batch_loss: 0.5856 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.5843 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.5847 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.5842 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.5831 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.5823 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.5821 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.5804 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.5784 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.5783 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.5778 [Training] 68/114 [================>.............] - ETA: 11s  batch_loss: 0.5776 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.5783 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.5796 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.5789 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.5786 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.5785 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.5794 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.5784 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.5772 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.5760 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.5753 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.5759 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.5758 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.5763 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.5753 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.5746 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.5745 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.5730 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.5728 [Training] 87/114 [=====================>........] - ETA: 7s  batch_loss: 0.5740 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.5739 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.5736 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.5729 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.5718 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.5715 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.5725 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.5729 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.5727 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.5733 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.5725 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.5723 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.5718 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.5714 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.5714 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.5716 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.5715 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.5711 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.5713 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.5709 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.5705 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.5714 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.5714 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.5712 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.5709 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.5705 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.5700 [Training] 114/114 [==============================] 258.7ms/step  batch_loss: 0.5700 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:43:53 - INFO - root -   The F1-score is 0.5222661396574441
09/25/2022 13:43:53 - INFO - root -   the best eval f1 is 0.5223, saving model !!
09/25/2022 13:43:56 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 2:01  batch_loss: 0.4825 [Training] 2/114 [..............................] - ETA: 1:14  batch_loss: 0.5019 [Training] 3/114 [..............................] - ETA: 58s  batch_loss: 0.4868 [Training] 4/114 [>.............................] - ETA: 50s  batch_loss: 0.4823 [Training] 5/114 [>.............................] - ETA: 45s  batch_loss: 0.4947 [Training] 6/114 [>.............................] - ETA: 42s  batch_loss: 0.5022 [Training] 7/114 [>.............................] - ETA: 39s  batch_loss: 0.5072 [Training] 8/114 [=>............................] - ETA: 37s  batch_loss: 0.5080 [Training] 9/114 [=>............................] - ETA: 36s  batch_loss: 0.5110 [Training] 10/114 [=>............................] - ETA: 34s  batch_loss: 0.5084 [Training] 11/114 [=>............................] - ETA: 33s  batch_loss: 0.5077 [Training] 12/114 [==>...........................] - ETA: 32s  batch_loss: 0.5107 [Training] 13/114 [==>...........................] - ETA: 31s  batch_loss: 0.5128 [Training] 14/114 [==>...........................] - ETA: 30s  batch_loss: 0.5138 [Training] 15/114 [==>...........................] - ETA: 30s  batch_loss: 0.5118 [Training] 16/114 [===>..........................] - ETA: 29s  batch_loss: 0.5223 [Training] 17/114 [===>..........................] - ETA: 29s  batch_loss: 0.5248 [Training] 18/114 [===>..........................] - ETA: 28s  batch_loss: 0.5286 [Training] 19/114 [====>.........................] - ETA: 28s  batch_loss: 0.5296 [Training] 20/114 [====>.........................] - ETA: 27s  batch_loss: 0.5228 [Training] 21/114 [====>.........................] - ETA: 27s  batch_loss: 0.5207 [Training] 22/114 [====>.........................] - ETA: 26s  batch_loss: 0.5197 [Training] 23/114 [=====>........................] - ETA: 26s  batch_loss: 0.5216 [Training] 24/114 [=====>........................] - ETA: 25s  batch_loss: 0.5201 [Training] 25/114 [=====>........................] - ETA: 25s  batch_loss: 0.5215 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.5197 [Training] 27/114 [======>.......................] - ETA: 24s  batch_loss: 0.5175 [Training] 28/114 [======>.......................] - ETA: 24s  batch_loss: 0.5198 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 0.5178 [Training] 30/114 [======>.......................] - ETA: 23s  batch_loss: 0.5222 [Training] 31/114 [=======>......................] - ETA: 23s  batch_loss: 0.5244 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.5213 [Training] 33/114 [=======>......................] - ETA: 22s  batch_loss: 0.5193 [Training] 34/114 [=======>......................] - ETA: 22s  batch_loss: 0.5189 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.5222 [Training] 36/114 [========>.....................] - ETA: 21s  batch_loss: 0.5222 [Training] 37/114 [========>.....................] - ETA: 21s  batch_loss: 0.5202 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.5179 [Training] 39/114 [=========>....................] - ETA: 20s  batch_loss: 0.5170 [Training] 40/114 [=========>....................] - ETA: 20s  batch_loss: 0.5147 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.5135 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.5164 [Training] 43/114 [==========>...................] - ETA: 19s  batch_loss: 0.5150 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.5136 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.5148 [Training] 46/114 [===========>..................] - ETA: 18s  batch_loss: 0.5160 [Training] 47/114 [===========>..................] - ETA: 18s  batch_loss: 0.5154 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.5143 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.5128 [Training] 50/114 [============>.................] - ETA: 17s  batch_loss: 0.5124 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.5123 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.5121 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.5124 [Training] 54/114 [=============>................] - ETA: 16s  batch_loss: 0.5121 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.5118 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.5111 [Training] 57/114 [==============>...............] - ETA: 15s  batch_loss: 0.5109 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.5130 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.5121 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.5128 [Training] 61/114 [===============>..............] - ETA: 14s  batch_loss: 0.5125 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.5129 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.5125 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.5128 [Training] 65/114 [================>.............] - ETA: 13s  batch_loss: 0.5118 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.5104 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.5102 [Training] 68/114 [================>.............] - ETA: 12s  batch_loss: 0.5102 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.5106 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.5109 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.5109 [Training] 72/114 [=================>............] - ETA: 11s  batch_loss: 0.5110 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.5103 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.5108 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.5107 [Training] 76/114 [===================>..........] - ETA: 10s  batch_loss: 0.5110 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.5104 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.5100 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.5089 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.5083 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.5067 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.5059 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.5061 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.5055 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.5052 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.5056 [Training] 87/114 [=====================>........] - ETA: 7s  batch_loss: 0.5059 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.5062 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.5075 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.5080 [Training] 91/114 [======================>.......] - ETA: 6s  batch_loss: 0.5077 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.5069 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.5060 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.5064 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.5067 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.5056 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.5050 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.5040 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.5040 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.5035 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.5028 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.5025 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.5024 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.5032 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.5033 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.5040 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.5042 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.5037 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.5036 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.5037 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.5029 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.5025 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.5018 [Training] 114/114 [==============================] 260.8ms/step  batch_loss: 0.5019 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:44:32 - INFO - root -   The F1-score is 0.5539924973204716
09/25/2022 13:44:32 - INFO - root -   the best eval f1 is 0.5540, saving model !!
09/25/2022 13:44:35 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:34  batch_loss: 0.5001 [Training] 2/114 [..............................] - ETA: 1:01  batch_loss: 0.4847 [Training] 3/114 [..............................] - ETA: 50s  batch_loss: 0.4875 [Training] 4/114 [>.............................] - ETA: 44s  batch_loss: 0.4870 [Training] 5/114 [>.............................] - ETA: 40s  batch_loss: 0.4719 [Training] 6/114 [>.............................] - ETA: 38s  batch_loss: 0.4837 [Training] 7/114 [>.............................] - ETA: 36s  batch_loss: 0.4826 [Training] 8/114 [=>............................] - ETA: 35s  batch_loss: 0.4883 [Training] 9/114 [=>............................] - ETA: 33s  batch_loss: 0.4869 [Training] 10/114 [=>............................] - ETA: 32s  batch_loss: 0.4784 [Training] 11/114 [=>............................] - ETA: 31s  batch_loss: 0.4763 [Training] 12/114 [==>...........................] - ETA: 31s  batch_loss: 0.4744 [Training] 13/114 [==>...........................] - ETA: 30s  batch_loss: 0.4773 [Training] 14/114 [==>...........................] - ETA: 29s  batch_loss: 0.4711 [Training] 15/114 [==>...........................] - ETA: 29s  batch_loss: 0.4669 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 0.4622 [Training] 17/114 [===>..........................] - ETA: 27s  batch_loss: 0.4572 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 0.4531 [Training] 19/114 [====>.........................] - ETA: 27s  batch_loss: 0.4566 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.4564 [Training] 21/114 [====>.........................] - ETA: 26s  batch_loss: 0.4585 [Training] 22/114 [====>.........................] - ETA: 25s  batch_loss: 0.4593 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.4594 [Training] 24/114 [=====>........................] - ETA: 24s  batch_loss: 0.4638 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 0.4615 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.4589 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 0.4571 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 0.4578 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 0.4618 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.4629 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.4639 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.4653 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.4677 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.4666 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.4636 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 0.4628 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.4610 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.4601 [Training] 39/114 [=========>....................] - ETA: 19s  batch_loss: 0.4600 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.4605 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.4596 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.4611 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.4627 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.4613 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.4614 [Training] 46/114 [===========>..................] - ETA: 17s  batch_loss: 0.4596 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.4577 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.4581 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.4569 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.4580 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.4578 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.4570 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.4569 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.4564 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.4569 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.4559 [Training] 57/114 [==============>...............] - ETA: 14s  batch_loss: 0.4553 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.4552 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.4534 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.4541 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.4538 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.4535 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.4526 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.4518 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.4503 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.4507 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.4514 [Training] 68/114 [================>.............] - ETA: 12s  batch_loss: 0.4503 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.4502 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.4502 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.4500 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.4508 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.4513 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.4501 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.4495 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.4484 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.4483 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.4494 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.4514 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.4504 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.4501 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.4498 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.4493 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.4484 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.4485 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.4481 [Training] 87/114 [=====================>........] - ETA: 7s  batch_loss: 0.4491 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.4497 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.4497 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.4501 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.4502 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.4505 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.4505 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.4507 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.4507 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.4507 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.4501 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.4496 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.4487 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.4485 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.4491 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.4484 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.4476 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.4483 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.4482 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.4479 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.4477 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.4474 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.4472 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.4462 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.4458 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.4454 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.4455 [Training] 114/114 [==============================] 258.6ms/step  batch_loss: 0.4458 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:45:11 - INFO - root -   The F1-score is 0.5846174107730805
09/25/2022 13:45:11 - INFO - root -   the best eval f1 is 0.5846, saving model !!
09/25/2022 13:45:14 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:32  batch_loss: 0.3525 [Training] 2/114 [..............................] - ETA: 59s  batch_loss: 0.3652 [Training] 3/114 [..............................] - ETA: 48s  batch_loss: 0.3729 [Training] 4/114 [>.............................] - ETA: 42s  batch_loss: 0.3784 [Training] 5/114 [>.............................] - ETA: 39s  batch_loss: 0.3958 [Training] 6/114 [>.............................] - ETA: 36s  batch_loss: 0.3881 [Training] 7/114 [>.............................] - ETA: 34s  batch_loss: 0.3981 [Training] 8/114 [=>............................] - ETA: 33s  batch_loss: 0.3893 [Training] 9/114 [=>............................] - ETA: 32s  batch_loss: 0.3932 [Training] 10/114 [=>............................] - ETA: 31s  batch_loss: 0.3944 [Training] 11/114 [=>............................] - ETA: 30s  batch_loss: 0.3922 [Training] 12/114 [==>...........................] - ETA: 29s  batch_loss: 0.3954 [Training] 13/114 [==>...........................] - ETA: 28s  batch_loss: 0.3961 [Training] 14/114 [==>...........................] - ETA: 28s  batch_loss: 0.3969 [Training] 15/114 [==>...........................] - ETA: 27s  batch_loss: 0.3966 [Training] 16/114 [===>..........................] - ETA: 27s  batch_loss: 0.3983 [Training] 17/114 [===>..........................] - ETA: 26s  batch_loss: 0.4004 [Training] 18/114 [===>..........................] - ETA: 26s  batch_loss: 0.4033 [Training] 19/114 [====>.........................] - ETA: 25s  batch_loss: 0.4057 [Training] 20/114 [====>.........................] - ETA: 25s  batch_loss: 0.4096 [Training] 21/114 [====>.........................] - ETA: 25s  batch_loss: 0.4091 [Training] 22/114 [====>.........................] - ETA: 24s  batch_loss: 0.4090 [Training] 23/114 [=====>........................] - ETA: 24s  batch_loss: 0.4070 [Training] 24/114 [=====>........................] - ETA: 23s  batch_loss: 0.4032 [Training] 25/114 [=====>........................] - ETA: 23s  batch_loss: 0.4030 [Training] 26/114 [=====>........................] - ETA: 23s  batch_loss: 0.4025 [Training] 27/114 [======>.......................] - ETA: 22s  batch_loss: 0.4019 [Training] 28/114 [======>.......................] - ETA: 22s  batch_loss: 0.4019 [Training] 29/114 [======>.......................] - ETA: 22s  batch_loss: 0.4031 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.4021 [Training] 31/114 [=======>......................] - ETA: 21s  batch_loss: 0.4030 [Training] 32/114 [=======>......................] - ETA: 21s  batch_loss: 0.4053 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.4066 [Training] 34/114 [=======>......................] - ETA: 20s  batch_loss: 0.4095 [Training] 35/114 [========>.....................] - ETA: 20s  batch_loss: 0.4118 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 0.4114 [Training] 37/114 [========>.....................] - ETA: 19s  batch_loss: 0.4118 [Training] 38/114 [=========>....................] - ETA: 19s  batch_loss: 0.4121 [Training] 39/114 [=========>....................] - ETA: 19s  batch_loss: 0.4140 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.4129 [Training] 41/114 [=========>....................] - ETA: 18s  batch_loss: 0.4123 [Training] 42/114 [==========>...................] - ETA: 18s  batch_loss: 0.4106 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.4104 [Training] 44/114 [==========>...................] - ETA: 17s  batch_loss: 0.4114 [Training] 45/114 [==========>...................] - ETA: 17s  batch_loss: 0.4111 [Training] 46/114 [===========>..................] - ETA: 17s  batch_loss: 0.4120 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.4112 [Training] 48/114 [===========>..................] - ETA: 16s  batch_loss: 0.4104 [Training] 49/114 [===========>..................] - ETA: 16s  batch_loss: 0.4092 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.4097 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.4098 [Training] 52/114 [============>.................] - ETA: 15s  batch_loss: 0.4081 [Training] 53/114 [============>.................] - ETA: 15s  batch_loss: 0.4079 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.4093 [Training] 55/114 [=============>................] - ETA: 14s  batch_loss: 0.4089 [Training] 56/114 [=============>................] - ETA: 14s  batch_loss: 0.4090 [Training] 57/114 [==============>...............] - ETA: 14s  batch_loss: 0.4088 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.4096 [Training] 59/114 [==============>...............] - ETA: 13s  batch_loss: 0.4079 [Training] 60/114 [==============>...............] - ETA: 13s  batch_loss: 0.4070 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.4058 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.4045 [Training] 63/114 [===============>..............] - ETA: 12s  batch_loss: 0.4045 [Training] 64/114 [===============>..............] - ETA: 12s  batch_loss: 0.4033 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.4027 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.4022 [Training] 67/114 [================>.............] - ETA: 11s  batch_loss: 0.4017 [Training] 68/114 [================>.............] - ETA: 11s  batch_loss: 0.4023 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.4022 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.4016 [Training] 71/114 [=================>............] - ETA: 10s  batch_loss: 0.4014 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.4007 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.4022 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.4024 [Training] 75/114 [==================>...........] - ETA: 9s  batch_loss: 0.4037 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.4041 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.4032 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.4019 [Training] 79/114 [===================>..........] - ETA: 8s  batch_loss: 0.4025 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.4016 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.4011 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.3999 [Training] 83/114 [====================>.........] - ETA: 7s  batch_loss: 0.3999 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.3994 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.3993 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.3992 [Training] 87/114 [=====================>........] - ETA: 6s  batch_loss: 0.3985 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.3980 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.3990 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.3991 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.3994 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.3996 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.3990 [Training] 94/114 [=======================>......] - ETA: 4s  batch_loss: 0.3989 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.3989 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.3993 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.3987 [Training] 98/114 [========================>.....] - ETA: 3s  batch_loss: 0.3986 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.3973 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.3966 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.3963 [Training] 102/114 [=========================>....] - ETA: 2s  batch_loss: 0.3958 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.3958 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.3962 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.3962 [Training] 106/114 [==========================>...] - ETA: 1s  batch_loss: 0.3955 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.3951 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.3958 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.3965 [Training] 110/114 [===========================>..] - ETA: 0s  batch_loss: 0.3964 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.3967 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.3970 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.3971 [Training] 114/114 [==============================] 249.1ms/step  batch_loss: 0.3972 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:45:49 - INFO - root -   The F1-score is 0.5923934426229508
09/25/2022 13:45:49 - INFO - root -   the best eval f1 is 0.5924, saving model !!
09/25/2022 13:45:52 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:29  batch_loss: 0.3397 [Training] 2/114 [..............................] - ETA: 59s  batch_loss: 0.3865 [Training] 3/114 [..............................] - ETA: 49s  batch_loss: 0.3806 [Training] 4/114 [>.............................] - ETA: 43s  batch_loss: 0.3762 [Training] 5/114 [>.............................] - ETA: 40s  batch_loss: 0.3681 [Training] 6/114 [>.............................] - ETA: 38s  batch_loss: 0.3593 [Training] 7/114 [>.............................] - ETA: 36s  batch_loss: 0.3569 [Training] 8/114 [=>............................] - ETA: 35s  batch_loss: 0.3571 [Training] 9/114 [=>............................] - ETA: 34s  batch_loss: 0.3652 [Training] 10/114 [=>............................] - ETA: 33s  batch_loss: 0.3721 [Training] 11/114 [=>............................] - ETA: 32s  batch_loss: 0.3736 [Training] 12/114 [==>...........................] - ETA: 31s  batch_loss: 0.3788 [Training] 13/114 [==>...........................] - ETA: 30s  batch_loss: 0.3755 [Training] 14/114 [==>...........................] - ETA: 30s  batch_loss: 0.3752 [Training] 15/114 [==>...........................] - ETA: 29s  batch_loss: 0.3737 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 0.3720 [Training] 17/114 [===>..........................] - ETA: 28s  batch_loss: 0.3731 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 0.3738 [Training] 19/114 [====>.........................] - ETA: 27s  batch_loss: 0.3726 [Training] 20/114 [====>.........................] - ETA: 27s  batch_loss: 0.3742 [Training] 21/114 [====>.........................] - ETA: 26s  batch_loss: 0.3726 [Training] 22/114 [====>.........................] - ETA: 26s  batch_loss: 0.3760 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.3738 [Training] 24/114 [=====>........................] - ETA: 25s  batch_loss: 0.3699 [Training] 25/114 [=====>........................] - ETA: 25s  batch_loss: 0.3676 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.3684 [Training] 27/114 [======>.......................] - ETA: 24s  batch_loss: 0.3689 [Training] 28/114 [======>.......................] - ETA: 24s  batch_loss: 0.3664 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 0.3642 [Training] 30/114 [======>.......................] - ETA: 23s  batch_loss: 0.3632 [Training] 31/114 [=======>......................] - ETA: 23s  batch_loss: 0.3607 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.3600 [Training] 33/114 [=======>......................] - ETA: 22s  batch_loss: 0.3600 [Training] 34/114 [=======>......................] - ETA: 22s  batch_loss: 0.3589 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.3583 [Training] 36/114 [========>.....................] - ETA: 21s  batch_loss: 0.3583 [Training] 37/114 [========>.....................] - ETA: 21s  batch_loss: 0.3582 [Training] 38/114 [=========>....................] - ETA: 21s  batch_loss: 0.3584 [Training] 39/114 [=========>....................] - ETA: 20s  batch_loss: 0.3600 [Training] 40/114 [=========>....................] - ETA: 20s  batch_loss: 0.3601 [Training] 41/114 [=========>....................] - ETA: 20s  batch_loss: 0.3620 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.3613 [Training] 43/114 [==========>...................] - ETA: 19s  batch_loss: 0.3617 [Training] 44/114 [==========>...................] - ETA: 19s  batch_loss: 0.3615 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.3625 [Training] 46/114 [===========>..................] - ETA: 18s  batch_loss: 0.3618 [Training] 47/114 [===========>..................] - ETA: 18s  batch_loss: 0.3611 [Training] 48/114 [===========>..................] - ETA: 18s  batch_loss: 0.3631 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.3618 [Training] 50/114 [============>.................] - ETA: 17s  batch_loss: 0.3624 [Training] 51/114 [============>.................] - ETA: 17s  batch_loss: 0.3626 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.3635 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.3631 [Training] 54/114 [=============>................] - ETA: 16s  batch_loss: 0.3629 [Training] 55/114 [=============>................] - ETA: 16s  batch_loss: 0.3631 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.3634 [Training] 57/114 [==============>...............] - ETA: 15s  batch_loss: 0.3625 [Training] 58/114 [==============>...............] - ETA: 15s  batch_loss: 0.3617 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.3607 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.3611 [Training] 61/114 [===============>..............] - ETA: 14s  batch_loss: 0.3620 [Training] 62/114 [===============>..............] - ETA: 14s  batch_loss: 0.3618 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.3621 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.3628 [Training] 65/114 [================>.............] - ETA: 13s  batch_loss: 0.3625 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.3624 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.3617 [Training] 68/114 [================>.............] - ETA: 12s  batch_loss: 0.3616 [Training] 69/114 [=================>............] - ETA: 12s  batch_loss: 0.3625 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.3632 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.3639 [Training] 72/114 [=================>............] - ETA: 11s  batch_loss: 0.3642 [Training] 73/114 [==================>...........] - ETA: 11s  batch_loss: 0.3647 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.3639 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.3639 [Training] 76/114 [===================>..........] - ETA: 10s  batch_loss: 0.3637 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.3633 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.3635 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.3633 [Training] 80/114 [====================>.........] - ETA: 9s  batch_loss: 0.3638 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.3635 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.3635 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.3627 [Training] 84/114 [=====================>........] - ETA: 8s  batch_loss: 0.3625 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.3622 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.3625 [Training] 87/114 [=====================>........] - ETA: 7s  batch_loss: 0.3619 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.3614 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.3610 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.3612 [Training] 91/114 [======================>.......] - ETA: 6s  batch_loss: 0.3611 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.3611 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.3605 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.3601 [Training] 95/114 [========================>.....] - ETA: 5s  batch_loss: 0.3600 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.3591 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.3588 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.3593 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.3588 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.3591 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.3592 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.3587 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.3584 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.3586 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.3585 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.3591 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.3591 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.3589 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.3589 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.3586 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.3588 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.3584 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.3585 [Training] 114/114 [==============================] 263.0ms/step  batch_loss: 0.3577 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:46:28 - INFO - root -   The F1-score is 0.605302199574276
09/25/2022 13:46:28 - INFO - root -   the best eval f1 is 0.6053, saving model !!
09/25/2022 13:46:31 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:31  batch_loss: 0.2861 [Training] 2/114 [..............................] - ETA: 59s  batch_loss: 0.3171 [Training] 3/114 [..............................] - ETA: 48s  batch_loss: 0.3090 [Training] 4/114 [>.............................] - ETA: 42s  batch_loss: 0.3055 [Training] 5/114 [>.............................] - ETA: 39s  batch_loss: 0.3150 [Training] 6/114 [>.............................] - ETA: 36s  batch_loss: 0.3021 [Training] 7/114 [>.............................] - ETA: 34s  batch_loss: 0.3139 [Training] 8/114 [=>............................] - ETA: 33s  batch_loss: 0.3170 [Training] 9/114 [=>............................] - ETA: 32s  batch_loss: 0.3128 [Training] 10/114 [=>............................] - ETA: 31s  batch_loss: 0.3176 [Training] 11/114 [=>............................] - ETA: 30s  batch_loss: 0.3182 [Training] 12/114 [==>...........................] - ETA: 29s  batch_loss: 0.3125 [Training] 13/114 [==>...........................] - ETA: 29s  batch_loss: 0.3122 [Training] 14/114 [==>...........................] - ETA: 28s  batch_loss: 0.3142 [Training] 15/114 [==>...........................] - ETA: 27s  batch_loss: 0.3181 [Training] 16/114 [===>..........................] - ETA: 27s  batch_loss: 0.3169 [Training] 17/114 [===>..........................] - ETA: 26s  batch_loss: 0.3139 [Training] 18/114 [===>..........................] - ETA: 26s  batch_loss: 0.3156 [Training] 19/114 [====>.........................] - ETA: 26s  batch_loss: 0.3175 [Training] 20/114 [====>.........................] - ETA: 25s  batch_loss: 0.3184 [Training] 21/114 [====>.........................] - ETA: 25s  batch_loss: 0.3193 [Training] 22/114 [====>.........................] - ETA: 24s  batch_loss: 0.3190 [Training] 23/114 [=====>........................] - ETA: 24s  batch_loss: 0.3201 [Training] 24/114 [=====>........................] - ETA: 24s  batch_loss: 0.3222 [Training] 25/114 [=====>........................] - ETA: 23s  batch_loss: 0.3234 [Training] 26/114 [=====>........................] - ETA: 23s  batch_loss: 0.3225 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 0.3223 [Training] 28/114 [======>.......................] - ETA: 22s  batch_loss: 0.3226 [Training] 29/114 [======>.......................] - ETA: 22s  batch_loss: 0.3218 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.3189 [Training] 31/114 [=======>......................] - ETA: 21s  batch_loss: 0.3187 [Training] 32/114 [=======>......................] - ETA: 21s  batch_loss: 0.3186 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.3192 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.3214 [Training] 35/114 [========>.....................] - ETA: 20s  batch_loss: 0.3224 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 0.3244 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.3247 [Training] 38/114 [=========>....................] - ETA: 19s  batch_loss: 0.3244 [Training] 39/114 [=========>....................] - ETA: 19s  batch_loss: 0.3257 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.3267 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.3267 [Training] 42/114 [==========>...................] - ETA: 18s  batch_loss: 0.3272 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.3270 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.3281 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.3278 [Training] 46/114 [===========>..................] - ETA: 17s  batch_loss: 0.3275 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.3262 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.3258 [Training] 49/114 [===========>..................] - ETA: 16s  batch_loss: 0.3270 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.3264 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.3259 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.3268 [Training] 53/114 [============>.................] - ETA: 15s  batch_loss: 0.3260 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.3256 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.3254 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.3249 [Training] 57/114 [==============>...............] - ETA: 14s  batch_loss: 0.3245 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.3251 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.3252 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.3256 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.3273 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.3278 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.3289 [Training] 64/114 [===============>..............] - ETA: 12s  batch_loss: 0.3278 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.3282 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.3290 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.3282 [Training] 68/114 [================>.............] - ETA: 11s  batch_loss: 0.3271 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.3268 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.3268 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.3269 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.3263 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.3267 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.3262 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.3264 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.3272 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.3274 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.3274 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.3281 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.3278 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.3276 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.3277 [Training] 83/114 [====================>.........] - ETA: 7s  batch_loss: 0.3279 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.3275 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.3279 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.3282 [Training] 87/114 [=====================>........] - ETA: 6s  batch_loss: 0.3276 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.3278 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.3283 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.3279 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.3286 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.3280 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.3282 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.3271 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.3270 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.3270 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.3268 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.3267 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.3272 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.3270 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.3270 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.3269 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.3268 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.3268 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.3267 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.3268 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.3270 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.3267 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.3267 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.3260 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.3261 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.3258 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.3254 [Training] 114/114 [==============================] 256.5ms/step  batch_loss: 0.3255 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:47:07 - INFO - root -   The F1-score is 0.6089518555667002
09/25/2022 13:47:07 - INFO - root -   the best eval f1 is 0.6090, saving model !!
09/25/2022 13:47:10 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:28  batch_loss: 0.2872 [Training] 2/114 [..............................] - ETA: 58s  batch_loss: 0.3078 [Training] 3/114 [..............................] - ETA: 47s  batch_loss: 0.2939 [Training] 4/114 [>.............................] - ETA: 42s  batch_loss: 0.2911 [Training] 5/114 [>.............................] - ETA: 39s  batch_loss: 0.2892 [Training] 6/114 [>.............................] - ETA: 36s  batch_loss: 0.2964 [Training] 7/114 [>.............................] - ETA: 35s  batch_loss: 0.2947 [Training] 8/114 [=>............................] - ETA: 33s  batch_loss: 0.2966 [Training] 9/114 [=>............................] - ETA: 32s  batch_loss: 0.2935 [Training] 10/114 [=>............................] - ETA: 31s  batch_loss: 0.2992 [Training] 11/114 [=>............................] - ETA: 30s  batch_loss: 0.3041 [Training] 12/114 [==>...........................] - ETA: 30s  batch_loss: 0.3023 [Training] 13/114 [==>...........................] - ETA: 29s  batch_loss: 0.2992 [Training] 14/114 [==>...........................] - ETA: 28s  batch_loss: 0.2987 [Training] 15/114 [==>...........................] - ETA: 28s  batch_loss: 0.2980 [Training] 16/114 [===>..........................] - ETA: 27s  batch_loss: 0.2970 [Training] 17/114 [===>..........................] - ETA: 27s  batch_loss: 0.2974 [Training] 18/114 [===>..........................] - ETA: 26s  batch_loss: 0.2982 [Training] 19/114 [====>.........................] - ETA: 26s  batch_loss: 0.2976 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.2976 [Training] 21/114 [====>.........................] - ETA: 25s  batch_loss: 0.2954 [Training] 22/114 [====>.........................] - ETA: 25s  batch_loss: 0.2969 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.2979 [Training] 24/114 [=====>........................] - ETA: 24s  batch_loss: 0.3006 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 0.2989 [Training] 26/114 [=====>........................] - ETA: 23s  batch_loss: 0.2983 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 0.2977 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 0.2949 [Training] 29/114 [======>.......................] - ETA: 22s  batch_loss: 0.2955 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.2972 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.2974 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.2994 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.3019 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.3015 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.3017 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 0.3011 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.3011 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.3028 [Training] 39/114 [=========>....................] - ETA: 19s  batch_loss: 0.3044 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.3037 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.3033 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.3034 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.3031 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.3015 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.3009 [Training] 46/114 [===========>..................] - ETA: 17s  batch_loss: 0.3007 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.3007 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.3011 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.3020 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.3008 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.2998 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.2994 [Training] 53/114 [============>.................] - ETA: 15s  batch_loss: 0.2985 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.2984 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.2980 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.2976 [Training] 57/114 [==============>...............] - ETA: 14s  batch_loss: 0.2984 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.2990 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.2984 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.2984 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.2988 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.2978 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.2974 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.2975 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.2979 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.2984 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.2987 [Training] 68/114 [================>.............] - ETA: 11s  batch_loss: 0.2992 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.2999 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.3010 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.3013 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.3017 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.3012 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.3007 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.3008 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.3006 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.3002 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.2999 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.3000 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.3003 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.3016 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.3011 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.3014 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.3013 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.3013 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.3010 [Training] 87/114 [=====================>........] - ETA: 6s  batch_loss: 0.3009 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.3009 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.3002 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.3007 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.3013 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.3010 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.3009 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.3008 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.3001 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.3002 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.3002 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.3002 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.3004 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.3005 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.3009 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.3002 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.3001 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.3000 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.2999 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.2998 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.2998 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.2994 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.2989 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.2993 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.3000 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.3000 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.2996 [Training] 114/114 [==============================] 257.7ms/step  batch_loss: 0.2993 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:47:46 - INFO - root -   The F1-score is 0.6181493754779506
09/25/2022 13:47:46 - INFO - root -   the best eval f1 is 0.6181, saving model !!
09/25/2022 13:47:49 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:31  batch_loss: 0.2619 [Training] 2/114 [..............................] - ETA: 59s  batch_loss: 0.3114 [Training] 3/114 [..............................] - ETA: 49s  batch_loss: 0.2983 [Training] 4/114 [>.............................] - ETA: 43s  batch_loss: 0.2922 [Training] 5/114 [>.............................] - ETA: 40s  batch_loss: 0.2912 [Training] 6/114 [>.............................] - ETA: 38s  batch_loss: 0.2841 [Training] 7/114 [>.............................] - ETA: 36s  batch_loss: 0.2778 [Training] 8/114 [=>............................] - ETA: 35s  batch_loss: 0.2850 [Training] 9/114 [=>............................] - ETA: 33s  batch_loss: 0.2887 [Training] 10/114 [=>............................] - ETA: 32s  batch_loss: 0.2871 [Training] 11/114 [=>............................] - ETA: 32s  batch_loss: 0.2847 [Training] 12/114 [==>...........................] - ETA: 31s  batch_loss: 0.2824 [Training] 13/114 [==>...........................] - ETA: 30s  batch_loss: 0.2846 [Training] 14/114 [==>...........................] - ETA: 30s  batch_loss: 0.2807 [Training] 15/114 [==>...........................] - ETA: 29s  batch_loss: 0.2781 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 0.2758 [Training] 17/114 [===>..........................] - ETA: 28s  batch_loss: 0.2736 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 0.2767 [Training] 19/114 [====>.........................] - ETA: 27s  batch_loss: 0.2774 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.2757 [Training] 21/114 [====>.........................] - ETA: 26s  batch_loss: 0.2751 [Training] 22/114 [====>.........................] - ETA: 26s  batch_loss: 0.2742 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.2730 [Training] 24/114 [=====>........................] - ETA: 25s  batch_loss: 0.2752 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 0.2740 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.2740 [Training] 27/114 [======>.......................] - ETA: 24s  batch_loss: 0.2729 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 0.2755 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 0.2764 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.2774 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.2765 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.2758 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.2763 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.2757 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.2764 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 0.2760 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.2755 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.2747 [Training] 39/114 [=========>....................] - ETA: 20s  batch_loss: 0.2747 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.2746 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.2763 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.2766 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.2757 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.2763 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.2762 [Training] 46/114 [===========>..................] - ETA: 17s  batch_loss: 0.2756 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.2761 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.2765 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.2757 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.2755 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.2740 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.2747 [Training] 53/114 [============>.................] - ETA: 15s  batch_loss: 0.2737 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.2739 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.2740 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.2731 [Training] 57/114 [==============>...............] - ETA: 14s  batch_loss: 0.2737 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.2728 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.2727 [Training] 60/114 [==============>...............] - ETA: 13s  batch_loss: 0.2723 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.2720 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.2716 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.2714 [Training] 64/114 [===============>..............] - ETA: 12s  batch_loss: 0.2711 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.2711 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.2713 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.2718 [Training] 68/114 [================>.............] - ETA: 11s  batch_loss: 0.2706 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.2709 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.2713 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.2712 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.2711 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.2711 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.2708 [Training] 75/114 [==================>...........] - ETA: 9s  batch_loss: 0.2712 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.2716 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.2719 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.2719 [Training] 79/114 [===================>..........] - ETA: 8s  batch_loss: 0.2719 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.2720 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.2719 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.2711 [Training] 83/114 [====================>.........] - ETA: 7s  batch_loss: 0.2723 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.2720 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.2719 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.2719 [Training] 87/114 [=====================>........] - ETA: 6s  batch_loss: 0.2721 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.2718 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.2719 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.2716 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.2716 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.2711 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.2706 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.2703 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.2700 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.2701 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.2701 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.2703 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.2703 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.2702 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.2708 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.2712 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.2710 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.2711 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.2716 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.2719 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.2721 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.2728 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.2725 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.2726 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.2724 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.2718 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.2719 [Training] 114/114 [==============================] 251.8ms/step  batch_loss: 0.2719 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:48:24 - INFO - root -   The F1-score is 0.6185487000388048
09/25/2022 13:48:24 - INFO - root -   the best eval f1 is 0.6185, saving model !!
09/25/2022 13:48:27 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:30  batch_loss: 0.2406 [Training] 2/114 [..............................] - ETA: 59s  batch_loss: 0.2435 [Training] 3/114 [..............................] - ETA: 48s  batch_loss: 0.2685 [Training] 4/114 [>.............................] - ETA: 43s  batch_loss: 0.2703 [Training] 5/114 [>.............................] - ETA: 39s  batch_loss: 0.2779 [Training] 6/114 [>.............................] - ETA: 37s  batch_loss: 0.2708 [Training] 7/114 [>.............................] - ETA: 35s  batch_loss: 0.2624 [Training] 8/114 [=>............................] - ETA: 34s  batch_loss: 0.2632 [Training] 9/114 [=>............................] - ETA: 33s  batch_loss: 0.2588 [Training] 10/114 [=>............................] - ETA: 32s  batch_loss: 0.2636 [Training] 11/114 [=>............................] - ETA: 31s  batch_loss: 0.2561 [Training] 12/114 [==>...........................] - ETA: 30s  batch_loss: 0.2565 [Training] 13/114 [==>...........................] - ETA: 29s  batch_loss: 0.2560 [Training] 14/114 [==>...........................] - ETA: 29s  batch_loss: 0.2585 [Training] 15/114 [==>...........................] - ETA: 28s  batch_loss: 0.2553 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 0.2516 [Training] 17/114 [===>..........................] - ETA: 27s  batch_loss: 0.2518 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 0.2502 [Training] 19/114 [====>.........................] - ETA: 26s  batch_loss: 0.2478 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.2503 [Training] 21/114 [====>.........................] - ETA: 26s  batch_loss: 0.2479 [Training] 22/114 [====>.........................] - ETA: 25s  batch_loss: 0.2484 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.2513 [Training] 24/114 [=====>........................] - ETA: 24s  batch_loss: 0.2501 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 0.2479 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.2474 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 0.2470 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 0.2464 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 0.2450 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.2459 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.2469 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.2479 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.2472 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.2458 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.2454 [Training] 36/114 [========>.....................] - ETA: 21s  batch_loss: 0.2450 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.2448 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.2448 [Training] 39/114 [=========>....................] - ETA: 20s  batch_loss: 0.2453 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.2448 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.2439 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.2434 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.2427 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.2434 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.2448 [Training] 46/114 [===========>..................] - ETA: 18s  batch_loss: 0.2448 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.2458 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.2448 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.2451 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.2450 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.2480 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.2479 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.2475 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.2481 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.2484 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.2486 [Training] 57/114 [==============>...............] - ETA: 15s  batch_loss: 0.2486 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.2487 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.2492 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.2503 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.2496 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.2493 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.2489 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.2486 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.2477 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.2483 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.2478 [Training] 68/114 [================>.............] - ETA: 12s  batch_loss: 0.2478 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.2476 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.2474 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.2474 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.2482 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.2483 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.2487 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.2484 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.2479 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.2484 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.2486 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.2487 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.2489 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.2490 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.2486 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.2484 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.2479 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.2475 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.2477 [Training] 87/114 [=====================>........] - ETA: 7s  batch_loss: 0.2479 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.2476 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.2475 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.2475 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.2484 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.2483 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.2477 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.2482 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.2477 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.2476 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.2475 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.2478 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.2482 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.2481 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.2481 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.2494 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.2496 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.2491 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.2497 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.2507 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.2505 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.2508 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.2510 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.2509 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.2511 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.2510 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.2512 [Training] 114/114 [==============================] 257.7ms/step  batch_loss: 0.2516 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:49:03 - INFO - root -   The F1-score is 0.6255791812123136
09/25/2022 13:49:03 - INFO - root -   the best eval f1 is 0.6256, saving model !!
09/25/2022 13:49:06 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:36  batch_loss: 0.1917 [Training] 2/114 [..............................] - ETA: 1:02  batch_loss: 0.1860 [Training] 3/114 [..............................] - ETA: 50s  batch_loss: 0.1993 [Training] 4/114 [>.............................] - ETA: 44s  batch_loss: 0.2101 [Training] 5/114 [>.............................] - ETA: 40s  batch_loss: 0.2191 [Training] 6/114 [>.............................] - ETA: 38s  batch_loss: 0.2281 [Training] 7/114 [>.............................] - ETA: 36s  batch_loss: 0.2240 [Training] 8/114 [=>............................] - ETA: 34s  batch_loss: 0.2296 [Training] 9/114 [=>............................] - ETA: 33s  batch_loss: 0.2260 [Training] 10/114 [=>............................] - ETA: 32s  batch_loss: 0.2229 [Training] 11/114 [=>............................] - ETA: 31s  batch_loss: 0.2272 [Training] 12/114 [==>...........................] - ETA: 30s  batch_loss: 0.2260 [Training] 13/114 [==>...........................] - ETA: 30s  batch_loss: 0.2269 [Training] 14/114 [==>...........................] - ETA: 29s  batch_loss: 0.2260 [Training] 15/114 [==>...........................] - ETA: 28s  batch_loss: 0.2295 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 0.2338 [Training] 17/114 [===>..........................] - ETA: 27s  batch_loss: 0.2355 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 0.2391 [Training] 19/114 [====>.........................] - ETA: 26s  batch_loss: 0.2359 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.2352 [Training] 21/114 [====>.........................] - ETA: 26s  batch_loss: 0.2348 [Training] 22/114 [====>.........................] - ETA: 25s  batch_loss: 0.2354 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.2341 [Training] 24/114 [=====>........................] - ETA: 24s  batch_loss: 0.2371 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 0.2367 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.2382 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 0.2391 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 0.2412 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 0.2378 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.2383 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.2372 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.2382 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.2378 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.2393 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.2388 [Training] 36/114 [========>.....................] - ETA: 21s  batch_loss: 0.2389 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.2374 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.2364 [Training] 39/114 [=========>....................] - ETA: 20s  batch_loss: 0.2361 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.2367 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.2367 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.2375 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.2372 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.2363 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.2365 [Training] 46/114 [===========>..................] - ETA: 18s  batch_loss: 0.2362 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.2364 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.2364 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.2369 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.2375 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.2367 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.2370 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.2373 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.2375 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.2366 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.2361 [Training] 57/114 [==============>...............] - ETA: 15s  batch_loss: 0.2364 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.2368 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.2366 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.2364 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.2356 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.2355 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.2357 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.2366 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.2366 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.2368 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.2364 [Training] 68/114 [================>.............] - ETA: 12s  batch_loss: 0.2358 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.2358 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.2361 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.2356 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.2358 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.2360 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.2356 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.2354 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.2356 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.2356 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.2356 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.2352 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.2351 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.2349 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.2344 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.2342 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.2340 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.2345 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.2353 [Training] 87/114 [=====================>........] - ETA: 7s  batch_loss: 0.2349 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.2354 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.2359 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.2357 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.2356 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.2351 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.2347 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.2344 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.2355 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.2355 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.2349 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.2352 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.2351 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.2364 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.2364 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.2362 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.2357 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.2360 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.2370 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.2366 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.2366 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.2361 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.2358 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.2360 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.2358 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.2359 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.2363 [Training] 114/114 [==============================] 259.6ms/step  batch_loss: 0.2361 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:49:42 - INFO - root -   The F1-score is 0.6340019730351857
09/25/2022 13:49:42 - INFO - root -   the best eval f1 is 0.6340, saving model !!
09/25/2022 13:49:46 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:31  batch_loss: 0.1760 [Training] 2/114 [..............................] - ETA: 59s  batch_loss: 0.1879 [Training] 3/114 [..............................] - ETA: 48s  batch_loss: 0.1925 [Training] 4/114 [>.............................] - ETA: 42s  batch_loss: 0.1965 [Training] 5/114 [>.............................] - ETA: 39s  batch_loss: 0.2014 [Training] 6/114 [>.............................] - ETA: 37s  batch_loss: 0.2060 [Training] 7/114 [>.............................] - ETA: 35s  batch_loss: 0.2099 [Training] 8/114 [=>............................] - ETA: 34s  batch_loss: 0.2151 [Training] 9/114 [=>............................] - ETA: 33s  batch_loss: 0.2154 [Training] 10/114 [=>............................] - ETA: 32s  batch_loss: 0.2125 [Training] 11/114 [=>............................] - ETA: 31s  batch_loss: 0.2143 [Training] 12/114 [==>...........................] - ETA: 30s  batch_loss: 0.2154 [Training] 13/114 [==>...........................] - ETA: 29s  batch_loss: 0.2109 [Training] 14/114 [==>...........................] - ETA: 29s  batch_loss: 0.2101 [Training] 15/114 [==>...........................] - ETA: 28s  batch_loss: 0.2140 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 0.2124 [Training] 17/114 [===>..........................] - ETA: 27s  batch_loss: 0.2152 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 0.2165 [Training] 19/114 [====>.........................] - ETA: 26s  batch_loss: 0.2141 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.2146 [Training] 21/114 [====>.........................] - ETA: 26s  batch_loss: 0.2107 [Training] 22/114 [====>.........................] - ETA: 25s  batch_loss: 0.2107 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.2114 [Training] 24/114 [=====>........................] - ETA: 24s  batch_loss: 0.2115 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 0.2149 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.2140 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 0.2143 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 0.2150 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 0.2148 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.2139 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.2135 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.2123 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.2121 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.2138 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.2142 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 0.2130 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.2135 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.2133 [Training] 39/114 [=========>....................] - ETA: 20s  batch_loss: 0.2117 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.2129 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.2127 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.2135 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.2130 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.2135 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.2145 [Training] 46/114 [===========>..................] - ETA: 18s  batch_loss: 0.2166 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.2185 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.2178 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.2181 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.2176 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.2177 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.2179 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.2172 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.2175 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.2175 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.2176 [Training] 57/114 [==============>...............] - ETA: 15s  batch_loss: 0.2182 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.2182 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.2180 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.2179 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.2185 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.2178 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.2179 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.2179 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.2175 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.2168 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.2171 [Training] 68/114 [================>.............] - ETA: 12s  batch_loss: 0.2176 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.2171 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.2173 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.2174 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.2173 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.2174 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.2171 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.2177 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.2178 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.2175 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.2179 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.2178 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.2180 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.2181 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.2177 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.2174 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.2175 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.2172 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.2169 [Training] 87/114 [=====================>........] - ETA: 7s  batch_loss: 0.2169 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.2176 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.2181 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.2178 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.2178 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.2181 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.2188 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.2190 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.2187 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.2182 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.2186 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.2183 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.2182 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.2181 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.2181 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.2179 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.2181 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.2186 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.2181 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.2185 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.2185 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.2185 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.2185 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.2190 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.2194 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.2198 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.2198 [Training] 114/114 [==============================] 258.1ms/step  batch_loss: 0.2200 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:50:22 - INFO - root -   The F1-score is 0.6446089475378178
09/25/2022 13:50:22 - INFO - root -   the best eval f1 is 0.6446, saving model !!
09/25/2022 13:50:24 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:30  batch_loss: 0.1635 [Training] 2/114 [..............................] - ETA: 58s  batch_loss: 0.1701 [Training] 3/114 [..............................] - ETA: 48s  batch_loss: 0.1893 [Training] 4/114 [>.............................] - ETA: 43s  batch_loss: 0.2005 [Training] 5/114 [>.............................] - ETA: 39s  batch_loss: 0.2081 [Training] 6/114 [>.............................] - ETA: 37s  batch_loss: 0.2100 [Training] 7/114 [>.............................] - ETA: 35s  batch_loss: 0.2061 [Training] 8/114 [=>............................] - ETA: 34s  batch_loss: 0.2061 [Training] 9/114 [=>............................] - ETA: 33s  batch_loss: 0.2053 [Training] 10/114 [=>............................] - ETA: 32s  batch_loss: 0.2019 [Training] 11/114 [=>............................] - ETA: 31s  batch_loss: 0.2035 [Training] 12/114 [==>...........................] - ETA: 30s  batch_loss: 0.2056 [Training] 13/114 [==>...........................] - ETA: 29s  batch_loss: 0.2076 [Training] 14/114 [==>...........................] - ETA: 29s  batch_loss: 0.2082 [Training] 15/114 [==>...........................] - ETA: 28s  batch_loss: 0.2080 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 0.2137 [Training] 17/114 [===>..........................] - ETA: 27s  batch_loss: 0.2121 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 0.2101 [Training] 19/114 [====>.........................] - ETA: 26s  batch_loss: 0.2116 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.2104 [Training] 21/114 [====>.........................] - ETA: 25s  batch_loss: 0.2104 [Training] 22/114 [====>.........................] - ETA: 25s  batch_loss: 0.2111 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.2094 [Training] 24/114 [=====>........................] - ETA: 24s  batch_loss: 0.2078 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 0.2078 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.2075 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 0.2087 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 0.2082 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 0.2089 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.2078 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.2096 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.2093 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.2074 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.2082 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.2070 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 0.2080 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.2086 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.2084 [Training] 39/114 [=========>....................] - ETA: 19s  batch_loss: 0.2087 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.2093 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.2092 [Training] 42/114 [==========>...................] - ETA: 18s  batch_loss: 0.2091 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.2092 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.2093 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.2087 [Training] 46/114 [===========>..................] - ETA: 17s  batch_loss: 0.2083 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.2082 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.2078 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.2084 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.2090 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.2082 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.2080 [Training] 53/114 [============>.................] - ETA: 15s  batch_loss: 0.2075 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.2066 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.2073 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.2077 [Training] 57/114 [==============>...............] - ETA: 14s  batch_loss: 0.2080 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.2073 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.2078 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.2071 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.2074 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.2068 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.2066 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.2066 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.2069 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.2064 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.2067 [Training] 68/114 [================>.............] - ETA: 11s  batch_loss: 0.2063 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.2055 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.2056 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.2050 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.2050 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.2051 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.2056 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.2057 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.2069 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.2067 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.2067 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.2066 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.2064 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.2062 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.2066 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.2065 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.2059 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.2058 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.2063 [Training] 87/114 [=====================>........] - ETA: 6s  batch_loss: 0.2058 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.2050 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.2052 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.2056 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.2060 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.2061 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.2054 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.2055 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.2055 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.2059 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.2062 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.2060 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.2059 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.2063 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.2064 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.2062 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.2060 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.2059 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.2060 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.2057 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.2058 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.2058 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.2057 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.2057 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.2062 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.2062 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.2060 [Training] 114/114 [==============================] 257.4ms/step  batch_loss: 0.2065 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:51:00 - INFO - root -   The F1-score is 0.6478435435824375
09/25/2022 13:51:00 - INFO - root -   the best eval f1 is 0.6478, saving model !!
09/25/2022 13:51:03 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:31  batch_loss: 0.1842 [Training] 2/114 [..............................] - ETA: 59s  batch_loss: 0.1869 [Training] 3/114 [..............................] - ETA: 48s  batch_loss: 0.1814 [Training] 4/114 [>.............................] - ETA: 43s  batch_loss: 0.1725 [Training] 5/114 [>.............................] - ETA: 39s  batch_loss: 0.1750 [Training] 6/114 [>.............................] - ETA: 37s  batch_loss: 0.1774 [Training] 7/114 [>.............................] - ETA: 35s  batch_loss: 0.1751 [Training] 8/114 [=>............................] - ETA: 34s  batch_loss: 0.1757 [Training] 9/114 [=>............................] - ETA: 33s  batch_loss: 0.1780 [Training] 10/114 [=>............................] - ETA: 32s  batch_loss: 0.1770 [Training] 11/114 [=>............................] - ETA: 31s  batch_loss: 0.1775 [Training] 12/114 [==>...........................] - ETA: 30s  batch_loss: 0.1737 [Training] 13/114 [==>...........................] - ETA: 29s  batch_loss: 0.1769 [Training] 14/114 [==>...........................] - ETA: 29s  batch_loss: 0.1756 [Training] 15/114 [==>...........................] - ETA: 28s  batch_loss: 0.1792 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 0.1807 [Training] 17/114 [===>..........................] - ETA: 27s  batch_loss: 0.1871 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 0.1885 [Training] 19/114 [====>.........................] - ETA: 26s  batch_loss: 0.1872 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.1879 [Training] 21/114 [====>.........................] - ETA: 25s  batch_loss: 0.1882 [Training] 22/114 [====>.........................] - ETA: 25s  batch_loss: 0.1895 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.1888 [Training] 24/114 [=====>........................] - ETA: 24s  batch_loss: 0.1884 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 0.1902 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.1891 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 0.1902 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 0.1926 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 0.1910 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.1905 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.1901 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.1898 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.1906 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.1893 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.1895 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 0.1890 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.1888 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.1884 [Training] 39/114 [=========>....................] - ETA: 20s  batch_loss: 0.1888 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.1889 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.1883 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.1883 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.1872 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.1863 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.1874 [Training] 46/114 [===========>..................] - ETA: 18s  batch_loss: 0.1870 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.1883 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.1892 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.1887 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.1890 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.1894 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.1892 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.1886 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.1885 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.1887 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.1888 [Training] 57/114 [==============>...............] - ETA: 14s  batch_loss: 0.1896 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.1897 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.1891 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.1892 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.1890 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.1891 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.1898 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.1899 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.1899 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.1900 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.1908 [Training] 68/114 [================>.............] - ETA: 11s  batch_loss: 0.1910 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.1915 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.1908 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.1909 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.1916 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.1920 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.1918 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.1916 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.1914 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.1915 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.1918 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.1922 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.1918 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.1920 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.1917 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.1917 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.1920 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.1925 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.1923 [Training] 87/114 [=====================>........] - ETA: 6s  batch_loss: 0.1920 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.1921 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.1917 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.1915 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.1916 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.1919 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.1918 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.1920 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.1923 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.1926 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.1926 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.1923 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.1927 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.1929 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.1936 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.1937 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.1941 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.1943 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.1939 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.1937 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.1932 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.1938 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.1936 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.1935 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.1933 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.1939 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.1939 [Training] 114/114 [==============================] 257.9ms/step  batch_loss: 0.1944 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:51:39 - INFO - root -   The F1-score is 0.6424486612945152
09/25/2022 13:51:39 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:26  batch_loss: 0.1627 [Training] 2/114 [..............................] - ETA: 58s  batch_loss: 0.1737 [Training] 3/114 [..............................] - ETA: 48s  batch_loss: 0.1703 [Training] 4/114 [>.............................] - ETA: 43s  batch_loss: 0.1582 [Training] 5/114 [>.............................] - ETA: 39s  batch_loss: 0.1704 [Training] 6/114 [>.............................] - ETA: 37s  batch_loss: 0.1753 [Training] 7/114 [>.............................] - ETA: 36s  batch_loss: 0.1721 [Training] 8/114 [=>............................] - ETA: 34s  batch_loss: 0.1751 [Training] 9/114 [=>............................] - ETA: 33s  batch_loss: 0.1752 [Training] 10/114 [=>............................] - ETA: 32s  batch_loss: 0.1781 [Training] 11/114 [=>............................] - ETA: 31s  batch_loss: 0.1775 [Training] 12/114 [==>...........................] - ETA: 31s  batch_loss: 0.1756 [Training] 13/114 [==>...........................] - ETA: 30s  batch_loss: 0.1729 [Training] 14/114 [==>...........................] - ETA: 29s  batch_loss: 0.1729 [Training] 15/114 [==>...........................] - ETA: 29s  batch_loss: 0.1734 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 0.1800 [Training] 17/114 [===>..........................] - ETA: 28s  batch_loss: 0.1819 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 0.1860 [Training] 19/114 [====>.........................] - ETA: 27s  batch_loss: 0.1853 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.1852 [Training] 21/114 [====>.........................] - ETA: 26s  batch_loss: 0.1857 [Training] 22/114 [====>.........................] - ETA: 26s  batch_loss: 0.1837 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.1837 [Training] 24/114 [=====>........................] - ETA: 25s  batch_loss: 0.1843 [Training] 25/114 [=====>........................] - ETA: 25s  batch_loss: 0.1828 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.1816 [Training] 27/114 [======>.......................] - ETA: 24s  batch_loss: 0.1816 [Training] 28/114 [======>.......................] - ETA: 24s  batch_loss: 0.1815 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 0.1809 [Training] 30/114 [======>.......................] - ETA: 23s  batch_loss: 0.1807 [Training] 31/114 [=======>......................] - ETA: 23s  batch_loss: 0.1806 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.1806 [Training] 33/114 [=======>......................] - ETA: 22s  batch_loss: 0.1804 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.1812 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.1803 [Training] 36/114 [========>.....................] - ETA: 21s  batch_loss: 0.1804 [Training] 37/114 [========>.....................] - ETA: 21s  batch_loss: 0.1805 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.1792 [Training] 39/114 [=========>....................] - ETA: 20s  batch_loss: 0.1806 [Training] 40/114 [=========>....................] - ETA: 20s  batch_loss: 0.1806 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.1804 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.1806 [Training] 43/114 [==========>...................] - ETA: 19s  batch_loss: 0.1806 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.1820 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.1824 [Training] 46/114 [===========>..................] - ETA: 18s  batch_loss: 0.1817 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.1813 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.1820 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.1821 [Training] 50/114 [============>.................] - ETA: 17s  batch_loss: 0.1825 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.1825 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.1824 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.1821 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.1826 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.1827 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.1824 [Training] 57/114 [==============>...............] - ETA: 15s  batch_loss: 0.1833 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.1829 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.1831 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.1831 [Training] 61/114 [===============>..............] - ETA: 14s  batch_loss: 0.1843 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.1840 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.1834 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.1833 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.1832 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.1830 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.1835 [Training] 68/114 [================>.............] - ETA: 12s  batch_loss: 0.1828 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.1839 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.1845 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.1843 [Training] 72/114 [=================>............] - ETA: 11s  batch_loss: 0.1847 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.1845 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.1844 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.1847 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.1847 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.1857 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.1861 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.1863 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.1860 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.1859 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.1855 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.1855 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.1853 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.1848 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.1844 [Training] 87/114 [=====================>........] - ETA: 7s  batch_loss: 0.1846 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.1847 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.1847 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.1840 [Training] 91/114 [======================>.......] - ETA: 6s  batch_loss: 0.1845 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.1845 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.1846 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.1847 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.1845 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.1840 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.1838 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.1832 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.1833 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.1831 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.1834 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.1833 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.1831 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.1828 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.1830 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.1835 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.1840 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.1847 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.1843 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.1843 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.1844 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.1848 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.1846 [Training] 114/114 [==============================] 259.5ms/step  batch_loss: 0.1851 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:52:16 - INFO - root -   The F1-score is 0.6422928771952732
09/25/2022 13:52:16 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:29  batch_loss: 0.1921 [Training] 2/114 [..............................] - ETA: 59s  batch_loss: 0.1931 [Training] 3/114 [..............................] - ETA: 48s  batch_loss: 0.1908 [Training] 4/114 [>.............................] - ETA: 43s  batch_loss: 0.1763 [Training] 5/114 [>.............................] - ETA: 40s  batch_loss: 0.1756 [Training] 6/114 [>.............................] - ETA: 37s  batch_loss: 0.1729 [Training] 7/114 [>.............................] - ETA: 35s  batch_loss: 0.1755 [Training] 8/114 [=>............................] - ETA: 34s  batch_loss: 0.1750 [Training] 9/114 [=>............................] - ETA: 33s  batch_loss: 0.1786 [Training] 10/114 [=>............................] - ETA: 32s  batch_loss: 0.1748 [Training] 11/114 [=>............................] - ETA: 31s  batch_loss: 0.1749 [Training] 12/114 [==>...........................] - ETA: 30s  batch_loss: 0.1765 [Training] 13/114 [==>...........................] - ETA: 29s  batch_loss: 0.1767 [Training] 14/114 [==>...........................] - ETA: 29s  batch_loss: 0.1732 [Training] 15/114 [==>...........................] - ETA: 28s  batch_loss: 0.1721 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 0.1748 [Training] 17/114 [===>..........................] - ETA: 27s  batch_loss: 0.1752 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 0.1752 [Training] 19/114 [====>.........................] - ETA: 26s  batch_loss: 0.1752 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.1765 [Training] 21/114 [====>.........................] - ETA: 26s  batch_loss: 0.1776 [Training] 22/114 [====>.........................] - ETA: 25s  batch_loss: 0.1778 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.1775 [Training] 24/114 [=====>........................] - ETA: 24s  batch_loss: 0.1750 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 0.1747 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.1769 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 0.1762 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 0.1763 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 0.1752 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.1774 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.1773 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.1779 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.1789 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.1788 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.1807 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 0.1813 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.1804 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.1795 [Training] 39/114 [=========>....................] - ETA: 20s  batch_loss: 0.1792 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.1778 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.1792 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.1788 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.1774 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.1762 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.1762 [Training] 46/114 [===========>..................] - ETA: 18s  batch_loss: 0.1770 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.1769 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.1768 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.1780 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.1784 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.1787 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.1784 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.1791 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.1788 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.1785 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.1790 [Training] 57/114 [==============>...............] - ETA: 14s  batch_loss: 0.1783 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.1777 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.1776 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.1778 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.1779 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.1780 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.1775 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.1779 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.1777 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.1771 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.1772 [Training] 68/114 [================>.............] - ETA: 12s  batch_loss: 0.1773 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.1770 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.1771 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.1770 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.1766 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.1768 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.1771 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.1774 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.1779 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.1783 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.1782 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.1777 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.1777 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.1775 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.1779 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.1776 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.1776 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.1778 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.1776 [Training] 87/114 [=====================>........] - ETA: 7s  batch_loss: 0.1778 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.1782 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.1780 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.1775 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.1768 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.1766 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.1763 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.1765 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.1767 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.1762 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.1763 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.1762 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.1759 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.1754 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.1754 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.1753 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.1757 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.1760 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.1758 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.1761 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.1758 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.1758 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.1757 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.1756 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.1759 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.1756 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.1756 [Training] 114/114 [==============================] 261.1ms/step  batch_loss: 0.1757 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:52:52 - INFO - root -   The F1-score is 0.6452947259565667
09/25/2022 13:52:52 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:30  batch_loss: 0.2265 [Training] 2/114 [..............................] - ETA: 59s  batch_loss: 0.2174 [Training] 3/114 [..............................] - ETA: 48s  batch_loss: 0.1914 [Training] 4/114 [>.............................] - ETA: 42s  batch_loss: 0.1808 [Training] 5/114 [>.............................] - ETA: 39s  batch_loss: 0.1733 [Training] 6/114 [>.............................] - ETA: 36s  batch_loss: 0.1740 [Training] 7/114 [>.............................] - ETA: 35s  batch_loss: 0.1678 [Training] 8/114 [=>............................] - ETA: 33s  batch_loss: 0.1696 [Training] 9/114 [=>............................] - ETA: 32s  batch_loss: 0.1671 [Training] 10/114 [=>............................] - ETA: 31s  batch_loss: 0.1633 [Training] 11/114 [=>............................] - ETA: 31s  batch_loss: 0.1649 [Training] 12/114 [==>...........................] - ETA: 30s  batch_loss: 0.1641 [Training] 13/114 [==>...........................] - ETA: 29s  batch_loss: 0.1616 [Training] 14/114 [==>...........................] - ETA: 29s  batch_loss: 0.1633 [Training] 15/114 [==>...........................] - ETA: 28s  batch_loss: 0.1650 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 0.1635 [Training] 17/114 [===>..........................] - ETA: 27s  batch_loss: 0.1629 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 0.1632 [Training] 19/114 [====>.........................] - ETA: 26s  batch_loss: 0.1632 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.1614 [Training] 21/114 [====>.........................] - ETA: 25s  batch_loss: 0.1608 [Training] 22/114 [====>.........................] - ETA: 25s  batch_loss: 0.1622 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.1655 [Training] 24/114 [=====>........................] - ETA: 24s  batch_loss: 0.1642 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 0.1640 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.1647 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 0.1659 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 0.1643 [Training] 29/114 [======>.......................] - ETA: 22s  batch_loss: 0.1657 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.1657 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.1656 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.1668 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.1663 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.1646 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.1651 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 0.1640 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.1646 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.1645 [Training] 39/114 [=========>....................] - ETA: 19s  batch_loss: 0.1649 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.1652 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.1667 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.1669 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.1666 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.1668 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.1680 [Training] 46/114 [===========>..................] - ETA: 18s  batch_loss: 0.1677 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.1678 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.1678 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.1678 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.1683 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.1691 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.1690 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.1683 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.1682 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.1680 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.1677 [Training] 57/114 [==============>...............] - ETA: 14s  batch_loss: 0.1679 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.1667 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.1669 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.1668 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.1666 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.1662 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.1658 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.1659 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.1659 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.1658 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.1657 [Training] 68/114 [================>.............] - ETA: 12s  batch_loss: 0.1657 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.1659 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.1652 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.1658 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.1660 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.1664 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.1665 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.1665 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.1662 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.1656 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.1654 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.1655 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.1661 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.1665 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.1667 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.1664 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.1665 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.1664 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.1656 [Training] 87/114 [=====================>........] - ETA: 7s  batch_loss: 0.1654 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.1655 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.1657 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.1658 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.1659 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.1659 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.1657 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.1654 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.1653 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.1653 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.1653 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.1648 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.1650 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.1648 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.1647 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.1649 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.1646 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.1648 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.1649 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.1652 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.1653 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.1655 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.1655 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.1651 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.1651 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.1653 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.1660 [Training] 114/114 [==============================] 258.5ms/step  batch_loss: 0.1663 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:53:28 - INFO - root -   The F1-score is 0.6462391864227696
09/25/2022 13:53:28 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:31  batch_loss: 0.1226 [Training] 2/114 [..............................] - ETA: 1:00  batch_loss: 0.1379 [Training] 3/114 [..............................] - ETA: 49s  batch_loss: 0.1283 [Training] 4/114 [>.............................] - ETA: 44s  batch_loss: 0.1401 [Training] 5/114 [>.............................] - ETA: 40s  batch_loss: 0.1528 [Training] 6/114 [>.............................] - ETA: 38s  batch_loss: 0.1542 [Training] 7/114 [>.............................] - ETA: 36s  batch_loss: 0.1512 [Training] 8/114 [=>............................] - ETA: 35s  batch_loss: 0.1501 [Training] 9/114 [=>............................] - ETA: 34s  batch_loss: 0.1491 [Training] 10/114 [=>............................] - ETA: 33s  batch_loss: 0.1473 [Training] 11/114 [=>............................] - ETA: 32s  batch_loss: 0.1557 [Training] 12/114 [==>...........................] - ETA: 31s  batch_loss: 0.1533 [Training] 13/114 [==>...........................] - ETA: 30s  batch_loss: 0.1539 [Training] 14/114 [==>...........................] - ETA: 30s  batch_loss: 0.1504 [Training] 15/114 [==>...........................] - ETA: 29s  batch_loss: 0.1503 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 0.1511 [Training] 17/114 [===>..........................] - ETA: 28s  batch_loss: 0.1533 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 0.1521 [Training] 19/114 [====>.........................] - ETA: 27s  batch_loss: 0.1532 [Training] 20/114 [====>.........................] - ETA: 27s  batch_loss: 0.1520 [Training] 21/114 [====>.........................] - ETA: 26s  batch_loss: 0.1519 [Training] 22/114 [====>.........................] - ETA: 26s  batch_loss: 0.1539 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.1546 [Training] 24/114 [=====>........................] - ETA: 25s  batch_loss: 0.1542 [Training] 25/114 [=====>........................] - ETA: 25s  batch_loss: 0.1542 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.1530 [Training] 27/114 [======>.......................] - ETA: 24s  batch_loss: 0.1529 [Training] 28/114 [======>.......................] - ETA: 24s  batch_loss: 0.1546 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 0.1542 [Training] 30/114 [======>.......................] - ETA: 23s  batch_loss: 0.1536 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.1547 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.1537 [Training] 33/114 [=======>......................] - ETA: 22s  batch_loss: 0.1538 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.1534 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.1534 [Training] 36/114 [========>.....................] - ETA: 21s  batch_loss: 0.1535 [Training] 37/114 [========>.....................] - ETA: 21s  batch_loss: 0.1537 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.1528 [Training] 39/114 [=========>....................] - ETA: 20s  batch_loss: 0.1526 [Training] 40/114 [=========>....................] - ETA: 20s  batch_loss: 0.1527 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.1520 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.1511 [Training] 43/114 [==========>...................] - ETA: 19s  batch_loss: 0.1516 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.1512 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.1519 [Training] 46/114 [===========>..................] - ETA: 18s  batch_loss: 0.1517 [Training] 47/114 [===========>..................] - ETA: 18s  batch_loss: 0.1516 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.1505 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.1497 [Training] 50/114 [============>.................] - ETA: 17s  batch_loss: 0.1504 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.1509 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.1502 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.1507 [Training] 54/114 [=============>................] - ETA: 16s  batch_loss: 0.1520 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.1522 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.1529 [Training] 57/114 [==============>...............] - ETA: 15s  batch_loss: 0.1521 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.1522 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.1519 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.1516 [Training] 61/114 [===============>..............] - ETA: 14s  batch_loss: 0.1516 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.1514 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.1509 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.1505 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.1503 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.1503 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.1505 [Training] 68/114 [================>.............] - ETA: 12s  batch_loss: 0.1505 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.1507 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.1512 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.1519 [Training] 72/114 [=================>............] - ETA: 11s  batch_loss: 0.1514 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.1516 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.1518 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.1524 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.1523 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.1523 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.1521 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.1520 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.1519 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.1520 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.1522 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.1521 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.1523 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.1528 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.1524 [Training] 87/114 [=====================>........] - ETA: 7s  batch_loss: 0.1536 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.1539 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.1546 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.1550 [Training] 91/114 [======================>.......] - ETA: 6s  batch_loss: 0.1554 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.1560 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.1571 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.1570 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.1572 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.1575 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.1570 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.1577 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.1584 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.1587 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.1589 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.1587 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.1587 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.1588 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.1589 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.1587 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.1589 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.1590 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.1593 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.1592 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.1595 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.1593 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.1596 [Training] 114/114 [==============================] 259.3ms/step  batch_loss: 0.1600 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:54:04 - INFO - root -   The F1-score is 0.6534225019669552
09/25/2022 13:54:04 - INFO - root -   the best eval f1 is 0.6534, saving model !!
09/25/2022 13:54:07 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:32  batch_loss: 0.1604 [Training] 2/114 [..............................] - ETA: 1:01  batch_loss: 0.1350 [Training] 3/114 [..............................] - ETA: 50s  batch_loss: 0.1542 [Training] 4/114 [>.............................] - ETA: 44s  batch_loss: 0.1500 [Training] 5/114 [>.............................] - ETA: 40s  batch_loss: 0.1466 [Training] 6/114 [>.............................] - ETA: 38s  batch_loss: 0.1488 [Training] 7/114 [>.............................] - ETA: 36s  batch_loss: 0.1529 [Training] 8/114 [=>............................] - ETA: 35s  batch_loss: 0.1564 [Training] 9/114 [=>............................] - ETA: 33s  batch_loss: 0.1553 [Training] 10/114 [=>............................] - ETA: 32s  batch_loss: 0.1597 [Training] 11/114 [=>............................] - ETA: 32s  batch_loss: 0.1584 [Training] 12/114 [==>...........................] - ETA: 31s  batch_loss: 0.1564 [Training] 13/114 [==>...........................] - ETA: 30s  batch_loss: 0.1576 [Training] 14/114 [==>...........................] - ETA: 29s  batch_loss: 0.1578 [Training] 15/114 [==>...........................] - ETA: 29s  batch_loss: 0.1587 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 0.1563 [Training] 17/114 [===>..........................] - ETA: 28s  batch_loss: 0.1543 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 0.1532 [Training] 19/114 [====>.........................] - ETA: 27s  batch_loss: 0.1539 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.1536 [Training] 21/114 [====>.........................] - ETA: 26s  batch_loss: 0.1546 [Training] 22/114 [====>.........................] - ETA: 26s  batch_loss: 0.1518 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.1495 [Training] 24/114 [=====>........................] - ETA: 25s  batch_loss: 0.1509 [Training] 25/114 [=====>........................] - ETA: 25s  batch_loss: 0.1524 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.1508 [Training] 27/114 [======>.......................] - ETA: 24s  batch_loss: 0.1528 [Training] 28/114 [======>.......................] - ETA: 24s  batch_loss: 0.1517 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 0.1508 [Training] 30/114 [======>.......................] - ETA: 23s  batch_loss: 0.1504 [Training] 31/114 [=======>......................] - ETA: 23s  batch_loss: 0.1500 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.1508 [Training] 33/114 [=======>......................] - ETA: 22s  batch_loss: 0.1510 [Training] 34/114 [=======>......................] - ETA: 22s  batch_loss: 0.1499 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.1500 [Training] 36/114 [========>.....................] - ETA: 21s  batch_loss: 0.1495 [Training] 37/114 [========>.....................] - ETA: 21s  batch_loss: 0.1502 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.1505 [Training] 39/114 [=========>....................] - ETA: 20s  batch_loss: 0.1499 [Training] 40/114 [=========>....................] - ETA: 20s  batch_loss: 0.1495 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.1492 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.1491 [Training] 43/114 [==========>...................] - ETA: 19s  batch_loss: 0.1491 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.1492 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.1497 [Training] 46/114 [===========>..................] - ETA: 18s  batch_loss: 0.1498 [Training] 47/114 [===========>..................] - ETA: 18s  batch_loss: 0.1499 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.1495 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.1498 [Training] 50/114 [============>.................] - ETA: 17s  batch_loss: 0.1506 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.1503 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.1496 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.1495 [Training] 54/114 [=============>................] - ETA: 16s  batch_loss: 0.1498 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.1490 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.1497 [Training] 57/114 [==============>...............] - ETA: 15s  batch_loss: 0.1494 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.1497 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.1498 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.1498 [Training] 61/114 [===============>..............] - ETA: 14s  batch_loss: 0.1493 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.1503 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.1506 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.1514 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.1514 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.1510 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.1504 [Training] 68/114 [================>.............] - ETA: 12s  batch_loss: 0.1507 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.1503 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.1509 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.1508 [Training] 72/114 [=================>............] - ETA: 11s  batch_loss: 0.1518 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.1512 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.1511 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.1509 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.1509 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.1504 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.1505 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.1503 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.1502 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.1503 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.1502 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.1500 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.1501 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.1501 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.1502 [Training] 87/114 [=====================>........] - ETA: 7s  batch_loss: 0.1501 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.1503 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.1502 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.1505 [Training] 91/114 [======================>.......] - ETA: 6s  batch_loss: 0.1501 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.1508 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.1508 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.1509 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.1507 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.1511 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.1508 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.1513 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.1511 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.1511 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.1514 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.1511 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.1523 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.1522 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.1521 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.1518 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.1515 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.1518 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.1520 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.1518 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.1518 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.1516 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.1516 [Training] 114/114 [==============================] 259.6ms/step  batch_loss: 0.1514 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:54:43 - INFO - root -   The F1-score is 0.6525948876839659
09/25/2022 13:54:43 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:31  batch_loss: 0.1151 [Training] 2/114 [..............................] - ETA: 59s  batch_loss: 0.1255 [Training] 3/114 [..............................] - ETA: 49s  batch_loss: 0.1333 [Training] 4/114 [>.............................] - ETA: 43s  batch_loss: 0.1336 [Training] 5/114 [>.............................] - ETA: 40s  batch_loss: 0.1337 [Training] 6/114 [>.............................] - ETA: 37s  batch_loss: 0.1379 [Training] 7/114 [>.............................] - ETA: 36s  batch_loss: 0.1331 [Training] 8/114 [=>............................] - ETA: 34s  batch_loss: 0.1325 [Training] 9/114 [=>............................] - ETA: 33s  batch_loss: 0.1328 [Training] 10/114 [=>............................] - ETA: 32s  batch_loss: 0.1323 [Training] 11/114 [=>............................] - ETA: 31s  batch_loss: 0.1307 [Training] 12/114 [==>...........................] - ETA: 30s  batch_loss: 0.1326 [Training] 13/114 [==>...........................] - ETA: 30s  batch_loss: 0.1309 [Training] 14/114 [==>...........................] - ETA: 29s  batch_loss: 0.1309 [Training] 15/114 [==>...........................] - ETA: 28s  batch_loss: 0.1330 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 0.1327 [Training] 17/114 [===>..........................] - ETA: 27s  batch_loss: 0.1325 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 0.1348 [Training] 19/114 [====>.........................] - ETA: 26s  batch_loss: 0.1340 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.1366 [Training] 21/114 [====>.........................] - ETA: 26s  batch_loss: 0.1368 [Training] 22/114 [====>.........................] - ETA: 25s  batch_loss: 0.1371 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.1390 [Training] 24/114 [=====>........................] - ETA: 25s  batch_loss: 0.1397 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 0.1415 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.1405 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 0.1409 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 0.1412 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 0.1392 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.1400 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.1398 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.1394 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.1395 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.1397 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.1401 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 0.1392 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.1388 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.1386 [Training] 39/114 [=========>....................] - ETA: 20s  batch_loss: 0.1387 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.1388 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.1388 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.1389 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.1394 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.1402 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.1401 [Training] 46/114 [===========>..................] - ETA: 18s  batch_loss: 0.1403 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.1408 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.1398 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.1399 [Training] 50/114 [============>.................] - ETA: 17s  batch_loss: 0.1391 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.1396 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.1396 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.1397 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.1399 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.1394 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.1388 [Training] 57/114 [==============>...............] - ETA: 15s  batch_loss: 0.1386 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.1390 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.1399 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.1397 [Training] 61/114 [===============>..............] - ETA: 14s  batch_loss: 0.1404 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.1411 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.1412 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.1410 [Training] 65/114 [================>.............] - ETA: 13s  batch_loss: 0.1411 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.1412 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.1412 [Training] 68/114 [================>.............] - ETA: 12s  batch_loss: 0.1412 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.1416 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.1412 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.1411 [Training] 72/114 [=================>............] - ETA: 11s  batch_loss: 0.1408 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.1407 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.1408 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.1410 [Training] 76/114 [===================>..........] - ETA: 10s  batch_loss: 0.1408 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.1401 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.1398 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.1397 [Training] 80/114 [====================>.........] - ETA: 9s  batch_loss: 0.1402 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.1403 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.1403 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.1407 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.1405 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.1413 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.1415 [Training] 87/114 [=====================>........] - ETA: 7s  batch_loss: 0.1414 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.1414 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.1411 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.1407 [Training] 91/114 [======================>.......] - ETA: 6s  batch_loss: 0.1408 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.1410 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.1413 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.1414 [Training] 95/114 [========================>.....] - ETA: 5s  batch_loss: 0.1413 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.1410 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.1412 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.1411 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.1412 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.1413 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.1415 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.1415 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.1415 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.1415 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.1418 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.1425 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.1425 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.1423 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.1427 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.1430 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.1432 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.1432 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.1430 [Training] 114/114 [==============================] 264.1ms/step  batch_loss: 0.1434 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:55:20 - INFO - root -   The F1-score is 0.6569202035164552
09/25/2022 13:55:20 - INFO - root -   the best eval f1 is 0.6569, saving model !!
09/25/2022 13:55:23 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:36  batch_loss: 0.1247 [Training] 2/114 [..............................] - ETA: 1:02  batch_loss: 0.1406 [Training] 3/114 [..............................] - ETA: 50s  batch_loss: 0.1339 [Training] 4/114 [>.............................] - ETA: 44s  batch_loss: 0.1338 [Training] 5/114 [>.............................] - ETA: 40s  batch_loss: 0.1465 [Training] 6/114 [>.............................] - ETA: 38s  batch_loss: 0.1486 [Training] 7/114 [>.............................] - ETA: 36s  batch_loss: 0.1494 [Training] 8/114 [=>............................] - ETA: 34s  batch_loss: 0.1453 [Training] 9/114 [=>............................] - ETA: 33s  batch_loss: 0.1399 [Training] 10/114 [=>............................] - ETA: 32s  batch_loss: 0.1377 [Training] 11/114 [=>............................] - ETA: 31s  batch_loss: 0.1347 [Training] 12/114 [==>...........................] - ETA: 30s  batch_loss: 0.1340 [Training] 13/114 [==>...........................] - ETA: 30s  batch_loss: 0.1365 [Training] 14/114 [==>...........................] - ETA: 29s  batch_loss: 0.1381 [Training] 15/114 [==>...........................] - ETA: 28s  batch_loss: 0.1399 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 0.1378 [Training] 17/114 [===>..........................] - ETA: 27s  batch_loss: 0.1381 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 0.1365 [Training] 19/114 [====>.........................] - ETA: 26s  batch_loss: 0.1366 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.1345 [Training] 21/114 [====>.........................] - ETA: 26s  batch_loss: 0.1346 [Training] 22/114 [====>.........................] - ETA: 25s  batch_loss: 0.1348 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.1349 [Training] 24/114 [=====>........................] - ETA: 24s  batch_loss: 0.1350 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 0.1361 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.1355 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 0.1348 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 0.1338 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 0.1337 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.1334 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.1325 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.1317 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.1326 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.1323 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.1325 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 0.1326 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.1315 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.1307 [Training] 39/114 [=========>....................] - ETA: 20s  batch_loss: 0.1311 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.1313 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.1316 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.1310 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.1342 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.1345 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.1343 [Training] 46/114 [===========>..................] - ETA: 17s  batch_loss: 0.1336 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.1344 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.1345 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.1347 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.1359 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.1355 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.1350 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.1347 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.1355 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.1353 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.1350 [Training] 57/114 [==============>...............] - ETA: 14s  batch_loss: 0.1348 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.1349 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.1347 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.1345 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.1351 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.1348 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.1343 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.1343 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.1346 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.1350 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.1345 [Training] 68/114 [================>.............] - ETA: 11s  batch_loss: 0.1345 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.1348 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.1354 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.1362 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.1359 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.1361 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.1363 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.1360 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.1360 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.1361 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.1361 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.1358 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.1358 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.1371 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.1369 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.1372 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.1368 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.1370 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.1369 [Training] 87/114 [=====================>........] - ETA: 6s  batch_loss: 0.1367 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.1363 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.1363 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.1360 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.1358 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.1358 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.1362 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.1368 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.1369 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.1368 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.1365 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.1372 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.1372 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.1376 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.1375 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.1375 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.1376 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.1376 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.1382 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.1385 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.1384 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.1383 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.1386 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.1385 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.1384 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.1381 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.1382 [Training] 114/114 [==============================] 257.2ms/step  batch_loss: 0.1386 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:55:59 - INFO - root -   The F1-score is 0.6488588550299631
09/25/2022 13:55:59 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:25  batch_loss: 0.1198 [Training] 2/114 [..............................] - ETA: 57s  batch_loss: 0.1205 [Training] 3/114 [..............................] - ETA: 47s  batch_loss: 0.1252 [Training] 4/114 [>.............................] - ETA: 42s  batch_loss: 0.1286 [Training] 5/114 [>.............................] - ETA: 39s  batch_loss: 0.1324 [Training] 6/114 [>.............................] - ETA: 37s  batch_loss: 0.1292 [Training] 7/114 [>.............................] - ETA: 35s  batch_loss: 0.1273 [Training] 8/114 [=>............................] - ETA: 34s  batch_loss: 0.1266 [Training] 9/114 [=>............................] - ETA: 33s  batch_loss: 0.1290 [Training] 10/114 [=>............................] - ETA: 32s  batch_loss: 0.1267 [Training] 11/114 [=>............................] - ETA: 31s  batch_loss: 0.1249 [Training] 12/114 [==>...........................] - ETA: 30s  batch_loss: 0.1289 [Training] 13/114 [==>...........................] - ETA: 30s  batch_loss: 0.1297 [Training] 14/114 [==>...........................] - ETA: 29s  batch_loss: 0.1291 [Training] 15/114 [==>...........................] - ETA: 28s  batch_loss: 0.1290 [Training] 16/114 [===>..........................] - ETA: 28s  batch_loss: 0.1285 [Training] 17/114 [===>..........................] - ETA: 27s  batch_loss: 0.1270 [Training] 18/114 [===>..........................] - ETA: 27s  batch_loss: 0.1282 [Training] 19/114 [====>.........................] - ETA: 27s  batch_loss: 0.1263 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.1281 [Training] 21/114 [====>.........................] - ETA: 26s  batch_loss: 0.1284 [Training] 22/114 [====>.........................] - ETA: 25s  batch_loss: 0.1288 [Training] 23/114 [=====>........................] - ETA: 25s  batch_loss: 0.1276 [Training] 24/114 [=====>........................] - ETA: 25s  batch_loss: 0.1274 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 0.1270 [Training] 26/114 [=====>........................] - ETA: 24s  batch_loss: 0.1276 [Training] 27/114 [======>.......................] - ETA: 24s  batch_loss: 0.1276 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 0.1284 [Training] 29/114 [======>.......................] - ETA: 23s  batch_loss: 0.1279 [Training] 30/114 [======>.......................] - ETA: 23s  batch_loss: 0.1278 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.1289 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.1278 [Training] 33/114 [=======>......................] - ETA: 22s  batch_loss: 0.1277 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.1283 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.1275 [Training] 36/114 [========>.....................] - ETA: 21s  batch_loss: 0.1278 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.1270 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.1277 [Training] 39/114 [=========>....................] - ETA: 20s  batch_loss: 0.1282 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.1278 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.1287 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.1279 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.1280 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.1280 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.1280 [Training] 46/114 [===========>..................] - ETA: 18s  batch_loss: 0.1277 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.1277 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.1278 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.1280 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.1276 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.1282 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.1277 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.1278 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.1273 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.1275 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.1272 [Training] 57/114 [==============>...............] - ETA: 15s  batch_loss: 0.1268 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.1271 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.1290 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.1295 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.1297 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.1301 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.1298 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.1298 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.1298 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.1300 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.1299 [Training] 68/114 [================>.............] - ETA: 12s  batch_loss: 0.1301 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.1302 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.1301 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.1305 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.1306 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.1310 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.1308 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.1308 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.1310 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.1311 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.1313 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.1309 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.1309 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.1309 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.1309 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.1310 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.1311 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.1317 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.1318 [Training] 87/114 [=====================>........] - ETA: 7s  batch_loss: 0.1319 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.1320 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.1320 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.1325 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.1326 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.1323 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.1328 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.1328 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.1329 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.1327 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.1327 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.1328 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.1327 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.1326 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.1328 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.1329 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.1328 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.1330 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.1335 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.1336 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.1334 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.1339 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.1341 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.1337 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.1339 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.1342 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.1345 [Training] 114/114 [==============================] 257.1ms/step  batch_loss: 0.1343 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:56:35 - INFO - root -   The F1-score is 0.6524084533857967
09/25/2022 13:56:35 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:25  batch_loss: 0.1213 [Training] 2/114 [..............................] - ETA: 56s  batch_loss: 0.1388 [Training] 3/114 [..............................] - ETA: 46s  batch_loss: 0.1352 [Training] 4/114 [>.............................] - ETA: 41s  batch_loss: 0.1400 [Training] 5/114 [>.............................] - ETA: 38s  batch_loss: 0.1323 [Training] 6/114 [>.............................] - ETA: 36s  batch_loss: 0.1408 [Training] 7/114 [>.............................] - ETA: 34s  batch_loss: 0.1346 [Training] 8/114 [=>............................] - ETA: 33s  batch_loss: 0.1309 [Training] 9/114 [=>............................] - ETA: 32s  batch_loss: 0.1312 [Training] 10/114 [=>............................] - ETA: 31s  batch_loss: 0.1293 [Training] 11/114 [=>............................] - ETA: 30s  batch_loss: 0.1270 [Training] 12/114 [==>...........................] - ETA: 30s  batch_loss: 0.1270 [Training] 13/114 [==>...........................] - ETA: 29s  batch_loss: 0.1239 [Training] 14/114 [==>...........................] - ETA: 28s  batch_loss: 0.1218 [Training] 15/114 [==>...........................] - ETA: 28s  batch_loss: 0.1198 [Training] 16/114 [===>..........................] - ETA: 27s  batch_loss: 0.1218 [Training] 17/114 [===>..........................] - ETA: 27s  batch_loss: 0.1214 [Training] 18/114 [===>..........................] - ETA: 26s  batch_loss: 0.1208 [Training] 19/114 [====>.........................] - ETA: 26s  batch_loss: 0.1214 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.1212 [Training] 21/114 [====>.........................] - ETA: 25s  batch_loss: 0.1197 [Training] 22/114 [====>.........................] - ETA: 25s  batch_loss: 0.1215 [Training] 23/114 [=====>........................] - ETA: 24s  batch_loss: 0.1210 [Training] 24/114 [=====>........................] - ETA: 24s  batch_loss: 0.1197 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 0.1207 [Training] 26/114 [=====>........................] - ETA: 23s  batch_loss: 0.1203 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 0.1190 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 0.1194 [Training] 29/114 [======>.......................] - ETA: 22s  batch_loss: 0.1198 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.1194 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.1191 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.1189 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.1195 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.1202 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.1218 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 0.1222 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.1217 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.1221 [Training] 39/114 [=========>....................] - ETA: 19s  batch_loss: 0.1228 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.1230 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.1228 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.1226 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.1228 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.1222 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.1224 [Training] 46/114 [===========>..................] - ETA: 17s  batch_loss: 0.1217 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.1219 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.1219 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.1212 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.1221 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.1217 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.1227 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.1240 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.1247 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.1256 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.1260 [Training] 57/114 [==============>...............] - ETA: 14s  batch_loss: 0.1256 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.1256 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.1258 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.1255 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.1260 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.1265 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.1266 [Training] 64/114 [===============>..............] - ETA: 13s  batch_loss: 0.1265 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.1265 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.1263 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.1258 [Training] 68/114 [================>.............] - ETA: 11s  batch_loss: 0.1254 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.1253 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.1250 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.1245 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.1253 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.1254 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.1257 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.1261 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.1261 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.1264 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.1254 [Training] 79/114 [===================>..........] - ETA: 9s  batch_loss: 0.1254 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.1254 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.1254 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.1254 [Training] 83/114 [====================>.........] - ETA: 8s  batch_loss: 0.1254 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.1251 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.1251 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.1246 [Training] 87/114 [=====================>........] - ETA: 6s  batch_loss: 0.1254 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.1250 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.1246 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.1250 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.1250 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.1255 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.1253 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.1248 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.1247 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.1247 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.1254 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.1256 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.1255 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.1262 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.1266 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.1267 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.1267 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.1269 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.1266 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.1267 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.1269 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.1273 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.1274 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.1280 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.1282 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.1285 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.1287 [Training] 114/114 [==============================] 257.5ms/step  batch_loss: 0.1291 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:57:11 - INFO - root -   The F1-score is 0.6532438478747203
09/25/2022 13:57:11 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/114 [..............................] - ETA: 1:31  batch_loss: 0.1089 [Training] 2/114 [..............................] - ETA: 59s  batch_loss: 0.1039 [Training] 3/114 [..............................] - ETA: 48s  batch_loss: 0.1093 [Training] 4/114 [>.............................] - ETA: 42s  batch_loss: 0.1150 [Training] 5/114 [>.............................] - ETA: 39s  batch_loss: 0.1131 [Training] 6/114 [>.............................] - ETA: 36s  batch_loss: 0.1151 [Training] 7/114 [>.............................] - ETA: 35s  batch_loss: 0.1187 [Training] 8/114 [=>............................] - ETA: 33s  batch_loss: 0.1220 [Training] 9/114 [=>............................] - ETA: 32s  batch_loss: 0.1234 [Training] 10/114 [=>............................] - ETA: 31s  batch_loss: 0.1232 [Training] 11/114 [=>............................] - ETA: 30s  batch_loss: 0.1226 [Training] 12/114 [==>...........................] - ETA: 29s  batch_loss: 0.1218 [Training] 13/114 [==>...........................] - ETA: 29s  batch_loss: 0.1234 [Training] 14/114 [==>...........................] - ETA: 28s  batch_loss: 0.1228 [Training] 15/114 [==>...........................] - ETA: 28s  batch_loss: 0.1246 [Training] 16/114 [===>..........................] - ETA: 27s  batch_loss: 0.1247 [Training] 17/114 [===>..........................] - ETA: 27s  batch_loss: 0.1255 [Training] 18/114 [===>..........................] - ETA: 26s  batch_loss: 0.1244 [Training] 19/114 [====>.........................] - ETA: 26s  batch_loss: 0.1233 [Training] 20/114 [====>.........................] - ETA: 26s  batch_loss: 0.1247 [Training] 21/114 [====>.........................] - ETA: 25s  batch_loss: 0.1226 [Training] 22/114 [====>.........................] - ETA: 25s  batch_loss: 0.1250 [Training] 23/114 [=====>........................] - ETA: 24s  batch_loss: 0.1237 [Training] 24/114 [=====>........................] - ETA: 24s  batch_loss: 0.1238 [Training] 25/114 [=====>........................] - ETA: 24s  batch_loss: 0.1232 [Training] 26/114 [=====>........................] - ETA: 23s  batch_loss: 0.1242 [Training] 27/114 [======>.......................] - ETA: 23s  batch_loss: 0.1256 [Training] 28/114 [======>.......................] - ETA: 23s  batch_loss: 0.1247 [Training] 29/114 [======>.......................] - ETA: 22s  batch_loss: 0.1259 [Training] 30/114 [======>.......................] - ETA: 22s  batch_loss: 0.1256 [Training] 31/114 [=======>......................] - ETA: 22s  batch_loss: 0.1255 [Training] 32/114 [=======>......................] - ETA: 22s  batch_loss: 0.1260 [Training] 33/114 [=======>......................] - ETA: 21s  batch_loss: 0.1255 [Training] 34/114 [=======>......................] - ETA: 21s  batch_loss: 0.1253 [Training] 35/114 [========>.....................] - ETA: 21s  batch_loss: 0.1241 [Training] 36/114 [========>.....................] - ETA: 20s  batch_loss: 0.1234 [Training] 37/114 [========>.....................] - ETA: 20s  batch_loss: 0.1229 [Training] 38/114 [=========>....................] - ETA: 20s  batch_loss: 0.1224 [Training] 39/114 [=========>....................] - ETA: 19s  batch_loss: 0.1229 [Training] 40/114 [=========>....................] - ETA: 19s  batch_loss: 0.1223 [Training] 41/114 [=========>....................] - ETA: 19s  batch_loss: 0.1218 [Training] 42/114 [==========>...................] - ETA: 19s  batch_loss: 0.1225 [Training] 43/114 [==========>...................] - ETA: 18s  batch_loss: 0.1234 [Training] 44/114 [==========>...................] - ETA: 18s  batch_loss: 0.1232 [Training] 45/114 [==========>...................] - ETA: 18s  batch_loss: 0.1234 [Training] 46/114 [===========>..................] - ETA: 18s  batch_loss: 0.1232 [Training] 47/114 [===========>..................] - ETA: 17s  batch_loss: 0.1226 [Training] 48/114 [===========>..................] - ETA: 17s  batch_loss: 0.1221 [Training] 49/114 [===========>..................] - ETA: 17s  batch_loss: 0.1226 [Training] 50/114 [============>.................] - ETA: 16s  batch_loss: 0.1235 [Training] 51/114 [============>.................] - ETA: 16s  batch_loss: 0.1241 [Training] 52/114 [============>.................] - ETA: 16s  batch_loss: 0.1237 [Training] 53/114 [============>.................] - ETA: 16s  batch_loss: 0.1233 [Training] 54/114 [=============>................] - ETA: 15s  batch_loss: 0.1233 [Training] 55/114 [=============>................] - ETA: 15s  batch_loss: 0.1235 [Training] 56/114 [=============>................] - ETA: 15s  batch_loss: 0.1238 [Training] 57/114 [==============>...............] - ETA: 14s  batch_loss: 0.1234 [Training] 58/114 [==============>...............] - ETA: 14s  batch_loss: 0.1233 [Training] 59/114 [==============>...............] - ETA: 14s  batch_loss: 0.1229 [Training] 60/114 [==============>...............] - ETA: 14s  batch_loss: 0.1233 [Training] 61/114 [===============>..............] - ETA: 13s  batch_loss: 0.1233 [Training] 62/114 [===============>..............] - ETA: 13s  batch_loss: 0.1232 [Training] 63/114 [===============>..............] - ETA: 13s  batch_loss: 0.1235 [Training] 64/114 [===============>..............] - ETA: 12s  batch_loss: 0.1231 [Training] 65/114 [================>.............] - ETA: 12s  batch_loss: 0.1234 [Training] 66/114 [================>.............] - ETA: 12s  batch_loss: 0.1239 [Training] 67/114 [================>.............] - ETA: 12s  batch_loss: 0.1235 [Training] 68/114 [================>.............] - ETA: 11s  batch_loss: 0.1232 [Training] 69/114 [=================>............] - ETA: 11s  batch_loss: 0.1232 [Training] 70/114 [=================>............] - ETA: 11s  batch_loss: 0.1227 [Training] 71/114 [=================>............] - ETA: 11s  batch_loss: 0.1222 [Training] 72/114 [=================>............] - ETA: 10s  batch_loss: 0.1220 [Training] 73/114 [==================>...........] - ETA: 10s  batch_loss: 0.1217 [Training] 74/114 [==================>...........] - ETA: 10s  batch_loss: 0.1220 [Training] 75/114 [==================>...........] - ETA: 10s  batch_loss: 0.1220 [Training] 76/114 [===================>..........] - ETA: 9s  batch_loss: 0.1224 [Training] 77/114 [===================>..........] - ETA: 9s  batch_loss: 0.1223 [Training] 78/114 [===================>..........] - ETA: 9s  batch_loss: 0.1224 [Training] 79/114 [===================>..........] - ETA: 8s  batch_loss: 0.1223 [Training] 80/114 [====================>.........] - ETA: 8s  batch_loss: 0.1222 [Training] 81/114 [====================>.........] - ETA: 8s  batch_loss: 0.1219 [Training] 82/114 [====================>.........] - ETA: 8s  batch_loss: 0.1224 [Training] 83/114 [====================>.........] - ETA: 7s  batch_loss: 0.1220 [Training] 84/114 [=====================>........] - ETA: 7s  batch_loss: 0.1218 [Training] 85/114 [=====================>........] - ETA: 7s  batch_loss: 0.1218 [Training] 86/114 [=====================>........] - ETA: 7s  batch_loss: 0.1216 [Training] 87/114 [=====================>........] - ETA: 6s  batch_loss: 0.1227 [Training] 88/114 [======================>.......] - ETA: 6s  batch_loss: 0.1224 [Training] 89/114 [======================>.......] - ETA: 6s  batch_loss: 0.1221 [Training] 90/114 [======================>.......] - ETA: 6s  batch_loss: 0.1227 [Training] 91/114 [======================>.......] - ETA: 5s  batch_loss: 0.1226 [Training] 92/114 [=======================>......] - ETA: 5s  batch_loss: 0.1227 [Training] 93/114 [=======================>......] - ETA: 5s  batch_loss: 0.1226 [Training] 94/114 [=======================>......] - ETA: 5s  batch_loss: 0.1229 [Training] 95/114 [========================>.....] - ETA: 4s  batch_loss: 0.1231 [Training] 96/114 [========================>.....] - ETA: 4s  batch_loss: 0.1231 [Training] 97/114 [========================>.....] - ETA: 4s  batch_loss: 0.1232 [Training] 98/114 [========================>.....] - ETA: 4s  batch_loss: 0.1237 [Training] 99/114 [=========================>....] - ETA: 3s  batch_loss: 0.1234 [Training] 100/114 [=========================>....] - ETA: 3s  batch_loss: 0.1238 [Training] 101/114 [=========================>....] - ETA: 3s  batch_loss: 0.1244 [Training] 102/114 [=========================>....] - ETA: 3s  batch_loss: 0.1242 [Training] 103/114 [==========================>...] - ETA: 2s  batch_loss: 0.1244 [Training] 104/114 [==========================>...] - ETA: 2s  batch_loss: 0.1242 [Training] 105/114 [==========================>...] - ETA: 2s  batch_loss: 0.1244 [Training] 106/114 [==========================>...] - ETA: 2s  batch_loss: 0.1250 [Training] 107/114 [===========================>..] - ETA: 1s  batch_loss: 0.1250 [Training] 108/114 [===========================>..] - ETA: 1s  batch_loss: 0.1248 [Training] 109/114 [===========================>..] - ETA: 1s  batch_loss: 0.1244 [Training] 110/114 [===========================>..] - ETA: 1s  batch_loss: 0.1244 [Training] 111/114 [============================>.] - ETA: 0s  batch_loss: 0.1248 [Training] 112/114 [============================>.] - ETA: 0s  batch_loss: 0.1246 [Training] 113/114 [============================>.] - ETA: 0s  batch_loss: 0.1253 [Training] 114/114 [==============================] 252.9ms/step  batch_loss: 0.1251 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 13:57:46 - INFO - root -   The F1-score is 0.6528705806033084
********** predict start ***********
********** Done ***************
