nohup: ignoring input
*********** data_prepare *************
开始数据预处理
开始处理tag.dict 和 tsv文件

=================DUEE 1.0 DATASET==============

=================start schema process==============
input path ./conf/DuEE1.0/event_schema.json
save trigger tag 379 at ./conf/DuEE1.0/role_tag.dict
=================end schema process===============

=================start sentence process==============

----trigger------for dir ./data/DuEE1.0 to ./data/DuEE1.0/trigger
train 5860 dev 1549 test 2551

----role------for dir ./data/DuEE1.0 to ./data/DuEE1.0/role
train 6983 dev 1841 test 2551
=================end schema process==============
********** train start *************
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 5, 
    "event_type": "role", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 1e-05, 
    "max_len": 150, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 30.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 48, 
    "per_gpu_train_batch_size": 64, 
    "seed": 66, 
    "stride": 100, 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/6982 [00:00<?, ?it/s]tokenizing...:   4%|▍         | 293/6982 [00:00<00:02, 2928.43it/s]tokenizing...:   8%|▊         | 586/6982 [00:00<00:02, 2671.27it/s]tokenizing...:  12%|█▏        | 855/6982 [00:00<00:02, 2591.03it/s]tokenizing...:  16%|█▌        | 1120/6982 [00:00<00:02, 2611.81it/s]tokenizing...:  20%|█▉        | 1382/6982 [00:00<00:02, 2583.44it/s]tokenizing...:  24%|██▎       | 1641/6982 [00:00<00:02, 2527.54it/s]tokenizing...:  27%|██▋       | 1895/6982 [00:00<00:02, 2481.30it/s]tokenizing...:  31%|███       | 2162/6982 [00:00<00:01, 2538.00it/s]tokenizing...:  35%|███▍      | 2417/6982 [00:00<00:01, 2432.27it/s]tokenizing...:  38%|███▊      | 2681/6982 [00:01<00:01, 2491.82it/s]tokenizing...:  42%|████▏     | 2932/6982 [00:01<00:01, 2494.21it/s]tokenizing...:  46%|████▌     | 3198/6982 [00:01<00:01, 2540.77it/s]tokenizing...:  49%|████▉     | 3453/6982 [00:01<00:01, 2466.02it/s]tokenizing...:  53%|█████▎    | 3701/6982 [00:01<00:01, 2433.11it/s]tokenizing...:  57%|█████▋    | 3945/6982 [00:01<00:01, 2401.63it/s]tokenizing...:  60%|██████    | 4204/6982 [00:01<00:01, 2456.17it/s]tokenizing...:  64%|██████▎   | 4451/6982 [00:01<00:01, 2420.09it/s]tokenizing...:  67%|██████▋   | 4702/6982 [00:01<00:00, 2445.11it/s]tokenizing...:  71%|███████▏  | 4982/6982 [00:01<00:00, 2548.38it/s]tokenizing...:  75%|███████▌  | 5238/6982 [00:02<00:00, 2517.97it/s]tokenizing...:  79%|███████▊  | 5491/6982 [00:02<00:00, 2420.84it/s]tokenizing...:  82%|████████▏ | 5735/6982 [00:02<00:00, 1689.96it/s]tokenizing...:  86%|████████▌ | 5992/6982 [00:02<00:00, 1884.63it/s]tokenizing...:  90%|████████▉ | 6253/6982 [00:02<00:00, 2059.01it/s]tokenizing...:  93%|█████████▎| 6483/6982 [00:02<00:00, 2023.16it/s]tokenizing...:  96%|█████████▌| 6702/6982 [00:02<00:00, 2060.39it/s]tokenizing...: 100%|██████████| 6982/6982 [00:02<00:00, 2347.72it/s]
tokenizing...:   0%|          | 0/1840 [00:00<?, ?it/s]tokenizing...:  14%|█▍        | 259/1840 [00:00<00:00, 2585.72it/s]tokenizing...:  28%|██▊       | 518/1840 [00:00<00:00, 2562.79it/s]tokenizing...:  42%|████▏     | 775/1840 [00:00<00:00, 2524.32it/s]tokenizing...:  56%|█████▌    | 1028/1840 [00:00<00:00, 2450.58it/s]tokenizing...:  69%|██████▉   | 1274/1840 [00:00<00:00, 2389.08it/s]tokenizing...:  82%|████████▏ | 1514/1840 [00:00<00:00, 2291.95it/s]tokenizing...:  95%|█████████▍| 1744/1840 [00:00<00:00, 2116.14it/s]tokenizing...: 100%|██████████| 1840/1840 [00:00<00:00, 2298.97it/s]
09/25/2022 11:48:10 - INFO - root -   The nums of the train_dataset features is 6982
09/25/2022 11:48:10 - INFO - root -   The nums of the eval_dataset features is 1840
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
09/25/2022 11:48:14 - INFO - root -   ***** Running train *****
********************************************************************************
/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:58  batch_loss: 6.0401 [Training] 2/110 [..............................] - ETA: 1:10  batch_loss: 6.0232 [Training] 3/110 [..............................] - ETA: 54s  batch_loss: 6.0179 [Training] 4/110 [>.............................] - ETA: 47s  batch_loss: 6.0230 [Training] 5/110 [>.............................] - ETA: 42s  batch_loss: 5.9935 [Training] 6/110 [>.............................] - ETA: 39s  batch_loss: 5.9530 [Training] 7/110 [>.............................] - ETA: 37s  batch_loss: 5.9035 [Training] 8/110 [=>............................] - ETA: 35s  batch_loss: 5.8560 [Training] 9/110 [=>............................] - ETA: 33s  batch_loss: 5.7988 [Training] 10/110 [=>............................] - ETA: 32s  batch_loss: 5.7358 [Training] 11/110 [==>...........................] - ETA: 31s  batch_loss: 5.6682 [Training] 12/110 [==>...........................] - ETA: 30s  batch_loss: 5.5978 [Training] 13/110 [==>...........................] - ETA: 29s  batch_loss: 5.5217 [Training] 14/110 [==>...........................] - ETA: 29s  batch_loss: 5.4547 [Training] 15/110 [===>..........................] - ETA: 28s  batch_loss: 5.3717 [Training] 16/110 [===>..........................] - ETA: 27s  batch_loss: 5.2961 [Training] 17/110 [===>..........................] - ETA: 27s  batch_loss: 5.2364 [Training] 18/110 [===>..........................] - ETA: 26s  batch_loss: 5.1632 [Training] 19/110 [====>.........................] - ETA: 26s  batch_loss: 5.0878 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 5.0163 [Training] 21/110 [====>.........................] - ETA: 25s  batch_loss: 4.9434 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 4.8726 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 4.7836 [Training] 24/110 [=====>........................] - ETA: 24s  batch_loss: 4.7005 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 4.6327 [Training] 26/110 [======>.......................] - ETA: 23s  batch_loss: 4.5584 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 4.4888 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 4.4112 [Training] 29/110 [======>.......................] - ETA: 22s  batch_loss: 4.3432 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 4.2784 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 4.2150 [Training] 32/110 [=======>......................] - ETA: 21s  batch_loss: 4.1568 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 4.0931 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 4.0345 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 3.9871 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 3.9529 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 3.9054 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 3.8655 [Training] 39/110 [=========>....................] - ETA: 19s  batch_loss: 3.8185 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 3.7771 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 3.7480 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 3.7170 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 3.6886 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 3.6580 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 3.6263 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 3.6004 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 3.5743 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 3.5469 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 3.5224 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 3.4924 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 3.4640 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 3.4390 [Training] 53/110 [=============>................] - ETA: 14s  batch_loss: 3.4183 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 3.3970 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 3.3777 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 3.3555 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 3.3301 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 3.3055 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 3.2915 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 3.2719 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 3.2560 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 3.2379 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 3.2204 [Training] 64/110 [================>.............] - ETA: 11s  batch_loss: 3.2059 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 3.1869 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 3.1763 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 3.1630 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 3.1511 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 3.1336 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 3.1191 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 3.1045 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 3.0909 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 3.0741 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 3.0587 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 3.0485 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 3.0345 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 3.0201 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 3.0067 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 2.9955 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 2.9863 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 2.9768 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 2.9655 [Training] 83/110 [=====================>........] - ETA: 6s  batch_loss: 2.9519 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 2.9403 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 2.9293 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 2.9226 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 2.9131 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 2.9059 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 2.8955 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 2.8849 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 2.8754 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 2.8654 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 2.8545 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 2.8450 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 2.8366 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 2.8275 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 2.8185 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 2.8100 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 2.8020 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 2.7937 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 2.7867 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 2.7776 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 2.7691 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 2.7633 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 2.7561 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 2.7488 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 2.7415 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 2.7350 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 2.7272 [Training] 110/110 [==============================] 254.2ms/step  batch_loss: 2.7203 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 11:48:49 - INFO - root -   The F1-score is 0.0109347913055074
09/25/2022 11:48:49 - INFO - root -   the best eval f1 is 0.0109, saving model !!
09/25/2022 11:48:51 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:29  batch_loss: 1.8914 [Training] 2/110 [..............................] - ETA: 58s  batch_loss: 1.8634 [Training] 3/110 [..............................] - ETA: 47s  batch_loss: 1.8901 [Training] 4/110 [>.............................] - ETA: 41s  batch_loss: 1.9569 [Training] 5/110 [>.............................] - ETA: 38s  batch_loss: 2.0073 [Training] 6/110 [>.............................] - ETA: 36s  batch_loss: 2.0080 [Training] 7/110 [>.............................] - ETA: 34s  batch_loss: 2.0022 [Training] 8/110 [=>............................] - ETA: 33s  batch_loss: 1.9977 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 2.0121 [Training] 10/110 [=>............................] - ETA: 30s  batch_loss: 2.0102 [Training] 11/110 [==>...........................] - ETA: 30s  batch_loss: 2.0146 [Training] 12/110 [==>...........................] - ETA: 29s  batch_loss: 2.0167 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 2.0233 [Training] 14/110 [==>...........................] - ETA: 27s  batch_loss: 2.0203 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 2.0182 [Training] 16/110 [===>..........................] - ETA: 26s  batch_loss: 2.0015 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 1.9978 [Training] 18/110 [===>..........................] - ETA: 25s  batch_loss: 1.9895 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 1.9754 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 1.9592 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 1.9560 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 1.9505 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 1.9391 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 1.9215 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 1.9129 [Training] 26/110 [======>.......................] - ETA: 22s  batch_loss: 1.9082 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 1.8989 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 1.9019 [Training] 29/110 [======>.......................] - ETA: 21s  batch_loss: 1.9130 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 1.9108 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 1.9103 [Training] 32/110 [=======>......................] - ETA: 21s  batch_loss: 1.9050 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 1.9103 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 1.9051 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 1.9002 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 1.8956 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 1.8971 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 1.8843 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 1.8858 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 1.8813 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 1.8769 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 1.8754 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 1.8687 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 1.8696 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 1.8662 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 1.8637 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 1.8638 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 1.8624 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 1.8611 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 1.8533 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 1.8540 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 1.8533 [Training] 53/110 [=============>................] - ETA: 14s  batch_loss: 1.8505 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 1.8482 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 1.8451 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 1.8407 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 1.8382 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 1.8375 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 1.8349 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 1.8307 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 1.8307 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 1.8249 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 1.8203 [Training] 64/110 [================>.............] - ETA: 11s  batch_loss: 1.8204 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 1.8174 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 1.8172 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 1.8124 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 1.8114 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 1.8081 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 1.8049 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 1.8009 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 1.7981 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 1.7964 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 1.7907 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 1.7850 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 1.7810 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 1.7777 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 1.7713 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 1.7689 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 1.7666 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 1.7626 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 1.7601 [Training] 83/110 [=====================>........] - ETA: 6s  batch_loss: 1.7599 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 1.7563 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 1.7551 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 1.7523 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 1.7497 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 1.7498 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 1.7463 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 1.7445 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 1.7436 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 1.7410 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 1.7405 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 1.7381 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 1.7362 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 1.7342 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 1.7323 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 1.7321 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 1.7312 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 1.7301 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 1.7295 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 1.7256 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 1.7245 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 1.7234 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 1.7207 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 1.7192 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 1.7192 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 1.7194 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 1.7159 [Training] 110/110 [==============================] 255.3ms/step  batch_loss: 1.7144 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 11:49:26 - INFO - root -   The F1-score is 0.11547097188067999
09/25/2022 11:49:26 - INFO - root -   the best eval f1 is 0.1155, saving model !!
09/25/2022 11:49:29 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:27  batch_loss: 1.5780 [Training] 2/110 [..............................] - ETA: 56s  batch_loss: 1.5701 [Training] 3/110 [..............................] - ETA: 46s  batch_loss: 1.5146 [Training] 4/110 [>.............................] - ETA: 41s  batch_loss: 1.5252 [Training] 5/110 [>.............................] - ETA: 37s  batch_loss: 1.5079 [Training] 6/110 [>.............................] - ETA: 35s  batch_loss: 1.5139 [Training] 7/110 [>.............................] - ETA: 33s  batch_loss: 1.5352 [Training] 8/110 [=>............................] - ETA: 32s  batch_loss: 1.5490 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 1.5424 [Training] 10/110 [=>............................] - ETA: 29s  batch_loss: 1.5321 [Training] 11/110 [==>...........................] - ETA: 29s  batch_loss: 1.5167 [Training] 12/110 [==>...........................] - ETA: 28s  batch_loss: 1.5146 [Training] 13/110 [==>...........................] - ETA: 27s  batch_loss: 1.5136 [Training] 14/110 [==>...........................] - ETA: 27s  batch_loss: 1.5148 [Training] 15/110 [===>..........................] - ETA: 26s  batch_loss: 1.5124 [Training] 16/110 [===>..........................] - ETA: 26s  batch_loss: 1.5100 [Training] 17/110 [===>..........................] - ETA: 25s  batch_loss: 1.4997 [Training] 18/110 [===>..........................] - ETA: 25s  batch_loss: 1.4857 [Training] 19/110 [====>.........................] - ETA: 24s  batch_loss: 1.4828 [Training] 20/110 [====>.........................] - ETA: 24s  batch_loss: 1.4813 [Training] 21/110 [====>.........................] - ETA: 23s  batch_loss: 1.4734 [Training] 22/110 [=====>........................] - ETA: 23s  batch_loss: 1.4626 [Training] 23/110 [=====>........................] - ETA: 23s  batch_loss: 1.4661 [Training] 24/110 [=====>........................] - ETA: 22s  batch_loss: 1.4632 [Training] 25/110 [=====>........................] - ETA: 22s  batch_loss: 1.4593 [Training] 26/110 [======>.......................] - ETA: 22s  batch_loss: 1.4563 [Training] 27/110 [======>.......................] - ETA: 21s  batch_loss: 1.4554 [Training] 28/110 [======>.......................] - ETA: 21s  batch_loss: 1.4642 [Training] 29/110 [======>.......................] - ETA: 21s  batch_loss: 1.4573 [Training] 30/110 [=======>......................] - ETA: 20s  batch_loss: 1.4586 [Training] 31/110 [=======>......................] - ETA: 20s  batch_loss: 1.4569 [Training] 32/110 [=======>......................] - ETA: 20s  batch_loss: 1.4558 [Training] 33/110 [========>.....................] - ETA: 19s  batch_loss: 1.4482 [Training] 34/110 [========>.....................] - ETA: 19s  batch_loss: 1.4447 [Training] 35/110 [========>.....................] - ETA: 19s  batch_loss: 1.4426 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 1.4437 [Training] 37/110 [=========>....................] - ETA: 18s  batch_loss: 1.4418 [Training] 38/110 [=========>....................] - ETA: 18s  batch_loss: 1.4355 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 1.4361 [Training] 40/110 [=========>....................] - ETA: 17s  batch_loss: 1.4324 [Training] 41/110 [==========>...................] - ETA: 17s  batch_loss: 1.4269 [Training] 42/110 [==========>...................] - ETA: 17s  batch_loss: 1.4240 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 1.4232 [Training] 44/110 [===========>..................] - ETA: 16s  batch_loss: 1.4236 [Training] 45/110 [===========>..................] - ETA: 16s  batch_loss: 1.4212 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 1.4163 [Training] 47/110 [===========>..................] - ETA: 15s  batch_loss: 1.4160 [Training] 48/110 [============>.................] - ETA: 15s  batch_loss: 1.4120 [Training] 49/110 [============>.................] - ETA: 15s  batch_loss: 1.4097 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 1.4093 [Training] 51/110 [============>.................] - ETA: 14s  batch_loss: 1.4078 [Training] 52/110 [=============>................] - ETA: 14s  batch_loss: 1.4085 [Training] 53/110 [=============>................] - ETA: 14s  batch_loss: 1.4057 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 1.4043 [Training] 55/110 [==============>...............] - ETA: 13s  batch_loss: 1.4028 [Training] 56/110 [==============>...............] - ETA: 13s  batch_loss: 1.3987 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 1.3962 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 1.3933 [Training] 59/110 [===============>..............] - ETA: 12s  batch_loss: 1.3910 [Training] 60/110 [===============>..............] - ETA: 12s  batch_loss: 1.3906 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 1.3897 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 1.3879 [Training] 63/110 [================>.............] - ETA: 11s  batch_loss: 1.3881 [Training] 64/110 [================>.............] - ETA: 11s  batch_loss: 1.3859 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 1.3851 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 1.3845 [Training] 67/110 [=================>............] - ETA: 10s  batch_loss: 1.3815 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 1.3799 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 1.3773 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 1.3765 [Training] 71/110 [==================>...........] - ETA: 9s  batch_loss: 1.3777 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 1.3804 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 1.3799 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 1.3786 [Training] 75/110 [===================>..........] - ETA: 8s  batch_loss: 1.3769 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 1.3765 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 1.3761 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 1.3751 [Training] 79/110 [====================>.........] - ETA: 7s  batch_loss: 1.3727 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 1.3729 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 1.3715 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 1.3705 [Training] 83/110 [=====================>........] - ETA: 6s  batch_loss: 1.3689 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 1.3658 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 1.3648 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 1.3638 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 1.3613 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 1.3603 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 1.3571 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 1.3567 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 1.3544 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 1.3539 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 1.3528 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 1.3527 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 1.3505 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 1.3497 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 1.3476 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 1.3465 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 1.3451 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 1.3448 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 1.3422 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 1.3407 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 1.3389 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 1.3378 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 1.3375 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 1.3377 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 1.3357 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 1.3320 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 1.3312 [Training] 110/110 [==============================] 250.2ms/step  batch_loss: 1.3300 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 11:50:02 - INFO - root -   The F1-score is 0.1864328564836179
09/25/2022 11:50:02 - INFO - root -   the best eval f1 is 0.1864, saving model !!
09/25/2022 11:50:05 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:25  batch_loss: 1.2323 [Training] 2/110 [..............................] - ETA: 57s  batch_loss: 1.2095 [Training] 3/110 [..............................] - ETA: 46s  batch_loss: 1.2190 [Training] 4/110 [>.............................] - ETA: 41s  batch_loss: 1.1497 [Training] 5/110 [>.............................] - ETA: 38s  batch_loss: 1.1380 [Training] 6/110 [>.............................] - ETA: 35s  batch_loss: 1.1191 [Training] 7/110 [>.............................] - ETA: 34s  batch_loss: 1.1139 [Training] 8/110 [=>............................] - ETA: 32s  batch_loss: 1.1086 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 1.1132 [Training] 10/110 [=>............................] - ETA: 30s  batch_loss: 1.1134 [Training] 11/110 [==>...........................] - ETA: 29s  batch_loss: 1.1359 [Training] 12/110 [==>...........................] - ETA: 29s  batch_loss: 1.1362 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 1.1291 [Training] 14/110 [==>...........................] - ETA: 27s  batch_loss: 1.1291 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 1.1381 [Training] 16/110 [===>..........................] - ETA: 26s  batch_loss: 1.1279 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 1.1250 [Training] 18/110 [===>..........................] - ETA: 25s  batch_loss: 1.1256 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 1.1307 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 1.1370 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 1.1352 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 1.1362 [Training] 23/110 [=====>........................] - ETA: 23s  batch_loss: 1.1367 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 1.1359 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 1.1361 [Training] 26/110 [======>.......................] - ETA: 22s  batch_loss: 1.1364 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 1.1373 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 1.1377 [Training] 29/110 [======>.......................] - ETA: 21s  batch_loss: 1.1362 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 1.1351 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 1.1319 [Training] 32/110 [=======>......................] - ETA: 20s  batch_loss: 1.1353 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 1.1312 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 1.1268 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 1.1258 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 1.1284 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 1.1350 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 1.1344 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 1.1325 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 1.1338 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 1.1312 [Training] 42/110 [==========>...................] - ETA: 17s  batch_loss: 1.1326 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 1.1353 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 1.1359 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 1.1342 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 1.1324 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 1.1306 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 1.1278 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 1.1281 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 1.1259 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 1.1236 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 1.1194 [Training] 53/110 [=============>................] - ETA: 14s  batch_loss: 1.1226 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 1.1188 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 1.1183 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 1.1167 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 1.1155 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 1.1137 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 1.1142 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 1.1139 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 1.1162 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 1.1153 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 1.1169 [Training] 64/110 [================>.............] - ETA: 11s  batch_loss: 1.1139 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 1.1146 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 1.1147 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 1.1149 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 1.1135 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 1.1137 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 1.1120 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 1.1098 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 1.1101 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 1.1125 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 1.1131 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 1.1124 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 1.1124 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 1.1127 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 1.1112 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 1.1109 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 1.1099 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 1.1096 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 1.1090 [Training] 83/110 [=====================>........] - ETA: 7s  batch_loss: 1.1072 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 1.1060 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 1.1049 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 1.1034 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 1.1016 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 1.1007 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 1.0992 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 1.0969 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 1.0958 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 1.0963 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 1.0976 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 1.0959 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 1.0955 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 1.0947 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 1.0948 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 1.0933 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 1.0930 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 1.0905 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 1.0897 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 1.0881 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 1.0865 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 1.0860 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 1.0851 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 1.0849 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 1.0835 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 1.0845 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 1.0838 [Training] 110/110 [==============================] 259.7ms/step  batch_loss: 1.0852 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 11:50:40 - INFO - root -   The F1-score is 0.3202260419119378
09/25/2022 11:50:40 - INFO - root -   the best eval f1 is 0.3202, saving model !!
09/25/2022 11:50:43 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:28  batch_loss: 0.8802 [Training] 2/110 [..............................] - ETA: 57s  batch_loss: 0.9249 [Training] 3/110 [..............................] - ETA: 46s  batch_loss: 0.9643 [Training] 4/110 [>.............................] - ETA: 41s  batch_loss: 0.9749 [Training] 5/110 [>.............................] - ETA: 37s  batch_loss: 1.0427 [Training] 6/110 [>.............................] - ETA: 35s  batch_loss: 1.0167 [Training] 7/110 [>.............................] - ETA: 33s  batch_loss: 0.9987 [Training] 8/110 [=>............................] - ETA: 32s  batch_loss: 1.0003 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 0.9999 [Training] 10/110 [=>............................] - ETA: 30s  batch_loss: 0.9889 [Training] 11/110 [==>...........................] - ETA: 29s  batch_loss: 0.9910 [Training] 12/110 [==>...........................] - ETA: 28s  batch_loss: 0.9778 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.9688 [Training] 14/110 [==>...........................] - ETA: 27s  batch_loss: 0.9579 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 0.9443 [Training] 16/110 [===>..........................] - ETA: 26s  batch_loss: 0.9470 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 0.9436 [Training] 18/110 [===>..........................] - ETA: 25s  batch_loss: 0.9489 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.9559 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 0.9611 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.9568 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.9611 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 0.9636 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.9613 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.9602 [Training] 26/110 [======>.......................] - ETA: 23s  batch_loss: 0.9574 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.9566 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.9531 [Training] 29/110 [======>.......................] - ETA: 22s  batch_loss: 0.9486 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.9512 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.9476 [Training] 32/110 [=======>......................] - ETA: 21s  batch_loss: 0.9499 [Training] 33/110 [========>.....................] - ETA: 21s  batch_loss: 0.9515 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.9484 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.9496 [Training] 36/110 [========>.....................] - ETA: 20s  batch_loss: 0.9501 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.9510 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.9486 [Training] 39/110 [=========>....................] - ETA: 19s  batch_loss: 0.9491 [Training] 40/110 [=========>....................] - ETA: 19s  batch_loss: 0.9473 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.9467 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 0.9442 [Training] 43/110 [==========>...................] - ETA: 18s  batch_loss: 0.9449 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.9431 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.9408 [Training] 46/110 [===========>..................] - ETA: 17s  batch_loss: 0.9414 [Training] 47/110 [===========>..................] - ETA: 17s  batch_loss: 0.9384 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.9370 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.9359 [Training] 50/110 [============>.................] - ETA: 16s  batch_loss: 0.9391 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.9385 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.9405 [Training] 53/110 [=============>................] - ETA: 15s  batch_loss: 0.9403 [Training] 54/110 [=============>................] - ETA: 15s  batch_loss: 0.9397 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.9384 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.9434 [Training] 57/110 [==============>...............] - ETA: 14s  batch_loss: 0.9400 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.9437 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.9454 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.9426 [Training] 61/110 [===============>..............] - ETA: 13s  batch_loss: 0.9415 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.9392 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.9369 [Training] 64/110 [================>.............] - ETA: 12s  batch_loss: 0.9363 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.9375 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.9386 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.9374 [Training] 68/110 [=================>............] - ETA: 11s  batch_loss: 0.9366 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.9369 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.9355 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.9357 [Training] 72/110 [==================>...........] - ETA: 10s  batch_loss: 0.9359 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.9350 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.9350 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.9349 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.9336 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.9324 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.9328 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.9325 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.9307 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.9289 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.9271 [Training] 83/110 [=====================>........] - ETA: 7s  batch_loss: 0.9275 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.9272 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.9272 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.9262 [Training] 87/110 [======================>.......] - ETA: 6s  batch_loss: 0.9259 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.9248 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.9245 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.9231 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.9218 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.9214 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.9209 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.9201 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.9192 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.9193 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.9191 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.9183 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.9181 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.9167 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.9157 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.9153 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.9139 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.9128 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.9114 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.9101 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.9106 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.9099 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.9101 [Training] 110/110 [==============================] 258.7ms/step  batch_loss: 0.9106 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 11:51:18 - INFO - root -   The F1-score is 0.4221186070231677
09/25/2022 11:51:18 - INFO - root -   the best eval f1 is 0.4221, saving model !!
09/25/2022 11:51:21 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:34  batch_loss: 0.7819 [Training] 2/110 [..............................] - ETA: 1:00  batch_loss: 0.8075 [Training] 3/110 [..............................] - ETA: 49s  batch_loss: 0.8566 [Training] 4/110 [>.............................] - ETA: 43s  batch_loss: 0.8250 [Training] 5/110 [>.............................] - ETA: 40s  batch_loss: 0.8335 [Training] 6/110 [>.............................] - ETA: 37s  batch_loss: 0.8148 [Training] 7/110 [>.............................] - ETA: 35s  batch_loss: 0.8090 [Training] 8/110 [=>............................] - ETA: 34s  batch_loss: 0.8107 [Training] 9/110 [=>............................] - ETA: 33s  batch_loss: 0.8132 [Training] 10/110 [=>............................] - ETA: 32s  batch_loss: 0.8158 [Training] 11/110 [==>...........................] - ETA: 31s  batch_loss: 0.8190 [Training] 12/110 [==>...........................] - ETA: 30s  batch_loss: 0.8219 [Training] 13/110 [==>...........................] - ETA: 29s  batch_loss: 0.8256 [Training] 14/110 [==>...........................] - ETA: 29s  batch_loss: 0.8304 [Training] 15/110 [===>..........................] - ETA: 28s  batch_loss: 0.8239 [Training] 16/110 [===>..........................] - ETA: 28s  batch_loss: 0.8225 [Training] 17/110 [===>..........................] - ETA: 27s  batch_loss: 0.8222 [Training] 18/110 [===>..........................] - ETA: 27s  batch_loss: 0.8236 [Training] 19/110 [====>.........................] - ETA: 26s  batch_loss: 0.8155 [Training] 20/110 [====>.........................] - ETA: 26s  batch_loss: 0.8131 [Training] 21/110 [====>.........................] - ETA: 25s  batch_loss: 0.8139 [Training] 22/110 [=====>........................] - ETA: 25s  batch_loss: 0.8141 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 0.8152 [Training] 24/110 [=====>........................] - ETA: 24s  batch_loss: 0.8133 [Training] 25/110 [=====>........................] - ETA: 24s  batch_loss: 0.8155 [Training] 26/110 [======>.......................] - ETA: 23s  batch_loss: 0.8184 [Training] 27/110 [======>.......................] - ETA: 23s  batch_loss: 0.8138 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.8113 [Training] 29/110 [======>.......................] - ETA: 22s  batch_loss: 0.8089 [Training] 30/110 [=======>......................] - ETA: 22s  batch_loss: 0.8143 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.8141 [Training] 32/110 [=======>......................] - ETA: 21s  batch_loss: 0.8109 [Training] 33/110 [========>.....................] - ETA: 21s  batch_loss: 0.8104 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.8117 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.8128 [Training] 36/110 [========>.....................] - ETA: 20s  batch_loss: 0.8081 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.8080 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.8073 [Training] 39/110 [=========>....................] - ETA: 19s  batch_loss: 0.8064 [Training] 40/110 [=========>....................] - ETA: 19s  batch_loss: 0.8078 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.8084 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 0.8068 [Training] 43/110 [==========>...................] - ETA: 18s  batch_loss: 0.8086 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.8051 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.8063 [Training] 46/110 [===========>..................] - ETA: 17s  batch_loss: 0.8053 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.8058 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.8072 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.8031 [Training] 50/110 [============>.................] - ETA: 16s  batch_loss: 0.8046 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.8037 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.8035 [Training] 53/110 [=============>................] - ETA: 15s  batch_loss: 0.8042 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.8031 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.8016 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.8027 [Training] 57/110 [==============>...............] - ETA: 14s  batch_loss: 0.8014 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.7993 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.7991 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.7993 [Training] 61/110 [===============>..............] - ETA: 13s  batch_loss: 0.7978 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.7977 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.7958 [Training] 64/110 [================>.............] - ETA: 12s  batch_loss: 0.7941 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.7941 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.7916 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.7914 [Training] 68/110 [=================>............] - ETA: 11s  batch_loss: 0.7927 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.7915 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.7908 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.7901 [Training] 72/110 [==================>...........] - ETA: 10s  batch_loss: 0.7905 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.7881 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.7863 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.7879 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.7862 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.7880 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.7877 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.7870 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.7872 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.7876 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.7855 [Training] 83/110 [=====================>........] - ETA: 7s  batch_loss: 0.7860 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.7861 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.7858 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.7847 [Training] 87/110 [======================>.......] - ETA: 6s  batch_loss: 0.7856 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.7871 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.7866 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.7858 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.7843 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.7838 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.7835 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.7830 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.7821 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.7817 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.7815 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.7813 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.7803 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.7796 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.7804 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.7788 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.7772 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.7755 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.7757 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.7745 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.7741 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.7738 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.7730 [Training] 110/110 [==============================] 258.1ms/step  batch_loss: 0.7730 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 11:51:56 - INFO - root -   The F1-score is 0.479090189303966
09/25/2022 11:51:56 - INFO - root -   the best eval f1 is 0.4791, saving model !!
09/25/2022 11:51:59 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:29  batch_loss: 0.7344 [Training] 2/110 [..............................] - ETA: 57s  batch_loss: 0.7809 [Training] 3/110 [..............................] - ETA: 46s  batch_loss: 0.7526 [Training] 4/110 [>.............................] - ETA: 41s  batch_loss: 0.7457 [Training] 5/110 [>.............................] - ETA: 38s  batch_loss: 0.7529 [Training] 6/110 [>.............................] - ETA: 35s  batch_loss: 0.7542 [Training] 7/110 [>.............................] - ETA: 34s  batch_loss: 0.7501 [Training] 8/110 [=>............................] - ETA: 32s  batch_loss: 0.7371 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 0.7255 [Training] 10/110 [=>............................] - ETA: 30s  batch_loss: 0.7162 [Training] 11/110 [==>...........................] - ETA: 29s  batch_loss: 0.7180 [Training] 12/110 [==>...........................] - ETA: 29s  batch_loss: 0.7127 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.7137 [Training] 14/110 [==>...........................] - ETA: 27s  batch_loss: 0.7076 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 0.7017 [Training] 16/110 [===>..........................] - ETA: 26s  batch_loss: 0.6999 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 0.6983 [Training] 18/110 [===>..........................] - ETA: 26s  batch_loss: 0.6910 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.6912 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 0.6909 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.6891 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.6912 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 0.6905 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.6913 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.6905 [Training] 26/110 [======>.......................] - ETA: 22s  batch_loss: 0.6932 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.6930 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.6908 [Training] 29/110 [======>.......................] - ETA: 21s  batch_loss: 0.6970 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.6934 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.6905 [Training] 32/110 [=======>......................] - ETA: 21s  batch_loss: 0.6912 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.6900 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.6897 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.6897 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.6897 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.6894 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.6894 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 0.6885 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.6893 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.6872 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 0.6859 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.6865 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.6841 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.6835 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 0.6839 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.6842 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.6837 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.6840 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.6826 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.6844 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.6848 [Training] 53/110 [=============>................] - ETA: 14s  batch_loss: 0.6848 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.6839 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.6853 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.6848 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.6853 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.6836 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.6816 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.6809 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.6814 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.6812 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.6809 [Training] 64/110 [================>.............] - ETA: 12s  batch_loss: 0.6818 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.6816 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.6823 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.6823 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 0.6818 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.6806 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.6804 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.6791 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.6792 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.6798 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.6793 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.6792 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.6792 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.6782 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.6777 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.6782 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.6776 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.6779 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.6780 [Training] 83/110 [=====================>........] - ETA: 6s  batch_loss: 0.6778 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.6765 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.6750 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.6741 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.6731 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.6738 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.6725 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.6713 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.6719 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.6717 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.6716 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.6709 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.6702 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.6704 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.6691 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.6684 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.6680 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.6671 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.6679 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.6672 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.6669 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.6670 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.6668 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.6665 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.6662 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.6659 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.6649 [Training] 110/110 [==============================] 255.7ms/step  batch_loss: 0.6658 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 11:52:33 - INFO - root -   The F1-score is 0.49798471160528146
09/25/2022 11:52:33 - INFO - root -   the best eval f1 is 0.4980, saving model !!
09/25/2022 11:52:36 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:30  batch_loss: 0.5587 [Training] 2/110 [..............................] - ETA: 59s  batch_loss: 0.5807 [Training] 3/110 [..............................] - ETA: 48s  batch_loss: 0.5786 [Training] 4/110 [>.............................] - ETA: 42s  batch_loss: 0.5893 [Training] 5/110 [>.............................] - ETA: 39s  batch_loss: 0.6101 [Training] 6/110 [>.............................] - ETA: 36s  batch_loss: 0.6048 [Training] 7/110 [>.............................] - ETA: 35s  batch_loss: 0.6031 [Training] 8/110 [=>............................] - ETA: 33s  batch_loss: 0.6089 [Training] 9/110 [=>............................] - ETA: 32s  batch_loss: 0.6168 [Training] 10/110 [=>............................] - ETA: 31s  batch_loss: 0.6184 [Training] 11/110 [==>...........................] - ETA: 30s  batch_loss: 0.6173 [Training] 12/110 [==>...........................] - ETA: 30s  batch_loss: 0.6099 [Training] 13/110 [==>...........................] - ETA: 29s  batch_loss: 0.6002 [Training] 14/110 [==>...........................] - ETA: 28s  batch_loss: 0.6020 [Training] 15/110 [===>..........................] - ETA: 28s  batch_loss: 0.6019 [Training] 16/110 [===>..........................] - ETA: 27s  batch_loss: 0.5995 [Training] 17/110 [===>..........................] - ETA: 27s  batch_loss: 0.6009 [Training] 18/110 [===>..........................] - ETA: 26s  batch_loss: 0.6019 [Training] 19/110 [====>.........................] - ETA: 26s  batch_loss: 0.6017 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 0.6040 [Training] 21/110 [====>.........................] - ETA: 25s  batch_loss: 0.6060 [Training] 22/110 [=====>........................] - ETA: 25s  batch_loss: 0.6049 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 0.6061 [Training] 24/110 [=====>........................] - ETA: 24s  batch_loss: 0.6029 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.6026 [Training] 26/110 [======>.......................] - ETA: 23s  batch_loss: 0.6012 [Training] 27/110 [======>.......................] - ETA: 23s  batch_loss: 0.5982 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.5953 [Training] 29/110 [======>.......................] - ETA: 22s  batch_loss: 0.5949 [Training] 30/110 [=======>......................] - ETA: 22s  batch_loss: 0.5956 [Training] 31/110 [=======>......................] - ETA: 22s  batch_loss: 0.5937 [Training] 32/110 [=======>......................] - ETA: 21s  batch_loss: 0.5952 [Training] 33/110 [========>.....................] - ETA: 21s  batch_loss: 0.5942 [Training] 34/110 [========>.....................] - ETA: 21s  batch_loss: 0.5940 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.5949 [Training] 36/110 [========>.....................] - ETA: 20s  batch_loss: 0.5944 [Training] 37/110 [=========>....................] - ETA: 20s  batch_loss: 0.5996 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.5971 [Training] 39/110 [=========>....................] - ETA: 19s  batch_loss: 0.5969 [Training] 40/110 [=========>....................] - ETA: 19s  batch_loss: 0.5963 [Training] 41/110 [==========>...................] - ETA: 19s  batch_loss: 0.5979 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 0.5948 [Training] 43/110 [==========>...................] - ETA: 18s  batch_loss: 0.5956 [Training] 44/110 [===========>..................] - ETA: 18s  batch_loss: 0.5934 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.5953 [Training] 46/110 [===========>..................] - ETA: 17s  batch_loss: 0.5934 [Training] 47/110 [===========>..................] - ETA: 17s  batch_loss: 0.5927 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.5911 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.5916 [Training] 50/110 [============>.................] - ETA: 16s  batch_loss: 0.5906 [Training] 51/110 [============>.................] - ETA: 16s  batch_loss: 0.5923 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.5917 [Training] 53/110 [=============>................] - ETA: 15s  batch_loss: 0.5914 [Training] 54/110 [=============>................] - ETA: 15s  batch_loss: 0.5897 [Training] 55/110 [==============>...............] - ETA: 15s  batch_loss: 0.5885 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.5887 [Training] 57/110 [==============>...............] - ETA: 14s  batch_loss: 0.5901 [Training] 58/110 [==============>...............] - ETA: 14s  batch_loss: 0.5899 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.5901 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.5903 [Training] 61/110 [===============>..............] - ETA: 13s  batch_loss: 0.5912 [Training] 62/110 [===============>..............] - ETA: 13s  batch_loss: 0.5902 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.5893 [Training] 64/110 [================>.............] - ETA: 12s  batch_loss: 0.5904 [Training] 65/110 [================>.............] - ETA: 12s  batch_loss: 0.5910 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.5894 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.5881 [Training] 68/110 [=================>............] - ETA: 11s  batch_loss: 0.5874 [Training] 69/110 [=================>............] - ETA: 11s  batch_loss: 0.5868 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.5877 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.5877 [Training] 72/110 [==================>...........] - ETA: 10s  batch_loss: 0.5883 [Training] 73/110 [==================>...........] - ETA: 10s  batch_loss: 0.5874 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.5868 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.5864 [Training] 76/110 [===================>..........] - ETA: 9s  batch_loss: 0.5872 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.5867 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.5869 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.5861 [Training] 80/110 [====================>.........] - ETA: 8s  batch_loss: 0.5860 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.5865 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.5864 [Training] 83/110 [=====================>........] - ETA: 7s  batch_loss: 0.5853 [Training] 84/110 [=====================>........] - ETA: 7s  batch_loss: 0.5841 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.5837 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.5849 [Training] 87/110 [======================>.......] - ETA: 6s  batch_loss: 0.5850 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.5847 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.5839 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.5825 [Training] 91/110 [=======================>......] - ETA: 5s  batch_loss: 0.5828 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.5824 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.5826 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.5823 [Training] 95/110 [========================>.....] - ETA: 4s  batch_loss: 0.5819 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.5814 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.5811 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.5801 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.5800 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.5797 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.5798 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.5795 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.5799 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.5787 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.5791 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.5791 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.5784 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.5784 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.5782 [Training] 110/110 [==============================] 266.4ms/step  batch_loss: 0.5784 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 11:53:12 - INFO - root -   The F1-score is 0.5390187235205685
09/25/2022 11:53:12 - INFO - root -   the best eval f1 is 0.5390, saving model !!
09/25/2022 11:53:15 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:32  batch_loss: 0.5104 [Training] 2/110 [..............................] - ETA: 58s  batch_loss: 0.5130 [Training] 3/110 [..............................] - ETA: 47s  batch_loss: 0.5033 [Training] 4/110 [>.............................] - ETA: 42s  batch_loss: 0.5061 [Training] 5/110 [>.............................] - ETA: 38s  batch_loss: 0.5211 [Training] 6/110 [>.............................] - ETA: 36s  batch_loss: 0.5074 [Training] 7/110 [>.............................] - ETA: 34s  batch_loss: 0.5113 [Training] 8/110 [=>............................] - ETA: 33s  batch_loss: 0.5141 [Training] 9/110 [=>............................] - ETA: 32s  batch_loss: 0.5120 [Training] 10/110 [=>............................] - ETA: 31s  batch_loss: 0.5090 [Training] 11/110 [==>...........................] - ETA: 30s  batch_loss: 0.5133 [Training] 12/110 [==>...........................] - ETA: 29s  batch_loss: 0.5179 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.5217 [Training] 14/110 [==>...........................] - ETA: 28s  batch_loss: 0.5229 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 0.5188 [Training] 16/110 [===>..........................] - ETA: 27s  batch_loss: 0.5131 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 0.5161 [Training] 18/110 [===>..........................] - ETA: 26s  batch_loss: 0.5180 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.5176 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 0.5178 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.5192 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.5211 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 0.5216 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.5208 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.5167 [Training] 26/110 [======>.......................] - ETA: 23s  batch_loss: 0.5172 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.5162 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.5138 [Training] 29/110 [======>.......................] - ETA: 22s  batch_loss: 0.5148 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.5135 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.5140 [Training] 32/110 [=======>......................] - ETA: 21s  batch_loss: 0.5155 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.5178 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.5156 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.5156 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.5150 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.5157 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.5154 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 0.5178 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.5191 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.5201 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 0.5193 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.5209 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.5223 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.5227 [Training] 46/110 [===========>..................] - ETA: 17s  batch_loss: 0.5189 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.5186 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.5210 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.5199 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.5191 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.5193 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.5207 [Training] 53/110 [=============>................] - ETA: 15s  batch_loss: 0.5234 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.5241 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.5231 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.5236 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.5240 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.5241 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.5229 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.5229 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.5226 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.5217 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.5210 [Training] 64/110 [================>.............] - ETA: 12s  batch_loss: 0.5204 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.5203 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.5189 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.5191 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 0.5182 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.5171 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.5178 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.5174 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.5174 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.5161 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.5152 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.5151 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.5143 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.5147 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.5137 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.5141 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.5140 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.5145 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.5144 [Training] 83/110 [=====================>........] - ETA: 7s  batch_loss: 0.5141 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.5131 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.5123 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.5109 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.5107 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.5107 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.5104 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.5101 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.5101 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.5099 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.5097 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.5106 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.5094 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.5104 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.5100 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.5100 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.5095 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.5092 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.5085 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.5093 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.5089 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.5077 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.5076 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.5074 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.5076 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.5073 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.5072 [Training] 110/110 [==============================] 257.7ms/step  batch_loss: 0.5055 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 11:53:49 - INFO - root -   The F1-score is 0.5517607268386499
09/25/2022 11:53:49 - INFO - root -   the best eval f1 is 0.5518, saving model !!
09/25/2022 11:53:52 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:26  batch_loss: 0.5229 [Training] 2/110 [..............................] - ETA: 56s  batch_loss: 0.5038 [Training] 3/110 [..............................] - ETA: 46s  batch_loss: 0.4710 [Training] 4/110 [>.............................] - ETA: 41s  batch_loss: 0.4775 [Training] 5/110 [>.............................] - ETA: 37s  batch_loss: 0.4635 [Training] 6/110 [>.............................] - ETA: 35s  batch_loss: 0.4535 [Training] 7/110 [>.............................] - ETA: 34s  batch_loss: 0.4479 [Training] 8/110 [=>............................] - ETA: 32s  batch_loss: 0.4586 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 0.4534 [Training] 10/110 [=>............................] - ETA: 30s  batch_loss: 0.4552 [Training] 11/110 [==>...........................] - ETA: 29s  batch_loss: 0.4558 [Training] 12/110 [==>...........................] - ETA: 29s  batch_loss: 0.4466 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.4475 [Training] 14/110 [==>...........................] - ETA: 27s  batch_loss: 0.4493 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 0.4502 [Training] 16/110 [===>..........................] - ETA: 26s  batch_loss: 0.4572 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 0.4584 [Training] 18/110 [===>..........................] - ETA: 25s  batch_loss: 0.4564 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.4587 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 0.4588 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.4572 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.4568 [Training] 23/110 [=====>........................] - ETA: 23s  batch_loss: 0.4566 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.4590 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.4584 [Training] 26/110 [======>.......................] - ETA: 22s  batch_loss: 0.4583 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.4602 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.4622 [Training] 29/110 [======>.......................] - ETA: 21s  batch_loss: 0.4635 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.4644 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.4611 [Training] 32/110 [=======>......................] - ETA: 20s  batch_loss: 0.4598 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.4596 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.4585 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.4590 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.4574 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.4562 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.4555 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 0.4543 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.4534 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.4523 [Training] 42/110 [==========>...................] - ETA: 17s  batch_loss: 0.4552 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.4547 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.4537 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.4534 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 0.4537 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.4536 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.4542 [Training] 49/110 [============>.................] - ETA: 15s  batch_loss: 0.4537 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.4555 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.4561 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.4575 [Training] 53/110 [=============>................] - ETA: 14s  batch_loss: 0.4561 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.4574 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.4570 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.4560 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.4566 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.4567 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.4564 [Training] 60/110 [===============>..............] - ETA: 12s  batch_loss: 0.4565 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.4572 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.4571 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.4579 [Training] 64/110 [================>.............] - ETA: 11s  batch_loss: 0.4578 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.4569 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.4582 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.4578 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 0.4568 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.4566 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.4568 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.4569 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.4578 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.4580 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.4571 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.4570 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.4566 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.4559 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.4567 [Training] 79/110 [====================>.........] - ETA: 7s  batch_loss: 0.4562 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.4569 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.4555 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.4556 [Training] 83/110 [=====================>........] - ETA: 6s  batch_loss: 0.4550 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.4550 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.4550 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.4549 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.4545 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.4537 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.4533 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.4536 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.4533 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.4530 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.4533 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.4530 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.4532 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.4533 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.4530 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.4521 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.4516 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.4512 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.4513 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.4508 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.4502 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.4498 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.4496 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.4496 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.4491 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.4496 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.4494 [Training] 110/110 [==============================] 254.7ms/step  batch_loss: 0.4493 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 11:54:27 - INFO - root -   The F1-score is 0.5860971079862033
09/25/2022 11:54:27 - INFO - root -   the best eval f1 is 0.5861, saving model !!
09/25/2022 11:54:30 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:30  batch_loss: 0.4099 [Training] 2/110 [..............................] - ETA: 57s  batch_loss: 0.4558 [Training] 3/110 [..............................] - ETA: 47s  batch_loss: 0.4380 [Training] 4/110 [>.............................] - ETA: 41s  batch_loss: 0.4476 [Training] 5/110 [>.............................] - ETA: 38s  batch_loss: 0.4361 [Training] 6/110 [>.............................] - ETA: 36s  batch_loss: 0.4355 [Training] 7/110 [>.............................] - ETA: 34s  batch_loss: 0.4446 [Training] 8/110 [=>............................] - ETA: 33s  batch_loss: 0.4317 [Training] 9/110 [=>............................] - ETA: 32s  batch_loss: 0.4311 [Training] 10/110 [=>............................] - ETA: 31s  batch_loss: 0.4253 [Training] 11/110 [==>...........................] - ETA: 30s  batch_loss: 0.4252 [Training] 12/110 [==>...........................] - ETA: 29s  batch_loss: 0.4244 [Training] 13/110 [==>...........................] - ETA: 29s  batch_loss: 0.4264 [Training] 14/110 [==>...........................] - ETA: 28s  batch_loss: 0.4291 [Training] 15/110 [===>..........................] - ETA: 28s  batch_loss: 0.4282 [Training] 16/110 [===>..........................] - ETA: 27s  batch_loss: 0.4262 [Training] 17/110 [===>..........................] - ETA: 27s  batch_loss: 0.4281 [Training] 18/110 [===>..........................] - ETA: 26s  batch_loss: 0.4260 [Training] 19/110 [====>.........................] - ETA: 26s  batch_loss: 0.4237 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 0.4237 [Training] 21/110 [====>.........................] - ETA: 25s  batch_loss: 0.4241 [Training] 22/110 [=====>........................] - ETA: 25s  batch_loss: 0.4251 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 0.4241 [Training] 24/110 [=====>........................] - ETA: 24s  batch_loss: 0.4249 [Training] 25/110 [=====>........................] - ETA: 24s  batch_loss: 0.4222 [Training] 26/110 [======>.......................] - ETA: 23s  batch_loss: 0.4196 [Training] 27/110 [======>.......................] - ETA: 23s  batch_loss: 0.4195 [Training] 28/110 [======>.......................] - ETA: 23s  batch_loss: 0.4206 [Training] 29/110 [======>.......................] - ETA: 22s  batch_loss: 0.4178 [Training] 30/110 [=======>......................] - ETA: 22s  batch_loss: 0.4154 [Training] 31/110 [=======>......................] - ETA: 22s  batch_loss: 0.4157 [Training] 32/110 [=======>......................] - ETA: 21s  batch_loss: 0.4137 [Training] 33/110 [========>.....................] - ETA: 21s  batch_loss: 0.4135 [Training] 34/110 [========>.....................] - ETA: 21s  batch_loss: 0.4147 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.4122 [Training] 36/110 [========>.....................] - ETA: 20s  batch_loss: 0.4112 [Training] 37/110 [=========>....................] - ETA: 20s  batch_loss: 0.4097 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.4148 [Training] 39/110 [=========>....................] - ETA: 19s  batch_loss: 0.4122 [Training] 40/110 [=========>....................] - ETA: 19s  batch_loss: 0.4118 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.4117 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 0.4116 [Training] 43/110 [==========>...................] - ETA: 18s  batch_loss: 0.4127 [Training] 44/110 [===========>..................] - ETA: 18s  batch_loss: 0.4121 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.4128 [Training] 46/110 [===========>..................] - ETA: 17s  batch_loss: 0.4135 [Training] 47/110 [===========>..................] - ETA: 17s  batch_loss: 0.4122 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.4113 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.4118 [Training] 50/110 [============>.................] - ETA: 16s  batch_loss: 0.4119 [Training] 51/110 [============>.................] - ETA: 16s  batch_loss: 0.4126 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.4124 [Training] 53/110 [=============>................] - ETA: 15s  batch_loss: 0.4125 [Training] 54/110 [=============>................] - ETA: 15s  batch_loss: 0.4124 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.4137 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.4134 [Training] 57/110 [==============>...............] - ETA: 14s  batch_loss: 0.4126 [Training] 58/110 [==============>...............] - ETA: 14s  batch_loss: 0.4124 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.4110 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.4113 [Training] 61/110 [===============>..............] - ETA: 13s  batch_loss: 0.4129 [Training] 62/110 [===============>..............] - ETA: 13s  batch_loss: 0.4120 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.4132 [Training] 64/110 [================>.............] - ETA: 12s  batch_loss: 0.4141 [Training] 65/110 [================>.............] - ETA: 12s  batch_loss: 0.4139 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.4139 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.4144 [Training] 68/110 [=================>............] - ETA: 11s  batch_loss: 0.4151 [Training] 69/110 [=================>............] - ETA: 11s  batch_loss: 0.4132 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.4143 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.4134 [Training] 72/110 [==================>...........] - ETA: 10s  batch_loss: 0.4127 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.4119 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.4128 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.4142 [Training] 76/110 [===================>..........] - ETA: 9s  batch_loss: 0.4132 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.4128 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.4123 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.4119 [Training] 80/110 [====================>.........] - ETA: 8s  batch_loss: 0.4116 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.4109 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.4104 [Training] 83/110 [=====================>........] - ETA: 7s  batch_loss: 0.4100 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.4105 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.4107 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.4107 [Training] 87/110 [======================>.......] - ETA: 6s  batch_loss: 0.4109 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.4112 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.4113 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.4105 [Training] 91/110 [=======================>......] - ETA: 5s  batch_loss: 0.4105 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.4103 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.4091 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.4083 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.4075 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.4068 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.4067 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.4061 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.4057 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.4055 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.4059 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.4057 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.4052 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.4043 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.4038 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.4033 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.4033 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.4039 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.4036 [Training] 110/110 [==============================] 259.0ms/step  batch_loss: 0.4028 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 11:55:05 - INFO - root -   The F1-score is 0.5867381111855324
09/25/2022 11:55:05 - INFO - root -   the best eval f1 is 0.5867, saving model !!
09/25/2022 11:55:07 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:28  batch_loss: 0.3759 [Training] 2/110 [..............................] - ETA: 57s  batch_loss: 0.3859 [Training] 3/110 [..............................] - ETA: 47s  batch_loss: 0.3972 [Training] 4/110 [>.............................] - ETA: 42s  batch_loss: 0.3778 [Training] 5/110 [>.............................] - ETA: 38s  batch_loss: 0.3618 [Training] 6/110 [>.............................] - ETA: 36s  batch_loss: 0.3716 [Training] 7/110 [>.............................] - ETA: 34s  batch_loss: 0.3684 [Training] 8/110 [=>............................] - ETA: 33s  batch_loss: 0.3602 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 0.3579 [Training] 10/110 [=>............................] - ETA: 31s  batch_loss: 0.3558 [Training] 11/110 [==>...........................] - ETA: 30s  batch_loss: 0.3619 [Training] 12/110 [==>...........................] - ETA: 29s  batch_loss: 0.3599 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.3588 [Training] 14/110 [==>...........................] - ETA: 28s  batch_loss: 0.3533 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 0.3525 [Training] 16/110 [===>..........................] - ETA: 27s  batch_loss: 0.3571 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 0.3573 [Training] 18/110 [===>..........................] - ETA: 26s  batch_loss: 0.3597 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.3554 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 0.3565 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.3595 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.3595 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 0.3577 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.3567 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.3578 [Training] 26/110 [======>.......................] - ETA: 23s  batch_loss: 0.3579 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.3576 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.3574 [Training] 29/110 [======>.......................] - ETA: 22s  batch_loss: 0.3575 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.3584 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.3554 [Training] 32/110 [=======>......................] - ETA: 21s  batch_loss: 0.3564 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.3554 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.3573 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.3585 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.3582 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.3577 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.3582 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 0.3578 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.3580 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.3571 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 0.3582 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.3598 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.3615 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.3601 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 0.3589 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.3620 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.3617 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.3621 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.3613 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.3615 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.3616 [Training] 53/110 [=============>................] - ETA: 15s  batch_loss: 0.3614 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.3616 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.3617 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.3614 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.3600 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.3594 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.3591 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.3595 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.3596 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.3599 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.3598 [Training] 64/110 [================>.............] - ETA: 12s  batch_loss: 0.3583 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.3587 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.3590 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.3595 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 0.3585 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.3588 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.3595 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.3604 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.3600 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.3592 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.3590 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.3593 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.3609 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.3617 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.3613 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.3610 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.3618 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.3629 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.3622 [Training] 83/110 [=====================>........] - ETA: 7s  batch_loss: 0.3627 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.3619 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.3628 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.3621 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.3619 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.3612 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.3612 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.3612 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.3621 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.3619 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.3629 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.3635 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.3639 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.3639 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.3636 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.3644 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.3644 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.3643 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.3639 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.3641 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.3637 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.3639 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.3642 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.3644 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.3650 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.3652 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.3655 [Training] 110/110 [==============================] 256.5ms/step  batch_loss: 0.3653 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 11:55:42 - INFO - root -   The F1-score is 0.5975459326105305
09/25/2022 11:55:42 - INFO - root -   the best eval f1 is 0.5975, saving model !!
09/25/2022 11:55:45 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:30  batch_loss: 0.4087 [Training] 2/110 [..............................] - ETA: 58s  batch_loss: 0.3534 [Training] 3/110 [..............................] - ETA: 47s  batch_loss: 0.3491 [Training] 4/110 [>.............................] - ETA: 42s  batch_loss: 0.3453 [Training] 5/110 [>.............................] - ETA: 38s  batch_loss: 0.3414 [Training] 6/110 [>.............................] - ETA: 36s  batch_loss: 0.3376 [Training] 7/110 [>.............................] - ETA: 34s  batch_loss: 0.3407 [Training] 8/110 [=>............................] - ETA: 33s  batch_loss: 0.3450 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 0.3388 [Training] 10/110 [=>............................] - ETA: 30s  batch_loss: 0.3369 [Training] 11/110 [==>...........................] - ETA: 30s  batch_loss: 0.3393 [Training] 12/110 [==>...........................] - ETA: 29s  batch_loss: 0.3371 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.3342 [Training] 14/110 [==>...........................] - ETA: 28s  batch_loss: 0.3314 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 0.3307 [Training] 16/110 [===>..........................] - ETA: 27s  batch_loss: 0.3288 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 0.3287 [Training] 18/110 [===>..........................] - ETA: 26s  batch_loss: 0.3303 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.3323 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 0.3300 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.3299 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.3317 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 0.3297 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.3294 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.3279 [Training] 26/110 [======>.......................] - ETA: 23s  batch_loss: 0.3314 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.3315 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.3307 [Training] 29/110 [======>.......................] - ETA: 22s  batch_loss: 0.3298 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.3299 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.3291 [Training] 32/110 [=======>......................] - ETA: 21s  batch_loss: 0.3276 [Training] 33/110 [========>.....................] - ETA: 21s  batch_loss: 0.3269 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.3273 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.3289 [Training] 36/110 [========>.....................] - ETA: 20s  batch_loss: 0.3300 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.3291 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.3286 [Training] 39/110 [=========>....................] - ETA: 19s  batch_loss: 0.3297 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.3306 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.3293 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 0.3296 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.3300 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.3323 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.3330 [Training] 46/110 [===========>..................] - ETA: 17s  batch_loss: 0.3340 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.3330 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.3328 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.3321 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.3335 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.3336 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.3345 [Training] 53/110 [=============>................] - ETA: 15s  batch_loss: 0.3348 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.3353 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.3346 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.3351 [Training] 57/110 [==============>...............] - ETA: 14s  batch_loss: 0.3345 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.3334 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.3344 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.3344 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.3351 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.3350 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.3354 [Training] 64/110 [================>.............] - ETA: 12s  batch_loss: 0.3351 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.3344 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.3337 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.3340 [Training] 68/110 [=================>............] - ETA: 11s  batch_loss: 0.3343 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.3344 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.3346 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.3349 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.3343 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.3338 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.3339 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.3336 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.3337 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.3333 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.3332 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.3324 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.3323 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.3317 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.3321 [Training] 83/110 [=====================>........] - ETA: 7s  batch_loss: 0.3317 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.3315 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.3309 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.3309 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.3305 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.3310 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.3312 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.3314 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.3313 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.3312 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.3311 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.3309 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.3307 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.3301 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.3305 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.3308 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.3307 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.3306 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.3303 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.3305 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.3308 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.3304 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.3312 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.3307 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.3296 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.3288 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.3287 [Training] 110/110 [==============================] 257.5ms/step  batch_loss: 0.3283 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 11:56:20 - INFO - root -   The F1-score is 0.598705055792809
09/25/2022 11:56:20 - INFO - root -   the best eval f1 is 0.5987, saving model !!
09/25/2022 11:56:23 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:20  batch_loss: 0.3588 [Training] 2/110 [..............................] - ETA: 53s  batch_loss: 0.3272 [Training] 3/110 [..............................] - ETA: 44s  batch_loss: 0.3384 [Training] 4/110 [>.............................] - ETA: 39s  batch_loss: 0.3265 [Training] 5/110 [>.............................] - ETA: 36s  batch_loss: 0.3246 [Training] 6/110 [>.............................] - ETA: 34s  batch_loss: 0.3211 [Training] 7/110 [>.............................] - ETA: 32s  batch_loss: 0.3165 [Training] 8/110 [=>............................] - ETA: 31s  batch_loss: 0.3148 [Training] 9/110 [=>............................] - ETA: 30s  batch_loss: 0.3078 [Training] 10/110 [=>............................] - ETA: 29s  batch_loss: 0.3062 [Training] 11/110 [==>...........................] - ETA: 28s  batch_loss: 0.3023 [Training] 12/110 [==>...........................] - ETA: 28s  batch_loss: 0.2984 [Training] 13/110 [==>...........................] - ETA: 27s  batch_loss: 0.2978 [Training] 14/110 [==>...........................] - ETA: 26s  batch_loss: 0.3006 [Training] 15/110 [===>..........................] - ETA: 26s  batch_loss: 0.2995 [Training] 16/110 [===>..........................] - ETA: 25s  batch_loss: 0.3057 [Training] 17/110 [===>..........................] - ETA: 25s  batch_loss: 0.3060 [Training] 18/110 [===>..........................] - ETA: 24s  batch_loss: 0.3058 [Training] 19/110 [====>.........................] - ETA: 24s  batch_loss: 0.3059 [Training] 20/110 [====>.........................] - ETA: 24s  batch_loss: 0.3051 [Training] 21/110 [====>.........................] - ETA: 23s  batch_loss: 0.3040 [Training] 22/110 [=====>........................] - ETA: 23s  batch_loss: 0.3055 [Training] 23/110 [=====>........................] - ETA: 23s  batch_loss: 0.3060 [Training] 24/110 [=====>........................] - ETA: 22s  batch_loss: 0.3045 [Training] 25/110 [=====>........................] - ETA: 22s  batch_loss: 0.3045 [Training] 26/110 [======>.......................] - ETA: 22s  batch_loss: 0.3020 [Training] 27/110 [======>.......................] - ETA: 21s  batch_loss: 0.3001 [Training] 28/110 [======>.......................] - ETA: 21s  batch_loss: 0.2998 [Training] 29/110 [======>.......................] - ETA: 21s  batch_loss: 0.3003 [Training] 30/110 [=======>......................] - ETA: 20s  batch_loss: 0.2979 [Training] 31/110 [=======>......................] - ETA: 20s  batch_loss: 0.2969 [Training] 32/110 [=======>......................] - ETA: 20s  batch_loss: 0.2986 [Training] 33/110 [========>.....................] - ETA: 19s  batch_loss: 0.2989 [Training] 34/110 [========>.....................] - ETA: 19s  batch_loss: 0.3007 [Training] 35/110 [========>.....................] - ETA: 19s  batch_loss: 0.2998 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.3018 [Training] 37/110 [=========>....................] - ETA: 18s  batch_loss: 0.3046 [Training] 38/110 [=========>....................] - ETA: 18s  batch_loss: 0.3036 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 0.3028 [Training] 40/110 [=========>....................] - ETA: 17s  batch_loss: 0.3023 [Training] 41/110 [==========>...................] - ETA: 17s  batch_loss: 0.3019 [Training] 42/110 [==========>...................] - ETA: 17s  batch_loss: 0.3017 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.3027 [Training] 44/110 [===========>..................] - ETA: 16s  batch_loss: 0.3025 [Training] 45/110 [===========>..................] - ETA: 16s  batch_loss: 0.3021 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 0.3022 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.3015 [Training] 48/110 [============>.................] - ETA: 15s  batch_loss: 0.3017 [Training] 49/110 [============>.................] - ETA: 15s  batch_loss: 0.3017 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.3018 [Training] 51/110 [============>.................] - ETA: 14s  batch_loss: 0.3013 [Training] 52/110 [=============>................] - ETA: 14s  batch_loss: 0.3017 [Training] 53/110 [=============>................] - ETA: 14s  batch_loss: 0.3005 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.3002 [Training] 55/110 [==============>...............] - ETA: 13s  batch_loss: 0.3012 [Training] 56/110 [==============>...............] - ETA: 13s  batch_loss: 0.3023 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.3012 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.3017 [Training] 59/110 [===============>..............] - ETA: 12s  batch_loss: 0.3027 [Training] 60/110 [===============>..............] - ETA: 12s  batch_loss: 0.3032 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.3037 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.3033 [Training] 63/110 [================>.............] - ETA: 11s  batch_loss: 0.3035 [Training] 64/110 [================>.............] - ETA: 11s  batch_loss: 0.3036 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.3036 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.3032 [Training] 67/110 [=================>............] - ETA: 10s  batch_loss: 0.3031 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 0.3026 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.3030 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.3033 [Training] 71/110 [==================>...........] - ETA: 9s  batch_loss: 0.3036 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.3032 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.3027 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.3027 [Training] 75/110 [===================>..........] - ETA: 8s  batch_loss: 0.3020 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.3015 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.3013 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.3007 [Training] 79/110 [====================>.........] - ETA: 7s  batch_loss: 0.2997 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.2998 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.2999 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.3004 [Training] 83/110 [=====================>........] - ETA: 6s  batch_loss: 0.3005 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.3004 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.3005 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.3007 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.3005 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.3006 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.3007 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.3005 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.3007 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.3004 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.3002 [Training] 94/110 [========================>.....] - ETA: 3s  batch_loss: 0.3008 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.3012 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.3015 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.3014 [Training] 98/110 [=========================>....] - ETA: 2s  batch_loss: 0.3010 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.3012 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.3014 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.3018 [Training] 102/110 [==========================>...] - ETA: 1s  batch_loss: 0.3015 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.3013 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.3006 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.3005 [Training] 106/110 [===========================>..] - ETA: 0s  batch_loss: 0.3006 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.3007 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.3005 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.3004 [Training] 110/110 [==============================] 247.3ms/step  batch_loss: 0.2999 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 11:56:56 - INFO - root -   The F1-score is 0.6098109215919608
09/25/2022 11:56:56 - INFO - root -   the best eval f1 is 0.6098, saving model !!
09/25/2022 11:56:59 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:29  batch_loss: 0.2245 [Training] 2/110 [..............................] - ETA: 58s  batch_loss: 0.2570 [Training] 3/110 [..............................] - ETA: 47s  batch_loss: 0.2789 [Training] 4/110 [>.............................] - ETA: 41s  batch_loss: 0.2676 [Training] 5/110 [>.............................] - ETA: 38s  batch_loss: 0.2699 [Training] 6/110 [>.............................] - ETA: 36s  batch_loss: 0.2718 [Training] 7/110 [>.............................] - ETA: 34s  batch_loss: 0.2782 [Training] 8/110 [=>............................] - ETA: 33s  batch_loss: 0.2780 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 0.2860 [Training] 10/110 [=>............................] - ETA: 30s  batch_loss: 0.2864 [Training] 11/110 [==>...........................] - ETA: 30s  batch_loss: 0.2877 [Training] 12/110 [==>...........................] - ETA: 29s  batch_loss: 0.2868 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.2828 [Training] 14/110 [==>...........................] - ETA: 28s  batch_loss: 0.2816 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 0.2808 [Training] 16/110 [===>..........................] - ETA: 26s  batch_loss: 0.2803 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 0.2793 [Training] 18/110 [===>..........................] - ETA: 25s  batch_loss: 0.2788 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.2800 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 0.2808 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.2816 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.2822 [Training] 23/110 [=====>........................] - ETA: 23s  batch_loss: 0.2800 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.2813 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.2804 [Training] 26/110 [======>.......................] - ETA: 22s  batch_loss: 0.2804 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.2799 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.2790 [Training] 29/110 [======>.......................] - ETA: 21s  batch_loss: 0.2786 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.2777 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.2780 [Training] 32/110 [=======>......................] - ETA: 20s  batch_loss: 0.2785 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.2795 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.2788 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.2791 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.2791 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.2787 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.2795 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 0.2777 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.2778 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.2786 [Training] 42/110 [==========>...................] - ETA: 17s  batch_loss: 0.2784 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.2774 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.2774 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.2769 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 0.2772 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.2768 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.2766 [Training] 49/110 [============>.................] - ETA: 15s  batch_loss: 0.2766 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.2764 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.2766 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.2765 [Training] 53/110 [=============>................] - ETA: 14s  batch_loss: 0.2772 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.2760 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.2753 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.2748 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.2747 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.2746 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.2745 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.2751 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.2747 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.2753 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.2767 [Training] 64/110 [================>.............] - ETA: 11s  batch_loss: 0.2773 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.2774 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.2778 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.2773 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 0.2771 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.2794 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.2790 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.2793 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.2794 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.2789 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.2792 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.2785 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.2784 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.2782 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.2789 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.2786 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.2780 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.2780 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.2770 [Training] 83/110 [=====================>........] - ETA: 6s  batch_loss: 0.2766 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.2763 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.2754 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.2753 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.2755 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.2758 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.2759 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.2757 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.2753 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.2748 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.2749 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.2752 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.2751 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.2751 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.2754 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.2752 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.2753 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.2762 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.2757 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.2761 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.2762 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.2767 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.2762 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.2764 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.2768 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.2774 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.2776 [Training] 110/110 [==============================] 255.0ms/step  batch_loss: 0.2782 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 11:57:34 - INFO - root -   The F1-score is 0.6119810754981009
09/25/2022 11:57:34 - INFO - root -   the best eval f1 is 0.6120, saving model !!
09/25/2022 11:57:37 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:30  batch_loss: 0.2186 [Training] 2/110 [..............................] - ETA: 59s  batch_loss: 0.2780 [Training] 3/110 [..............................] - ETA: 48s  batch_loss: 0.2582 [Training] 4/110 [>.............................] - ETA: 42s  batch_loss: 0.2602 [Training] 5/110 [>.............................] - ETA: 39s  batch_loss: 0.2559 [Training] 6/110 [>.............................] - ETA: 37s  batch_loss: 0.2543 [Training] 7/110 [>.............................] - ETA: 35s  batch_loss: 0.2507 [Training] 8/110 [=>............................] - ETA: 34s  batch_loss: 0.2539 [Training] 9/110 [=>............................] - ETA: 32s  batch_loss: 0.2534 [Training] 10/110 [=>............................] - ETA: 31s  batch_loss: 0.2486 [Training] 11/110 [==>...........................] - ETA: 30s  batch_loss: 0.2493 [Training] 12/110 [==>...........................] - ETA: 30s  batch_loss: 0.2456 [Training] 13/110 [==>...........................] - ETA: 29s  batch_loss: 0.2453 [Training] 14/110 [==>...........................] - ETA: 28s  batch_loss: 0.2472 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 0.2462 [Training] 16/110 [===>..........................] - ETA: 27s  batch_loss: 0.2459 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 0.2470 [Training] 18/110 [===>..........................] - ETA: 26s  batch_loss: 0.2440 [Training] 19/110 [====>.........................] - ETA: 26s  batch_loss: 0.2461 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 0.2443 [Training] 21/110 [====>.........................] - ETA: 25s  batch_loss: 0.2470 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.2458 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 0.2490 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.2470 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.2451 [Training] 26/110 [======>.......................] - ETA: 23s  batch_loss: 0.2452 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.2459 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.2473 [Training] 29/110 [======>.......................] - ETA: 22s  batch_loss: 0.2482 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.2473 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.2487 [Training] 32/110 [=======>......................] - ETA: 21s  batch_loss: 0.2486 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.2513 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.2519 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.2515 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.2523 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.2521 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.2520 [Training] 39/110 [=========>....................] - ETA: 19s  batch_loss: 0.2515 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.2517 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.2511 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 0.2509 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.2505 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.2500 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.2500 [Training] 46/110 [===========>..................] - ETA: 17s  batch_loss: 0.2507 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.2496 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.2494 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.2498 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.2494 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.2498 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.2496 [Training] 53/110 [=============>................] - ETA: 15s  batch_loss: 0.2501 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.2504 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.2499 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.2498 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.2500 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.2500 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.2499 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.2515 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.2516 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.2513 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.2518 [Training] 64/110 [================>.............] - ETA: 12s  batch_loss: 0.2527 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.2521 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.2527 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.2525 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 0.2533 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.2536 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.2539 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.2544 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.2547 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.2540 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.2551 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.2554 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.2562 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.2558 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.2550 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.2548 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.2549 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.2551 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.2560 [Training] 83/110 [=====================>........] - ETA: 6s  batch_loss: 0.2560 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.2563 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.2561 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.2560 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.2559 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.2557 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.2556 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.2556 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.2555 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.2561 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.2571 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.2577 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.2576 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.2577 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.2572 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.2567 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.2568 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.2565 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.2569 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.2568 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.2573 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.2570 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.2574 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.2573 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.2570 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.2568 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.2565 [Training] 110/110 [==============================] 252.6ms/step  batch_loss: 0.2568 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 11:58:11 - INFO - root -   The F1-score is 0.6184939840475868
09/25/2022 11:58:11 - INFO - root -   the best eval f1 is 0.6185, saving model !!
09/25/2022 11:58:14 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:29  batch_loss: 0.1795 [Training] 2/110 [..............................] - ETA: 57s  batch_loss: 0.1961 [Training] 3/110 [..............................] - ETA: 47s  batch_loss: 0.2205 [Training] 4/110 [>.............................] - ETA: 41s  batch_loss: 0.2164 [Training] 5/110 [>.............................] - ETA: 38s  batch_loss: 0.2219 [Training] 6/110 [>.............................] - ETA: 36s  batch_loss: 0.2272 [Training] 7/110 [>.............................] - ETA: 34s  batch_loss: 0.2345 [Training] 8/110 [=>............................] - ETA: 33s  batch_loss: 0.2345 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 0.2401 [Training] 10/110 [=>............................] - ETA: 30s  batch_loss: 0.2390 [Training] 11/110 [==>...........................] - ETA: 30s  batch_loss: 0.2436 [Training] 12/110 [==>...........................] - ETA: 29s  batch_loss: 0.2446 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.2480 [Training] 14/110 [==>...........................] - ETA: 28s  batch_loss: 0.2510 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 0.2495 [Training] 16/110 [===>..........................] - ETA: 26s  batch_loss: 0.2444 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 0.2465 [Training] 18/110 [===>..........................] - ETA: 26s  batch_loss: 0.2444 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.2448 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 0.2460 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.2435 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.2413 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 0.2414 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.2406 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.2405 [Training] 26/110 [======>.......................] - ETA: 22s  batch_loss: 0.2421 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.2414 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.2397 [Training] 29/110 [======>.......................] - ETA: 21s  batch_loss: 0.2383 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.2408 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.2401 [Training] 32/110 [=======>......................] - ETA: 20s  batch_loss: 0.2393 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.2383 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.2383 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.2391 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.2379 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.2376 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.2376 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 0.2365 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.2376 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.2380 [Training] 42/110 [==========>...................] - ETA: 17s  batch_loss: 0.2392 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.2383 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.2392 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.2397 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 0.2396 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.2392 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.2388 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.2381 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.2381 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.2380 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.2376 [Training] 53/110 [=============>................] - ETA: 14s  batch_loss: 0.2377 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.2383 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.2380 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.2378 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.2377 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.2376 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.2371 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.2374 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.2374 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.2369 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.2368 [Training] 64/110 [================>.............] - ETA: 11s  batch_loss: 0.2362 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.2359 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.2364 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.2361 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 0.2356 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.2368 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.2371 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.2367 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.2369 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.2370 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.2371 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.2360 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.2365 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.2365 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.2365 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.2366 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.2366 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.2362 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.2372 [Training] 83/110 [=====================>........] - ETA: 6s  batch_loss: 0.2364 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.2367 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.2364 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.2367 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.2368 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.2373 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.2367 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.2368 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.2367 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.2375 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.2378 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.2377 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.2382 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.2382 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.2378 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.2381 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.2386 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.2388 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.2388 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.2390 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.2395 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.2392 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.2397 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.2400 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.2400 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.2396 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.2393 [Training] 110/110 [==============================] 254.9ms/step  batch_loss: 0.2380 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 11:58:48 - INFO - root -   The F1-score is 0.6165147598966128
09/25/2022 11:58:48 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:28  batch_loss: 0.2360 [Training] 2/110 [..............................] - ETA: 57s  batch_loss: 0.2356 [Training] 3/110 [..............................] - ETA: 47s  batch_loss: 0.2438 [Training] 4/110 [>.............................] - ETA: 41s  batch_loss: 0.2278 [Training] 5/110 [>.............................] - ETA: 38s  batch_loss: 0.2205 [Training] 6/110 [>.............................] - ETA: 36s  batch_loss: 0.2174 [Training] 7/110 [>.............................] - ETA: 34s  batch_loss: 0.2169 [Training] 8/110 [=>............................] - ETA: 33s  batch_loss: 0.2214 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 0.2201 [Training] 10/110 [=>............................] - ETA: 31s  batch_loss: 0.2256 [Training] 11/110 [==>...........................] - ETA: 30s  batch_loss: 0.2295 [Training] 12/110 [==>...........................] - ETA: 29s  batch_loss: 0.2256 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.2240 [Training] 14/110 [==>...........................] - ETA: 28s  batch_loss: 0.2209 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 0.2200 [Training] 16/110 [===>..........................] - ETA: 27s  batch_loss: 0.2174 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 0.2160 [Training] 18/110 [===>..........................] - ETA: 26s  batch_loss: 0.2182 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.2197 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 0.2210 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.2203 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.2198 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 0.2176 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.2159 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.2153 [Training] 26/110 [======>.......................] - ETA: 23s  batch_loss: 0.2147 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.2141 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.2128 [Training] 29/110 [======>.......................] - ETA: 22s  batch_loss: 0.2131 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.2124 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.2123 [Training] 32/110 [=======>......................] - ETA: 21s  batch_loss: 0.2127 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.2145 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.2153 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.2149 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.2143 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.2157 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.2147 [Training] 39/110 [=========>....................] - ETA: 19s  batch_loss: 0.2151 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.2159 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.2161 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 0.2172 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.2188 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.2184 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.2191 [Training] 46/110 [===========>..................] - ETA: 17s  batch_loss: 0.2197 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.2191 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.2185 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.2188 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.2190 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.2181 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.2180 [Training] 53/110 [=============>................] - ETA: 15s  batch_loss: 0.2183 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.2173 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.2171 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.2169 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.2177 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.2177 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.2172 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.2176 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.2179 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.2182 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.2188 [Training] 64/110 [================>.............] - ETA: 12s  batch_loss: 0.2188 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.2188 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.2192 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.2200 [Training] 68/110 [=================>............] - ETA: 11s  batch_loss: 0.2209 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.2206 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.2214 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.2219 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.2215 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.2210 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.2214 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.2216 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.2219 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.2221 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.2215 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.2216 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.2217 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.2220 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.2218 [Training] 83/110 [=====================>........] - ETA: 7s  batch_loss: 0.2226 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.2222 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.2231 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.2224 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.2222 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.2226 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.2233 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.2229 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.2224 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.2225 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.2234 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.2242 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.2242 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.2243 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.2245 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.2241 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.2240 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.2237 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.2237 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.2236 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.2233 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.2236 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.2235 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.2236 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.2236 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.2232 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.2234 [Training] 110/110 [==============================] 257.4ms/step  batch_loss: 0.2231 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 11:59:23 - INFO - root -   The F1-score is 0.6184497816593886
09/25/2022 11:59:23 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:30  batch_loss: 0.2648 [Training] 2/110 [..............................] - ETA: 58s  batch_loss: 0.2279 [Training] 3/110 [..............................] - ETA: 47s  batch_loss: 0.2132 [Training] 4/110 [>.............................] - ETA: 41s  batch_loss: 0.2002 [Training] 5/110 [>.............................] - ETA: 38s  batch_loss: 0.2076 [Training] 6/110 [>.............................] - ETA: 36s  batch_loss: 0.2134 [Training] 7/110 [>.............................] - ETA: 34s  batch_loss: 0.2109 [Training] 8/110 [=>............................] - ETA: 33s  batch_loss: 0.2081 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 0.2039 [Training] 10/110 [=>............................] - ETA: 30s  batch_loss: 0.2025 [Training] 11/110 [==>...........................] - ETA: 30s  batch_loss: 0.2092 [Training] 12/110 [==>...........................] - ETA: 29s  batch_loss: 0.2079 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.2065 [Training] 14/110 [==>...........................] - ETA: 28s  batch_loss: 0.2127 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 0.2097 [Training] 16/110 [===>..........................] - ETA: 27s  batch_loss: 0.2130 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 0.2132 [Training] 18/110 [===>..........................] - ETA: 26s  batch_loss: 0.2135 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.2120 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 0.2144 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.2135 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.2120 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 0.2094 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.2088 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.2078 [Training] 26/110 [======>.......................] - ETA: 22s  batch_loss: 0.2069 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.2064 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.2066 [Training] 29/110 [======>.......................] - ETA: 21s  batch_loss: 0.2063 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.2060 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.2055 [Training] 32/110 [=======>......................] - ETA: 20s  batch_loss: 0.2052 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.2068 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.2068 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.2071 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.2076 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.2070 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.2071 [Training] 39/110 [=========>....................] - ETA: 19s  batch_loss: 0.2065 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.2084 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.2085 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 0.2093 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.2094 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.2090 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.2090 [Training] 46/110 [===========>..................] - ETA: 17s  batch_loss: 0.2094 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.2098 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.2097 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.2097 [Training] 50/110 [============>.................] - ETA: 16s  batch_loss: 0.2094 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.2085 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.2082 [Training] 53/110 [=============>................] - ETA: 15s  batch_loss: 0.2085 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.2084 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.2075 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.2076 [Training] 57/110 [==============>...............] - ETA: 14s  batch_loss: 0.2070 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.2072 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.2071 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.2069 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.2085 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.2092 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.2094 [Training] 64/110 [================>.............] - ETA: 12s  batch_loss: 0.2092 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.2087 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.2091 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.2091 [Training] 68/110 [=================>............] - ETA: 11s  batch_loss: 0.2094 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.2094 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.2088 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.2087 [Training] 72/110 [==================>...........] - ETA: 10s  batch_loss: 0.2089 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.2089 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.2086 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.2092 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.2095 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.2095 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.2092 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.2091 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.2090 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.2087 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.2082 [Training] 83/110 [=====================>........] - ETA: 7s  batch_loss: 0.2082 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.2087 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.2094 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.2094 [Training] 87/110 [======================>.......] - ETA: 6s  batch_loss: 0.2095 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.2091 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.2092 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.2097 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.2093 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.2095 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.2094 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.2090 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.2096 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.2096 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.2094 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.2092 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.2093 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.2088 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.2090 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.2086 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.2088 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.2084 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.2089 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.2087 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.2088 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.2089 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.2086 [Training] 110/110 [==============================] 258.3ms/step  batch_loss: 0.2076 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 11:59:58 - INFO - root -   The F1-score is 0.626285868976719
09/25/2022 11:59:58 - INFO - root -   the best eval f1 is 0.6263, saving model !!
09/25/2022 12:00:01 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:28  batch_loss: 0.2053 [Training] 2/110 [..............................] - ETA: 56s  batch_loss: 0.2115 [Training] 3/110 [..............................] - ETA: 46s  batch_loss: 0.2133 [Training] 4/110 [>.............................] - ETA: 40s  batch_loss: 0.2173 [Training] 5/110 [>.............................] - ETA: 37s  batch_loss: 0.2152 [Training] 6/110 [>.............................] - ETA: 35s  batch_loss: 0.2072 [Training] 7/110 [>.............................] - ETA: 33s  batch_loss: 0.1996 [Training] 8/110 [=>............................] - ETA: 32s  batch_loss: 0.2029 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 0.2007 [Training] 10/110 [=>............................] - ETA: 30s  batch_loss: 0.2061 [Training] 11/110 [==>...........................] - ETA: 29s  batch_loss: 0.2069 [Training] 12/110 [==>...........................] - ETA: 28s  batch_loss: 0.2045 [Training] 13/110 [==>...........................] - ETA: 27s  batch_loss: 0.1990 [Training] 14/110 [==>...........................] - ETA: 27s  batch_loss: 0.1975 [Training] 15/110 [===>..........................] - ETA: 26s  batch_loss: 0.1968 [Training] 16/110 [===>..........................] - ETA: 26s  batch_loss: 0.1974 [Training] 17/110 [===>..........................] - ETA: 25s  batch_loss: 0.1963 [Training] 18/110 [===>..........................] - ETA: 25s  batch_loss: 0.1991 [Training] 19/110 [====>.........................] - ETA: 24s  batch_loss: 0.2008 [Training] 20/110 [====>.........................] - ETA: 24s  batch_loss: 0.2023 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.2017 [Training] 22/110 [=====>........................] - ETA: 23s  batch_loss: 0.2007 [Training] 23/110 [=====>........................] - ETA: 23s  batch_loss: 0.1998 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.1997 [Training] 25/110 [=====>........................] - ETA: 22s  batch_loss: 0.1988 [Training] 26/110 [======>.......................] - ETA: 22s  batch_loss: 0.1977 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.1967 [Training] 28/110 [======>.......................] - ETA: 21s  batch_loss: 0.1977 [Training] 29/110 [======>.......................] - ETA: 21s  batch_loss: 0.2007 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.1989 [Training] 31/110 [=======>......................] - ETA: 20s  batch_loss: 0.1979 [Training] 32/110 [=======>......................] - ETA: 20s  batch_loss: 0.1969 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.1949 [Training] 34/110 [========>.....................] - ETA: 19s  batch_loss: 0.1957 [Training] 35/110 [========>.....................] - ETA: 19s  batch_loss: 0.1953 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.1953 [Training] 37/110 [=========>....................] - ETA: 18s  batch_loss: 0.1954 [Training] 38/110 [=========>....................] - ETA: 18s  batch_loss: 0.1947 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 0.1949 [Training] 40/110 [=========>....................] - ETA: 17s  batch_loss: 0.1941 [Training] 41/110 [==========>...................] - ETA: 17s  batch_loss: 0.1947 [Training] 42/110 [==========>...................] - ETA: 17s  batch_loss: 0.1942 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.1936 [Training] 44/110 [===========>..................] - ETA: 16s  batch_loss: 0.1932 [Training] 45/110 [===========>..................] - ETA: 16s  batch_loss: 0.1945 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 0.1946 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.1954 [Training] 48/110 [============>.................] - ETA: 15s  batch_loss: 0.1951 [Training] 49/110 [============>.................] - ETA: 15s  batch_loss: 0.1956 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.1945 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.1952 [Training] 52/110 [=============>................] - ETA: 14s  batch_loss: 0.1952 [Training] 53/110 [=============>................] - ETA: 14s  batch_loss: 0.1959 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.1954 [Training] 55/110 [==============>...............] - ETA: 13s  batch_loss: 0.1950 [Training] 56/110 [==============>...............] - ETA: 13s  batch_loss: 0.1953 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.1953 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.1961 [Training] 59/110 [===============>..............] - ETA: 12s  batch_loss: 0.1963 [Training] 60/110 [===============>..............] - ETA: 12s  batch_loss: 0.1957 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.1954 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.1951 [Training] 63/110 [================>.............] - ETA: 11s  batch_loss: 0.1952 [Training] 64/110 [================>.............] - ETA: 11s  batch_loss: 0.1949 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.1951 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.1948 [Training] 67/110 [=================>............] - ETA: 10s  batch_loss: 0.1958 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 0.1963 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.1971 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.1974 [Training] 71/110 [==================>...........] - ETA: 9s  batch_loss: 0.1970 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.1965 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.1965 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.1969 [Training] 75/110 [===================>..........] - ETA: 8s  batch_loss: 0.1967 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.1970 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.1972 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.1975 [Training] 79/110 [====================>.........] - ETA: 7s  batch_loss: 0.1974 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.1974 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.1969 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.1967 [Training] 83/110 [=====================>........] - ETA: 6s  batch_loss: 0.1965 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.1973 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.1977 [Training] 86/110 [======================>.......] - ETA: 5s  batch_loss: 0.1981 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.1975 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.1970 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.1974 [Training] 90/110 [=======================>......] - ETA: 4s  batch_loss: 0.1973 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.1972 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.1977 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.1979 [Training] 94/110 [========================>.....] - ETA: 3s  batch_loss: 0.1979 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.1978 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.1973 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.1976 [Training] 98/110 [=========================>....] - ETA: 2s  batch_loss: 0.1971 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.1969 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.1969 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.1969 [Training] 102/110 [==========================>...] - ETA: 1s  batch_loss: 0.1968 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.1970 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.1972 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.1971 [Training] 106/110 [===========================>..] - ETA: 0s  batch_loss: 0.1972 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.1974 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.1970 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.1970 [Training] 110/110 [==============================] 247.1ms/step  batch_loss: 0.1962 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:00:34 - INFO - root -   The F1-score is 0.6366273187183811
09/25/2022 12:00:34 - INFO - root -   the best eval f1 is 0.6366, saving model !!
09/25/2022 12:00:37 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:32  batch_loss: 0.1883 [Training] 2/110 [..............................] - ETA: 59s  batch_loss: 0.1832 [Training] 3/110 [..............................] - ETA: 48s  batch_loss: 0.1838 [Training] 4/110 [>.............................] - ETA: 42s  batch_loss: 0.1732 [Training] 5/110 [>.............................] - ETA: 39s  batch_loss: 0.1764 [Training] 6/110 [>.............................] - ETA: 36s  batch_loss: 0.1694 [Training] 7/110 [>.............................] - ETA: 34s  batch_loss: 0.1679 [Training] 8/110 [=>............................] - ETA: 33s  batch_loss: 0.1632 [Training] 9/110 [=>............................] - ETA: 32s  batch_loss: 0.1631 [Training] 10/110 [=>............................] - ETA: 31s  batch_loss: 0.1666 [Training] 11/110 [==>...........................] - ETA: 30s  batch_loss: 0.1695 [Training] 12/110 [==>...........................] - ETA: 29s  batch_loss: 0.1660 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.1738 [Training] 14/110 [==>...........................] - ETA: 28s  batch_loss: 0.1754 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 0.1760 [Training] 16/110 [===>..........................] - ETA: 27s  batch_loss: 0.1816 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 0.1843 [Training] 18/110 [===>..........................] - ETA: 26s  batch_loss: 0.1880 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.1873 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 0.1881 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.1871 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.1873 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 0.1874 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.1879 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.1881 [Training] 26/110 [======>.......................] - ETA: 23s  batch_loss: 0.1876 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.1872 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.1869 [Training] 29/110 [======>.......................] - ETA: 22s  batch_loss: 0.1855 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.1870 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.1870 [Training] 32/110 [=======>......................] - ETA: 21s  batch_loss: 0.1865 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.1859 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.1856 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.1841 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.1825 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.1822 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.1830 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 0.1823 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.1817 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.1810 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 0.1813 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.1818 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.1819 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.1816 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 0.1813 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.1820 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.1818 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.1818 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.1827 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.1830 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.1834 [Training] 53/110 [=============>................] - ETA: 15s  batch_loss: 0.1839 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.1835 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.1844 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.1843 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.1845 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.1846 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.1854 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.1851 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.1867 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.1864 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.1867 [Training] 64/110 [================>.............] - ETA: 12s  batch_loss: 0.1870 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.1878 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.1872 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.1874 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 0.1870 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.1870 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.1870 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.1876 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.1878 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.1874 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.1870 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.1870 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.1864 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.1865 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.1868 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.1861 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.1867 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.1867 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.1875 [Training] 83/110 [=====================>........] - ETA: 7s  batch_loss: 0.1877 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.1874 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.1873 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.1874 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.1874 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.1879 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.1879 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.1879 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.1876 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.1874 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.1874 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.1872 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.1874 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.1868 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.1867 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.1863 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.1861 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.1863 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.1866 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.1866 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.1867 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.1868 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.1870 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.1869 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.1864 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.1862 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.1868 [Training] 110/110 [==============================] 256.8ms/step  batch_loss: 0.1861 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:01:12 - INFO - root -   The F1-score is 0.6340304809658904
09/25/2022 12:01:12 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:31  batch_loss: 0.2147 [Training] 2/110 [..............................] - ETA: 59s  batch_loss: 0.1842 [Training] 3/110 [..............................] - ETA: 48s  batch_loss: 0.1912 [Training] 4/110 [>.............................] - ETA: 42s  batch_loss: 0.1900 [Training] 5/110 [>.............................] - ETA: 38s  batch_loss: 0.1797 [Training] 6/110 [>.............................] - ETA: 36s  batch_loss: 0.1710 [Training] 7/110 [>.............................] - ETA: 34s  batch_loss: 0.1632 [Training] 8/110 [=>............................] - ETA: 33s  batch_loss: 0.1613 [Training] 9/110 [=>............................] - ETA: 32s  batch_loss: 0.1654 [Training] 10/110 [=>............................] - ETA: 31s  batch_loss: 0.1688 [Training] 11/110 [==>...........................] - ETA: 30s  batch_loss: 0.1689 [Training] 12/110 [==>...........................] - ETA: 29s  batch_loss: 0.1683 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.1709 [Training] 14/110 [==>...........................] - ETA: 28s  batch_loss: 0.1695 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 0.1702 [Training] 16/110 [===>..........................] - ETA: 27s  batch_loss: 0.1703 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 0.1694 [Training] 18/110 [===>..........................] - ETA: 26s  batch_loss: 0.1702 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.1713 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 0.1710 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.1727 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.1731 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 0.1746 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.1733 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.1738 [Training] 26/110 [======>.......................] - ETA: 23s  batch_loss: 0.1734 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.1735 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.1729 [Training] 29/110 [======>.......................] - ETA: 22s  batch_loss: 0.1709 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.1705 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.1706 [Training] 32/110 [=======>......................] - ETA: 21s  batch_loss: 0.1704 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.1707 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.1715 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.1718 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.1708 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.1705 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.1696 [Training] 39/110 [=========>....................] - ETA: 19s  batch_loss: 0.1714 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.1708 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.1703 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 0.1708 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.1713 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.1711 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.1711 [Training] 46/110 [===========>..................] - ETA: 17s  batch_loss: 0.1724 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.1732 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.1738 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.1736 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.1730 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.1733 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.1731 [Training] 53/110 [=============>................] - ETA: 15s  batch_loss: 0.1731 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.1732 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.1734 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.1736 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.1745 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.1746 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.1743 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.1743 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.1738 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.1737 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.1728 [Training] 64/110 [================>.............] - ETA: 12s  batch_loss: 0.1734 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.1728 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.1728 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.1732 [Training] 68/110 [=================>............] - ETA: 11s  batch_loss: 0.1728 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.1725 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.1726 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.1731 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.1728 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.1738 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.1746 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.1742 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.1742 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.1747 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.1746 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.1754 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.1758 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.1762 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.1765 [Training] 83/110 [=====================>........] - ETA: 7s  batch_loss: 0.1764 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.1757 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.1756 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.1756 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.1756 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.1761 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.1760 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.1761 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.1763 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.1766 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.1762 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.1761 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.1760 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.1760 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.1768 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.1766 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.1764 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.1765 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.1767 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.1770 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.1770 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.1771 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.1773 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.1776 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.1776 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.1774 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.1773 [Training] 110/110 [==============================] 256.7ms/step  batch_loss: 0.1773 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:01:46 - INFO - root -   The F1-score is 0.640882491423959
09/25/2022 12:01:46 - INFO - root -   the best eval f1 is 0.6409, saving model !!
09/25/2022 12:01:49 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:29  batch_loss: 0.1091 [Training] 2/110 [..............................] - ETA: 57s  batch_loss: 0.1523 [Training] 3/110 [..............................] - ETA: 46s  batch_loss: 0.1680 [Training] 4/110 [>.............................] - ETA: 41s  batch_loss: 0.1627 [Training] 5/110 [>.............................] - ETA: 38s  batch_loss: 0.1631 [Training] 6/110 [>.............................] - ETA: 35s  batch_loss: 0.1600 [Training] 7/110 [>.............................] - ETA: 33s  batch_loss: 0.1574 [Training] 8/110 [=>............................] - ETA: 32s  batch_loss: 0.1622 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 0.1628 [Training] 10/110 [=>............................] - ETA: 30s  batch_loss: 0.1631 [Training] 11/110 [==>...........................] - ETA: 29s  batch_loss: 0.1650 [Training] 12/110 [==>...........................] - ETA: 28s  batch_loss: 0.1689 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.1681 [Training] 14/110 [==>...........................] - ETA: 27s  batch_loss: 0.1687 [Training] 15/110 [===>..........................] - ETA: 26s  batch_loss: 0.1720 [Training] 16/110 [===>..........................] - ETA: 26s  batch_loss: 0.1720 [Training] 17/110 [===>..........................] - ETA: 25s  batch_loss: 0.1740 [Training] 18/110 [===>..........................] - ETA: 25s  batch_loss: 0.1763 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.1773 [Training] 20/110 [====>.........................] - ETA: 24s  batch_loss: 0.1733 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.1712 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.1725 [Training] 23/110 [=====>........................] - ETA: 23s  batch_loss: 0.1723 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.1711 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.1700 [Training] 26/110 [======>.......................] - ETA: 22s  batch_loss: 0.1697 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.1702 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.1690 [Training] 29/110 [======>.......................] - ETA: 21s  batch_loss: 0.1687 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.1685 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.1686 [Training] 32/110 [=======>......................] - ETA: 20s  batch_loss: 0.1689 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.1692 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.1689 [Training] 35/110 [========>.....................] - ETA: 19s  batch_loss: 0.1684 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.1684 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.1680 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.1672 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 0.1674 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.1676 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.1673 [Training] 42/110 [==========>...................] - ETA: 17s  batch_loss: 0.1669 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.1666 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.1673 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.1675 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 0.1670 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.1660 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.1663 [Training] 49/110 [============>.................] - ETA: 15s  batch_loss: 0.1661 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.1668 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.1677 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.1667 [Training] 53/110 [=============>................] - ETA: 14s  batch_loss: 0.1661 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.1660 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.1666 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.1673 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.1668 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.1660 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.1661 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.1652 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.1656 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.1664 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.1673 [Training] 64/110 [================>.............] - ETA: 11s  batch_loss: 0.1678 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.1673 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.1674 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.1667 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 0.1668 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.1667 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.1669 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.1677 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.1679 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.1675 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.1674 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.1672 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.1680 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.1681 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.1682 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.1683 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.1685 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.1683 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.1685 [Training] 83/110 [=====================>........] - ETA: 6s  batch_loss: 0.1680 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.1681 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.1682 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.1681 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.1679 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.1676 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.1670 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.1670 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.1669 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.1666 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.1661 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.1658 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.1656 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.1655 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.1656 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.1657 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.1658 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.1660 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.1658 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.1669 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.1665 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.1664 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.1662 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.1659 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.1658 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.1663 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.1661 [Training] 110/110 [==============================] 255.2ms/step  batch_loss: 0.1658 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:02:23 - INFO - root -   The F1-score is 0.6367813074297716
09/25/2022 12:02:23 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:27  batch_loss: 0.1676 [Training] 2/110 [..............................] - ETA: 56s  batch_loss: 0.1483 [Training] 3/110 [..............................] - ETA: 46s  batch_loss: 0.1517 [Training] 4/110 [>.............................] - ETA: 41s  batch_loss: 0.1469 [Training] 5/110 [>.............................] - ETA: 38s  batch_loss: 0.1579 [Training] 6/110 [>.............................] - ETA: 35s  batch_loss: 0.1588 [Training] 7/110 [>.............................] - ETA: 34s  batch_loss: 0.1554 [Training] 8/110 [=>............................] - ETA: 32s  batch_loss: 0.1560 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 0.1555 [Training] 10/110 [=>............................] - ETA: 30s  batch_loss: 0.1563 [Training] 11/110 [==>...........................] - ETA: 29s  batch_loss: 0.1510 [Training] 12/110 [==>...........................] - ETA: 28s  batch_loss: 0.1478 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.1492 [Training] 14/110 [==>...........................] - ETA: 27s  batch_loss: 0.1464 [Training] 15/110 [===>..........................] - ETA: 26s  batch_loss: 0.1482 [Training] 16/110 [===>..........................] - ETA: 26s  batch_loss: 0.1475 [Training] 17/110 [===>..........................] - ETA: 25s  batch_loss: 0.1505 [Training] 18/110 [===>..........................] - ETA: 25s  batch_loss: 0.1520 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.1514 [Training] 20/110 [====>.........................] - ETA: 24s  batch_loss: 0.1513 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.1504 [Training] 22/110 [=====>........................] - ETA: 23s  batch_loss: 0.1502 [Training] 23/110 [=====>........................] - ETA: 23s  batch_loss: 0.1495 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.1480 [Training] 25/110 [=====>........................] - ETA: 22s  batch_loss: 0.1492 [Training] 26/110 [======>.......................] - ETA: 22s  batch_loss: 0.1497 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.1502 [Training] 28/110 [======>.......................] - ETA: 21s  batch_loss: 0.1490 [Training] 29/110 [======>.......................] - ETA: 21s  batch_loss: 0.1496 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.1506 [Training] 31/110 [=======>......................] - ETA: 20s  batch_loss: 0.1509 [Training] 32/110 [=======>......................] - ETA: 20s  batch_loss: 0.1524 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.1538 [Training] 34/110 [========>.....................] - ETA: 19s  batch_loss: 0.1540 [Training] 35/110 [========>.....................] - ETA: 19s  batch_loss: 0.1543 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.1547 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.1542 [Training] 38/110 [=========>....................] - ETA: 18s  batch_loss: 0.1541 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 0.1531 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.1533 [Training] 41/110 [==========>...................] - ETA: 17s  batch_loss: 0.1536 [Training] 42/110 [==========>...................] - ETA: 17s  batch_loss: 0.1531 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.1526 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.1533 [Training] 45/110 [===========>..................] - ETA: 16s  batch_loss: 0.1532 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 0.1531 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.1538 [Training] 48/110 [============>.................] - ETA: 15s  batch_loss: 0.1534 [Training] 49/110 [============>.................] - ETA: 15s  batch_loss: 0.1532 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.1538 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.1545 [Training] 52/110 [=============>................] - ETA: 14s  batch_loss: 0.1547 [Training] 53/110 [=============>................] - ETA: 14s  batch_loss: 0.1549 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.1550 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.1556 [Training] 56/110 [==============>...............] - ETA: 13s  batch_loss: 0.1559 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.1561 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.1554 [Training] 59/110 [===============>..............] - ETA: 12s  batch_loss: 0.1566 [Training] 60/110 [===============>..............] - ETA: 12s  batch_loss: 0.1574 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.1577 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.1578 [Training] 63/110 [================>.............] - ETA: 11s  batch_loss: 0.1576 [Training] 64/110 [================>.............] - ETA: 11s  batch_loss: 0.1572 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.1575 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.1577 [Training] 67/110 [=================>............] - ETA: 10s  batch_loss: 0.1574 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 0.1572 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.1582 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.1584 [Training] 71/110 [==================>...........] - ETA: 9s  batch_loss: 0.1588 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.1589 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.1592 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.1589 [Training] 75/110 [===================>..........] - ETA: 8s  batch_loss: 0.1583 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.1584 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.1586 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.1586 [Training] 79/110 [====================>.........] - ETA: 7s  batch_loss: 0.1585 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.1592 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.1594 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.1592 [Training] 83/110 [=====================>........] - ETA: 6s  batch_loss: 0.1594 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.1591 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.1588 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.1592 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.1594 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.1594 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.1597 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.1596 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.1594 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.1594 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.1591 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.1589 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.1591 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.1591 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.1595 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.1594 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.1594 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.1594 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.1589 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.1589 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.1587 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.1586 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.1587 [Training] 106/110 [===========================>..] - ETA: 0s  batch_loss: 0.1587 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.1593 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.1595 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.1596 [Training] 110/110 [==============================] 247.8ms/step  batch_loss: 0.1603 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:02:57 - INFO - root -   The F1-score is 0.6423495973472287
09/25/2022 12:02:57 - INFO - root -   the best eval f1 is 0.6423, saving model !!
09/25/2022 12:03:00 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:29  batch_loss: 0.1357 [Training] 2/110 [..............................] - ETA: 57s  batch_loss: 0.1325 [Training] 3/110 [..............................] - ETA: 47s  batch_loss: 0.1352 [Training] 4/110 [>.............................] - ETA: 41s  batch_loss: 0.1338 [Training] 5/110 [>.............................] - ETA: 37s  batch_loss: 0.1345 [Training] 6/110 [>.............................] - ETA: 35s  batch_loss: 0.1409 [Training] 7/110 [>.............................] - ETA: 33s  batch_loss: 0.1453 [Training] 8/110 [=>............................] - ETA: 32s  batch_loss: 0.1544 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 0.1569 [Training] 10/110 [=>............................] - ETA: 30s  batch_loss: 0.1561 [Training] 11/110 [==>...........................] - ETA: 29s  batch_loss: 0.1532 [Training] 12/110 [==>...........................] - ETA: 28s  batch_loss: 0.1495 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.1493 [Training] 14/110 [==>...........................] - ETA: 27s  batch_loss: 0.1517 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 0.1520 [Training] 16/110 [===>..........................] - ETA: 26s  batch_loss: 0.1494 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 0.1503 [Training] 18/110 [===>..........................] - ETA: 25s  batch_loss: 0.1503 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.1513 [Training] 20/110 [====>.........................] - ETA: 24s  batch_loss: 0.1504 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.1505 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.1485 [Training] 23/110 [=====>........................] - ETA: 23s  batch_loss: 0.1491 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.1502 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.1502 [Training] 26/110 [======>.......................] - ETA: 22s  batch_loss: 0.1502 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.1509 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.1512 [Training] 29/110 [======>.......................] - ETA: 21s  batch_loss: 0.1506 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.1511 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.1531 [Training] 32/110 [=======>......................] - ETA: 20s  batch_loss: 0.1524 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.1510 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.1527 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.1522 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.1509 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.1509 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.1496 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 0.1490 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.1486 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.1489 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 0.1491 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.1486 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.1491 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.1509 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 0.1509 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.1515 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.1526 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.1540 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.1536 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.1530 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.1530 [Training] 53/110 [=============>................] - ETA: 14s  batch_loss: 0.1525 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.1525 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.1521 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.1528 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.1525 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.1516 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.1525 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.1526 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.1529 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.1528 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.1523 [Training] 64/110 [================>.............] - ETA: 11s  batch_loss: 0.1524 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.1529 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.1529 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.1529 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 0.1526 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.1526 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.1525 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.1519 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.1516 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.1509 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.1504 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.1509 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.1512 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.1510 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.1515 [Training] 79/110 [====================>.........] - ETA: 7s  batch_loss: 0.1511 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.1516 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.1516 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.1519 [Training] 83/110 [=====================>........] - ETA: 6s  batch_loss: 0.1518 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.1523 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.1521 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.1521 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.1521 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.1519 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.1523 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.1524 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.1525 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.1527 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.1525 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.1529 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.1527 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.1530 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.1530 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.1534 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.1534 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.1533 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.1536 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.1532 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.1532 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.1532 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.1533 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.1532 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.1535 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.1530 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.1534 [Training] 110/110 [==============================] 252.0ms/step  batch_loss: 0.1535 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:03:34 - INFO - root -   The F1-score is 0.635465981333694
09/25/2022 12:03:34 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:27  batch_loss: 0.1313 [Training] 2/110 [..............................] - ETA: 56s  batch_loss: 0.1387 [Training] 3/110 [..............................] - ETA: 46s  batch_loss: 0.1372 [Training] 4/110 [>.............................] - ETA: 41s  batch_loss: 0.1389 [Training] 5/110 [>.............................] - ETA: 37s  batch_loss: 0.1446 [Training] 6/110 [>.............................] - ETA: 35s  batch_loss: 0.1446 [Training] 7/110 [>.............................] - ETA: 34s  batch_loss: 0.1382 [Training] 8/110 [=>............................] - ETA: 32s  batch_loss: 0.1378 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 0.1405 [Training] 10/110 [=>............................] - ETA: 30s  batch_loss: 0.1394 [Training] 11/110 [==>...........................] - ETA: 29s  batch_loss: 0.1431 [Training] 12/110 [==>...........................] - ETA: 29s  batch_loss: 0.1423 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.1445 [Training] 14/110 [==>...........................] - ETA: 27s  batch_loss: 0.1440 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 0.1443 [Training] 16/110 [===>..........................] - ETA: 26s  batch_loss: 0.1482 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 0.1483 [Training] 18/110 [===>..........................] - ETA: 25s  batch_loss: 0.1487 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.1474 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 0.1500 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.1501 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.1496 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 0.1514 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.1501 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.1499 [Training] 26/110 [======>.......................] - ETA: 22s  batch_loss: 0.1487 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.1490 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.1495 [Training] 29/110 [======>.......................] - ETA: 21s  batch_loss: 0.1490 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.1481 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.1474 [Training] 32/110 [=======>......................] - ETA: 21s  batch_loss: 0.1472 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.1466 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.1477 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.1492 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.1497 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.1491 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.1483 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 0.1498 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.1485 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.1476 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 0.1483 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.1486 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.1478 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.1477 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 0.1475 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.1464 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.1453 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.1451 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.1459 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.1468 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.1468 [Training] 53/110 [=============>................] - ETA: 15s  batch_loss: 0.1465 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.1472 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.1477 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.1479 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.1485 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.1500 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.1491 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.1488 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.1485 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.1490 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.1488 [Training] 64/110 [================>.............] - ETA: 12s  batch_loss: 0.1488 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.1480 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.1476 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.1476 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 0.1474 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.1474 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.1472 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.1468 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.1474 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.1473 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.1474 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.1474 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.1470 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.1474 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.1481 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.1481 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.1475 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.1475 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.1472 [Training] 83/110 [=====================>........] - ETA: 7s  batch_loss: 0.1473 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.1471 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.1470 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.1470 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.1470 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.1465 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.1462 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.1462 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.1463 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.1459 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.1459 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.1462 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.1460 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.1456 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.1455 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.1454 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.1453 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.1453 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.1452 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.1453 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.1455 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.1452 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.1450 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.1451 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.1454 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.1454 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.1458 [Training] 110/110 [==============================] 256.6ms/step  batch_loss: 0.1465 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:04:09 - INFO - root -   The F1-score is 0.6423787838199839
09/25/2022 12:04:09 - INFO - root -   the best eval f1 is 0.6424, saving model !!
09/25/2022 12:04:11 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:28  batch_loss: 0.1358 [Training] 2/110 [..............................] - ETA: 58s  batch_loss: 0.1224 [Training] 3/110 [..............................] - ETA: 47s  batch_loss: 0.1245 [Training] 4/110 [>.............................] - ETA: 41s  batch_loss: 0.1250 [Training] 5/110 [>.............................] - ETA: 38s  batch_loss: 0.1276 [Training] 6/110 [>.............................] - ETA: 35s  batch_loss: 0.1376 [Training] 7/110 [>.............................] - ETA: 33s  batch_loss: 0.1391 [Training] 8/110 [=>............................] - ETA: 32s  batch_loss: 0.1412 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 0.1423 [Training] 10/110 [=>............................] - ETA: 30s  batch_loss: 0.1410 [Training] 11/110 [==>...........................] - ETA: 29s  batch_loss: 0.1407 [Training] 12/110 [==>...........................] - ETA: 28s  batch_loss: 0.1403 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.1425 [Training] 14/110 [==>...........................] - ETA: 27s  batch_loss: 0.1409 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 0.1410 [Training] 16/110 [===>..........................] - ETA: 26s  batch_loss: 0.1403 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 0.1422 [Training] 18/110 [===>..........................] - ETA: 25s  batch_loss: 0.1438 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.1428 [Training] 20/110 [====>.........................] - ETA: 24s  batch_loss: 0.1426 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.1427 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.1422 [Training] 23/110 [=====>........................] - ETA: 23s  batch_loss: 0.1429 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.1436 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.1437 [Training] 26/110 [======>.......................] - ETA: 22s  batch_loss: 0.1447 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.1452 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.1459 [Training] 29/110 [======>.......................] - ETA: 21s  batch_loss: 0.1453 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.1441 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.1444 [Training] 32/110 [=======>......................] - ETA: 20s  batch_loss: 0.1426 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.1423 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.1428 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.1423 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.1421 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.1427 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.1432 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 0.1438 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.1436 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.1449 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 0.1453 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.1447 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.1441 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.1437 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 0.1431 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.1440 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.1431 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.1425 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.1433 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.1429 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.1434 [Training] 53/110 [=============>................] - ETA: 14s  batch_loss: 0.1432 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.1427 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.1427 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.1423 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.1418 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.1425 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.1420 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.1417 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.1412 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.1412 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.1422 [Training] 64/110 [================>.............] - ETA: 11s  batch_loss: 0.1425 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.1426 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.1424 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.1421 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 0.1419 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.1414 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.1419 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.1424 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.1424 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.1425 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.1422 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.1417 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.1416 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.1411 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.1409 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.1405 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.1407 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.1410 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.1407 [Training] 83/110 [=====================>........] - ETA: 6s  batch_loss: 0.1406 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.1406 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.1404 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.1404 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.1406 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.1415 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.1415 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.1413 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.1411 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.1406 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.1408 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.1410 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.1408 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.1405 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.1406 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.1404 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.1403 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.1401 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.1402 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.1404 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.1403 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.1408 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.1408 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.1406 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.1404 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.1405 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.1403 [Training] 110/110 [==============================] 255.1ms/step  batch_loss: 0.1400 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:04:46 - INFO - root -   The F1-score is 0.6470706394375627
09/25/2022 12:04:46 - INFO - root -   the best eval f1 is 0.6471, saving model !!
09/25/2022 12:04:48 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:28  batch_loss: 0.1450 [Training] 2/110 [..............................] - ETA: 57s  batch_loss: 0.1433 [Training] 3/110 [..............................] - ETA: 46s  batch_loss: 0.1256 [Training] 4/110 [>.............................] - ETA: 41s  batch_loss: 0.1289 [Training] 5/110 [>.............................] - ETA: 38s  batch_loss: 0.1285 [Training] 6/110 [>.............................] - ETA: 35s  batch_loss: 0.1320 [Training] 7/110 [>.............................] - ETA: 34s  batch_loss: 0.1383 [Training] 8/110 [=>............................] - ETA: 32s  batch_loss: 0.1332 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 0.1298 [Training] 10/110 [=>............................] - ETA: 30s  batch_loss: 0.1325 [Training] 11/110 [==>...........................] - ETA: 29s  batch_loss: 0.1309 [Training] 12/110 [==>...........................] - ETA: 29s  batch_loss: 0.1331 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.1333 [Training] 14/110 [==>...........................] - ETA: 27s  batch_loss: 0.1331 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 0.1321 [Training] 16/110 [===>..........................] - ETA: 26s  batch_loss: 0.1333 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 0.1329 [Training] 18/110 [===>..........................] - ETA: 26s  batch_loss: 0.1319 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.1312 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 0.1334 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.1321 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.1325 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 0.1337 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.1328 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.1314 [Training] 26/110 [======>.......................] - ETA: 22s  batch_loss: 0.1316 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.1318 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.1321 [Training] 29/110 [======>.......................] - ETA: 21s  batch_loss: 0.1317 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.1308 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.1295 [Training] 32/110 [=======>......................] - ETA: 21s  batch_loss: 0.1292 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.1304 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.1304 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.1314 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.1324 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.1343 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.1346 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 0.1354 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.1348 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.1341 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 0.1339 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.1334 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.1331 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.1323 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 0.1327 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.1341 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.1335 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.1335 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.1338 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.1346 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.1347 [Training] 53/110 [=============>................] - ETA: 15s  batch_loss: 0.1349 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.1348 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.1345 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.1343 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.1338 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.1341 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.1343 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.1340 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.1335 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.1334 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.1331 [Training] 64/110 [================>.............] - ETA: 12s  batch_loss: 0.1327 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.1328 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.1328 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.1325 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 0.1331 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.1325 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.1327 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.1326 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.1328 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.1326 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.1328 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.1325 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.1325 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.1327 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.1330 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.1329 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.1328 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.1328 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.1326 [Training] 83/110 [=====================>........] - ETA: 7s  batch_loss: 0.1325 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.1333 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.1333 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.1333 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.1330 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.1334 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.1332 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.1332 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.1332 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.1333 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.1333 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.1331 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.1330 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.1335 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.1339 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.1339 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.1335 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.1338 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.1336 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.1340 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.1339 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.1346 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.1353 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.1355 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.1356 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.1354 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.1356 [Training] 110/110 [==============================] 256.2ms/step  batch_loss: 0.1352 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:05:23 - INFO - root -   The F1-score is 0.6446237270202354
09/25/2022 12:05:23 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:26  batch_loss: 0.1117 [Training] 2/110 [..............................] - ETA: 56s  batch_loss: 0.1138 [Training] 3/110 [..............................] - ETA: 46s  batch_loss: 0.1068 [Training] 4/110 [>.............................] - ETA: 41s  batch_loss: 0.1066 [Training] 5/110 [>.............................] - ETA: 37s  batch_loss: 0.1135 [Training] 6/110 [>.............................] - ETA: 35s  batch_loss: 0.1129 [Training] 7/110 [>.............................] - ETA: 34s  batch_loss: 0.1206 [Training] 8/110 [=>............................] - ETA: 32s  batch_loss: 0.1150 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 0.1152 [Training] 10/110 [=>............................] - ETA: 30s  batch_loss: 0.1163 [Training] 11/110 [==>...........................] - ETA: 29s  batch_loss: 0.1176 [Training] 12/110 [==>...........................] - ETA: 29s  batch_loss: 0.1194 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.1199 [Training] 14/110 [==>...........................] - ETA: 27s  batch_loss: 0.1198 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 0.1198 [Training] 16/110 [===>..........................] - ETA: 26s  batch_loss: 0.1207 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 0.1200 [Training] 18/110 [===>..........................] - ETA: 26s  batch_loss: 0.1201 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.1214 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 0.1222 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.1230 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.1225 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 0.1232 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.1222 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.1211 [Training] 26/110 [======>.......................] - ETA: 23s  batch_loss: 0.1200 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.1204 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.1221 [Training] 29/110 [======>.......................] - ETA: 22s  batch_loss: 0.1213 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.1223 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.1225 [Training] 32/110 [=======>......................] - ETA: 21s  batch_loss: 0.1247 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.1257 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.1251 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.1264 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.1255 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.1246 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.1253 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 0.1253 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.1254 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.1255 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 0.1260 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.1264 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.1266 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.1269 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 0.1267 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.1264 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.1259 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.1255 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.1265 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.1258 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.1260 [Training] 53/110 [=============>................] - ETA: 15s  batch_loss: 0.1261 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.1260 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.1259 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.1264 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.1267 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.1270 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.1267 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.1268 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.1269 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.1267 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.1267 [Training] 64/110 [================>.............] - ETA: 12s  batch_loss: 0.1265 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.1267 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.1283 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.1282 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 0.1283 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.1284 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.1284 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.1291 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.1294 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.1293 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.1294 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.1300 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.1295 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.1293 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.1294 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.1293 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.1294 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.1290 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.1297 [Training] 83/110 [=====================>........] - ETA: 6s  batch_loss: 0.1304 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.1309 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.1309 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.1308 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.1306 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.1306 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.1307 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.1304 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.1305 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.1303 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.1303 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.1305 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.1301 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.1300 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.1302 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.1304 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.1304 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.1305 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.1305 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.1309 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.1311 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.1310 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.1311 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.1312 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.1310 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.1313 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.1314 [Training] 110/110 [==============================] 256.3ms/step  batch_loss: 0.1309 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:05:57 - INFO - root -   The F1-score is 0.6412223752377206
09/25/2022 12:05:57 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/110 [..............................] - ETA: 1:27  batch_loss: 0.0980 [Training] 2/110 [..............................] - ETA: 57s  batch_loss: 0.1071 [Training] 3/110 [..............................] - ETA: 47s  batch_loss: 0.1057 [Training] 4/110 [>.............................] - ETA: 41s  batch_loss: 0.1031 [Training] 5/110 [>.............................] - ETA: 38s  batch_loss: 0.1024 [Training] 6/110 [>.............................] - ETA: 36s  batch_loss: 0.1061 [Training] 7/110 [>.............................] - ETA: 34s  batch_loss: 0.1124 [Training] 8/110 [=>............................] - ETA: 32s  batch_loss: 0.1142 [Training] 9/110 [=>............................] - ETA: 31s  batch_loss: 0.1161 [Training] 10/110 [=>............................] - ETA: 30s  batch_loss: 0.1173 [Training] 11/110 [==>...........................] - ETA: 30s  batch_loss: 0.1184 [Training] 12/110 [==>...........................] - ETA: 29s  batch_loss: 0.1205 [Training] 13/110 [==>...........................] - ETA: 28s  batch_loss: 0.1206 [Training] 14/110 [==>...........................] - ETA: 28s  batch_loss: 0.1185 [Training] 15/110 [===>..........................] - ETA: 27s  batch_loss: 0.1164 [Training] 16/110 [===>..........................] - ETA: 26s  batch_loss: 0.1161 [Training] 17/110 [===>..........................] - ETA: 26s  batch_loss: 0.1167 [Training] 18/110 [===>..........................] - ETA: 26s  batch_loss: 0.1175 [Training] 19/110 [====>.........................] - ETA: 25s  batch_loss: 0.1179 [Training] 20/110 [====>.........................] - ETA: 25s  batch_loss: 0.1188 [Training] 21/110 [====>.........................] - ETA: 24s  batch_loss: 0.1182 [Training] 22/110 [=====>........................] - ETA: 24s  batch_loss: 0.1184 [Training] 23/110 [=====>........................] - ETA: 24s  batch_loss: 0.1175 [Training] 24/110 [=====>........................] - ETA: 23s  batch_loss: 0.1175 [Training] 25/110 [=====>........................] - ETA: 23s  batch_loss: 0.1190 [Training] 26/110 [======>.......................] - ETA: 23s  batch_loss: 0.1182 [Training] 27/110 [======>.......................] - ETA: 22s  batch_loss: 0.1188 [Training] 28/110 [======>.......................] - ETA: 22s  batch_loss: 0.1187 [Training] 29/110 [======>.......................] - ETA: 22s  batch_loss: 0.1211 [Training] 30/110 [=======>......................] - ETA: 21s  batch_loss: 0.1200 [Training] 31/110 [=======>......................] - ETA: 21s  batch_loss: 0.1194 [Training] 32/110 [=======>......................] - ETA: 21s  batch_loss: 0.1199 [Training] 33/110 [========>.....................] - ETA: 20s  batch_loss: 0.1207 [Training] 34/110 [========>.....................] - ETA: 20s  batch_loss: 0.1203 [Training] 35/110 [========>.....................] - ETA: 20s  batch_loss: 0.1206 [Training] 36/110 [========>.....................] - ETA: 19s  batch_loss: 0.1208 [Training] 37/110 [=========>....................] - ETA: 19s  batch_loss: 0.1203 [Training] 38/110 [=========>....................] - ETA: 19s  batch_loss: 0.1200 [Training] 39/110 [=========>....................] - ETA: 18s  batch_loss: 0.1202 [Training] 40/110 [=========>....................] - ETA: 18s  batch_loss: 0.1190 [Training] 41/110 [==========>...................] - ETA: 18s  batch_loss: 0.1189 [Training] 42/110 [==========>...................] - ETA: 18s  batch_loss: 0.1190 [Training] 43/110 [==========>...................] - ETA: 17s  batch_loss: 0.1187 [Training] 44/110 [===========>..................] - ETA: 17s  batch_loss: 0.1187 [Training] 45/110 [===========>..................] - ETA: 17s  batch_loss: 0.1196 [Training] 46/110 [===========>..................] - ETA: 16s  batch_loss: 0.1190 [Training] 47/110 [===========>..................] - ETA: 16s  batch_loss: 0.1194 [Training] 48/110 [============>.................] - ETA: 16s  batch_loss: 0.1201 [Training] 49/110 [============>.................] - ETA: 16s  batch_loss: 0.1208 [Training] 50/110 [============>.................] - ETA: 15s  batch_loss: 0.1203 [Training] 51/110 [============>.................] - ETA: 15s  batch_loss: 0.1218 [Training] 52/110 [=============>................] - ETA: 15s  batch_loss: 0.1213 [Training] 53/110 [=============>................] - ETA: 15s  batch_loss: 0.1210 [Training] 54/110 [=============>................] - ETA: 14s  batch_loss: 0.1211 [Training] 55/110 [==============>...............] - ETA: 14s  batch_loss: 0.1214 [Training] 56/110 [==============>...............] - ETA: 14s  batch_loss: 0.1223 [Training] 57/110 [==============>...............] - ETA: 13s  batch_loss: 0.1221 [Training] 58/110 [==============>...............] - ETA: 13s  batch_loss: 0.1221 [Training] 59/110 [===============>..............] - ETA: 13s  batch_loss: 0.1216 [Training] 60/110 [===============>..............] - ETA: 13s  batch_loss: 0.1218 [Training] 61/110 [===============>..............] - ETA: 12s  batch_loss: 0.1222 [Training] 62/110 [===============>..............] - ETA: 12s  batch_loss: 0.1233 [Training] 63/110 [================>.............] - ETA: 12s  batch_loss: 0.1231 [Training] 64/110 [================>.............] - ETA: 12s  batch_loss: 0.1231 [Training] 65/110 [================>.............] - ETA: 11s  batch_loss: 0.1231 [Training] 66/110 [=================>............] - ETA: 11s  batch_loss: 0.1232 [Training] 67/110 [=================>............] - ETA: 11s  batch_loss: 0.1233 [Training] 68/110 [=================>............] - ETA: 10s  batch_loss: 0.1230 [Training] 69/110 [=================>............] - ETA: 10s  batch_loss: 0.1230 [Training] 70/110 [==================>...........] - ETA: 10s  batch_loss: 0.1246 [Training] 71/110 [==================>...........] - ETA: 10s  batch_loss: 0.1245 [Training] 72/110 [==================>...........] - ETA: 9s  batch_loss: 0.1248 [Training] 73/110 [==================>...........] - ETA: 9s  batch_loss: 0.1247 [Training] 74/110 [===================>..........] - ETA: 9s  batch_loss: 0.1247 [Training] 75/110 [===================>..........] - ETA: 9s  batch_loss: 0.1247 [Training] 76/110 [===================>..........] - ETA: 8s  batch_loss: 0.1246 [Training] 77/110 [====================>.........] - ETA: 8s  batch_loss: 0.1251 [Training] 78/110 [====================>.........] - ETA: 8s  batch_loss: 0.1251 [Training] 79/110 [====================>.........] - ETA: 8s  batch_loss: 0.1250 [Training] 80/110 [====================>.........] - ETA: 7s  batch_loss: 0.1247 [Training] 81/110 [=====================>........] - ETA: 7s  batch_loss: 0.1251 [Training] 82/110 [=====================>........] - ETA: 7s  batch_loss: 0.1251 [Training] 83/110 [=====================>........] - ETA: 7s  batch_loss: 0.1248 [Training] 84/110 [=====================>........] - ETA: 6s  batch_loss: 0.1246 [Training] 85/110 [======================>.......] - ETA: 6s  batch_loss: 0.1246 [Training] 86/110 [======================>.......] - ETA: 6s  batch_loss: 0.1245 [Training] 87/110 [======================>.......] - ETA: 5s  batch_loss: 0.1244 [Training] 88/110 [=======================>......] - ETA: 5s  batch_loss: 0.1247 [Training] 89/110 [=======================>......] - ETA: 5s  batch_loss: 0.1246 [Training] 90/110 [=======================>......] - ETA: 5s  batch_loss: 0.1245 [Training] 91/110 [=======================>......] - ETA: 4s  batch_loss: 0.1248 [Training] 92/110 [========================>.....] - ETA: 4s  batch_loss: 0.1249 [Training] 93/110 [========================>.....] - ETA: 4s  batch_loss: 0.1255 [Training] 94/110 [========================>.....] - ETA: 4s  batch_loss: 0.1258 [Training] 95/110 [========================>.....] - ETA: 3s  batch_loss: 0.1259 [Training] 96/110 [=========================>....] - ETA: 3s  batch_loss: 0.1256 [Training] 97/110 [=========================>....] - ETA: 3s  batch_loss: 0.1255 [Training] 98/110 [=========================>....] - ETA: 3s  batch_loss: 0.1255 [Training] 99/110 [==========================>...] - ETA: 2s  batch_loss: 0.1259 [Training] 100/110 [==========================>...] - ETA: 2s  batch_loss: 0.1260 [Training] 101/110 [==========================>...] - ETA: 2s  batch_loss: 0.1259 [Training] 102/110 [==========================>...] - ETA: 2s  batch_loss: 0.1264 [Training] 103/110 [===========================>..] - ETA: 1s  batch_loss: 0.1263 [Training] 104/110 [===========================>..] - ETA: 1s  batch_loss: 0.1264 [Training] 105/110 [===========================>..] - ETA: 1s  batch_loss: 0.1261 [Training] 106/110 [===========================>..] - ETA: 1s  batch_loss: 0.1261 [Training] 107/110 [============================>.] - ETA: 0s  batch_loss: 0.1262 [Training] 108/110 [============================>.] - ETA: 0s  batch_loss: 0.1261 [Training] 109/110 [============================>.] - ETA: 0s  batch_loss: 0.1262 [Training] 110/110 [==============================] 258.2ms/step  batch_loss: 0.1263 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/25/2022 12:06:32 - INFO - root -   The F1-score is 0.6438919210295803
********** predict start ***********
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 8, 
    "event_type": "role", 
    "fine_tunning_model_path": "./output/DuEE1.0/role/best_model.pkl", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 0.001, 
    "max_len": 250, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 50.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 128, 
    "per_gpu_train_batch_size": 8, 
    "predict_save_path": "./output/DuEE1.0/role/test_result.json", 
    "seed": 66, 
    "stride": 100, 
    "test_json": "./data/DuEE1.0/subor_news.json", 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/2550 [00:00<?, ?it/s]tokenizing...:  11%|█         | 284/2550 [00:00<00:00, 2832.56it/s]tokenizing...:  24%|██▎       | 600/2550 [00:00<00:00, 3021.62it/s]tokenizing...:  36%|███▌      | 912/2550 [00:00<00:00, 3065.25it/s]tokenizing...:  48%|████▊     | 1219/2550 [00:00<00:00, 3017.22it/s]tokenizing...:  60%|██████    | 1541/2550 [00:00<00:00, 3087.17it/s]tokenizing...:  73%|███████▎  | 1850/2550 [00:00<00:00, 3067.64it/s]tokenizing...:  85%|████████▍ | 2157/2550 [00:00<00:00, 3061.27it/s]tokenizing...:  97%|█████████▋| 2464/2550 [00:00<00:00, 2983.00it/s]tokenizing...: 100%|██████████| 2550/2550 [00:00<00:00, 3021.38it/s]
09/25/2022 12:06:36 - INFO - root -   The nums of the test_dataset features is 2550
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:00<00:18,  1.03it/s] 10%|█         | 2/20 [00:01<00:13,  1.35it/s] 15%|█▌        | 3/20 [00:02<00:12,  1.34it/s] 20%|██        | 4/20 [00:02<00:10,  1.48it/s] 25%|██▌       | 5/20 [00:03<00:09,  1.62it/s] 30%|███       | 6/20 [00:03<00:08,  1.74it/s] 35%|███▌      | 7/20 [00:04<00:07,  1.85it/s] 40%|████      | 8/20 [00:04<00:06,  1.92it/s] 45%|████▌     | 9/20 [00:05<00:05,  1.98it/s] 50%|█████     | 10/20 [00:05<00:05,  1.85it/s] 55%|█████▌    | 11/20 [00:06<00:05,  1.80it/s] 60%|██████    | 12/20 [00:07<00:04,  1.82it/s] 65%|██████▌   | 13/20 [00:07<00:03,  1.89it/s] 70%|███████   | 14/20 [00:08<00:03,  1.88it/s] 75%|███████▌  | 15/20 [00:08<00:02,  1.92it/s] 80%|████████  | 16/20 [00:09<00:02,  1.77it/s] 85%|████████▌ | 17/20 [00:09<00:01,  1.86it/s] 90%|█████████ | 18/20 [00:10<00:01,  1.84it/s] 95%|█████████▌| 19/20 [00:10<00:00,  1.90it/s]100%|██████████| 20/20 [00:11<00:00,  1.98it/s]100%|██████████| 20/20 [00:11<00:00,  1.77it/s]
2550
147540
Traceback (most recent call last):
  File "predict_ner.py", line 114, in <module>
    main()
  File "predict_ner.py", line 103, in main
    assert len(results) == len(sentences)
AssertionError
trigger predict 147540 load from ./output/DuEE1.0/role/test_result.json
role predict 147540 load from ./output/DuEE1.0/role/test_result.json
schema 66 load from ./conf/DuEE1.0/event_schema.json
submit data 147540 save to ./output/DuEE1.0/surbot_news_together.json
********** Done ***************
