nohup: ignoring input
Traceback (most recent call last):
  File "/data/qingyang/event_extration/DuEE_merge/run_ner.py", line 7, in <module>
    from torch.cuda.amp import autocast, GradScaler
ModuleNotFoundError: No module named 'torch'
nohup: ignoring input
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 5, 
    "event_type": "role", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 1e-05, 
    "max_len": 150, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 30.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 48, 
    "per_gpu_train_batch_size": 64, 
    "seed": 66, 
    "stride": 100, 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/7225 [00:00<?, ?it/s]tokenizing...:   5%|▌         | 384/7225 [00:00<00:01, 3833.37it/s]tokenizing...:  11%|█         | 768/7225 [00:00<00:02, 3225.89it/s]tokenizing...:  15%|█▌        | 1097/7225 [00:00<00:02, 2847.16it/s]tokenizing...:  19%|█▉        | 1388/7225 [00:00<00:02, 2847.57it/s]tokenizing...:  23%|██▎       | 1677/7225 [00:00<00:01, 2831.30it/s]tokenizing...:  27%|██▋       | 1963/7225 [00:00<00:01, 2737.12it/s]tokenizing...:  31%|███       | 2239/7225 [00:00<00:01, 2636.45it/s]tokenizing...:  35%|███▍      | 2504/7225 [00:00<00:01, 2636.18it/s]tokenizing...:  39%|███▊      | 2782/7225 [00:00<00:01, 2677.10it/s]tokenizing...:  42%|████▏     | 3051/7225 [00:01<00:01, 2679.30it/s]tokenizing...:  46%|████▌     | 3320/7225 [00:01<00:01, 2529.51it/s]tokenizing...:  49%|████▉     | 3575/7225 [00:01<00:01, 2500.13it/s]tokenizing...:  53%|█████▎    | 3855/7225 [00:01<00:01, 2583.58it/s]tokenizing...:  57%|█████▋    | 4115/7225 [00:01<00:01, 2536.49it/s]tokenizing...:  60%|██████    | 4370/7225 [00:01<00:01, 2538.64it/s]tokenizing...:  64%|██████▍   | 4625/7225 [00:01<00:01, 2500.86it/s]tokenizing...:  68%|██████▊   | 4915/7225 [00:01<00:00, 2615.75it/s]tokenizing...:  72%|███████▏  | 5187/7225 [00:01<00:00, 2642.95it/s]tokenizing...:  75%|███████▌  | 5452/7225 [00:02<00:00, 1868.81it/s]tokenizing...:  79%|███████▊  | 5678/7225 [00:02<00:00, 1956.45it/s]tokenizing...:  82%|████████▏ | 5921/7225 [00:02<00:00, 2072.32it/s]tokenizing...:  86%|████████▌ | 6204/7225 [00:02<00:00, 2269.48it/s]tokenizing...:  90%|████████▉ | 6470/7225 [00:02<00:00, 2374.33it/s]tokenizing...:  93%|█████████▎| 6721/7225 [00:02<00:00, 2259.67it/s]tokenizing...:  96%|█████████▋| 6971/7225 [00:02<00:00, 2323.55it/s]tokenizing...: 100%|██████████| 7225/7225 [00:02<00:00, 2501.15it/s]
tokenizing...:   0%|          | 0/1870 [00:00<?, ?it/s]tokenizing...:  16%|█▌        | 302/1870 [00:00<00:00, 3013.13it/s]tokenizing...:  32%|███▏      | 604/1870 [00:00<00:00, 2726.71it/s]tokenizing...:  47%|████▋     | 879/1870 [00:00<00:00, 2653.22it/s]tokenizing...:  61%|██████▏   | 1146/1870 [00:00<00:00, 2616.41it/s]tokenizing...:  75%|███████▌  | 1409/1870 [00:00<00:00, 2589.54it/s]tokenizing...:  89%|████████▉ | 1669/1870 [00:00<00:00, 2394.65it/s]tokenizing...: 100%|██████████| 1870/1870 [00:00<00:00, 2526.24it/s]
11/26/2022 11:45:40 - INFO - root -   The nums of the train_dataset features is 7225
11/26/2022 11:45:40 - INFO - root -   The nums of the eval_dataset features is 1870
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/26/2022 11:45:53 - INFO - root -   ***** Running train *****
********************************************************************************
/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Traceback (most recent call last):
  File "run_ner.py", line 221, in <module>
    main()
  File "run_ner.py", line 198, in main
    train(args, train_iter, model)
  File "run_ner.py", line 91, in train
    logits = model(
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/event_extration/DuEE_merge/model/model.py", line 22, in forward
    output = self.bert(input_ids,
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1018, in forward
    encoder_outputs = self.encoder(
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 607, in forward
    layer_outputs = layer_module(
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 345, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 0; 10.76 GiB total capacity; 1.46 GiB already allocated; 22.44 MiB free; 1.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
nohup: ignoring input
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 5, 
    "event_type": "role", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 1e-05, 
    "max_len": 150, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 30.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 48, 
    "per_gpu_train_batch_size": 64, 
    "seed": 66, 
    "stride": 100, 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/7225 [00:00<?, ?it/s]tokenizing...:   5%|▍         | 361/7225 [00:00<00:01, 3599.43it/s]tokenizing...:  10%|▉         | 721/7225 [00:00<00:02, 2988.27it/s]tokenizing...:  14%|█▍        | 1027/7225 [00:00<00:02, 2764.29it/s]tokenizing...:  18%|█▊        | 1309/7225 [00:00<00:02, 2781.90it/s]tokenizing...:  22%|██▏       | 1604/7225 [00:00<00:01, 2836.58it/s]tokenizing...:  26%|██▌       | 1890/7225 [00:00<00:01, 2781.94it/s]tokenizing...:  30%|███       | 2170/7225 [00:00<00:01, 2650.59it/s]tokenizing...:  34%|███▍      | 2457/7225 [00:00<00:01, 2712.47it/s]tokenizing...:  38%|███▊      | 2730/7225 [00:00<00:01, 2701.03it/s]tokenizing...:  42%|████▏     | 3015/7225 [00:01<00:01, 2743.10it/s]tokenizing...:  46%|████▌     | 3291/7225 [00:01<00:01, 2697.38it/s]tokenizing...:  49%|████▉     | 3562/7225 [00:01<00:01, 2631.87it/s]tokenizing...:  53%|█████▎    | 3844/7225 [00:01<00:01, 2685.57it/s]tokenizing...:  57%|█████▋    | 4114/7225 [00:01<00:01, 2618.12it/s]tokenizing...:  61%|██████    | 4377/7225 [00:01<00:01, 2608.75it/s]tokenizing...:  64%|██████▍   | 4639/7225 [00:01<00:01, 2585.16it/s]tokenizing...:  68%|██████▊   | 4931/7225 [00:01<00:00, 2682.49it/s]tokenizing...:  72%|███████▏  | 5213/7225 [00:01<00:00, 2721.26it/s]tokenizing...:  76%|███████▌  | 5486/7225 [00:02<00:00, 1919.99it/s]tokenizing...:  79%|███████▉  | 5719/7225 [00:02<00:00, 2014.09it/s]tokenizing...:  83%|████████▎ | 5975/7225 [00:02<00:00, 2148.72it/s]tokenizing...:  87%|████████▋ | 6253/7225 [00:02<00:00, 2312.58it/s]tokenizing...:  90%|█████████ | 6504/7225 [00:02<00:00, 2364.46it/s]tokenizing...:  93%|█████████▎| 6753/7225 [00:02<00:00, 2295.25it/s]tokenizing...:  97%|█████████▋| 6999/7225 [00:02<00:00, 2339.97it/s]tokenizing...: 100%|██████████| 7225/7225 [00:02<00:00, 2540.50it/s]
tokenizing...:   0%|          | 0/1870 [00:00<?, ?it/s]tokenizing...:  17%|█▋        | 310/1870 [00:00<00:00, 3095.27it/s]tokenizing...:  33%|███▎      | 620/1870 [00:00<00:00, 2786.25it/s]tokenizing...:  48%|████▊     | 901/1870 [00:00<00:00, 2682.44it/s]tokenizing...:  63%|██████▎   | 1172/1870 [00:00<00:00, 2691.02it/s]tokenizing...:  77%|███████▋  | 1442/1870 [00:00<00:00, 2668.55it/s]tokenizing...:  91%|█████████▏| 1710/1870 [00:00<00:00, 2456.48it/s]tokenizing...: 100%|██████████| 1870/1870 [00:00<00:00, 2589.58it/s]
11/26/2022 11:46:44 - INFO - root -   The nums of the train_dataset features is 7225
11/26/2022 11:46:44 - INFO - root -   The nums of the eval_dataset features is 1870
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
11/26/2022 11:46:49 - INFO - root -   ***** Running train *****
********************************************************************************
/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 2:16  batch_loss: 6.2994 [Training] 2/113 [..............................] - ETA: 1:20  batch_loss: 6.3394 [Training] 3/113 [..............................] - ETA: 1:01  batch_loss: 6.3287 [Training] 4/113 [>.............................] - ETA: 52s  batch_loss: 6.3309 [Training] 5/113 [>.............................] - ETA: 47s  batch_loss: 6.3001 [Training] 6/113 [>.............................] - ETA: 43s  batch_loss: 6.2577 [Training] 7/113 [>.............................] - ETA: 40s  batch_loss: 6.2046 [Training] 8/113 [=>............................] - ETA: 38s  batch_loss: 6.1458 [Training] 9/113 [=>............................] - ETA: 36s  batch_loss: 6.0842 [Training] 10/113 [=>............................] - ETA: 35s  batch_loss: 6.0180 [Training] 11/113 [=>............................] - ETA: 33s  batch_loss: 5.9536 [Training] 12/113 [==>...........................] - ETA: 32s  batch_loss: 5.8814 [Training] 13/113 [==>...........................] - ETA: 31s  batch_loss: 5.8062 [Training] 14/113 [==>...........................] - ETA: 31s  batch_loss: 5.7274 [Training] 15/113 [==>...........................] - ETA: 30s  batch_loss: 5.6497 [Training] 16/113 [===>..........................] - ETA: 29s  batch_loss: 5.5721 [Training] 17/113 [===>..........................] - ETA: 28s  batch_loss: 5.4835 [Training] 18/113 [===>..........................] - ETA: 28s  batch_loss: 5.4044 [Training] 19/113 [====>.........................] - ETA: 27s  batch_loss: 5.3187 [Training] 20/113 [====>.........................] - ETA: 27s  batch_loss: 5.2385 [Training] 21/113 [====>.........................] - ETA: 26s  batch_loss: 5.1411 [Training] 22/113 [====>.........................] - ETA: 26s  batch_loss: 5.0555 [Training] 23/113 [=====>........................] - ETA: 25s  batch_loss: 4.9753 [Training] 24/113 [=====>........................] - ETA: 25s  batch_loss: 4.8752 [Training] 25/113 [=====>........................] - ETA: 25s  batch_loss: 4.7845 [Training] 26/113 [=====>........................] - ETA: 24s  batch_loss: 4.7062 [Training] 27/113 [======>.......................] - ETA: 24s  batch_loss: 4.6443 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 4.5744 [Training] 29/113 [======>.......................] - ETA: 23s  batch_loss: 4.5011 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 4.4302 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 4.3675 [Training] 32/113 [=======>......................] - ETA: 22s  batch_loss: 4.3026 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 4.2449 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 4.1932 [Training] 35/113 [========>.....................] - ETA: 21s  batch_loss: 4.1429 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 4.1065 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 4.0763 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 4.0286 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 3.9834 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 3.9445 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 3.9065 [Training] 42/113 [==========>...................] - ETA: 19s  batch_loss: 3.8736 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 3.8405 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 3.8178 [Training] 45/113 [==========>...................] - ETA: 18s  batch_loss: 3.7889 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 3.7526 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 3.7193 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 3.6914 [Training] 49/113 [============>.................] - ETA: 17s  batch_loss: 3.6622 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 3.6253 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 3.6009 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 3.5736 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 3.5498 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 3.5307 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 3.5074 [Training] 56/113 [=============>................] - ETA: 15s  batch_loss: 3.4822 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 3.4581 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 3.4340 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 3.4090 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 3.3932 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 3.3722 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 3.3520 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 3.3302 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 3.3103 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 3.2911 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 3.2767 [Training] 67/113 [================>.............] - ETA: 12s  batch_loss: 3.2600 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 3.2429 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 3.2253 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 3.2100 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 3.1949 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 3.1780 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 3.1669 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 3.1541 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 3.1399 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 3.1292 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 3.1149 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 3.1044 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 3.0905 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 3.0760 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 3.0617 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 3.0493 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 3.0355 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 3.0218 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 3.0142 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 3.0024 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 2.9969 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 2.9890 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 2.9811 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 2.9693 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 2.9621 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 2.9539 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 2.9411 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 2.9326 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 2.9240 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 2.9155 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 2.9076 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 2.9000 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 2.8892 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 2.8804 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 2.8733 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 2.8647 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 2.8548 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 2.8456 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 2.8368 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 2.8311 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 2.8221 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 2.8128 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 2.8050 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 2.7999 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 2.7902 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 2.7837 [Training] 113/113 [==============================] 256.5ms/step  batch_loss: 2.7793 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:47:25 - INFO - root -   The F1-score is 0.021463666925265062
11/26/2022 11:47:25 - INFO - root -   the best eval f1 is 0.0215, saving model !!
11/26/2022 11:47:27 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:35  batch_loss: 1.8612 [Training] 2/113 [..............................] - ETA: 59s  batch_loss: 1.9952 [Training] 3/113 [..............................] - ETA: 48s  batch_loss: 1.9882 [Training] 4/113 [>.............................] - ETA: 42s  batch_loss: 1.9221 [Training] 5/113 [>.............................] - ETA: 38s  batch_loss: 1.9256 [Training] 6/113 [>.............................] - ETA: 36s  batch_loss: 1.9144 [Training] 7/113 [>.............................] - ETA: 34s  batch_loss: 1.8946 [Training] 8/113 [=>............................] - ETA: 33s  batch_loss: 1.8823 [Training] 9/113 [=>............................] - ETA: 32s  batch_loss: 1.8868 [Training] 10/113 [=>............................] - ETA: 31s  batch_loss: 1.8627 [Training] 11/113 [=>............................] - ETA: 30s  batch_loss: 1.8567 [Training] 12/113 [==>...........................] - ETA: 29s  batch_loss: 1.8520 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 1.8533 [Training] 14/113 [==>...........................] - ETA: 28s  batch_loss: 1.8496 [Training] 15/113 [==>...........................] - ETA: 27s  batch_loss: 1.8528 [Training] 16/113 [===>..........................] - ETA: 27s  batch_loss: 1.8466 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 1.8320 [Training] 18/113 [===>..........................] - ETA: 26s  batch_loss: 1.8496 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 1.8508 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 1.8523 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 1.8625 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 1.8636 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 1.8613 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 1.8556 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 1.8450 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 1.8445 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 1.8397 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 1.8401 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 1.8392 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 1.8327 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 1.8354 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 1.8307 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 1.8382 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 1.8350 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 1.8344 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 1.8277 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 1.8337 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 1.8284 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 1.8222 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 1.8201 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 1.8233 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 1.8277 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 1.8231 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 1.8266 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 1.8218 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 1.8216 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 1.8180 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 1.8215 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 1.8190 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 1.8180 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 1.8177 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 1.8117 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 1.8087 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 1.8078 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 1.8038 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 1.8030 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 1.8043 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 1.7970 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 1.7985 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 1.7934 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 1.7900 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 1.7864 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 1.7836 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 1.7789 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 1.7762 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 1.7702 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 1.7693 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 1.7664 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 1.7658 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 1.7625 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 1.7606 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 1.7574 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 1.7545 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 1.7500 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 1.7464 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 1.7439 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 1.7397 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 1.7351 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 1.7314 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 1.7283 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 1.7267 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 1.7227 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 1.7179 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 1.7164 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 1.7154 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 1.7156 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 1.7148 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 1.7118 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 1.7091 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 1.7063 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 1.7063 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 1.7026 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 1.7015 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 1.6996 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 1.6955 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 1.6936 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 1.6922 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 1.6897 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 1.6890 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 1.6875 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 1.6859 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 1.6845 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 1.6813 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 1.6801 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 1.6772 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 1.6762 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 1.6749 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 1.6743 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 1.6754 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 1.6759 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 1.6743 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 1.6723 [Training] 113/113 [==============================] 255.9ms/step  batch_loss: 1.6696 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:48:03 - INFO - root -   The F1-score is 0.1136919315403423
11/26/2022 11:48:03 - INFO - root -   the best eval f1 is 0.1137, saving model !!
11/26/2022 11:48:05 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:38  batch_loss: 1.3167 [Training] 2/113 [..............................] - ETA: 1:03  batch_loss: 1.4190 [Training] 3/113 [..............................] - ETA: 50s  batch_loss: 1.3700 [Training] 4/113 [>.............................] - ETA: 44s  batch_loss: 1.3994 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 1.4134 [Training] 6/113 [>.............................] - ETA: 37s  batch_loss: 1.4182 [Training] 7/113 [>.............................] - ETA: 36s  batch_loss: 1.4031 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 1.3968 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 1.4051 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 1.3833 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 1.3710 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 1.3637 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 1.3621 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 1.3700 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 1.3646 [Training] 16/113 [===>..........................] - ETA: 28s  batch_loss: 1.3508 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 1.3475 [Training] 18/113 [===>..........................] - ETA: 27s  batch_loss: 1.3495 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 1.3515 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 1.3494 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 1.3472 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 1.3510 [Training] 23/113 [=====>........................] - ETA: 25s  batch_loss: 1.3421 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 1.3352 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 1.3415 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 1.3528 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 1.3576 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 1.3558 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 1.3613 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 1.3619 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 1.3552 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 1.3536 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 1.3525 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 1.3524 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 1.3503 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 1.3548 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 1.3546 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 1.3523 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 1.3515 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 1.3506 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 1.3508 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 1.3505 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 1.3503 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 1.3484 [Training] 45/113 [==========>...................] - ETA: 18s  batch_loss: 1.3495 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 1.3447 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 1.3489 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 1.3517 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 1.3549 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 1.3550 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 1.3565 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 1.3585 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 1.3564 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 1.3563 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 1.3557 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 1.3556 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 1.3555 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 1.3527 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 1.3509 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 1.3534 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 1.3529 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 1.3544 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 1.3548 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 1.3527 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 1.3517 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 1.3534 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 1.3500 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 1.3496 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 1.3471 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 1.3446 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 1.3435 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 1.3435 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 1.3416 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 1.3396 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 1.3373 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 1.3360 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 1.3352 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 1.3347 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 1.3336 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 1.3317 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 1.3309 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 1.3281 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 1.3268 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 1.3249 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 1.3230 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 1.3209 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 1.3196 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 1.3165 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 1.3155 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 1.3128 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 1.3116 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 1.3110 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 1.3102 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 1.3110 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 1.3104 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 1.3099 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 1.3075 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 1.3056 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 1.3048 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 1.3040 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 1.3032 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 1.3017 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 1.3019 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 1.3008 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 1.2995 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 1.2985 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 1.2978 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 1.2961 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 1.2944 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 1.2937 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 1.2926 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 1.2915 [Training] 113/113 [==============================] 257.2ms/step  batch_loss: 1.2905 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:48:41 - INFO - root -   The F1-score is 0.21267696267696268
11/26/2022 11:48:41 - INFO - root -   the best eval f1 is 0.2127, saving model !!
11/26/2022 11:48:43 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:38  batch_loss: 1.0953 [Training] 2/113 [..............................] - ETA: 1:03  batch_loss: 1.0913 [Training] 3/113 [..............................] - ETA: 51s  batch_loss: 1.0927 [Training] 4/113 [>.............................] - ETA: 45s  batch_loss: 1.1121 [Training] 5/113 [>.............................] - ETA: 41s  batch_loss: 1.1257 [Training] 6/113 [>.............................] - ETA: 38s  batch_loss: 1.1184 [Training] 7/113 [>.............................] - ETA: 36s  batch_loss: 1.1125 [Training] 8/113 [=>............................] - ETA: 35s  batch_loss: 1.1018 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 1.1235 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 1.1024 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 1.1084 [Training] 12/113 [==>...........................] - ETA: 31s  batch_loss: 1.1181 [Training] 13/113 [==>...........................] - ETA: 30s  batch_loss: 1.1190 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 1.1137 [Training] 15/113 [==>...........................] - ETA: 29s  batch_loss: 1.1129 [Training] 16/113 [===>..........................] - ETA: 28s  batch_loss: 1.1137 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 1.1176 [Training] 18/113 [===>..........................] - ETA: 27s  batch_loss: 1.1203 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 1.1201 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 1.1228 [Training] 21/113 [====>.........................] - ETA: 26s  batch_loss: 1.1066 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 1.1044 [Training] 23/113 [=====>........................] - ETA: 25s  batch_loss: 1.1097 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 1.1081 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 1.1054 [Training] 26/113 [=====>........................] - ETA: 24s  batch_loss: 1.1017 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 1.1076 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 1.1095 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 1.1047 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 1.1037 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 1.1081 [Training] 32/113 [=======>......................] - ETA: 22s  batch_loss: 1.1105 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 1.1097 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 1.1042 [Training] 35/113 [========>.....................] - ETA: 21s  batch_loss: 1.1041 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 1.1066 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 1.1062 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 1.1098 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 1.1077 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 1.1033 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 1.1001 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 1.1016 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 1.1021 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 1.1065 [Training] 45/113 [==========>...................] - ETA: 18s  batch_loss: 1.1049 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 1.1074 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 1.1060 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 1.1060 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 1.1025 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 1.0990 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 1.1008 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 1.0971 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 1.0996 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 1.0987 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 1.0969 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 1.0978 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 1.0985 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 1.0995 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 1.0983 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 1.0959 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 1.0946 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 1.0928 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 1.0925 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 1.0906 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 1.0876 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 1.0875 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 1.0872 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 1.0870 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 1.0843 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 1.0863 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 1.0860 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 1.0853 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 1.0828 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 1.0828 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 1.0859 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 1.0853 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 1.0829 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 1.0813 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 1.0793 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 1.0786 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 1.0763 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 1.0756 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 1.0751 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 1.0740 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 1.0721 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 1.0714 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 1.0704 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 1.0683 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 1.0672 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 1.0652 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 1.0653 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 1.0681 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 1.0665 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 1.0678 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 1.0672 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 1.0681 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 1.0683 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 1.0670 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 1.0667 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 1.0661 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 1.0649 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 1.0632 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 1.0638 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 1.0624 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 1.0615 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 1.0603 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 1.0589 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 1.0583 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 1.0582 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 1.0580 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 1.0562 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 1.0566 [Training] 113/113 [==============================] 257.2ms/step  batch_loss: 1.0562 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:49:19 - INFO - root -   The F1-score is 0.3390899534464634
11/26/2022 11:49:19 - INFO - root -   the best eval f1 is 0.3391, saving model !!
11/26/2022 11:49:21 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:43  batch_loss: 1.0717 [Training] 2/113 [..............................] - ETA: 1:02  batch_loss: 1.1481 [Training] 3/113 [..............................] - ETA: 50s  batch_loss: 1.1308 [Training] 4/113 [>.............................] - ETA: 44s  batch_loss: 1.0500 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 1.0174 [Training] 6/113 [>.............................] - ETA: 37s  batch_loss: 1.0052 [Training] 7/113 [>.............................] - ETA: 35s  batch_loss: 0.9966 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.9936 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.9760 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.9902 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.9976 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.9882 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.9822 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.9729 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.9758 [Training] 16/113 [===>..........................] - ETA: 27s  batch_loss: 0.9828 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.9797 [Training] 18/113 [===>..........................] - ETA: 26s  batch_loss: 0.9701 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.9746 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.9729 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.9678 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.9681 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.9681 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.9674 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.9655 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.9647 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.9624 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.9563 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.9518 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.9493 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.9466 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.9426 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.9446 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.9416 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.9370 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.9349 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.9326 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.9307 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.9341 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.9320 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.9295 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.9294 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.9279 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.9276 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.9272 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.9258 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.9224 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.9235 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.9221 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.9198 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.9177 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.9169 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.9151 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.9145 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.9124 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.9118 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.9095 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.9067 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.9064 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.9044 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.9059 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.9075 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.9059 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.9057 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.9074 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.9078 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.9062 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.9048 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.9047 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.9031 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.9021 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.9023 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.9030 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.9035 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.9027 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.9013 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.9001 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.8989 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.8978 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.8981 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.8988 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.8986 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.9000 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.8993 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.8986 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.8969 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.8957 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.8945 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.8936 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.8927 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.8923 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.8921 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.8930 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.8920 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.8926 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.8926 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.8931 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.8920 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.8915 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.8901 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.8902 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.8892 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.8893 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.8890 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.8873 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.8864 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.8865 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.8861 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.8862 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.8867 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.8864 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.8846 [Training] 113/113 [==============================] 256.8ms/step  batch_loss: 0.8852 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:49:57 - INFO - root -   The F1-score is 0.44844290657439445
11/26/2022 11:49:57 - INFO - root -   the best eval f1 is 0.4484, saving model !!
11/26/2022 11:49:59 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:38  batch_loss: 0.7375 [Training] 2/113 [..............................] - ETA: 1:02  batch_loss: 0.7448 [Training] 3/113 [..............................] - ETA: 50s  batch_loss: 0.7622 [Training] 4/113 [>.............................] - ETA: 43s  batch_loss: 0.7586 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 0.7518 [Training] 6/113 [>.............................] - ETA: 37s  batch_loss: 0.7569 [Training] 7/113 [>.............................] - ETA: 35s  batch_loss: 0.7534 [Training] 8/113 [=>............................] - ETA: 33s  batch_loss: 0.7669 [Training] 9/113 [=>............................] - ETA: 32s  batch_loss: 0.7587 [Training] 10/113 [=>............................] - ETA: 31s  batch_loss: 0.7585 [Training] 11/113 [=>............................] - ETA: 30s  batch_loss: 0.7636 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.7627 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.7567 [Training] 14/113 [==>...........................] - ETA: 28s  batch_loss: 0.7578 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.7570 [Training] 16/113 [===>..........................] - ETA: 27s  batch_loss: 0.7630 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.7627 [Training] 18/113 [===>..........................] - ETA: 26s  batch_loss: 0.7630 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.7603 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.7607 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.7586 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.7581 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.7598 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.7638 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.7634 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.7575 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.7592 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.7624 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.7642 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.7666 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.7680 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.7669 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.7672 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.7703 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.7705 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.7704 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.7713 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.7737 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.7740 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.7742 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.7711 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.7756 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.7727 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.7735 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.7754 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.7767 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.7752 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.7766 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.7756 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.7769 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.7772 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 0.7767 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.7757 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.7780 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.7768 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.7760 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.7754 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.7752 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.7734 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.7724 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.7761 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.7765 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.7753 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.7750 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.7741 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.7740 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.7729 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.7734 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.7729 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.7721 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.7725 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.7733 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.7723 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.7713 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.7725 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.7722 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.7719 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.7698 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.7690 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.7679 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.7672 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.7669 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.7650 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.7633 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.7630 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.7624 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.7617 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.7623 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.7610 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.7606 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.7602 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.7607 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.7598 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.7590 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.7578 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.7579 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.7586 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.7589 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.7588 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.7594 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.7590 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.7580 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.7577 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.7576 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.7578 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.7561 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.7555 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.7553 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.7545 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.7543 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.7534 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.7527 [Training] 113/113 [==============================] 256.9ms/step  batch_loss: 0.7517 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:50:34 - INFO - root -   The F1-score is 0.49061032863849763
11/26/2022 11:50:34 - INFO - root -   the best eval f1 is 0.4906, saving model !!
11/26/2022 11:50:37 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:35  batch_loss: 0.5748 [Training] 2/113 [..............................] - ETA: 1:02  batch_loss: 0.6562 [Training] 3/113 [..............................] - ETA: 50s  batch_loss: 0.6714 [Training] 4/113 [>.............................] - ETA: 44s  batch_loss: 0.6719 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 0.6793 [Training] 6/113 [>.............................] - ETA: 38s  batch_loss: 0.6740 [Training] 7/113 [>.............................] - ETA: 36s  batch_loss: 0.6785 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.6813 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.6942 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.6948 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.6886 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.6866 [Training] 13/113 [==>...........................] - ETA: 30s  batch_loss: 0.6816 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.6853 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.6804 [Training] 16/113 [===>..........................] - ETA: 28s  batch_loss: 0.6782 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.6751 [Training] 18/113 [===>..........................] - ETA: 27s  batch_loss: 0.6729 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.6718 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.6713 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.6738 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.6747 [Training] 23/113 [=====>........................] - ETA: 25s  batch_loss: 0.6778 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.6775 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.6790 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.6778 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.6747 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.6742 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.6747 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.6769 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.6760 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.6751 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.6736 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.6699 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.6708 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.6682 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.6678 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 0.6664 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.6651 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.6642 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.6632 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.6611 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.6601 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.6590 [Training] 45/113 [==========>...................] - ETA: 18s  batch_loss: 0.6602 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.6611 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.6599 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.6603 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.6613 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.6607 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.6605 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 0.6585 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.6589 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.6584 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.6574 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.6572 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.6565 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.6578 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.6572 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.6580 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.6591 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.6585 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.6578 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.6571 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.6571 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.6572 [Training] 67/113 [================>.............] - ETA: 12s  batch_loss: 0.6570 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.6562 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.6556 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.6562 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.6568 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.6551 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.6541 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.6545 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.6537 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.6524 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.6524 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.6520 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.6520 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.6525 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.6521 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.6520 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.6507 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.6512 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.6500 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.6502 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.6493 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.6487 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.6495 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.6497 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.6500 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.6502 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.6504 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.6510 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.6506 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.6508 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.6506 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.6508 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.6506 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.6502 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.6501 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.6502 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.6494 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.6487 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.6482 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.6490 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.6484 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.6470 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.6473 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.6469 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.6463 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.6460 [Training] 113/113 [==============================] 257.3ms/step  batch_loss: 0.6463 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:51:13 - INFO - root -   The F1-score is 0.5132486889318245
11/26/2022 11:51:13 - INFO - root -   the best eval f1 is 0.5132, saving model !!
11/26/2022 11:51:15 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:34  batch_loss: 0.5215 [Training] 2/113 [..............................] - ETA: 1:01  batch_loss: 0.5036 [Training] 3/113 [..............................] - ETA: 50s  batch_loss: 0.5280 [Training] 4/113 [>.............................] - ETA: 44s  batch_loss: 0.5322 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 0.5447 [Training] 6/113 [>.............................] - ETA: 38s  batch_loss: 0.5459 [Training] 7/113 [>.............................] - ETA: 36s  batch_loss: 0.5432 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.5526 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.5635 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.5666 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.5724 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.5721 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.5716 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.5771 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.5766 [Training] 16/113 [===>..........................] - ETA: 28s  batch_loss: 0.5726 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.5782 [Training] 18/113 [===>..........................] - ETA: 27s  batch_loss: 0.5774 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.5814 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.5812 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.5815 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.5817 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.5802 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.5797 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.5775 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.5771 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.5764 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.5786 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.5830 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.5824 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.5812 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.5813 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.5793 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.5763 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.5764 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.5776 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.5762 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 0.5780 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.5796 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.5772 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.5773 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.5776 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.5783 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.5767 [Training] 45/113 [==========>...................] - ETA: 18s  batch_loss: 0.5763 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.5771 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.5762 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.5767 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.5771 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.5748 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.5730 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 0.5730 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.5720 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.5719 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.5714 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.5728 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.5732 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.5737 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.5737 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.5734 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.5730 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.5727 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.5715 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.5716 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.5710 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.5712 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.5714 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.5716 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.5723 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.5712 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.5727 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.5742 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.5748 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.5747 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.5732 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.5724 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.5720 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.5721 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.5725 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.5728 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.5724 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.5716 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.5715 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.5709 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.5707 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.5699 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.5691 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.5685 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.5679 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.5665 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.5659 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.5659 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.5652 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.5643 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.5635 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.5644 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.5636 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.5626 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.5625 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.5632 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.5625 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.5633 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.5622 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.5618 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.5618 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.5617 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.5614 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.5609 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.5624 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.5624 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.5612 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.5608 [Training] 113/113 [==============================] 257.4ms/step  batch_loss: 0.5611 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:51:51 - INFO - root -   The F1-score is 0.5364715295372716
11/26/2022 11:51:51 - INFO - root -   the best eval f1 is 0.5365, saving model !!
11/26/2022 11:51:53 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:39  batch_loss: 0.5396 [Training] 2/113 [..............................] - ETA: 1:03  batch_loss: 0.5083 [Training] 3/113 [..............................] - ETA: 51s  batch_loss: 0.5177 [Training] 4/113 [>.............................] - ETA: 45s  batch_loss: 0.5055 [Training] 5/113 [>.............................] - ETA: 41s  batch_loss: 0.5120 [Training] 6/113 [>.............................] - ETA: 38s  batch_loss: 0.5098 [Training] 7/113 [>.............................] - ETA: 36s  batch_loss: 0.5157 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.5108 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.5154 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.5107 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.5153 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.5085 [Training] 13/113 [==>...........................] - ETA: 30s  batch_loss: 0.5079 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.5076 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.5029 [Training] 16/113 [===>..........................] - ETA: 28s  batch_loss: 0.5042 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.5069 [Training] 18/113 [===>..........................] - ETA: 27s  batch_loss: 0.5054 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.5019 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.5051 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.5002 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.5019 [Training] 23/113 [=====>........................] - ETA: 25s  batch_loss: 0.4970 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.4951 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.4972 [Training] 26/113 [=====>........................] - ETA: 24s  batch_loss: 0.4968 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.5036 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.5070 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.5048 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.5032 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.5039 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.5028 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.5017 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.5022 [Training] 35/113 [========>.....................] - ETA: 21s  batch_loss: 0.5011 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.5045 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.5028 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 0.5021 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.5028 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.5010 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.5003 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.5015 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.5007 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.5007 [Training] 45/113 [==========>...................] - ETA: 18s  batch_loss: 0.5036 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.5039 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.5041 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.5020 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.5014 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.5013 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.5005 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 0.5025 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.5024 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.5027 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.5040 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.5051 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.5039 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.5027 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.5018 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.5012 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.5014 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.5005 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.5003 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.4998 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.4995 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.4993 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.4999 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.5001 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.5003 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.4988 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.4991 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.4979 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.4972 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.4966 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.4965 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.4960 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.4956 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.4956 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.4960 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.4972 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.4962 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.4952 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.4951 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.4948 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.4936 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.4931 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.4929 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.4930 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.4927 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.4932 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.4925 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.4930 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.4928 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.4928 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.4927 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.4921 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.4923 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.4915 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.4910 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.4908 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.4916 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.4918 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.4919 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.4919 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.4929 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.4926 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.4930 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.4925 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.4934 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.4931 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.4943 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.4949 [Training] 113/113 [==============================] 258.2ms/step  batch_loss: 0.4940 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:52:29 - INFO - root -   The F1-score is 0.5695658018550881
11/26/2022 11:52:29 - INFO - root -   the best eval f1 is 0.5696, saving model !!
11/26/2022 11:52:31 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:39  batch_loss: 0.4405 [Training] 2/113 [..............................] - ETA: 1:03  batch_loss: 0.5060 [Training] 3/113 [..............................] - ETA: 51s  batch_loss: 0.4723 [Training] 4/113 [>.............................] - ETA: 44s  batch_loss: 0.4685 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 0.4817 [Training] 6/113 [>.............................] - ETA: 38s  batch_loss: 0.4745 [Training] 7/113 [>.............................] - ETA: 36s  batch_loss: 0.4591 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.4468 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.4383 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.4392 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.4413 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.4376 [Training] 13/113 [==>...........................] - ETA: 30s  batch_loss: 0.4383 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.4360 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.4354 [Training] 16/113 [===>..........................] - ETA: 28s  batch_loss: 0.4426 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.4470 [Training] 18/113 [===>..........................] - ETA: 27s  batch_loss: 0.4480 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.4482 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.4460 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.4456 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.4454 [Training] 23/113 [=====>........................] - ETA: 25s  batch_loss: 0.4428 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.4439 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.4450 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.4450 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.4438 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.4406 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.4403 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.4426 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.4426 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.4409 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.4419 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.4400 [Training] 35/113 [========>.....................] - ETA: 21s  batch_loss: 0.4413 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.4400 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.4402 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 0.4395 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.4406 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.4395 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.4394 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.4418 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.4422 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.4436 [Training] 45/113 [==========>...................] - ETA: 18s  batch_loss: 0.4438 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.4437 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.4436 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.4448 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.4433 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.4414 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.4413 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 0.4428 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.4447 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.4454 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.4463 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.4451 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.4454 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.4456 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.4455 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.4455 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.4457 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.4449 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.4441 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.4435 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.4426 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.4413 [Training] 67/113 [================>.............] - ETA: 12s  batch_loss: 0.4405 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.4413 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.4406 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.4406 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.4405 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.4420 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.4414 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.4404 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.4409 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.4411 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.4405 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.4417 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.4423 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.4430 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.4415 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.4412 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.4414 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.4425 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.4423 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.4419 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.4409 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.4407 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.4408 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.4401 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.4396 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.4403 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.4400 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.4400 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.4405 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.4399 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.4397 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.4399 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.4395 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.4398 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.4393 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.4388 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.4386 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.4381 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.4380 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.4377 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.4383 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.4382 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.4378 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.4373 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.4371 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.4372 [Training] 113/113 [==============================] 257.4ms/step  batch_loss: 0.4377 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:53:07 - INFO - root -   The F1-score is 0.5862384123253688
11/26/2022 11:53:07 - INFO - root -   the best eval f1 is 0.5862, saving model !!
11/26/2022 11:53:09 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:36  batch_loss: 0.3860 [Training] 2/113 [..............................] - ETA: 1:01  batch_loss: 0.4468 [Training] 3/113 [..............................] - ETA: 50s  batch_loss: 0.4407 [Training] 4/113 [>.............................] - ETA: 44s  batch_loss: 0.4297 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 0.4135 [Training] 6/113 [>.............................] - ETA: 38s  batch_loss: 0.4102 [Training] 7/113 [>.............................] - ETA: 36s  batch_loss: 0.4062 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.3960 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.4046 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.4043 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.4181 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.4169 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.4139 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.4168 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.4138 [Training] 16/113 [===>..........................] - ETA: 28s  batch_loss: 0.4135 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.4106 [Training] 18/113 [===>..........................] - ETA: 26s  batch_loss: 0.4106 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.4060 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.4069 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.4107 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.4101 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.4113 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.4116 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.4124 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.4073 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.4084 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.4094 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.4063 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.4060 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.4080 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.4072 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.4068 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.4064 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.4072 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.4059 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.4066 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 0.4075 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.4071 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.4075 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.4068 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.4061 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.4055 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.4037 [Training] 45/113 [==========>...................] - ETA: 18s  batch_loss: 0.4047 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.4046 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.4041 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.4051 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.4037 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.4040 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.4039 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 0.4023 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.4013 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.3998 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.3996 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.4003 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.3999 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.3997 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.3987 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.3974 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.3968 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.3970 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.3964 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.3961 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.3953 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.3957 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.3944 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.3941 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.3939 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.3943 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.3934 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.3930 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.3922 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.3925 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.3914 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.3915 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.3934 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.3926 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.3928 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.3918 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.3920 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.3915 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.3917 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.3921 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.3918 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.3914 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.3910 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.3909 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.3905 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.3908 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.3900 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.3907 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.3900 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.3895 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.3904 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.3909 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.3909 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.3910 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.3903 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.3906 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.3901 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.3894 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.3894 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.3899 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.3902 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.3908 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.3910 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.3908 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.3913 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.3909 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.3908 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.3907 [Training] 113/113 [==============================] 257.2ms/step  batch_loss: 0.3906 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:53:45 - INFO - root -   The F1-score is 0.5945555919973763
11/26/2022 11:53:45 - INFO - root -   the best eval f1 is 0.5946, saving model !!
11/26/2022 11:53:47 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:37  batch_loss: 0.4028 [Training] 2/113 [..............................] - ETA: 1:02  batch_loss: 0.3957 [Training] 3/113 [..............................] - ETA: 50s  batch_loss: 0.3801 [Training] 4/113 [>.............................] - ETA: 44s  batch_loss: 0.4060 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 0.3921 [Training] 6/113 [>.............................] - ETA: 37s  batch_loss: 0.3887 [Training] 7/113 [>.............................] - ETA: 35s  batch_loss: 0.3848 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.3783 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.3822 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.3754 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.3707 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.3742 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.3713 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.3696 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.3688 [Training] 16/113 [===>..........................] - ETA: 28s  batch_loss: 0.3679 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.3654 [Training] 18/113 [===>..........................] - ETA: 27s  batch_loss: 0.3626 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.3647 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.3706 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.3716 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.3712 [Training] 23/113 [=====>........................] - ETA: 25s  batch_loss: 0.3706 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.3699 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.3670 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.3674 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.3688 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.3707 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.3697 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.3674 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.3673 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.3673 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.3669 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.3659 [Training] 35/113 [========>.....................] - ETA: 21s  batch_loss: 0.3651 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.3639 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.3628 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 0.3639 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.3642 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.3627 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.3632 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.3626 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.3608 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.3611 [Training] 45/113 [==========>...................] - ETA: 18s  batch_loss: 0.3612 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.3609 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.3614 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.3608 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.3617 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.3615 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.3598 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 0.3604 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.3598 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.3597 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.3592 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.3580 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.3575 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.3580 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.3582 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.3587 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.3585 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.3571 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.3572 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.3575 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.3575 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.3565 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.3556 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.3554 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.3567 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.3564 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.3555 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.3552 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.3546 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.3552 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.3540 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.3530 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.3526 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.3525 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.3529 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.3530 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.3529 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.3521 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.3531 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.3530 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.3534 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.3536 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.3535 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.3542 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.3543 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.3541 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.3546 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.3552 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.3552 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.3548 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.3544 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.3541 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.3545 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.3541 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.3543 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.3543 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.3539 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.3534 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.3537 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.3540 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.3538 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.3536 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.3531 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.3529 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.3525 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.3527 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.3528 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.3527 [Training] 113/113 [==============================] 257.6ms/step  batch_loss: 0.3520 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:54:23 - INFO - root -   The F1-score is 0.6017501151391539
11/26/2022 11:54:23 - INFO - root -   the best eval f1 is 0.6018, saving model !!
11/26/2022 11:54:25 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:35  batch_loss: 0.3570 [Training] 2/113 [..............................] - ETA: 1:01  batch_loss: 0.3406 [Training] 3/113 [..............................] - ETA: 49s  batch_loss: 0.3308 [Training] 4/113 [>.............................] - ETA: 43s  batch_loss: 0.3286 [Training] 5/113 [>.............................] - ETA: 39s  batch_loss: 0.3196 [Training] 6/113 [>.............................] - ETA: 37s  batch_loss: 0.3222 [Training] 7/113 [>.............................] - ETA: 35s  batch_loss: 0.3322 [Training] 8/113 [=>............................] - ETA: 33s  batch_loss: 0.3343 [Training] 9/113 [=>............................] - ETA: 32s  batch_loss: 0.3338 [Training] 10/113 [=>............................] - ETA: 31s  batch_loss: 0.3296 [Training] 11/113 [=>............................] - ETA: 30s  batch_loss: 0.3304 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.3299 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.3315 [Training] 14/113 [==>...........................] - ETA: 28s  batch_loss: 0.3283 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.3288 [Training] 16/113 [===>..........................] - ETA: 27s  batch_loss: 0.3270 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.3266 [Training] 18/113 [===>..........................] - ETA: 26s  batch_loss: 0.3252 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.3265 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.3280 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.3292 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.3279 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.3267 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.3260 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.3250 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.3241 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.3240 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.3233 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.3253 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.3262 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.3258 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.3231 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.3227 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.3219 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.3219 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.3236 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.3243 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.3235 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.3244 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.3238 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.3223 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.3224 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.3240 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.3238 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.3247 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.3244 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.3256 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.3250 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.3263 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.3261 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.3263 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.3265 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.3261 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.3262 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.3263 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.3270 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.3264 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.3263 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.3263 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.3256 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.3256 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.3255 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.3257 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.3257 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.3261 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.3270 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.3272 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.3269 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.3272 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.3277 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.3273 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.3274 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.3272 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.3265 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.3258 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.3252 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.3249 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.3250 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.3255 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.3250 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.3251 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.3246 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.3243 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.3237 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.3235 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.3229 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.3233 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.3226 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.3225 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.3225 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.3227 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.3230 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.3227 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.3230 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.3231 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.3224 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.3224 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.3223 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.3223 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.3228 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.3227 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.3224 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.3223 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.3221 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.3219 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.3219 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.3217 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.3221 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.3222 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.3219 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.3218 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.3214 [Training] 113/113 [==============================] 256.3ms/step  batch_loss: 0.3209 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:55:00 - INFO - root -   The F1-score is 0.6045944558521561
11/26/2022 11:55:00 - INFO - root -   the best eval f1 is 0.6046, saving model !!
11/26/2022 11:55:03 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:37  batch_loss: 0.2692 [Training] 2/113 [..............................] - ETA: 1:02  batch_loss: 0.2897 [Training] 3/113 [..............................] - ETA: 50s  batch_loss: 0.3176 [Training] 4/113 [>.............................] - ETA: 44s  batch_loss: 0.3306 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 0.3362 [Training] 6/113 [>.............................] - ETA: 37s  batch_loss: 0.3308 [Training] 7/113 [>.............................] - ETA: 36s  batch_loss: 0.3295 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.3176 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.3137 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.3175 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.3144 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.3105 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.3091 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.3044 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.3047 [Training] 16/113 [===>..........................] - ETA: 28s  batch_loss: 0.3039 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.3005 [Training] 18/113 [===>..........................] - ETA: 27s  batch_loss: 0.2995 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.3007 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.2988 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.3012 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.3021 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.2999 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.2998 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.3020 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.2989 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.2998 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.3020 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.3004 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.3004 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.3002 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.2995 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.2986 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.2994 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.2997 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.3011 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.3010 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.3032 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.3018 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.3011 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.3002 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.2998 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.3010 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.3006 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.2989 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.3013 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.3013 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.3006 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.3003 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.2993 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.3001 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.2993 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.2985 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.2981 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.2979 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.2976 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.2986 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.2980 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.2977 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.2977 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.2987 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.2985 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.2974 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.2973 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.2977 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.2977 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.2988 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.2995 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.2992 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.2983 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.2976 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.2979 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.2993 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.2987 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.2985 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.2983 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.2977 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.2969 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.2967 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.2962 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.2953 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.2961 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.2954 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.2953 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.2945 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.2941 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.2948 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.2948 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.2949 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.2949 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.2949 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.2953 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.2952 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.2953 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.2951 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.2948 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.2960 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.2961 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.2966 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.2965 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.2959 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.2952 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.2942 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.2940 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.2941 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.2941 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.2943 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.2950 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.2951 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.2953 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.2950 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.2949 [Training] 113/113 [==============================] 257.0ms/step  batch_loss: 0.2951 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:55:39 - INFO - root -   The F1-score is 0.6200345423143351
11/26/2022 11:55:39 - INFO - root -   the best eval f1 is 0.6200, saving model !!
11/26/2022 11:55:41 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:36  batch_loss: 0.2493 [Training] 2/113 [..............................] - ETA: 1:02  batch_loss: 0.2290 [Training] 3/113 [..............................] - ETA: 50s  batch_loss: 0.2500 [Training] 4/113 [>.............................] - ETA: 44s  batch_loss: 0.2668 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 0.2718 [Training] 6/113 [>.............................] - ETA: 37s  batch_loss: 0.2651 [Training] 7/113 [>.............................] - ETA: 35s  batch_loss: 0.2652 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.2658 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.2683 [Training] 10/113 [=>............................] - ETA: 31s  batch_loss: 0.2790 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.2817 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.2825 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.2821 [Training] 14/113 [==>...........................] - ETA: 28s  batch_loss: 0.2801 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.2780 [Training] 16/113 [===>..........................] - ETA: 27s  batch_loss: 0.2799 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.2787 [Training] 18/113 [===>..........................] - ETA: 26s  batch_loss: 0.2768 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.2794 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.2757 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.2751 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.2737 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.2724 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.2704 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.2731 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.2727 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.2725 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.2734 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.2737 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.2731 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.2748 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.2732 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.2726 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.2735 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.2725 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.2713 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.2714 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 0.2705 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.2705 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.2686 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.2681 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.2691 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.2689 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.2685 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.2678 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.2676 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.2684 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.2685 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.2676 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.2685 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.2687 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 0.2697 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.2697 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.2698 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.2695 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.2696 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.2698 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.2697 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.2696 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.2689 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.2705 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.2701 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.2697 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.2705 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.2717 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.2712 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.2712 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.2725 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.2713 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.2712 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.2711 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.2711 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.2709 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.2704 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.2699 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.2707 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.2702 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.2702 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.2706 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.2706 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.2702 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.2706 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.2706 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.2709 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.2711 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.2709 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.2709 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.2711 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.2708 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.2704 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.2705 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.2700 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.2700 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.2707 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.2707 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.2707 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.2707 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.2715 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.2710 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.2716 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.2718 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.2717 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.2716 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.2712 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.2714 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.2711 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.2709 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.2706 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.2708 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.2707 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.2709 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.2712 [Training] 113/113 [==============================] 257.0ms/step  batch_loss: 0.2721 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:56:17 - INFO - root -   The F1-score is 0.626633932718466
11/26/2022 11:56:17 - INFO - root -   the best eval f1 is 0.6266, saving model !!
11/26/2022 11:56:19 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:38  batch_loss: 0.2952 [Training] 2/113 [..............................] - ETA: 1:03  batch_loss: 0.2676 [Training] 3/113 [..............................] - ETA: 51s  batch_loss: 0.2566 [Training] 4/113 [>.............................] - ETA: 45s  batch_loss: 0.2499 [Training] 5/113 [>.............................] - ETA: 41s  batch_loss: 0.2377 [Training] 6/113 [>.............................] - ETA: 38s  batch_loss: 0.2358 [Training] 7/113 [>.............................] - ETA: 36s  batch_loss: 0.2337 [Training] 8/113 [=>............................] - ETA: 35s  batch_loss: 0.2317 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.2381 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.2396 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.2413 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.2406 [Training] 13/113 [==>...........................] - ETA: 30s  batch_loss: 0.2456 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.2457 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.2437 [Training] 16/113 [===>..........................] - ETA: 28s  batch_loss: 0.2447 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.2428 [Training] 18/113 [===>..........................] - ETA: 27s  batch_loss: 0.2427 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.2432 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.2422 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.2424 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.2408 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.2419 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.2424 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.2430 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.2430 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.2433 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.2443 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.2440 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.2438 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.2441 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.2447 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.2440 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.2440 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.2453 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.2449 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.2449 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.2450 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.2455 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.2452 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.2444 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.2439 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.2431 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.2435 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.2439 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.2430 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.2431 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.2433 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.2435 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.2433 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.2436 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.2440 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.2433 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.2446 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.2472 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.2472 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.2474 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.2463 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.2458 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.2457 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.2463 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.2471 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.2471 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.2477 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.2471 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.2472 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.2480 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.2478 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.2481 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.2476 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.2474 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.2476 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.2472 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.2473 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.2477 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.2477 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.2476 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.2476 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.2483 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.2489 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.2488 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.2485 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.2497 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.2497 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.2500 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.2500 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.2494 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.2494 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.2497 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.2499 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.2496 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.2495 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.2502 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.2498 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.2502 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.2504 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.2501 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.2498 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.2499 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.2496 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.2494 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.2497 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.2502 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.2508 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.2506 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.2503 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.2505 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.2508 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.2510 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.2506 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.2508 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.2508 [Training] 113/113 [==============================] 257.7ms/step  batch_loss: 0.2508 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:56:58 - INFO - root -   The F1-score is 0.6329944244014432
11/26/2022 11:56:58 - INFO - root -   the best eval f1 is 0.6330, saving model !!
11/26/2022 11:57:00 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:35  batch_loss: 0.1940 [Training] 2/113 [..............................] - ETA: 1:01  batch_loss: 0.2089 [Training] 3/113 [..............................] - ETA: 49s  batch_loss: 0.1975 [Training] 4/113 [>.............................] - ETA: 43s  batch_loss: 0.2227 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 0.2174 [Training] 6/113 [>.............................] - ETA: 37s  batch_loss: 0.2381 [Training] 7/113 [>.............................] - ETA: 35s  batch_loss: 0.2400 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.2348 [Training] 9/113 [=>............................] - ETA: 32s  batch_loss: 0.2351 [Training] 10/113 [=>............................] - ETA: 31s  batch_loss: 0.2371 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.2407 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.2441 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.2401 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.2401 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.2366 [Training] 16/113 [===>..........................] - ETA: 27s  batch_loss: 0.2356 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.2340 [Training] 18/113 [===>..........................] - ETA: 26s  batch_loss: 0.2346 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.2360 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.2383 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.2354 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.2329 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.2315 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.2315 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.2324 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.2326 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.2318 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.2322 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.2303 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.2326 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.2326 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.2321 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.2319 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.2306 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.2297 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.2326 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.2340 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 0.2343 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.2356 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.2347 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.2346 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.2341 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.2337 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.2347 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.2343 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.2337 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.2349 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.2356 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.2366 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.2367 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.2358 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 0.2362 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.2363 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.2367 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.2362 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.2367 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.2364 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.2354 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.2354 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.2360 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.2354 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.2356 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.2355 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.2352 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.2342 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.2346 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.2342 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.2341 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.2336 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.2336 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.2335 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.2335 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.2334 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.2336 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.2340 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.2333 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.2343 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.2338 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.2338 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.2339 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.2332 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.2335 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.2328 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.2326 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.2326 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.2321 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.2323 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.2319 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.2322 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.2321 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.2322 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.2322 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.2321 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.2327 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.2327 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.2330 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.2332 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.2335 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.2344 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.2340 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.2336 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.2333 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.2336 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.2335 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.2331 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.2329 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.2333 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.2333 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.2332 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.2331 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.2337 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.2334 [Training] 113/113 [==============================] 257.1ms/step  batch_loss: 0.2336 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:57:35 - INFO - root -   The F1-score is 0.636545795359059
11/26/2022 11:57:35 - INFO - root -   the best eval f1 is 0.6365, saving model !!
11/26/2022 11:57:38 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:37  batch_loss: 0.2010 [Training] 2/113 [..............................] - ETA: 1:01  batch_loss: 0.2034 [Training] 3/113 [..............................] - ETA: 49s  batch_loss: 0.1984 [Training] 4/113 [>.............................] - ETA: 43s  batch_loss: 0.1955 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 0.1931 [Training] 6/113 [>.............................] - ETA: 37s  batch_loss: 0.2025 [Training] 7/113 [>.............................] - ETA: 35s  batch_loss: 0.2044 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.2061 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.2091 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.2114 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.2141 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.2170 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.2177 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.2125 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.2133 [Training] 16/113 [===>..........................] - ETA: 27s  batch_loss: 0.2128 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.2121 [Training] 18/113 [===>..........................] - ETA: 27s  batch_loss: 0.2110 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.2106 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.2103 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.2088 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.2070 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.2064 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.2069 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.2076 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.2090 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.2106 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.2100 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.2098 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.2093 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.2103 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.2098 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.2091 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.2098 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.2097 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.2097 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.2100 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 0.2104 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.2107 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.2124 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.2119 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.2119 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.2121 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.2121 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.2129 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.2134 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.2127 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.2123 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.2133 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.2126 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.2133 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.2142 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.2148 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.2147 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.2147 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.2146 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.2152 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.2148 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.2148 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.2160 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.2155 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.2155 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.2158 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.2150 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.2146 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.2144 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.2150 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.2153 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.2149 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.2153 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.2152 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.2160 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.2162 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.2157 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.2153 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.2157 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.2152 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.2143 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.2140 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.2133 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.2143 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.2147 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.2147 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.2149 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.2149 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.2150 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.2158 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.2168 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.2170 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.2171 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.2170 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.2172 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.2168 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.2168 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.2170 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.2168 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.2177 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.2176 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.2175 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.2175 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.2173 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.2179 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.2179 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.2178 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.2179 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.2182 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.2179 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.2175 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.2176 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.2174 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.2175 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.2176 [Training] 113/113 [==============================] 256.1ms/step  batch_loss: 0.2173 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:58:13 - INFO - root -   The F1-score is 0.6311300639658848
11/26/2022 11:58:13 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:39  batch_loss: 0.1991 [Training] 2/113 [..............................] - ETA: 1:03  batch_loss: 0.1797 [Training] 3/113 [..............................] - ETA: 51s  batch_loss: 0.1953 [Training] 4/113 [>.............................] - ETA: 44s  batch_loss: 0.1913 [Training] 5/113 [>.............................] - ETA: 41s  batch_loss: 0.1924 [Training] 6/113 [>.............................] - ETA: 38s  batch_loss: 0.1911 [Training] 7/113 [>.............................] - ETA: 36s  batch_loss: 0.1897 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.1914 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.1965 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.1942 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.1954 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.1940 [Training] 13/113 [==>...........................] - ETA: 30s  batch_loss: 0.1916 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.1908 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.1882 [Training] 16/113 [===>..........................] - ETA: 28s  batch_loss: 0.1870 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.1871 [Training] 18/113 [===>..........................] - ETA: 27s  batch_loss: 0.1884 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.1883 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.1878 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.1866 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.1868 [Training] 23/113 [=====>........................] - ETA: 25s  batch_loss: 0.1875 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.1872 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.1885 [Training] 26/113 [=====>........................] - ETA: 24s  batch_loss: 0.1894 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.1904 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.1904 [Training] 29/113 [======>.......................] - ETA: 23s  batch_loss: 0.1903 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.1897 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.1904 [Training] 32/113 [=======>......................] - ETA: 22s  batch_loss: 0.1912 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1927 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.1940 [Training] 35/113 [========>.....................] - ETA: 21s  batch_loss: 0.1946 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1949 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.1943 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 0.1946 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1958 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1968 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.1966 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1972 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1976 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.1984 [Training] 45/113 [==========>...................] - ETA: 18s  batch_loss: 0.1983 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1981 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1974 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.1968 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1966 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1961 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1969 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 0.1980 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1977 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1987 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.1995 [Training] 56/113 [=============>................] - ETA: 15s  batch_loss: 0.2012 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.2013 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.2013 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.2014 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.2009 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.2027 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.2028 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.2025 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.2025 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.2024 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.2024 [Training] 67/113 [================>.............] - ETA: 12s  batch_loss: 0.2022 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.2032 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.2027 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.2022 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.2023 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.2019 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.2016 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.2016 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.2015 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.2024 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.2022 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.2020 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.2019 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.2021 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.2020 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.2018 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.2022 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.2026 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.2029 [Training] 86/113 [=====================>........] - ETA: 7s  batch_loss: 0.2027 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.2026 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.2031 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.2027 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.2026 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.2032 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.2031 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.2033 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.2030 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.2033 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.2038 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.2039 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.2043 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.2045 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.2042 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.2040 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.2035 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.2032 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.2030 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.2027 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.2030 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.2030 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.2031 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.2030 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.2029 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.2033 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.2039 [Training] 113/113 [==============================] 258.0ms/step  batch_loss: 0.2039 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:58:49 - INFO - root -   The F1-score is 0.6438668490652075
11/26/2022 11:58:49 - INFO - root -   the best eval f1 is 0.6439, saving model !!
11/26/2022 11:58:51 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:34  batch_loss: 0.1850 [Training] 2/113 [..............................] - ETA: 1:00  batch_loss: 0.1785 [Training] 3/113 [..............................] - ETA: 49s  batch_loss: 0.1741 [Training] 4/113 [>.............................] - ETA: 43s  batch_loss: 0.1660 [Training] 5/113 [>.............................] - ETA: 39s  batch_loss: 0.1728 [Training] 6/113 [>.............................] - ETA: 37s  batch_loss: 0.1784 [Training] 7/113 [>.............................] - ETA: 35s  batch_loss: 0.1831 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.1833 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.1831 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.1822 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.1803 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.1806 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.1800 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.1820 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.1857 [Training] 16/113 [===>..........................] - ETA: 27s  batch_loss: 0.1870 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.1875 [Training] 18/113 [===>..........................] - ETA: 27s  batch_loss: 0.1898 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.1913 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.1912 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.1899 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.1898 [Training] 23/113 [=====>........................] - ETA: 25s  batch_loss: 0.1892 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.1889 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.1895 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.1900 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.1897 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.1898 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1918 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.1906 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.1904 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1903 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1918 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.1918 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1909 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1909 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.1908 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 0.1909 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1908 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1911 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.1912 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1927 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1932 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.1931 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.1931 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1926 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1934 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.1937 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1933 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1928 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1934 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 0.1936 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1942 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1937 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.1938 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.1940 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1933 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1927 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.1925 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1933 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1934 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1934 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.1935 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1933 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1940 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1945 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.1943 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1947 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1945 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1946 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1940 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1938 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1940 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.1939 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1944 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1943 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1944 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.1949 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1950 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1951 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1949 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.1948 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1951 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1953 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1953 [Training] 86/113 [=====================>........] - ETA: 7s  batch_loss: 0.1952 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1950 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1954 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1955 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1955 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1957 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1958 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1962 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1962 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1963 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1959 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1955 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1953 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1948 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1946 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1945 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1937 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1933 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1929 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1930 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1930 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1924 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1923 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1921 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1922 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1923 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1926 [Training] 113/113 [==============================] 258.0ms/step  batch_loss: 0.1925 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 11:59:27 - INFO - root -   The F1-score is 0.6477444846888377
11/26/2022 11:59:27 - INFO - root -   the best eval f1 is 0.6477, saving model !!
11/26/2022 11:59:29 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:34  batch_loss: 0.2049 [Training] 2/113 [..............................] - ETA: 1:01  batch_loss: 0.1971 [Training] 3/113 [..............................] - ETA: 49s  batch_loss: 0.1880 [Training] 4/113 [>.............................] - ETA: 43s  batch_loss: 0.1813 [Training] 5/113 [>.............................] - ETA: 39s  batch_loss: 0.1692 [Training] 6/113 [>.............................] - ETA: 37s  batch_loss: 0.1670 [Training] 7/113 [>.............................] - ETA: 35s  batch_loss: 0.1648 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.1635 [Training] 9/113 [=>............................] - ETA: 32s  batch_loss: 0.1687 [Training] 10/113 [=>............................] - ETA: 31s  batch_loss: 0.1691 [Training] 11/113 [=>............................] - ETA: 30s  batch_loss: 0.1712 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.1710 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.1714 [Training] 14/113 [==>...........................] - ETA: 28s  batch_loss: 0.1708 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.1709 [Training] 16/113 [===>..........................] - ETA: 27s  batch_loss: 0.1768 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.1753 [Training] 18/113 [===>..........................] - ETA: 26s  batch_loss: 0.1755 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.1756 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.1760 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.1761 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.1756 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.1774 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.1776 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.1758 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.1745 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.1751 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.1757 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1755 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.1771 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.1776 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1801 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1798 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.1805 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1809 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1817 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.1806 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.1797 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1795 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1799 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.1796 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1793 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1792 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.1794 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.1801 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1799 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1790 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.1795 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1795 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1797 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1802 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.1804 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1796 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1794 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.1795 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.1791 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1792 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1786 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.1785 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1811 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1811 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1814 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.1811 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1812 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1810 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1811 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.1811 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1810 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1807 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1804 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1802 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1806 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1813 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.1807 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1809 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1809 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1809 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.1814 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1808 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1807 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1810 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.1810 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1811 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1818 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1820 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1823 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1822 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1824 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1824 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1826 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1828 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1830 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1835 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1836 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1836 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1835 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1833 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1838 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1831 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1833 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1829 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1827 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1825 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1828 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1825 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1828 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1830 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1827 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1832 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1835 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1833 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1833 [Training] 113/113 [==============================] 256.8ms/step  batch_loss: 0.1836 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 12:00:04 - INFO - root -   The F1-score is 0.6439215686274509
11/26/2022 12:00:04 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:37  batch_loss: 0.1791 [Training] 2/113 [..............................] - ETA: 1:02  batch_loss: 0.1833 [Training] 3/113 [..............................] - ETA: 50s  batch_loss: 0.1803 [Training] 4/113 [>.............................] - ETA: 44s  batch_loss: 0.1685 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 0.1664 [Training] 6/113 [>.............................] - ETA: 38s  batch_loss: 0.1687 [Training] 7/113 [>.............................] - ETA: 36s  batch_loss: 0.1763 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.1718 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.1691 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.1671 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.1697 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.1687 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.1685 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.1671 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.1680 [Training] 16/113 [===>..........................] - ETA: 28s  batch_loss: 0.1685 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.1688 [Training] 18/113 [===>..........................] - ETA: 27s  batch_loss: 0.1686 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.1696 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.1698 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.1735 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.1726 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.1723 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.1742 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.1730 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.1714 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.1706 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.1698 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1684 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.1689 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.1674 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1685 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1686 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.1686 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1680 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1678 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.1685 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.1681 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1684 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1674 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.1670 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1673 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1667 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.1671 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.1666 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1662 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1676 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.1673 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1673 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1669 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1663 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 0.1659 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1651 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1651 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.1645 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.1646 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1650 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1646 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.1651 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1667 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1666 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1666 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.1670 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1670 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1678 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1680 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.1686 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1681 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1690 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1688 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1688 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1688 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1684 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.1688 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1691 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1691 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1691 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.1684 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1690 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1687 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1687 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.1690 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1695 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1693 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1696 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1696 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1704 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1705 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1707 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1710 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1705 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1713 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1713 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1708 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1709 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1714 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1710 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1712 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1710 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1710 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1714 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1717 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1717 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1719 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1722 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1723 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1723 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1725 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1727 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1730 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1734 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1734 [Training] 113/113 [==============================] 256.9ms/step  batch_loss: 0.1734 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 12:00:40 - INFO - root -   The F1-score is 0.6470471529660736
11/26/2022 12:00:40 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:35  batch_loss: 0.1203 [Training] 2/113 [..............................] - ETA: 1:01  batch_loss: 0.1209 [Training] 3/113 [..............................] - ETA: 50s  batch_loss: 0.1388 [Training] 4/113 [>.............................] - ETA: 44s  batch_loss: 0.1508 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 0.1560 [Training] 6/113 [>.............................] - ETA: 38s  batch_loss: 0.1566 [Training] 7/113 [>.............................] - ETA: 36s  batch_loss: 0.1555 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.1585 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.1597 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.1580 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.1595 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.1591 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.1643 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.1658 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.1654 [Training] 16/113 [===>..........................] - ETA: 28s  batch_loss: 0.1646 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.1636 [Training] 18/113 [===>..........................] - ETA: 27s  batch_loss: 0.1609 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.1605 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.1606 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.1595 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.1596 [Training] 23/113 [=====>........................] - ETA: 25s  batch_loss: 0.1593 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.1618 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.1601 [Training] 26/113 [=====>........................] - ETA: 24s  batch_loss: 0.1611 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.1633 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.1637 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1633 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.1636 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.1651 [Training] 32/113 [=======>......................] - ETA: 22s  batch_loss: 0.1646 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1643 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.1632 [Training] 35/113 [========>.....................] - ETA: 21s  batch_loss: 0.1627 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1616 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.1618 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 0.1614 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1625 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1636 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.1643 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1645 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1644 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.1639 [Training] 45/113 [==========>...................] - ETA: 18s  batch_loss: 0.1633 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1634 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1630 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.1626 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1635 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1635 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1635 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 0.1633 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1622 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1616 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.1620 [Training] 56/113 [=============>................] - ETA: 15s  batch_loss: 0.1616 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1612 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1613 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.1613 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1607 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1604 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1606 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.1602 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1606 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1601 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1598 [Training] 67/113 [================>.............] - ETA: 12s  batch_loss: 0.1595 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1599 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1601 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1603 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1612 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1616 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1615 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.1614 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1617 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1620 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1624 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.1627 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1633 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1632 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1634 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.1638 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1639 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1642 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1638 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1637 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1636 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1642 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1649 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1650 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1649 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1643 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1638 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1644 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1647 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1643 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1641 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1645 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1647 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1647 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1645 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1642 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1642 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1639 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1638 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1637 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1638 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1638 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1638 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1636 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1638 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1634 [Training] 113/113 [==============================] 257.4ms/step  batch_loss: 0.1635 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 12:01:16 - INFO - root -   The F1-score is 0.6409670042469781
11/26/2022 12:01:16 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:35  batch_loss: 0.1609 [Training] 2/113 [..............................] - ETA: 1:01  batch_loss: 0.1524 [Training] 3/113 [..............................] - ETA: 49s  batch_loss: 0.1470 [Training] 4/113 [>.............................] - ETA: 44s  batch_loss: 0.1500 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 0.1419 [Training] 6/113 [>.............................] - ETA: 37s  batch_loss: 0.1409 [Training] 7/113 [>.............................] - ETA: 35s  batch_loss: 0.1431 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.1465 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.1458 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.1449 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.1437 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.1476 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.1524 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.1529 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.1547 [Training] 16/113 [===>..........................] - ETA: 28s  batch_loss: 0.1556 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.1551 [Training] 18/113 [===>..........................] - ETA: 27s  batch_loss: 0.1552 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.1571 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.1562 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.1559 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.1549 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.1533 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.1538 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.1537 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.1551 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.1564 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.1570 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1556 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.1556 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.1539 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1546 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1537 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.1542 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1539 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1537 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.1524 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.1516 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1524 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1546 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.1575 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1573 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1573 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.1580 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.1582 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1592 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1590 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.1595 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1591 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1588 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1592 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 0.1588 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1584 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1582 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.1582 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.1584 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1577 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1579 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.1579 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1576 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1577 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1577 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.1569 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1572 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1571 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1571 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.1567 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1565 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1566 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1564 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1567 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1563 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1571 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.1573 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1573 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1573 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1569 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.1570 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1572 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1573 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1575 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.1570 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1563 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1568 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1570 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1568 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1570 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1568 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1568 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1573 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1577 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1578 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1580 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1575 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1572 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1570 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1569 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1570 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1566 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1564 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1572 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1568 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1567 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1562 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1567 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1564 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1564 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1566 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1566 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1563 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1563 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1571 [Training] 113/113 [==============================] 257.5ms/step  batch_loss: 0.1570 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 12:01:51 - INFO - root -   The F1-score is 0.6494629290018338
11/26/2022 12:01:51 - INFO - root -   the best eval f1 is 0.6495, saving model !!
11/26/2022 12:01:53 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:35  batch_loss: 0.1364 [Training] 2/113 [..............................] - ETA: 1:01  batch_loss: 0.1312 [Training] 3/113 [..............................] - ETA: 49s  batch_loss: 0.1481 [Training] 4/113 [>.............................] - ETA: 43s  batch_loss: 0.1381 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 0.1511 [Training] 6/113 [>.............................] - ETA: 37s  batch_loss: 0.1539 [Training] 7/113 [>.............................] - ETA: 35s  batch_loss: 0.1543 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.1519 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.1507 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.1454 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.1465 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.1433 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.1454 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.1424 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.1424 [Training] 16/113 [===>..........................] - ETA: 28s  batch_loss: 0.1434 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.1456 [Training] 18/113 [===>..........................] - ETA: 27s  batch_loss: 0.1468 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.1476 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.1464 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.1448 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.1463 [Training] 23/113 [=====>........................] - ETA: 25s  batch_loss: 0.1447 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.1439 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.1427 [Training] 26/113 [=====>........................] - ETA: 24s  batch_loss: 0.1430 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.1436 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.1433 [Training] 29/113 [======>.......................] - ETA: 23s  batch_loss: 0.1432 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.1430 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.1429 [Training] 32/113 [=======>......................] - ETA: 22s  batch_loss: 0.1429 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1436 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.1449 [Training] 35/113 [========>.....................] - ETA: 21s  batch_loss: 0.1449 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1451 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.1454 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 0.1460 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1457 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1450 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.1450 [Training] 42/113 [==========>...................] - ETA: 19s  batch_loss: 0.1446 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1454 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.1467 [Training] 45/113 [==========>...................] - ETA: 18s  batch_loss: 0.1464 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1465 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1467 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.1471 [Training] 49/113 [============>.................] - ETA: 17s  batch_loss: 0.1475 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1491 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1493 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 0.1493 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1487 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1485 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.1493 [Training] 56/113 [=============>................] - ETA: 15s  batch_loss: 0.1497 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1495 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1494 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.1499 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1498 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1508 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1514 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.1512 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1510 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1503 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1501 [Training] 67/113 [================>.............] - ETA: 12s  batch_loss: 0.1506 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1500 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1497 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1498 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1496 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1497 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1495 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.1492 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1491 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1493 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1498 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.1501 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1505 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1505 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1507 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.1507 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1506 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1508 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1507 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1509 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1508 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1511 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1513 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1514 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1511 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1513 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1510 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1507 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1506 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1506 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1507 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1510 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1508 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1509 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1510 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1512 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1510 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1516 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1516 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1514 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1515 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1512 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1510 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1513 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1510 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1511 [Training] 113/113 [==============================] 257.6ms/step  batch_loss: 0.1510 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 12:02:29 - INFO - root -   The F1-score is 0.6491468915473623
11/26/2022 12:02:29 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:38  batch_loss: 0.1322 [Training] 2/113 [..............................] - ETA: 1:02  batch_loss: 0.1153 [Training] 3/113 [..............................] - ETA: 50s  batch_loss: 0.1474 [Training] 4/113 [>.............................] - ETA: 44s  batch_loss: 0.1398 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 0.1373 [Training] 6/113 [>.............................] - ETA: 37s  batch_loss: 0.1344 [Training] 7/113 [>.............................] - ETA: 35s  batch_loss: 0.1376 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.1384 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.1375 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.1390 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.1367 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.1360 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.1335 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.1350 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.1349 [Training] 16/113 [===>..........................] - ETA: 28s  batch_loss: 0.1340 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.1334 [Training] 18/113 [===>..........................] - ETA: 27s  batch_loss: 0.1333 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.1332 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.1335 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.1336 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.1357 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.1355 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.1364 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.1372 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.1369 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.1382 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.1378 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1378 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.1374 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.1373 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1373 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1368 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.1364 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1354 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1352 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.1355 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 0.1359 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1363 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1376 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.1371 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1378 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1380 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.1384 [Training] 45/113 [==========>...................] - ETA: 18s  batch_loss: 0.1389 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1395 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1400 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.1403 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1403 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1404 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1404 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 0.1403 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1400 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1398 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.1397 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.1395 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1407 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1409 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.1406 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1411 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1416 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1415 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.1416 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1420 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1419 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1418 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.1417 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1422 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1422 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1429 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1428 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1424 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1428 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.1423 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1419 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1419 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1418 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.1418 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1416 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1414 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1412 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.1411 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1409 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1411 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1411 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1418 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1418 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1417 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1414 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1424 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1424 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1423 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1425 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1420 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1421 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1421 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1430 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1430 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1432 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1432 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1439 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1443 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1442 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1442 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1449 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1449 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1447 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1452 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1452 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1447 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1446 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1443 [Training] 113/113 [==============================] 257.1ms/step  batch_loss: 0.1444 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 12:03:05 - INFO - root -   The F1-score is 0.6439954657598187
11/26/2022 12:03:05 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:37  batch_loss: 0.1402 [Training] 2/113 [..............................] - ETA: 1:02  batch_loss: 0.1110 [Training] 3/113 [..............................] - ETA: 50s  batch_loss: 0.1168 [Training] 4/113 [>.............................] - ETA: 44s  batch_loss: 0.1406 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 0.1332 [Training] 6/113 [>.............................] - ETA: 38s  batch_loss: 0.1292 [Training] 7/113 [>.............................] - ETA: 36s  batch_loss: 0.1298 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.1278 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.1258 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.1313 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.1289 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.1312 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.1304 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.1307 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.1295 [Training] 16/113 [===>..........................] - ETA: 28s  batch_loss: 0.1286 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.1295 [Training] 18/113 [===>..........................] - ETA: 27s  batch_loss: 0.1296 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.1293 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.1298 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.1300 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.1310 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.1315 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.1315 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.1312 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.1307 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.1294 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.1297 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1303 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.1312 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.1309 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1302 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1303 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.1311 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1319 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1324 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.1325 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 0.1321 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1327 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1326 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.1333 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1331 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1323 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.1320 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.1338 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1333 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1342 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.1346 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1350 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1356 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1351 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.1355 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1365 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1361 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.1364 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.1372 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1369 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1371 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.1367 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1364 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1365 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1363 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.1367 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1363 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1363 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1367 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.1364 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1362 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1364 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1365 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1368 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1369 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1375 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.1379 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1378 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1379 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1379 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.1382 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1384 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1385 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1384 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.1381 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1386 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1388 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1395 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1397 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1394 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1404 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1399 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1402 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1401 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1399 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1400 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1400 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1402 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1406 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1401 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1402 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1401 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1400 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1398 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1397 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1401 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1401 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1404 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1404 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1401 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1401 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1401 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1399 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1395 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1395 [Training] 113/113 [==============================] 256.6ms/step  batch_loss: 0.1398 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 12:03:40 - INFO - root -   The F1-score is 0.6445182724252492
11/26/2022 12:03:40 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:26  batch_loss: 0.1439 [Training] 2/113 [..............................] - ETA: 56s  batch_loss: 0.1421 [Training] 3/113 [..............................] - ETA: 46s  batch_loss: 0.1353 [Training] 4/113 [>.............................] - ETA: 41s  batch_loss: 0.1418 [Training] 5/113 [>.............................] - ETA: 38s  batch_loss: 0.1432 [Training] 6/113 [>.............................] - ETA: 36s  batch_loss: 0.1381 [Training] 7/113 [>.............................] - ETA: 34s  batch_loss: 0.1389 [Training] 8/113 [=>............................] - ETA: 33s  batch_loss: 0.1384 [Training] 9/113 [=>............................] - ETA: 32s  batch_loss: 0.1364 [Training] 10/113 [=>............................] - ETA: 31s  batch_loss: 0.1323 [Training] 11/113 [=>............................] - ETA: 30s  batch_loss: 0.1296 [Training] 12/113 [==>...........................] - ETA: 29s  batch_loss: 0.1330 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.1316 [Training] 14/113 [==>...........................] - ETA: 28s  batch_loss: 0.1321 [Training] 15/113 [==>...........................] - ETA: 27s  batch_loss: 0.1326 [Training] 16/113 [===>..........................] - ETA: 27s  batch_loss: 0.1359 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.1362 [Training] 18/113 [===>..........................] - ETA: 26s  batch_loss: 0.1354 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.1359 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.1361 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.1350 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.1361 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.1347 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.1351 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.1342 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.1339 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.1350 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.1369 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1361 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.1378 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.1379 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1394 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1395 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.1395 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1405 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1400 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.1395 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.1392 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1390 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1381 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.1378 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1380 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1395 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.1393 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.1389 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1383 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1374 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.1372 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1379 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1373 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1375 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.1371 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1363 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1369 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.1365 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.1357 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1362 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1360 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.1362 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1354 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1350 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1355 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.1354 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1354 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1358 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1355 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.1354 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1363 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1362 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1361 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1354 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1355 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1355 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.1352 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1352 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1353 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1352 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.1347 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1347 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1342 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1340 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.1339 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1337 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1337 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1335 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1337 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1336 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1335 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1338 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1333 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1331 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1333 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1334 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1334 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1335 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1334 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1339 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1340 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1340 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1340 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1337 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1338 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1336 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1335 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1338 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1337 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1336 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1339 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1337 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1339 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1338 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1338 [Training] 113/113 [==============================] 256.1ms/step  batch_loss: 0.1339 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 12:04:16 - INFO - root -   The F1-score is 0.6473746235432762
11/26/2022 12:04:16 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:36  batch_loss: 0.1297 [Training] 2/113 [..............................] - ETA: 1:02  batch_loss: 0.1483 [Training] 3/113 [..............................] - ETA: 50s  batch_loss: 0.1311 [Training] 4/113 [>.............................] - ETA: 44s  batch_loss: 0.1276 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 0.1216 [Training] 6/113 [>.............................] - ETA: 37s  batch_loss: 0.1186 [Training] 7/113 [>.............................] - ETA: 35s  batch_loss: 0.1186 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.1182 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.1160 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.1145 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.1154 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.1142 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.1200 [Training] 14/113 [==>...........................] - ETA: 29s  batch_loss: 0.1193 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.1186 [Training] 16/113 [===>..........................] - ETA: 27s  batch_loss: 0.1191 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.1175 [Training] 18/113 [===>..........................] - ETA: 26s  batch_loss: 0.1186 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.1211 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.1221 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.1220 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.1233 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.1223 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.1224 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.1237 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.1247 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.1246 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.1242 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1245 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.1246 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.1238 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1247 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1243 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.1232 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1228 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1217 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.1214 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 0.1210 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1212 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1204 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.1207 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1216 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1223 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.1227 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.1231 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1236 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1242 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.1237 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1244 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1246 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1251 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 0.1250 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1249 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1246 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.1247 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.1245 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1246 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1252 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.1250 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1254 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1252 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1257 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.1257 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1259 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1254 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1252 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.1257 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1259 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1260 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1256 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1253 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1254 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1252 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.1257 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1263 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1268 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1266 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.1264 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1261 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1264 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1273 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.1282 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1279 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1280 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1283 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1283 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1279 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1279 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1278 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1278 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1280 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1279 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1282 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1279 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1280 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1280 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1279 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1280 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1282 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1284 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1281 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1278 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1282 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1282 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1282 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1282 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1282 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1282 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1296 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1299 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1299 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1298 [Training] 113/113 [==============================] 255.6ms/step  batch_loss: 0.1304 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 12:04:51 - INFO - root -   The F1-score is 0.6495168451292765
11/26/2022 12:04:51 - INFO - root -   the best eval f1 is 0.6495, saving model !!
11/26/2022 12:05:32 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:38  batch_loss: 0.1253 [Training] 2/113 [..............................] - ETA: 1:02  batch_loss: 0.1246 [Training] 3/113 [..............................] - ETA: 50s  batch_loss: 0.1197 [Training] 4/113 [>.............................] - ETA: 44s  batch_loss: 0.1159 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 0.1199 [Training] 6/113 [>.............................] - ETA: 37s  batch_loss: 0.1217 [Training] 7/113 [>.............................] - ETA: 35s  batch_loss: 0.1179 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.1211 [Training] 9/113 [=>............................] - ETA: 33s  batch_loss: 0.1204 [Training] 10/113 [=>............................] - ETA: 32s  batch_loss: 0.1185 [Training] 11/113 [=>............................] - ETA: 31s  batch_loss: 0.1187 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.1179 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.1189 [Training] 14/113 [==>...........................] - ETA: 28s  batch_loss: 0.1205 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.1175 [Training] 16/113 [===>..........................] - ETA: 27s  batch_loss: 0.1189 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.1178 [Training] 18/113 [===>..........................] - ETA: 26s  batch_loss: 0.1191 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.1176 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.1169 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.1183 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.1173 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.1178 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.1210 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.1212 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.1225 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.1215 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.1206 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1221 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.1227 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.1233 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1226 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1234 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.1237 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1249 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1248 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.1248 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.1244 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1241 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1243 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.1248 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1243 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1237 [Training] 44/113 [==========>...................] - ETA: 17s  batch_loss: 0.1236 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.1232 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1233 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1235 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.1235 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1236 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1234 [Training] 51/113 [============>.................] - ETA: 15s  batch_loss: 0.1238 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.1234 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1237 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1236 [Training] 55/113 [=============>................] - ETA: 14s  batch_loss: 0.1235 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.1242 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1244 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1241 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 0.1261 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1255 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1253 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1254 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.1253 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1248 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1253 [Training] 66/113 [================>.............] - ETA: 11s  batch_loss: 0.1260 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.1256 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1261 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1255 [Training] 70/113 [=================>............] - ETA: 10s  batch_loss: 0.1252 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1249 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1247 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1249 [Training] 74/113 [==================>...........] - ETA: 9s  batch_loss: 0.1243 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1247 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1262 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1262 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 0.1265 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1268 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1268 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1271 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.1265 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1263 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1258 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1258 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1254 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1253 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1255 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1253 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1253 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1248 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1249 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1248 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1247 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1247 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1246 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1248 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1245 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1243 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1245 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1242 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1243 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1240 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1241 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1244 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1244 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1247 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1245 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1244 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1244 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1247 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1247 [Training] 113/113 [==============================] 252.5ms/step  batch_loss: 0.1249 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11/26/2022 12:06:07 - INFO - root -   The F1-score is 0.6582616236408512
11/26/2022 12:06:07 - INFO - root -   the best eval f1 is 0.6583, saving model !!
