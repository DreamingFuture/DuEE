nohup: ignoring input
*********** data_prepare *************
********** train start *************
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 5, 
    "event_type": "role", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 1e-05, 
    "max_len": 150, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 30.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 48, 
    "per_gpu_train_batch_size": 64, 
    "seed": 66, 
    "stride": 100, 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/7225 [00:00<?, ?it/s]tokenizing...:   4%|▍         | 278/7225 [00:00<00:02, 2775.72it/s]tokenizing...:   8%|▊         | 559/7225 [00:00<00:02, 2795.75it/s]tokenizing...:  12%|█▏        | 839/7225 [00:00<00:02, 2720.45it/s]tokenizing...:  15%|█▌        | 1112/7225 [00:00<00:02, 2723.88it/s]tokenizing...:  19%|█▉        | 1393/7225 [00:00<00:02, 2749.23it/s]tokenizing...:  23%|██▎       | 1686/7225 [00:00<00:01, 2808.22it/s]tokenizing...:  27%|██▋       | 1967/7225 [00:00<00:01, 2787.31it/s]tokenizing...:  31%|███       | 2246/7225 [00:00<00:01, 2761.60it/s]tokenizing...:  35%|███▍      | 2523/7225 [00:00<00:01, 2681.22it/s]tokenizing...:  39%|███▉      | 2816/7225 [00:01<00:01, 2753.23it/s]tokenizing...:  43%|████▎     | 3109/7225 [00:01<00:01, 2805.24it/s]tokenizing...:  47%|████▋     | 3397/7225 [00:01<00:01, 2827.47it/s]tokenizing...:  51%|█████     | 3681/7225 [00:01<00:01, 2796.84it/s]tokenizing...:  55%|█████▍    | 3963/7225 [00:01<00:01, 2801.67it/s]tokenizing...:  59%|█████▊    | 4244/7225 [00:01<00:01, 2727.07it/s]tokenizing...:  63%|██████▎   | 4518/7225 [00:01<00:01, 2442.99it/s]tokenizing...:  66%|██████▌   | 4768/7225 [00:01<00:01, 2418.40it/s]tokenizing...:  69%|██████▉   | 5014/7225 [00:01<00:00, 2380.11it/s]tokenizing...:  73%|███████▎  | 5255/7225 [00:02<00:00, 2283.58it/s]tokenizing...:  76%|███████▌  | 5486/7225 [00:02<00:00, 2132.43it/s]tokenizing...:  79%|███████▉  | 5702/7225 [00:02<00:00, 1601.73it/s]tokenizing...:  82%|████████▏ | 5952/7225 [00:02<00:00, 1801.21it/s]tokenizing...:  86%|████████▌ | 6224/7225 [00:02<00:00, 2021.31it/s]tokenizing...:  90%|████████▉ | 6501/7225 [00:02<00:00, 2210.83it/s]tokenizing...:  93%|█████████▎| 6740/7225 [00:02<00:00, 2137.38it/s]tokenizing...:  97%|█████████▋| 6979/7225 [00:02<00:00, 2203.96it/s]tokenizing...: 100%|██████████| 7225/7225 [00:02<00:00, 2439.10it/s]
tokenizing...:   0%|          | 0/1870 [00:00<?, ?it/s]tokenizing...:  16%|█▋        | 304/1870 [00:00<00:00, 3033.47it/s]tokenizing...:  33%|███▎      | 608/1870 [00:00<00:00, 2848.29it/s]tokenizing...:  48%|████▊     | 894/1870 [00:00<00:00, 2603.45it/s]tokenizing...:  62%|██████▏   | 1157/1870 [00:00<00:00, 2594.19it/s]tokenizing...:  76%|███████▌  | 1418/1870 [00:00<00:00, 2593.49it/s]tokenizing...:  90%|████████▉ | 1679/1870 [00:00<00:00, 2463.77it/s]tokenizing...: 100%|██████████| 1870/1870 [00:00<00:00, 2541.30it/s]
10/25/2022 04:19:29 - INFO - root -   The nums of the train_dataset features is 7225
10/25/2022 04:19:29 - INFO - root -   The nums of the eval_dataset features is 1870
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
10/25/2022 04:19:41 - INFO - root -   ***** Running train *****
********************************************************************************
/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 3:35  batch_loss: 6.2994 [Training] 2/113 [..............................] - ETA: 2:00  batch_loss: 6.3394 [Training] 3/113 [..............................] - ETA: 1:27  batch_loss: 6.3287 [Training] 4/113 [>.............................] - ETA: 1:12  batch_loss: 6.3309 [Training] 5/113 [>.............................] - ETA: 1:02  batch_loss: 6.3001 [Training] 6/113 [>.............................] - ETA: 56s  batch_loss: 6.2577 [Training] 7/113 [>.............................] - ETA: 51s  batch_loss: 6.2046 [Training] 8/113 [=>............................] - ETA: 47s  batch_loss: 6.1458 [Training] 9/113 [=>............................] - ETA: 44s  batch_loss: 6.0842 [Training] 10/113 [=>............................] - ETA: 42s  batch_loss: 6.0180 [Training] 11/113 [=>............................] - ETA: 40s  batch_loss: 5.9536 [Training] 12/113 [==>...........................] - ETA: 38s  batch_loss: 5.8814 [Training] 13/113 [==>...........................] - ETA: 37s  batch_loss: 5.8062 [Training] 14/113 [==>...........................] - ETA: 36s  batch_loss: 5.7274 [Training] 15/113 [==>...........................] - ETA: 35s  batch_loss: 5.6497 [Training] 16/113 [===>..........................] - ETA: 34s  batch_loss: 5.5721 [Training] 17/113 [===>..........................] - ETA: 33s  batch_loss: 5.4835 [Training] 18/113 [===>..........................] - ETA: 32s  batch_loss: 5.4044 [Training] 19/113 [====>.........................] - ETA: 31s  batch_loss: 5.3187 [Training] 20/113 [====>.........................] - ETA: 30s  batch_loss: 5.2385 [Training] 21/113 [====>.........................] - ETA: 30s  batch_loss: 5.1411 [Training] 22/113 [====>.........................] - ETA: 29s  batch_loss: 5.0555 [Training] 23/113 [=====>........................] - ETA: 29s  batch_loss: 4.9753 [Training] 24/113 [=====>........................] - ETA: 28s  batch_loss: 4.8752 [Training] 25/113 [=====>........................] - ETA: 28s  batch_loss: 4.7845 [Training] 26/113 [=====>........................] - ETA: 27s  batch_loss: 4.7062 [Training] 27/113 [======>.......................] - ETA: 26s  batch_loss: 4.6443 [Training] 28/113 [======>.......................] - ETA: 26s  batch_loss: 4.5744 [Training] 29/113 [======>.......................] - ETA: 25s  batch_loss: 4.5011 [Training] 30/113 [======>.......................] - ETA: 25s  batch_loss: 4.4302 [Training] 31/113 [=======>......................] - ETA: 24s  batch_loss: 4.3675 [Training] 32/113 [=======>......................] - ETA: 24s  batch_loss: 4.3026 [Training] 33/113 [=======>......................] - ETA: 24s  batch_loss: 4.2449 [Training] 34/113 [========>.....................] - ETA: 23s  batch_loss: 4.1932 [Training] 35/113 [========>.....................] - ETA: 23s  batch_loss: 4.1429 [Training] 36/113 [========>.....................] - ETA: 22s  batch_loss: 4.1065 [Training] 37/113 [========>.....................] - ETA: 22s  batch_loss: 4.0763 [Training] 38/113 [=========>....................] - ETA: 21s  batch_loss: 4.0286 [Training] 39/113 [=========>....................] - ETA: 21s  batch_loss: 3.9834 [Training] 40/113 [=========>....................] - ETA: 21s  batch_loss: 3.9445 [Training] 41/113 [=========>....................] - ETA: 20s  batch_loss: 3.9065 [Training] 42/113 [==========>...................] - ETA: 20s  batch_loss: 3.8736 [Training] 43/113 [==========>...................] - ETA: 20s  batch_loss: 3.8405 [Training] 44/113 [==========>...................] - ETA: 19s  batch_loss: 3.8178 [Training] 45/113 [==========>...................] - ETA: 19s  batch_loss: 3.7889 [Training] 46/113 [===========>..................] - ETA: 19s  batch_loss: 3.7526 [Training] 47/113 [===========>..................] - ETA: 18s  batch_loss: 3.7193 [Training] 48/113 [===========>..................] - ETA: 18s  batch_loss: 3.6914 [Training] 49/113 [============>.................] - ETA: 18s  batch_loss: 3.6622 [Training] 50/113 [============>.................] - ETA: 17s  batch_loss: 3.6253 [Training] 51/113 [============>.................] - ETA: 17s  batch_loss: 3.6009 [Training] 52/113 [============>.................] - ETA: 17s  batch_loss: 3.5736 [Training] 53/113 [=============>................] - ETA: 16s  batch_loss: 3.5498 [Training] 54/113 [=============>................] - ETA: 16s  batch_loss: 3.5307 [Training] 55/113 [=============>................] - ETA: 16s  batch_loss: 3.5074 [Training] 56/113 [=============>................] - ETA: 15s  batch_loss: 3.4822 [Training] 57/113 [==============>...............] - ETA: 15s  batch_loss: 3.4581 [Training] 58/113 [==============>...............] - ETA: 15s  batch_loss: 3.4340 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 3.4090 [Training] 60/113 [==============>...............] - ETA: 14s  batch_loss: 3.3932 [Training] 61/113 [===============>..............] - ETA: 14s  batch_loss: 3.3722 [Training] 62/113 [===============>..............] - ETA: 14s  batch_loss: 3.3520 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 3.3302 [Training] 64/113 [===============>..............] - ETA: 13s  batch_loss: 3.3103 [Training] 65/113 [================>.............] - ETA: 13s  batch_loss: 3.2911 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 3.2767 [Training] 67/113 [================>.............] - ETA: 12s  batch_loss: 3.2600 [Training] 68/113 [=================>............] - ETA: 12s  batch_loss: 3.2429 [Training] 69/113 [=================>............] - ETA: 12s  batch_loss: 3.2253 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 3.2100 [Training] 71/113 [=================>............] - ETA: 11s  batch_loss: 3.1949 [Training] 72/113 [==================>...........] - ETA: 11s  batch_loss: 3.1780 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 3.1669 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 3.1541 [Training] 75/113 [==================>...........] - ETA: 10s  batch_loss: 3.1399 [Training] 76/113 [===================>..........] - ETA: 10s  batch_loss: 3.1292 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 3.1149 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 3.1044 [Training] 79/113 [===================>..........] - ETA: 9s  batch_loss: 3.0905 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 3.0760 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 3.0617 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 3.0493 [Training] 83/113 [=====================>........] - ETA: 8s  batch_loss: 3.0355 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 3.0218 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 3.0142 [Training] 86/113 [=====================>........] - ETA: 7s  batch_loss: 3.0024 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 2.9969 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 2.9890 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 2.9811 [Training] 90/113 [======================>.......] - ETA: 6s  batch_loss: 2.9693 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 2.9621 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 2.9539 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 2.9411 [Training] 94/113 [=======================>......] - ETA: 5s  batch_loss: 2.9326 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 2.9240 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 2.9155 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 2.9076 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 2.9000 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 2.8892 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 2.8804 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 2.8733 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 2.8647 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 2.8548 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 2.8456 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 2.8368 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 2.8311 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 2.8221 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 2.8128 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 2.8050 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 2.7999 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 2.7902 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 2.7837 [Training] 113/113 [==============================] 264.6ms/step  batch_loss: 2.7793 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:20:17 - INFO - root -   The F1-score is 0.021463666925265062
10/25/2022 04:20:17 - INFO - root -   the best eval f1 is 0.0215, saving model !!
10/25/2022 04:20:18 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:03  batch_loss: 1.8612 [Training] 2/113 [..............................] - ETA: 43s  batch_loss: 1.9952 [Training] 3/113 [..............................] - ETA: 37s  batch_loss: 1.9882 [Training] 4/113 [>.............................] - ETA: 34s  batch_loss: 1.9221 [Training] 5/113 [>.............................] - ETA: 32s  batch_loss: 1.9256 [Training] 6/113 [>.............................] - ETA: 31s  batch_loss: 1.9144 [Training] 7/113 [>.............................] - ETA: 31s  batch_loss: 1.8946 [Training] 8/113 [=>............................] - ETA: 30s  batch_loss: 1.8823 [Training] 9/113 [=>............................] - ETA: 29s  batch_loss: 1.8868 [Training] 10/113 [=>............................] - ETA: 28s  batch_loss: 1.8627 [Training] 11/113 [=>............................] - ETA: 28s  batch_loss: 1.8567 [Training] 12/113 [==>...........................] - ETA: 27s  batch_loss: 1.8520 [Training] 13/113 [==>...........................] - ETA: 27s  batch_loss: 1.8533 [Training] 14/113 [==>...........................] - ETA: 26s  batch_loss: 1.8496 [Training] 15/113 [==>...........................] - ETA: 26s  batch_loss: 1.8528 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 1.8466 [Training] 17/113 [===>..........................] - ETA: 25s  batch_loss: 1.8320 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 1.8496 [Training] 19/113 [====>.........................] - ETA: 24s  batch_loss: 1.8508 [Training] 20/113 [====>.........................] - ETA: 24s  batch_loss: 1.8523 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 1.8625 [Training] 22/113 [====>.........................] - ETA: 23s  batch_loss: 1.8636 [Training] 23/113 [=====>........................] - ETA: 23s  batch_loss: 1.8613 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 1.8556 [Training] 25/113 [=====>........................] - ETA: 22s  batch_loss: 1.8450 [Training] 26/113 [=====>........................] - ETA: 22s  batch_loss: 1.8445 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 1.8397 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 1.8401 [Training] 29/113 [======>.......................] - ETA: 21s  batch_loss: 1.8392 [Training] 30/113 [======>.......................] - ETA: 21s  batch_loss: 1.8327 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 1.8354 [Training] 32/113 [=======>......................] - ETA: 20s  batch_loss: 1.8307 [Training] 33/113 [=======>......................] - ETA: 20s  batch_loss: 1.8382 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 1.8350 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 1.8344 [Training] 36/113 [========>.....................] - ETA: 19s  batch_loss: 1.8277 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 1.8337 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 1.8284 [Training] 39/113 [=========>....................] - ETA: 18s  batch_loss: 1.8222 [Training] 40/113 [=========>....................] - ETA: 18s  batch_loss: 1.8201 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 1.8233 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 1.8277 [Training] 43/113 [==========>...................] - ETA: 17s  batch_loss: 1.8231 [Training] 44/113 [==========>...................] - ETA: 17s  batch_loss: 1.8266 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 1.8218 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 1.8216 [Training] 47/113 [===========>..................] - ETA: 16s  batch_loss: 1.8180 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 1.8215 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 1.8190 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 1.8180 [Training] 51/113 [============>.................] - ETA: 15s  batch_loss: 1.8177 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 1.8117 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 1.8087 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 1.8078 [Training] 55/113 [=============>................] - ETA: 14s  batch_loss: 1.8038 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 1.8030 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 1.8043 [Training] 58/113 [==============>...............] - ETA: 13s  batch_loss: 1.7970 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 1.7985 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 1.7934 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 1.7900 [Training] 62/113 [===============>..............] - ETA: 12s  batch_loss: 1.7864 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 1.7836 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 1.7789 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 1.7762 [Training] 66/113 [================>.............] - ETA: 11s  batch_loss: 1.7702 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 1.7693 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 1.7664 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 1.7658 [Training] 70/113 [=================>............] - ETA: 10s  batch_loss: 1.7625 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 1.7606 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 1.7574 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 1.7545 [Training] 74/113 [==================>...........] - ETA: 9s  batch_loss: 1.7500 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 1.7464 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 1.7439 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 1.7397 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 1.7351 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 1.7314 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 1.7283 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 1.7267 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 1.7227 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 1.7179 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 1.7164 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 1.7154 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 1.7156 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 1.7148 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 1.7118 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 1.7091 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 1.7063 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 1.7063 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 1.7026 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 1.7015 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 1.6996 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 1.6955 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 1.6936 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 1.6922 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 1.6897 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 1.6890 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 1.6875 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 1.6859 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 1.6845 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 1.6813 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 1.6801 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 1.6772 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 1.6762 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 1.6749 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 1.6743 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 1.6754 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 1.6759 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 1.6743 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 1.6723 [Training] 113/113 [==============================] 254.4ms/step  batch_loss: 1.6696 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:20:53 - INFO - root -   The F1-score is 0.1136919315403423
10/25/2022 04:20:53 - INFO - root -   the best eval f1 is 0.1137, saving model !!
10/25/2022 04:20:55 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:08  batch_loss: 1.3167 [Training] 2/113 [..............................] - ETA: 48s  batch_loss: 1.4190 [Training] 3/113 [..............................] - ETA: 41s  batch_loss: 1.3700 [Training] 4/113 [>.............................] - ETA: 37s  batch_loss: 1.3994 [Training] 5/113 [>.............................] - ETA: 34s  batch_loss: 1.4134 [Training] 6/113 [>.............................] - ETA: 33s  batch_loss: 1.4182 [Training] 7/113 [>.............................] - ETA: 32s  batch_loss: 1.4031 [Training] 8/113 [=>............................] - ETA: 31s  batch_loss: 1.3968 [Training] 9/113 [=>............................] - ETA: 30s  batch_loss: 1.4051 [Training] 10/113 [=>............................] - ETA: 29s  batch_loss: 1.3833 [Training] 11/113 [=>............................] - ETA: 28s  batch_loss: 1.3710 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 1.3637 [Training] 13/113 [==>...........................] - ETA: 27s  batch_loss: 1.3621 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 1.3700 [Training] 15/113 [==>...........................] - ETA: 26s  batch_loss: 1.3646 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 1.3508 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 1.3475 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 1.3495 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 1.3515 [Training] 20/113 [====>.........................] - ETA: 24s  batch_loss: 1.3494 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 1.3472 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 1.3510 [Training] 23/113 [=====>........................] - ETA: 23s  batch_loss: 1.3421 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 1.3352 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 1.3415 [Training] 26/113 [=====>........................] - ETA: 22s  batch_loss: 1.3528 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 1.3576 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 1.3558 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 1.3613 [Training] 30/113 [======>.......................] - ETA: 21s  batch_loss: 1.3619 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 1.3552 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 1.3536 [Training] 33/113 [=======>......................] - ETA: 20s  batch_loss: 1.3525 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 1.3524 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 1.3503 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 1.3548 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 1.3546 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 1.3523 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 1.3515 [Training] 40/113 [=========>....................] - ETA: 18s  batch_loss: 1.3506 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 1.3508 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 1.3505 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 1.3503 [Training] 44/113 [==========>...................] - ETA: 17s  batch_loss: 1.3484 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 1.3495 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 1.3447 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 1.3489 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 1.3517 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 1.3549 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 1.3550 [Training] 51/113 [============>.................] - ETA: 15s  batch_loss: 1.3565 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 1.3585 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 1.3564 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 1.3563 [Training] 55/113 [=============>................] - ETA: 14s  batch_loss: 1.3557 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 1.3556 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 1.3555 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 1.3527 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 1.3509 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 1.3534 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 1.3529 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 1.3544 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 1.3548 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 1.3527 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 1.3517 [Training] 66/113 [================>.............] - ETA: 11s  batch_loss: 1.3534 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 1.3500 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 1.3496 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 1.3471 [Training] 70/113 [=================>............] - ETA: 10s  batch_loss: 1.3446 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 1.3435 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 1.3435 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 1.3416 [Training] 74/113 [==================>...........] - ETA: 9s  batch_loss: 1.3396 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 1.3373 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 1.3360 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 1.3352 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 1.3347 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 1.3336 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 1.3317 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 1.3309 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 1.3281 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 1.3268 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 1.3249 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 1.3230 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 1.3209 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 1.3196 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 1.3165 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 1.3155 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 1.3128 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 1.3116 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 1.3110 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 1.3102 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 1.3110 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 1.3104 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 1.3099 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 1.3075 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 1.3056 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 1.3048 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 1.3040 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 1.3032 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 1.3017 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 1.3019 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 1.3008 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 1.2995 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 1.2985 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 1.2978 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 1.2961 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 1.2944 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 1.2937 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 1.2926 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 1.2915 [Training] 113/113 [==============================] 254.0ms/step  batch_loss: 1.2905 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:21:30 - INFO - root -   The F1-score is 0.21267696267696268
10/25/2022 04:21:30 - INFO - root -   the best eval f1 is 0.2127, saving model !!
10/25/2022 04:21:32 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:06  batch_loss: 1.0953 [Training] 2/113 [..............................] - ETA: 47s  batch_loss: 1.0913 [Training] 3/113 [..............................] - ETA: 40s  batch_loss: 1.0927 [Training] 4/113 [>.............................] - ETA: 36s  batch_loss: 1.1121 [Training] 5/113 [>.............................] - ETA: 34s  batch_loss: 1.1257 [Training] 6/113 [>.............................] - ETA: 32s  batch_loss: 1.1184 [Training] 7/113 [>.............................] - ETA: 31s  batch_loss: 1.1125 [Training] 8/113 [=>............................] - ETA: 30s  batch_loss: 1.1018 [Training] 9/113 [=>............................] - ETA: 30s  batch_loss: 1.1235 [Training] 10/113 [=>............................] - ETA: 29s  batch_loss: 1.1024 [Training] 11/113 [=>............................] - ETA: 28s  batch_loss: 1.1084 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 1.1181 [Training] 13/113 [==>...........................] - ETA: 27s  batch_loss: 1.1190 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 1.1137 [Training] 15/113 [==>...........................] - ETA: 26s  batch_loss: 1.1129 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 1.1137 [Training] 17/113 [===>..........................] - ETA: 25s  batch_loss: 1.1176 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 1.1203 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 1.1201 [Training] 20/113 [====>.........................] - ETA: 24s  batch_loss: 1.1228 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 1.1066 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 1.1044 [Training] 23/113 [=====>........................] - ETA: 23s  batch_loss: 1.1097 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 1.1081 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 1.1054 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 1.1017 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 1.1076 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 1.1095 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 1.1047 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 1.1037 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 1.1081 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 1.1105 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 1.1097 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 1.1042 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 1.1041 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 1.1066 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 1.1062 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 1.1098 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 1.1077 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 1.1033 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 1.1001 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 1.1016 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 1.1021 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 1.1065 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 1.1049 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 1.1074 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 1.1060 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 1.1060 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 1.1025 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 1.0990 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 1.1008 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 1.0971 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 1.0996 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 1.0987 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 1.0969 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 1.0978 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 1.0985 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 1.0995 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 1.0983 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 1.0959 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 1.0946 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 1.0928 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 1.0925 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 1.0906 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 1.0876 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 1.0875 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 1.0872 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 1.0870 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 1.0843 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 1.0863 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 1.0860 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 1.0853 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 1.0828 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 1.0828 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 1.0859 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 1.0853 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 1.0829 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 1.0813 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 1.0793 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 1.0786 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 1.0763 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 1.0756 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 1.0751 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 1.0740 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 1.0721 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 1.0714 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 1.0704 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 1.0683 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 1.0672 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 1.0652 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 1.0653 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 1.0681 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 1.0665 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 1.0678 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 1.0672 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 1.0681 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 1.0683 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 1.0670 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 1.0667 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 1.0661 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 1.0649 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 1.0632 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 1.0638 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 1.0624 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 1.0615 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 1.0603 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 1.0589 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 1.0583 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 1.0582 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 1.0580 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 1.0562 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 1.0566 [Training] 113/113 [==============================] 256.1ms/step  batch_loss: 1.0562 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:22:07 - INFO - root -   The F1-score is 0.3390899534464634
10/25/2022 04:22:07 - INFO - root -   the best eval f1 is 0.3391, saving model !!
10/25/2022 04:22:10 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:11  batch_loss: 1.0717 [Training] 2/113 [..............................] - ETA: 47s  batch_loss: 1.1481 [Training] 3/113 [..............................] - ETA: 40s  batch_loss: 1.1308 [Training] 4/113 [>.............................] - ETA: 36s  batch_loss: 1.0500 [Training] 5/113 [>.............................] - ETA: 34s  batch_loss: 1.0174 [Training] 6/113 [>.............................] - ETA: 33s  batch_loss: 1.0052 [Training] 7/113 [>.............................] - ETA: 31s  batch_loss: 0.9966 [Training] 8/113 [=>............................] - ETA: 31s  batch_loss: 0.9936 [Training] 9/113 [=>............................] - ETA: 30s  batch_loss: 0.9760 [Training] 10/113 [=>............................] - ETA: 29s  batch_loss: 0.9902 [Training] 11/113 [=>............................] - ETA: 28s  batch_loss: 0.9976 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 0.9882 [Training] 13/113 [==>...........................] - ETA: 27s  batch_loss: 0.9822 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 0.9729 [Training] 15/113 [==>...........................] - ETA: 26s  batch_loss: 0.9758 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 0.9828 [Training] 17/113 [===>..........................] - ETA: 25s  batch_loss: 0.9797 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 0.9701 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.9746 [Training] 20/113 [====>.........................] - ETA: 24s  batch_loss: 0.9729 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 0.9678 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.9681 [Training] 23/113 [=====>........................] - ETA: 23s  batch_loss: 0.9681 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 0.9674 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.9655 [Training] 26/113 [=====>........................] - ETA: 22s  batch_loss: 0.9647 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 0.9624 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.9563 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.9518 [Training] 30/113 [======>.......................] - ETA: 21s  batch_loss: 0.9493 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.9466 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.9426 [Training] 33/113 [=======>......................] - ETA: 20s  batch_loss: 0.9446 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.9416 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.9370 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.9349 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.9326 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.9307 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.9341 [Training] 40/113 [=========>....................] - ETA: 18s  batch_loss: 0.9320 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.9295 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.9294 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.9279 [Training] 44/113 [==========>...................] - ETA: 17s  batch_loss: 0.9276 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.9272 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.9258 [Training] 47/113 [===========>..................] - ETA: 16s  batch_loss: 0.9224 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.9235 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.9221 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.9198 [Training] 51/113 [============>.................] - ETA: 15s  batch_loss: 0.9177 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.9169 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.9151 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.9145 [Training] 55/113 [=============>................] - ETA: 14s  batch_loss: 0.9124 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.9118 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.9095 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.9067 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 0.9064 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.9044 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.9059 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.9075 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.9059 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.9057 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.9074 [Training] 66/113 [================>.............] - ETA: 11s  batch_loss: 0.9078 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.9062 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.9048 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.9047 [Training] 70/113 [=================>............] - ETA: 10s  batch_loss: 0.9031 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.9021 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.9023 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.9030 [Training] 74/113 [==================>...........] - ETA: 9s  batch_loss: 0.9035 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.9027 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.9013 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.9001 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 0.8989 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.8978 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.8981 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.8988 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.8986 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.9000 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.8993 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.8986 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.8969 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.8957 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.8945 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.8936 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.8927 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.8923 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.8921 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.8930 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.8920 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.8926 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.8926 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.8931 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.8920 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.8915 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.8901 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.8902 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.8892 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.8893 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.8890 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.8873 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.8864 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.8865 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.8861 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.8862 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.8867 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.8864 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.8846 [Training] 113/113 [==============================] 251.3ms/step  batch_loss: 0.8852 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:22:44 - INFO - root -   The F1-score is 0.44844290657439445
10/25/2022 04:22:44 - INFO - root -   the best eval f1 is 0.4484, saving model !!
10/25/2022 04:22:46 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:07  batch_loss: 0.7375 [Training] 2/113 [..............................] - ETA: 47s  batch_loss: 0.7448 [Training] 3/113 [..............................] - ETA: 40s  batch_loss: 0.7622 [Training] 4/113 [>.............................] - ETA: 37s  batch_loss: 0.7586 [Training] 5/113 [>.............................] - ETA: 34s  batch_loss: 0.7518 [Training] 6/113 [>.............................] - ETA: 33s  batch_loss: 0.7569 [Training] 7/113 [>.............................] - ETA: 32s  batch_loss: 0.7534 [Training] 8/113 [=>............................] - ETA: 30s  batch_loss: 0.7669 [Training] 9/113 [=>............................] - ETA: 29s  batch_loss: 0.7587 [Training] 10/113 [=>............................] - ETA: 29s  batch_loss: 0.7585 [Training] 11/113 [=>............................] - ETA: 28s  batch_loss: 0.7636 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 0.7627 [Training] 13/113 [==>...........................] - ETA: 27s  batch_loss: 0.7567 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 0.7578 [Training] 15/113 [==>...........................] - ETA: 26s  batch_loss: 0.7570 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 0.7630 [Training] 17/113 [===>..........................] - ETA: 25s  batch_loss: 0.7627 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 0.7630 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.7603 [Training] 20/113 [====>.........................] - ETA: 24s  batch_loss: 0.7607 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 0.7586 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.7581 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.7598 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 0.7638 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.7634 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.7575 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 0.7592 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.7624 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.7642 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.7666 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.7680 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.7669 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.7672 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.7703 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.7705 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.7704 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.7713 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.7737 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.7740 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.7742 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.7711 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.7756 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.7727 [Training] 44/113 [==========>...................] - ETA: 17s  batch_loss: 0.7735 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.7754 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.7767 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.7752 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.7766 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.7756 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.7769 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.7772 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.7767 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.7757 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.7780 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.7768 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.7760 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.7754 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.7752 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.7734 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.7724 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.7761 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.7765 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.7753 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.7750 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.7741 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.7740 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.7729 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.7734 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.7729 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.7721 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.7725 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.7733 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.7723 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.7713 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.7725 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.7722 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.7719 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.7698 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.7690 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.7679 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.7672 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.7669 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.7650 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.7633 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.7630 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.7624 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.7617 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.7623 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.7610 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.7606 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.7602 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.7607 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.7598 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.7590 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.7578 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.7579 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.7586 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.7589 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.7588 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.7594 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.7590 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.7580 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.7577 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.7576 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.7578 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.7561 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.7555 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.7553 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.7545 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.7543 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.7534 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.7527 [Training] 113/113 [==============================] 256.2ms/step  batch_loss: 0.7517 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:23:22 - INFO - root -   The F1-score is 0.49061032863849763
10/25/2022 04:23:22 - INFO - root -   the best eval f1 is 0.4906, saving model !!
10/25/2022 04:23:24 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:09  batch_loss: 0.5748 [Training] 2/113 [..............................] - ETA: 49s  batch_loss: 0.6562 [Training] 3/113 [..............................] - ETA: 41s  batch_loss: 0.6714 [Training] 4/113 [>.............................] - ETA: 37s  batch_loss: 0.6719 [Training] 5/113 [>.............................] - ETA: 35s  batch_loss: 0.6793 [Training] 6/113 [>.............................] - ETA: 33s  batch_loss: 0.6740 [Training] 7/113 [>.............................] - ETA: 32s  batch_loss: 0.6785 [Training] 8/113 [=>............................] - ETA: 31s  batch_loss: 0.6813 [Training] 9/113 [=>............................] - ETA: 30s  batch_loss: 0.6942 [Training] 10/113 [=>............................] - ETA: 29s  batch_loss: 0.6948 [Training] 11/113 [=>............................] - ETA: 29s  batch_loss: 0.6886 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 0.6866 [Training] 13/113 [==>...........................] - ETA: 28s  batch_loss: 0.6816 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 0.6853 [Training] 15/113 [==>...........................] - ETA: 27s  batch_loss: 0.6804 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 0.6782 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.6751 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 0.6729 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.6718 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.6713 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 0.6738 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.6747 [Training] 23/113 [=====>........................] - ETA: 23s  batch_loss: 0.6778 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 0.6775 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.6790 [Training] 26/113 [=====>........................] - ETA: 22s  batch_loss: 0.6778 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 0.6747 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.6742 [Training] 29/113 [======>.......................] - ETA: 21s  batch_loss: 0.6747 [Training] 30/113 [======>.......................] - ETA: 21s  batch_loss: 0.6769 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.6760 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.6751 [Training] 33/113 [=======>......................] - ETA: 20s  batch_loss: 0.6736 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.6699 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.6708 [Training] 36/113 [========>.....................] - ETA: 19s  batch_loss: 0.6682 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.6678 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.6664 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.6651 [Training] 40/113 [=========>....................] - ETA: 18s  batch_loss: 0.6642 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.6632 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.6611 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.6601 [Training] 44/113 [==========>...................] - ETA: 17s  batch_loss: 0.6590 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.6602 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.6611 [Training] 47/113 [===========>..................] - ETA: 16s  batch_loss: 0.6599 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.6603 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.6613 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.6607 [Training] 51/113 [============>.................] - ETA: 15s  batch_loss: 0.6605 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.6585 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.6589 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.6584 [Training] 55/113 [=============>................] - ETA: 14s  batch_loss: 0.6574 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.6572 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.6565 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.6578 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 0.6572 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.6580 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.6591 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.6585 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.6578 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.6571 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.6571 [Training] 66/113 [================>.............] - ETA: 11s  batch_loss: 0.6572 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.6570 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.6562 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.6556 [Training] 70/113 [=================>............] - ETA: 10s  batch_loss: 0.6562 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.6568 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.6551 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.6541 [Training] 74/113 [==================>...........] - ETA: 9s  batch_loss: 0.6545 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.6537 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.6524 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.6524 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 0.6520 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.6520 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.6525 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.6521 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.6520 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.6507 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.6512 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.6500 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.6502 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.6493 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.6487 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.6495 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.6497 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.6500 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.6502 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.6504 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.6510 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.6506 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.6508 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.6506 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.6508 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.6506 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.6502 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.6501 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.6502 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.6494 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.6487 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.6482 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.6490 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.6484 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.6470 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.6473 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.6469 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.6463 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.6460 [Training] 113/113 [==============================] 254.5ms/step  batch_loss: 0.6463 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:23:59 - INFO - root -   The F1-score is 0.5132486889318245
10/25/2022 04:23:59 - INFO - root -   the best eval f1 is 0.5132, saving model !!
10/25/2022 04:24:01 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:09  batch_loss: 0.5215 [Training] 2/113 [..............................] - ETA: 48s  batch_loss: 0.5036 [Training] 3/113 [..............................] - ETA: 40s  batch_loss: 0.5280 [Training] 4/113 [>.............................] - ETA: 36s  batch_loss: 0.5322 [Training] 5/113 [>.............................] - ETA: 34s  batch_loss: 0.5447 [Training] 6/113 [>.............................] - ETA: 32s  batch_loss: 0.5459 [Training] 7/113 [>.............................] - ETA: 31s  batch_loss: 0.5432 [Training] 8/113 [=>............................] - ETA: 30s  batch_loss: 0.5526 [Training] 9/113 [=>............................] - ETA: 30s  batch_loss: 0.5635 [Training] 10/113 [=>............................] - ETA: 29s  batch_loss: 0.5666 [Training] 11/113 [=>............................] - ETA: 28s  batch_loss: 0.5724 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 0.5721 [Training] 13/113 [==>...........................] - ETA: 27s  batch_loss: 0.5716 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 0.5771 [Training] 15/113 [==>...........................] - ETA: 26s  batch_loss: 0.5766 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 0.5726 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.5782 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 0.5774 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.5814 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.5812 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 0.5815 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.5817 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.5802 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.5797 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.5775 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.5771 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.5764 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.5786 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.5830 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.5824 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.5812 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.5813 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.5793 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.5763 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.5764 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.5776 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.5762 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.5780 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.5796 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.5772 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.5773 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.5776 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.5783 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.5767 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.5763 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.5771 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.5762 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.5767 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.5771 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.5748 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.5730 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.5730 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.5720 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.5719 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.5714 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.5728 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.5732 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.5737 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.5737 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.5734 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.5730 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.5727 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.5715 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.5716 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.5710 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.5712 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.5714 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.5716 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.5723 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.5712 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.5727 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.5742 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.5748 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.5747 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.5732 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.5724 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.5720 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.5721 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.5725 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.5728 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.5724 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.5716 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.5715 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.5709 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.5707 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.5699 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.5691 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.5685 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.5679 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.5665 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.5659 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.5659 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.5652 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.5643 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.5635 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.5644 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.5636 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.5626 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.5625 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.5632 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.5625 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.5633 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.5622 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.5618 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.5618 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.5617 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.5614 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.5609 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.5624 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.5624 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.5612 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.5608 [Training] 113/113 [==============================] 256.5ms/step  batch_loss: 0.5611 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:24:37 - INFO - root -   The F1-score is 0.5364715295372716
10/25/2022 04:24:37 - INFO - root -   the best eval f1 is 0.5365, saving model !!
10/25/2022 04:24:39 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:40  batch_loss: 0.5396 [Training] 2/113 [..............................] - ETA: 1:03  batch_loss: 0.5083 [Training] 3/113 [..............................] - ETA: 50s  batch_loss: 0.5177 [Training] 4/113 [>.............................] - ETA: 44s  batch_loss: 0.5055 [Training] 5/113 [>.............................] - ETA: 40s  batch_loss: 0.5120 [Training] 6/113 [>.............................] - ETA: 37s  batch_loss: 0.5098 [Training] 7/113 [>.............................] - ETA: 35s  batch_loss: 0.5157 [Training] 8/113 [=>............................] - ETA: 34s  batch_loss: 0.5108 [Training] 9/113 [=>............................] - ETA: 32s  batch_loss: 0.5154 [Training] 10/113 [=>............................] - ETA: 31s  batch_loss: 0.5107 [Training] 11/113 [=>............................] - ETA: 30s  batch_loss: 0.5153 [Training] 12/113 [==>...........................] - ETA: 30s  batch_loss: 0.5085 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.5079 [Training] 14/113 [==>...........................] - ETA: 28s  batch_loss: 0.5076 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.5029 [Training] 16/113 [===>..........................] - ETA: 27s  batch_loss: 0.5042 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.5069 [Training] 18/113 [===>..........................] - ETA: 26s  batch_loss: 0.5054 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.5019 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.5051 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.5002 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.5019 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.4970 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.4951 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.4972 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.4968 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.5036 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.5070 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.5048 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.5032 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.5039 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.5028 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.5017 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.5022 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.5011 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.5045 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.5028 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.5021 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.5028 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.5010 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.5003 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.5015 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.5007 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.5007 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.5036 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.5039 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.5041 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.5020 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.5014 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.5013 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.5005 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.5025 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.5024 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.5027 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.5040 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.5051 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.5039 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.5027 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.5018 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.5012 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.5014 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.5005 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.5003 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.4998 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.4995 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.4993 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.4999 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.5001 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.5003 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.4988 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.4991 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.4979 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.4972 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.4966 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.4965 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.4960 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.4956 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.4956 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.4960 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.4972 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.4962 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.4952 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.4951 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.4948 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.4936 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.4931 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.4929 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.4930 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.4927 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.4932 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.4925 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.4930 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.4928 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.4928 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.4927 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.4921 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.4923 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.4915 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.4910 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.4908 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.4916 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.4918 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.4919 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.4919 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.4929 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.4926 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.4930 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.4925 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.4934 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.4931 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.4943 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.4949 [Training] 113/113 [==============================] 257.6ms/step  batch_loss: 0.4940 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:25:14 - INFO - root -   The F1-score is 0.5695658018550881
10/25/2022 04:25:14 - INFO - root -   the best eval f1 is 0.5696, saving model !!
10/25/2022 04:25:16 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:13  batch_loss: 0.4405 [Training] 2/113 [..............................] - ETA: 50s  batch_loss: 0.5060 [Training] 3/113 [..............................] - ETA: 42s  batch_loss: 0.4723 [Training] 4/113 [>.............................] - ETA: 38s  batch_loss: 0.4685 [Training] 5/113 [>.............................] - ETA: 35s  batch_loss: 0.4817 [Training] 6/113 [>.............................] - ETA: 34s  batch_loss: 0.4745 [Training] 7/113 [>.............................] - ETA: 32s  batch_loss: 0.4591 [Training] 8/113 [=>............................] - ETA: 31s  batch_loss: 0.4468 [Training] 9/113 [=>............................] - ETA: 30s  batch_loss: 0.4383 [Training] 10/113 [=>............................] - ETA: 30s  batch_loss: 0.4392 [Training] 11/113 [=>............................] - ETA: 29s  batch_loss: 0.4413 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 0.4376 [Training] 13/113 [==>...........................] - ETA: 28s  batch_loss: 0.4383 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 0.4360 [Training] 15/113 [==>...........................] - ETA: 27s  batch_loss: 0.4354 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 0.4426 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.4470 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 0.4480 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.4482 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.4460 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 0.4456 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.4454 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.4428 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 0.4439 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.4450 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.4450 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 0.4438 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.4406 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.4403 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.4426 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.4426 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.4409 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.4419 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.4400 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.4413 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.4400 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.4402 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.4395 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.4406 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.4395 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.4394 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.4418 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.4422 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.4436 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.4438 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.4437 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.4436 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.4448 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.4433 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.4414 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.4413 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.4428 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.4447 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.4454 [Training] 55/113 [=============>................] - ETA: 14s  batch_loss: 0.4463 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.4451 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.4454 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.4456 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 0.4455 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.4455 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.4457 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.4449 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.4441 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.4435 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.4426 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.4413 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.4405 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.4413 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.4406 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.4406 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.4405 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.4420 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.4414 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.4404 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.4409 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.4411 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.4405 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 0.4417 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.4423 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.4430 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.4415 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.4412 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.4414 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.4425 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.4423 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.4419 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.4409 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.4407 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.4408 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.4401 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.4396 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.4403 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.4400 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.4400 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.4405 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.4399 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.4397 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.4399 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.4395 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.4398 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.4393 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.4388 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.4386 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.4381 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.4380 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.4377 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.4383 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.4382 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.4378 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.4373 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.4371 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.4372 [Training] 113/113 [==============================] 253.8ms/step  batch_loss: 0.4377 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:25:51 - INFO - root -   The F1-score is 0.5862384123253688
10/25/2022 04:25:51 - INFO - root -   the best eval f1 is 0.5862, saving model !!
10/25/2022 04:25:53 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:12  batch_loss: 0.3860 [Training] 2/113 [..............................] - ETA: 49s  batch_loss: 0.4468 [Training] 3/113 [..............................] - ETA: 41s  batch_loss: 0.4407 [Training] 4/113 [>.............................] - ETA: 38s  batch_loss: 0.4297 [Training] 5/113 [>.............................] - ETA: 35s  batch_loss: 0.4135 [Training] 6/113 [>.............................] - ETA: 33s  batch_loss: 0.4102 [Training] 7/113 [>.............................] - ETA: 32s  batch_loss: 0.4062 [Training] 8/113 [=>............................] - ETA: 31s  batch_loss: 0.3960 [Training] 9/113 [=>............................] - ETA: 30s  batch_loss: 0.4046 [Training] 10/113 [=>............................] - ETA: 29s  batch_loss: 0.4043 [Training] 11/113 [=>............................] - ETA: 29s  batch_loss: 0.4181 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 0.4169 [Training] 13/113 [==>...........................] - ETA: 27s  batch_loss: 0.4139 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 0.4168 [Training] 15/113 [==>...........................] - ETA: 26s  batch_loss: 0.4138 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 0.4135 [Training] 17/113 [===>..........................] - ETA: 25s  batch_loss: 0.4106 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 0.4106 [Training] 19/113 [====>.........................] - ETA: 24s  batch_loss: 0.4060 [Training] 20/113 [====>.........................] - ETA: 24s  batch_loss: 0.4069 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 0.4107 [Training] 22/113 [====>.........................] - ETA: 23s  batch_loss: 0.4101 [Training] 23/113 [=====>........................] - ETA: 23s  batch_loss: 0.4113 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 0.4116 [Training] 25/113 [=====>........................] - ETA: 22s  batch_loss: 0.4124 [Training] 26/113 [=====>........................] - ETA: 22s  batch_loss: 0.4073 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 0.4084 [Training] 28/113 [======>.......................] - ETA: 21s  batch_loss: 0.4094 [Training] 29/113 [======>.......................] - ETA: 21s  batch_loss: 0.4063 [Training] 30/113 [======>.......................] - ETA: 21s  batch_loss: 0.4060 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.4080 [Training] 32/113 [=======>......................] - ETA: 20s  batch_loss: 0.4072 [Training] 33/113 [=======>......................] - ETA: 20s  batch_loss: 0.4068 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.4064 [Training] 35/113 [========>.....................] - ETA: 19s  batch_loss: 0.4072 [Training] 36/113 [========>.....................] - ETA: 19s  batch_loss: 0.4059 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.4066 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.4075 [Training] 39/113 [=========>....................] - ETA: 18s  batch_loss: 0.4071 [Training] 40/113 [=========>....................] - ETA: 18s  batch_loss: 0.4075 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.4068 [Training] 42/113 [==========>...................] - ETA: 17s  batch_loss: 0.4061 [Training] 43/113 [==========>...................] - ETA: 17s  batch_loss: 0.4055 [Training] 44/113 [==========>...................] - ETA: 17s  batch_loss: 0.4037 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.4047 [Training] 46/113 [===========>..................] - ETA: 16s  batch_loss: 0.4046 [Training] 47/113 [===========>..................] - ETA: 16s  batch_loss: 0.4041 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.4051 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.4037 [Training] 50/113 [============>.................] - ETA: 15s  batch_loss: 0.4040 [Training] 51/113 [============>.................] - ETA: 15s  batch_loss: 0.4039 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.4023 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.4013 [Training] 54/113 [=============>................] - ETA: 14s  batch_loss: 0.3998 [Training] 55/113 [=============>................] - ETA: 14s  batch_loss: 0.3996 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.4003 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.3999 [Training] 58/113 [==============>...............] - ETA: 13s  batch_loss: 0.3997 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 0.3987 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.3974 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.3968 [Training] 62/113 [===============>..............] - ETA: 12s  batch_loss: 0.3970 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.3964 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.3961 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.3953 [Training] 66/113 [================>.............] - ETA: 11s  batch_loss: 0.3957 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.3944 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.3941 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.3939 [Training] 70/113 [=================>............] - ETA: 10s  batch_loss: 0.3943 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.3934 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.3930 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.3922 [Training] 74/113 [==================>...........] - ETA: 9s  batch_loss: 0.3925 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.3914 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.3915 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.3934 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 0.3926 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.3928 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.3918 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.3920 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.3915 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.3917 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.3921 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.3918 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.3914 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.3910 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.3909 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.3905 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.3908 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.3900 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.3907 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.3900 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.3895 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.3904 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.3909 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.3909 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.3910 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.3903 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.3906 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.3901 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.3894 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.3894 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.3899 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.3902 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.3908 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.3910 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.3908 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.3913 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.3909 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.3908 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.3907 [Training] 113/113 [==============================] 252.4ms/step  batch_loss: 0.3906 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:26:28 - INFO - root -   The F1-score is 0.5945555919973763
10/25/2022 04:26:28 - INFO - root -   the best eval f1 is 0.5946, saving model !!
10/25/2022 04:26:30 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:06  batch_loss: 0.4028 [Training] 2/113 [..............................] - ETA: 47s  batch_loss: 0.3957 [Training] 3/113 [..............................] - ETA: 40s  batch_loss: 0.3801 [Training] 4/113 [>.............................] - ETA: 36s  batch_loss: 0.4060 [Training] 5/113 [>.............................] - ETA: 34s  batch_loss: 0.3921 [Training] 6/113 [>.............................] - ETA: 32s  batch_loss: 0.3887 [Training] 7/113 [>.............................] - ETA: 31s  batch_loss: 0.3848 [Training] 8/113 [=>............................] - ETA: 30s  batch_loss: 0.3783 [Training] 9/113 [=>............................] - ETA: 30s  batch_loss: 0.3822 [Training] 10/113 [=>............................] - ETA: 29s  batch_loss: 0.3754 [Training] 11/113 [=>............................] - ETA: 28s  batch_loss: 0.3707 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 0.3742 [Training] 13/113 [==>...........................] - ETA: 27s  batch_loss: 0.3713 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 0.3696 [Training] 15/113 [==>...........................] - ETA: 26s  batch_loss: 0.3688 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 0.3679 [Training] 17/113 [===>..........................] - ETA: 25s  batch_loss: 0.3654 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 0.3626 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.3647 [Training] 20/113 [====>.........................] - ETA: 24s  batch_loss: 0.3706 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 0.3716 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.3712 [Training] 23/113 [=====>........................] - ETA: 23s  batch_loss: 0.3706 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 0.3699 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.3670 [Training] 26/113 [=====>........................] - ETA: 22s  batch_loss: 0.3674 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 0.3688 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.3707 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.3697 [Training] 30/113 [======>.......................] - ETA: 21s  batch_loss: 0.3674 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.3673 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.3673 [Training] 33/113 [=======>......................] - ETA: 20s  batch_loss: 0.3669 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.3659 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.3651 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.3639 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.3628 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.3639 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.3642 [Training] 40/113 [=========>....................] - ETA: 18s  batch_loss: 0.3627 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.3632 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.3626 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.3608 [Training] 44/113 [==========>...................] - ETA: 17s  batch_loss: 0.3611 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.3612 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.3609 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.3614 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.3608 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.3617 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.3615 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.3598 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.3604 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.3598 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.3597 [Training] 55/113 [=============>................] - ETA: 14s  batch_loss: 0.3592 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.3580 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.3575 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.3580 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 0.3582 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.3587 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.3585 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.3571 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.3572 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.3575 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.3575 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.3565 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.3556 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.3554 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.3567 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.3564 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.3555 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.3552 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.3546 [Training] 74/113 [==================>...........] - ETA: 9s  batch_loss: 0.3552 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.3540 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.3530 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.3526 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 0.3525 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.3529 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.3530 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.3529 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.3521 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.3531 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.3530 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.3534 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.3536 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.3535 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.3542 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.3543 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.3541 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.3546 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.3552 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.3552 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.3548 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.3544 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.3541 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.3545 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.3541 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.3543 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.3543 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.3539 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.3534 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.3537 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.3540 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.3538 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.3536 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.3531 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.3529 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.3525 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.3527 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.3528 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.3527 [Training] 113/113 [==============================] 255.6ms/step  batch_loss: 0.3520 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:27:05 - INFO - root -   The F1-score is 0.6017501151391539
10/25/2022 04:27:05 - INFO - root -   the best eval f1 is 0.6018, saving model !!
10/25/2022 04:27:08 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:13  batch_loss: 0.3570 [Training] 2/113 [..............................] - ETA: 51s  batch_loss: 0.3406 [Training] 3/113 [..............................] - ETA: 43s  batch_loss: 0.3308 [Training] 4/113 [>.............................] - ETA: 39s  batch_loss: 0.3286 [Training] 5/113 [>.............................] - ETA: 37s  batch_loss: 0.3196 [Training] 6/113 [>.............................] - ETA: 35s  batch_loss: 0.3222 [Training] 7/113 [>.............................] - ETA: 34s  batch_loss: 0.3322 [Training] 8/113 [=>............................] - ETA: 33s  batch_loss: 0.3343 [Training] 9/113 [=>............................] - ETA: 31s  batch_loss: 0.3338 [Training] 10/113 [=>............................] - ETA: 31s  batch_loss: 0.3296 [Training] 11/113 [=>............................] - ETA: 30s  batch_loss: 0.3304 [Training] 12/113 [==>...........................] - ETA: 29s  batch_loss: 0.3299 [Training] 13/113 [==>...........................] - ETA: 29s  batch_loss: 0.3315 [Training] 14/113 [==>...........................] - ETA: 28s  batch_loss: 0.3283 [Training] 15/113 [==>...........................] - ETA: 28s  batch_loss: 0.3288 [Training] 16/113 [===>..........................] - ETA: 27s  batch_loss: 0.3270 [Training] 17/113 [===>..........................] - ETA: 27s  batch_loss: 0.3266 [Training] 18/113 [===>..........................] - ETA: 26s  batch_loss: 0.3252 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.3265 [Training] 20/113 [====>.........................] - ETA: 26s  batch_loss: 0.3280 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.3292 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.3279 [Training] 23/113 [=====>........................] - ETA: 25s  batch_loss: 0.3267 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.3260 [Training] 25/113 [=====>........................] - ETA: 24s  batch_loss: 0.3250 [Training] 26/113 [=====>........................] - ETA: 24s  batch_loss: 0.3241 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.3240 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.3233 [Training] 29/113 [======>.......................] - ETA: 23s  batch_loss: 0.3253 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.3262 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.3258 [Training] 32/113 [=======>......................] - ETA: 22s  batch_loss: 0.3231 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.3227 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.3219 [Training] 35/113 [========>.....................] - ETA: 21s  batch_loss: 0.3219 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.3236 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.3243 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 0.3235 [Training] 39/113 [=========>....................] - ETA: 20s  batch_loss: 0.3244 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.3238 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.3223 [Training] 42/113 [==========>...................] - ETA: 19s  batch_loss: 0.3224 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.3240 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.3238 [Training] 45/113 [==========>...................] - ETA: 18s  batch_loss: 0.3247 [Training] 46/113 [===========>..................] - ETA: 18s  batch_loss: 0.3244 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.3256 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.3250 [Training] 49/113 [============>.................] - ETA: 17s  batch_loss: 0.3263 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.3261 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.3263 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 0.3265 [Training] 53/113 [=============>................] - ETA: 16s  batch_loss: 0.3261 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.3262 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.3263 [Training] 56/113 [=============>................] - ETA: 15s  batch_loss: 0.3270 [Training] 57/113 [==============>...............] - ETA: 15s  batch_loss: 0.3264 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.3263 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.3263 [Training] 60/113 [==============>...............] - ETA: 14s  batch_loss: 0.3256 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.3256 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.3255 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.3257 [Training] 64/113 [===============>..............] - ETA: 13s  batch_loss: 0.3257 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.3261 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.3270 [Training] 67/113 [================>.............] - ETA: 12s  batch_loss: 0.3272 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.3269 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.3272 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.3277 [Training] 71/113 [=================>............] - ETA: 11s  batch_loss: 0.3273 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.3274 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.3272 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.3265 [Training] 75/113 [==================>...........] - ETA: 10s  batch_loss: 0.3258 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.3252 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.3249 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.3250 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.3255 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.3250 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.3251 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.3246 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.3243 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.3237 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.3235 [Training] 86/113 [=====================>........] - ETA: 7s  batch_loss: 0.3229 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.3233 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.3226 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.3225 [Training] 90/113 [======================>.......] - ETA: 6s  batch_loss: 0.3225 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.3227 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.3230 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.3227 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.3230 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.3231 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.3224 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.3224 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.3223 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.3223 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.3228 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.3227 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.3224 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.3223 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.3221 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.3219 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.3219 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.3217 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.3221 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.3222 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.3219 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.3218 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.3214 [Training] 113/113 [==============================] 260.7ms/step  batch_loss: 0.3209 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:27:43 - INFO - root -   The F1-score is 0.6045944558521561
10/25/2022 04:27:43 - INFO - root -   the best eval f1 is 0.6046, saving model !!
10/25/2022 04:27:46 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:13  batch_loss: 0.2692 [Training] 2/113 [..............................] - ETA: 50s  batch_loss: 0.2897 [Training] 3/113 [..............................] - ETA: 42s  batch_loss: 0.3176 [Training] 4/113 [>.............................] - ETA: 38s  batch_loss: 0.3306 [Training] 5/113 [>.............................] - ETA: 35s  batch_loss: 0.3362 [Training] 6/113 [>.............................] - ETA: 33s  batch_loss: 0.3308 [Training] 7/113 [>.............................] - ETA: 32s  batch_loss: 0.3295 [Training] 8/113 [=>............................] - ETA: 31s  batch_loss: 0.3176 [Training] 9/113 [=>............................] - ETA: 30s  batch_loss: 0.3137 [Training] 10/113 [=>............................] - ETA: 29s  batch_loss: 0.3175 [Training] 11/113 [=>............................] - ETA: 29s  batch_loss: 0.3144 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 0.3105 [Training] 13/113 [==>...........................] - ETA: 28s  batch_loss: 0.3091 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 0.3044 [Training] 15/113 [==>...........................] - ETA: 27s  batch_loss: 0.3047 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 0.3039 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.3005 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 0.2995 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.3007 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.2988 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 0.3012 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.3021 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.2999 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 0.2998 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.3020 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.2989 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 0.2998 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.3020 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.3004 [Training] 30/113 [======>.......................] - ETA: 21s  batch_loss: 0.3004 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.3002 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.2995 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.2986 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.2994 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.2997 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.3011 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.3010 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.3032 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.3018 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.3011 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.3002 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.2998 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.3010 [Training] 44/113 [==========>...................] - ETA: 17s  batch_loss: 0.3006 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.2989 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.3013 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.3013 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.3006 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.3003 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.2993 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.3001 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.2993 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.2985 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.2981 [Training] 55/113 [=============>................] - ETA: 14s  batch_loss: 0.2979 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.2976 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.2986 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.2980 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 0.2977 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.2977 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.2987 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.2985 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.2974 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.2973 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.2977 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.2977 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.2988 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.2995 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.2992 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.2983 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.2976 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.2979 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.2993 [Training] 74/113 [==================>...........] - ETA: 9s  batch_loss: 0.2987 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.2985 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.2983 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.2977 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 0.2969 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.2967 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.2962 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.2953 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.2961 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.2954 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.2953 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.2945 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.2941 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.2948 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.2948 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.2949 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.2949 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.2949 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.2953 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.2952 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.2953 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.2951 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.2948 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.2960 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.2961 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.2966 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.2965 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.2959 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.2952 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.2942 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.2940 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.2941 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.2941 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.2943 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.2950 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.2951 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.2953 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.2950 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.2949 [Training] 113/113 [==============================] 254.9ms/step  batch_loss: 0.2951 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:28:21 - INFO - root -   The F1-score is 0.6200345423143351
10/25/2022 04:28:21 - INFO - root -   the best eval f1 is 0.6200, saving model !!
10/25/2022 04:28:23 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:10  batch_loss: 0.2493 [Training] 2/113 [..............................] - ETA: 49s  batch_loss: 0.2290 [Training] 3/113 [..............................] - ETA: 41s  batch_loss: 0.2500 [Training] 4/113 [>.............................] - ETA: 38s  batch_loss: 0.2668 [Training] 5/113 [>.............................] - ETA: 35s  batch_loss: 0.2718 [Training] 6/113 [>.............................] - ETA: 33s  batch_loss: 0.2651 [Training] 7/113 [>.............................] - ETA: 32s  batch_loss: 0.2652 [Training] 8/113 [=>............................] - ETA: 31s  batch_loss: 0.2658 [Training] 9/113 [=>............................] - ETA: 30s  batch_loss: 0.2683 [Training] 10/113 [=>............................] - ETA: 29s  batch_loss: 0.2790 [Training] 11/113 [=>............................] - ETA: 28s  batch_loss: 0.2817 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 0.2825 [Training] 13/113 [==>...........................] - ETA: 27s  batch_loss: 0.2821 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 0.2801 [Training] 15/113 [==>...........................] - ETA: 26s  batch_loss: 0.2780 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 0.2799 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.2787 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 0.2768 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.2794 [Training] 20/113 [====>.........................] - ETA: 24s  batch_loss: 0.2757 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 0.2751 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.2737 [Training] 23/113 [=====>........................] - ETA: 23s  batch_loss: 0.2724 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 0.2704 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.2731 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.2727 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 0.2725 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.2734 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.2737 [Training] 30/113 [======>.......................] - ETA: 21s  batch_loss: 0.2731 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.2748 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.2732 [Training] 33/113 [=======>......................] - ETA: 20s  batch_loss: 0.2726 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.2735 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.2725 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.2713 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.2714 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.2705 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.2705 [Training] 40/113 [=========>....................] - ETA: 18s  batch_loss: 0.2686 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.2681 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.2691 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.2689 [Training] 44/113 [==========>...................] - ETA: 17s  batch_loss: 0.2685 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.2678 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.2676 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.2684 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.2685 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.2676 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.2685 [Training] 51/113 [============>.................] - ETA: 15s  batch_loss: 0.2687 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.2697 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.2697 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.2698 [Training] 55/113 [=============>................] - ETA: 14s  batch_loss: 0.2695 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.2696 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.2698 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.2697 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 0.2696 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.2689 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.2705 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.2701 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.2697 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.2705 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.2717 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.2712 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.2712 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.2725 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.2713 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.2712 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.2711 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.2711 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.2709 [Training] 74/113 [==================>...........] - ETA: 9s  batch_loss: 0.2704 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.2699 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.2707 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.2702 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 0.2702 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.2706 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.2706 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.2702 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.2706 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.2706 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.2709 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.2711 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.2709 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.2709 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.2711 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.2708 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.2704 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.2705 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.2700 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.2700 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.2707 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.2707 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.2707 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.2707 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.2715 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.2710 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.2716 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.2718 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.2717 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.2716 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.2712 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.2714 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.2711 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.2709 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.2706 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.2708 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.2707 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.2709 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.2712 [Training] 113/113 [==============================] 254.7ms/step  batch_loss: 0.2721 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:28:58 - INFO - root -   The F1-score is 0.626633932718466
10/25/2022 04:28:58 - INFO - root -   the best eval f1 is 0.6266, saving model !!
10/25/2022 04:29:00 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:12  batch_loss: 0.2952 [Training] 2/113 [..............................] - ETA: 50s  batch_loss: 0.2676 [Training] 3/113 [..............................] - ETA: 42s  batch_loss: 0.2566 [Training] 4/113 [>.............................] - ETA: 38s  batch_loss: 0.2499 [Training] 5/113 [>.............................] - ETA: 36s  batch_loss: 0.2377 [Training] 6/113 [>.............................] - ETA: 34s  batch_loss: 0.2358 [Training] 7/113 [>.............................] - ETA: 32s  batch_loss: 0.2337 [Training] 8/113 [=>............................] - ETA: 31s  batch_loss: 0.2317 [Training] 9/113 [=>............................] - ETA: 30s  batch_loss: 0.2381 [Training] 10/113 [=>............................] - ETA: 30s  batch_loss: 0.2396 [Training] 11/113 [=>............................] - ETA: 29s  batch_loss: 0.2413 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 0.2406 [Training] 13/113 [==>...........................] - ETA: 28s  batch_loss: 0.2456 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 0.2457 [Training] 15/113 [==>...........................] - ETA: 26s  batch_loss: 0.2437 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 0.2447 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.2428 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 0.2427 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.2432 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.2422 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 0.2424 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.2408 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.2419 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 0.2424 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.2430 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.2430 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 0.2433 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.2443 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.2440 [Training] 30/113 [======>.......................] - ETA: 21s  batch_loss: 0.2438 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.2441 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.2447 [Training] 33/113 [=======>......................] - ETA: 20s  batch_loss: 0.2440 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.2440 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.2453 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.2449 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.2449 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.2450 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.2455 [Training] 40/113 [=========>....................] - ETA: 18s  batch_loss: 0.2452 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.2444 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.2439 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.2431 [Training] 44/113 [==========>...................] - ETA: 17s  batch_loss: 0.2435 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.2439 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.2430 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.2431 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.2433 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.2435 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.2433 [Training] 51/113 [============>.................] - ETA: 15s  batch_loss: 0.2436 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.2440 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.2433 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.2446 [Training] 55/113 [=============>................] - ETA: 14s  batch_loss: 0.2472 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.2472 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.2474 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.2463 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 0.2458 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.2457 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.2463 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.2471 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.2471 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.2477 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.2471 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.2472 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.2480 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.2478 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.2481 [Training] 70/113 [=================>............] - ETA: 10s  batch_loss: 0.2476 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.2474 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.2476 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.2472 [Training] 74/113 [==================>...........] - ETA: 9s  batch_loss: 0.2473 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.2477 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.2477 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.2476 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 0.2476 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.2483 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.2489 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.2488 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.2485 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.2497 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.2497 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.2500 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.2500 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.2494 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.2494 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.2497 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.2499 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.2496 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.2495 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.2502 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.2498 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.2502 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.2504 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.2501 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.2498 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.2499 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.2496 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.2494 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.2497 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.2502 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.2508 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.2506 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.2503 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.2505 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.2508 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.2510 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.2506 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.2508 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.2508 [Training] 113/113 [==============================] 254.3ms/step  batch_loss: 0.2508 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:29:35 - INFO - root -   The F1-score is 0.6329944244014432
10/25/2022 04:29:35 - INFO - root -   the best eval f1 is 0.6330, saving model !!
10/25/2022 04:29:37 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:15  batch_loss: 0.1940 [Training] 2/113 [..............................] - ETA: 51s  batch_loss: 0.2089 [Training] 3/113 [..............................] - ETA: 43s  batch_loss: 0.1975 [Training] 4/113 [>.............................] - ETA: 39s  batch_loss: 0.2227 [Training] 5/113 [>.............................] - ETA: 37s  batch_loss: 0.2174 [Training] 6/113 [>.............................] - ETA: 35s  batch_loss: 0.2381 [Training] 7/113 [>.............................] - ETA: 33s  batch_loss: 0.2400 [Training] 8/113 [=>............................] - ETA: 32s  batch_loss: 0.2348 [Training] 9/113 [=>............................] - ETA: 31s  batch_loss: 0.2351 [Training] 10/113 [=>............................] - ETA: 30s  batch_loss: 0.2371 [Training] 11/113 [=>............................] - ETA: 29s  batch_loss: 0.2407 [Training] 12/113 [==>...........................] - ETA: 29s  batch_loss: 0.2441 [Training] 13/113 [==>...........................] - ETA: 28s  batch_loss: 0.2401 [Training] 14/113 [==>...........................] - ETA: 28s  batch_loss: 0.2401 [Training] 15/113 [==>...........................] - ETA: 27s  batch_loss: 0.2366 [Training] 16/113 [===>..........................] - ETA: 27s  batch_loss: 0.2356 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.2340 [Training] 18/113 [===>..........................] - ETA: 26s  batch_loss: 0.2346 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.2360 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.2383 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.2354 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.2329 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.2315 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.2315 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.2324 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.2326 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.2318 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.2322 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.2303 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.2326 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.2326 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.2321 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.2319 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.2306 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.2297 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.2326 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.2340 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.2343 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.2356 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.2347 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.2346 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.2341 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.2337 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.2347 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.2343 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.2337 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.2349 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.2356 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.2366 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.2367 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.2358 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.2362 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.2363 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.2367 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.2362 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.2367 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.2364 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.2354 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.2354 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.2360 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.2354 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.2356 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.2355 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.2352 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.2342 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.2346 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.2342 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.2341 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.2336 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.2336 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.2335 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.2335 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.2334 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.2336 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.2340 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.2333 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.2343 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 0.2338 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.2338 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.2339 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.2332 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.2335 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.2328 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.2326 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.2326 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.2321 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.2323 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.2319 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.2322 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.2321 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.2322 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.2322 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.2321 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.2327 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.2327 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.2330 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.2332 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.2335 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.2344 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.2340 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.2336 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.2333 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.2336 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.2335 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.2331 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.2329 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.2333 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.2333 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.2332 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.2331 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.2337 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.2334 [Training] 113/113 [==============================] 253.4ms/step  batch_loss: 0.2336 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:30:12 - INFO - root -   The F1-score is 0.636545795359059
10/25/2022 04:30:12 - INFO - root -   the best eval f1 is 0.6365, saving model !!
10/25/2022 04:30:15 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:13  batch_loss: 0.2010 [Training] 2/113 [..............................] - ETA: 49s  batch_loss: 0.2034 [Training] 3/113 [..............................] - ETA: 42s  batch_loss: 0.1984 [Training] 4/113 [>.............................] - ETA: 38s  batch_loss: 0.1955 [Training] 5/113 [>.............................] - ETA: 36s  batch_loss: 0.1931 [Training] 6/113 [>.............................] - ETA: 34s  batch_loss: 0.2025 [Training] 7/113 [>.............................] - ETA: 33s  batch_loss: 0.2044 [Training] 8/113 [=>............................] - ETA: 32s  batch_loss: 0.2061 [Training] 9/113 [=>............................] - ETA: 31s  batch_loss: 0.2091 [Training] 10/113 [=>............................] - ETA: 30s  batch_loss: 0.2114 [Training] 11/113 [=>............................] - ETA: 30s  batch_loss: 0.2141 [Training] 12/113 [==>...........................] - ETA: 29s  batch_loss: 0.2170 [Training] 13/113 [==>...........................] - ETA: 28s  batch_loss: 0.2177 [Training] 14/113 [==>...........................] - ETA: 28s  batch_loss: 0.2125 [Training] 15/113 [==>...........................] - ETA: 27s  batch_loss: 0.2133 [Training] 16/113 [===>..........................] - ETA: 27s  batch_loss: 0.2128 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.2121 [Training] 18/113 [===>..........................] - ETA: 26s  batch_loss: 0.2110 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.2106 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.2103 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.2088 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.2070 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.2064 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.2069 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.2076 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.2090 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.2106 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.2100 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.2098 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.2093 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.2103 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.2098 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.2091 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.2098 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.2097 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.2097 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.2100 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.2104 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.2107 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.2124 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.2119 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.2119 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.2121 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.2121 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.2129 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.2134 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.2127 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.2123 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.2133 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.2126 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.2133 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.2142 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.2148 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.2147 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.2147 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.2146 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.2152 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.2148 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 0.2148 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.2160 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.2155 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.2155 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.2158 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.2150 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.2146 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.2144 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.2150 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.2153 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.2149 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.2153 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.2152 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.2160 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.2162 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.2157 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.2153 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.2157 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.2152 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 0.2143 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.2140 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.2133 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.2143 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.2147 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.2147 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.2149 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.2149 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.2150 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.2158 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.2168 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.2170 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.2171 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.2170 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.2172 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.2168 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.2168 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.2170 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.2168 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.2177 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.2176 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.2175 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.2175 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.2173 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.2179 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.2179 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.2178 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.2179 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.2182 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.2179 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.2175 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.2176 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.2174 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.2175 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.2176 [Training] 113/113 [==============================] 255.6ms/step  batch_loss: 0.2173 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:30:50 - INFO - root -   The F1-score is 0.6311300639658848
10/25/2022 04:30:50 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:11  batch_loss: 0.1991 [Training] 2/113 [..............................] - ETA: 49s  batch_loss: 0.1797 [Training] 3/113 [..............................] - ETA: 41s  batch_loss: 0.1953 [Training] 4/113 [>.............................] - ETA: 37s  batch_loss: 0.1913 [Training] 5/113 [>.............................] - ETA: 35s  batch_loss: 0.1924 [Training] 6/113 [>.............................] - ETA: 33s  batch_loss: 0.1911 [Training] 7/113 [>.............................] - ETA: 32s  batch_loss: 0.1897 [Training] 8/113 [=>............................] - ETA: 31s  batch_loss: 0.1914 [Training] 9/113 [=>............................] - ETA: 30s  batch_loss: 0.1965 [Training] 10/113 [=>............................] - ETA: 29s  batch_loss: 0.1942 [Training] 11/113 [=>............................] - ETA: 29s  batch_loss: 0.1954 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 0.1940 [Training] 13/113 [==>...........................] - ETA: 28s  batch_loss: 0.1916 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 0.1908 [Training] 15/113 [==>...........................] - ETA: 27s  batch_loss: 0.1882 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 0.1870 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.1871 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 0.1884 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.1883 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.1878 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 0.1866 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.1868 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.1875 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 0.1872 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.1885 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.1894 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 0.1904 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.1904 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1903 [Training] 30/113 [======>.......................] - ETA: 21s  batch_loss: 0.1897 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.1904 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1912 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1927 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.1940 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1946 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1949 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.1943 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.1946 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1958 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1968 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.1966 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1972 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1976 [Training] 44/113 [==========>...................] - ETA: 17s  batch_loss: 0.1984 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.1983 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1981 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1974 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.1968 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1966 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1961 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1969 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.1980 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1977 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1987 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.1995 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.2012 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.2013 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.2013 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 0.2014 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.2009 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.2027 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.2028 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.2025 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.2025 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.2024 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.2024 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.2022 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.2032 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.2027 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.2022 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.2023 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.2019 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.2016 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.2016 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.2015 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.2024 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.2022 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 0.2020 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.2019 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.2021 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.2020 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.2018 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.2022 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.2026 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.2029 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.2027 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.2026 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.2031 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.2027 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.2026 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.2032 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.2031 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.2033 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.2030 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.2033 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.2038 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.2039 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.2043 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.2045 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.2042 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.2040 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.2035 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.2032 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.2030 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.2027 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.2030 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.2030 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.2031 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.2030 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.2029 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.2033 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.2039 [Training] 113/113 [==============================] 255.7ms/step  batch_loss: 0.2039 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:31:25 - INFO - root -   The F1-score is 0.6438668490652075
10/25/2022 04:31:25 - INFO - root -   the best eval f1 is 0.6439, saving model !!
10/25/2022 04:31:27 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:10  batch_loss: 0.1850 [Training] 2/113 [..............................] - ETA: 49s  batch_loss: 0.1785 [Training] 3/113 [..............................] - ETA: 42s  batch_loss: 0.1741 [Training] 4/113 [>.............................] - ETA: 38s  batch_loss: 0.1660 [Training] 5/113 [>.............................] - ETA: 36s  batch_loss: 0.1728 [Training] 6/113 [>.............................] - ETA: 34s  batch_loss: 0.1784 [Training] 7/113 [>.............................] - ETA: 33s  batch_loss: 0.1831 [Training] 8/113 [=>............................] - ETA: 32s  batch_loss: 0.1833 [Training] 9/113 [=>............................] - ETA: 31s  batch_loss: 0.1831 [Training] 10/113 [=>............................] - ETA: 30s  batch_loss: 0.1822 [Training] 11/113 [=>............................] - ETA: 29s  batch_loss: 0.1803 [Training] 12/113 [==>...........................] - ETA: 29s  batch_loss: 0.1806 [Training] 13/113 [==>...........................] - ETA: 28s  batch_loss: 0.1800 [Training] 14/113 [==>...........................] - ETA: 28s  batch_loss: 0.1820 [Training] 15/113 [==>...........................] - ETA: 27s  batch_loss: 0.1857 [Training] 16/113 [===>..........................] - ETA: 27s  batch_loss: 0.1870 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.1875 [Training] 18/113 [===>..........................] - ETA: 26s  batch_loss: 0.1898 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.1913 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.1912 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.1899 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.1898 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.1892 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.1889 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.1895 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.1900 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.1897 [Training] 28/113 [======>.......................] - ETA: 23s  batch_loss: 0.1898 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1918 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.1906 [Training] 31/113 [=======>......................] - ETA: 22s  batch_loss: 0.1904 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1903 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1918 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.1918 [Training] 35/113 [========>.....................] - ETA: 21s  batch_loss: 0.1909 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1909 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.1908 [Training] 38/113 [=========>....................] - ETA: 20s  batch_loss: 0.1909 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1908 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1911 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.1912 [Training] 42/113 [==========>...................] - ETA: 19s  batch_loss: 0.1927 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1932 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.1931 [Training] 45/113 [==========>...................] - ETA: 18s  batch_loss: 0.1931 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1926 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1934 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.1937 [Training] 49/113 [============>.................] - ETA: 17s  batch_loss: 0.1933 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1928 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1934 [Training] 52/113 [============>.................] - ETA: 16s  batch_loss: 0.1936 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1942 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1937 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.1938 [Training] 56/113 [=============>................] - ETA: 15s  batch_loss: 0.1940 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1933 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1927 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.1925 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1933 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1934 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1934 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.1935 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1933 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1940 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1945 [Training] 67/113 [================>.............] - ETA: 12s  batch_loss: 0.1943 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1947 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1945 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1946 [Training] 71/113 [=================>............] - ETA: 11s  batch_loss: 0.1940 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1938 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1940 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.1939 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1944 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1943 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1944 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.1949 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1950 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1951 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1949 [Training] 82/113 [====================>.........] - ETA: 8s  batch_loss: 0.1948 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1951 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1953 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1953 [Training] 86/113 [=====================>........] - ETA: 7s  batch_loss: 0.1952 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1950 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1954 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1955 [Training] 90/113 [======================>.......] - ETA: 6s  batch_loss: 0.1955 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1957 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1958 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1962 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1962 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1963 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1959 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1955 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1953 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1948 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1946 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1945 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1937 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1933 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1929 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1930 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1930 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1924 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1923 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1921 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1922 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1923 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1926 [Training] 113/113 [==============================] 259.7ms/step  batch_loss: 0.1925 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:32:02 - INFO - root -   The F1-score is 0.6477444846888377
10/25/2022 04:32:02 - INFO - root -   the best eval f1 is 0.6477, saving model !!
10/25/2022 04:32:05 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:12  batch_loss: 0.2049 [Training] 2/113 [..............................] - ETA: 50s  batch_loss: 0.1971 [Training] 3/113 [..............................] - ETA: 42s  batch_loss: 0.1880 [Training] 4/113 [>.............................] - ETA: 38s  batch_loss: 0.1813 [Training] 5/113 [>.............................] - ETA: 36s  batch_loss: 0.1692 [Training] 6/113 [>.............................] - ETA: 34s  batch_loss: 0.1670 [Training] 7/113 [>.............................] - ETA: 33s  batch_loss: 0.1648 [Training] 8/113 [=>............................] - ETA: 32s  batch_loss: 0.1635 [Training] 9/113 [=>............................] - ETA: 31s  batch_loss: 0.1687 [Training] 10/113 [=>............................] - ETA: 30s  batch_loss: 0.1691 [Training] 11/113 [=>............................] - ETA: 30s  batch_loss: 0.1712 [Training] 12/113 [==>...........................] - ETA: 29s  batch_loss: 0.1710 [Training] 13/113 [==>...........................] - ETA: 28s  batch_loss: 0.1714 [Training] 14/113 [==>...........................] - ETA: 28s  batch_loss: 0.1708 [Training] 15/113 [==>...........................] - ETA: 27s  batch_loss: 0.1709 [Training] 16/113 [===>..........................] - ETA: 27s  batch_loss: 0.1768 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.1753 [Training] 18/113 [===>..........................] - ETA: 26s  batch_loss: 0.1755 [Training] 19/113 [====>.........................] - ETA: 26s  batch_loss: 0.1756 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.1760 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.1761 [Training] 22/113 [====>.........................] - ETA: 25s  batch_loss: 0.1756 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.1774 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.1776 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.1758 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.1745 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.1751 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.1757 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1755 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.1771 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.1776 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1801 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1798 [Training] 34/113 [========>.....................] - ETA: 21s  batch_loss: 0.1805 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1809 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1817 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.1806 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.1797 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1795 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1799 [Training] 41/113 [=========>....................] - ETA: 19s  batch_loss: 0.1796 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1793 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1792 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.1794 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.1801 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1799 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1790 [Training] 48/113 [===========>..................] - ETA: 17s  batch_loss: 0.1795 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1795 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1797 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1802 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.1804 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1796 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1794 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.1795 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.1791 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1792 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1786 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.1785 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1811 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1811 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1814 [Training] 63/113 [===============>..............] - ETA: 13s  batch_loss: 0.1811 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1812 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1810 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1811 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.1811 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1810 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1807 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1804 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1802 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1806 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1813 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.1807 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1809 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1809 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1809 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.1814 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1808 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1807 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1810 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.1810 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1811 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1818 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1820 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1823 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1822 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1824 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1824 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1826 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1828 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1830 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1835 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1836 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1836 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1835 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1833 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1838 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1831 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1833 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1829 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1827 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1825 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1828 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1825 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1828 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1830 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1827 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1832 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1835 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1833 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1833 [Training] 113/113 [==============================] 257.6ms/step  batch_loss: 0.1836 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:32:40 - INFO - root -   The F1-score is 0.6439215686274509
10/25/2022 04:32:40 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:09  batch_loss: 0.1791 [Training] 2/113 [..............................] - ETA: 48s  batch_loss: 0.1833 [Training] 3/113 [..............................] - ETA: 41s  batch_loss: 0.1803 [Training] 4/113 [>.............................] - ETA: 37s  batch_loss: 0.1685 [Training] 5/113 [>.............................] - ETA: 35s  batch_loss: 0.1664 [Training] 6/113 [>.............................] - ETA: 33s  batch_loss: 0.1687 [Training] 7/113 [>.............................] - ETA: 32s  batch_loss: 0.1763 [Training] 8/113 [=>............................] - ETA: 31s  batch_loss: 0.1718 [Training] 9/113 [=>............................] - ETA: 30s  batch_loss: 0.1691 [Training] 10/113 [=>............................] - ETA: 29s  batch_loss: 0.1671 [Training] 11/113 [=>............................] - ETA: 29s  batch_loss: 0.1697 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 0.1687 [Training] 13/113 [==>...........................] - ETA: 27s  batch_loss: 0.1685 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 0.1671 [Training] 15/113 [==>...........................] - ETA: 27s  batch_loss: 0.1680 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 0.1685 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.1688 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 0.1686 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.1696 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.1698 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 0.1735 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.1726 [Training] 23/113 [=====>........................] - ETA: 23s  batch_loss: 0.1723 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 0.1742 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.1730 [Training] 26/113 [=====>........................] - ETA: 22s  batch_loss: 0.1714 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 0.1706 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.1698 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1684 [Training] 30/113 [======>.......................] - ETA: 21s  batch_loss: 0.1689 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.1674 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1685 [Training] 33/113 [=======>......................] - ETA: 20s  batch_loss: 0.1686 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.1686 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1680 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1678 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.1685 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.1681 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1684 [Training] 40/113 [=========>....................] - ETA: 18s  batch_loss: 0.1674 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.1670 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1673 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1667 [Training] 44/113 [==========>...................] - ETA: 17s  batch_loss: 0.1671 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.1666 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1662 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1676 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.1673 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1673 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1669 [Training] 51/113 [============>.................] - ETA: 15s  batch_loss: 0.1663 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.1659 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1651 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1651 [Training] 55/113 [=============>................] - ETA: 14s  batch_loss: 0.1645 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.1646 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1650 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1646 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 0.1651 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1667 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1666 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1666 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.1670 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1670 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1678 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1680 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.1686 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1681 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1690 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1688 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1688 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1688 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1684 [Training] 74/113 [==================>...........] - ETA: 9s  batch_loss: 0.1688 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1691 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1691 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1691 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 0.1684 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1690 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1687 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1687 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.1690 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1695 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1693 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1696 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1696 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1704 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1705 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1707 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1710 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1705 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1713 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1713 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1708 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1709 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1714 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1710 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1712 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1710 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1710 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1714 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1717 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1717 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1719 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1722 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1723 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1723 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1725 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1727 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1730 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1734 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1734 [Training] 113/113 [==============================] 254.9ms/step  batch_loss: 0.1734 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:33:15 - INFO - root -   The F1-score is 0.6470471529660736
10/25/2022 04:33:15 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:10  batch_loss: 0.1203 [Training] 2/113 [..............................] - ETA: 49s  batch_loss: 0.1209 [Training] 3/113 [..............................] - ETA: 42s  batch_loss: 0.1388 [Training] 4/113 [>.............................] - ETA: 38s  batch_loss: 0.1508 [Training] 5/113 [>.............................] - ETA: 35s  batch_loss: 0.1560 [Training] 6/113 [>.............................] - ETA: 34s  batch_loss: 0.1566 [Training] 7/113 [>.............................] - ETA: 32s  batch_loss: 0.1555 [Training] 8/113 [=>............................] - ETA: 31s  batch_loss: 0.1585 [Training] 9/113 [=>............................] - ETA: 30s  batch_loss: 0.1597 [Training] 10/113 [=>............................] - ETA: 29s  batch_loss: 0.1580 [Training] 11/113 [=>............................] - ETA: 29s  batch_loss: 0.1595 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 0.1591 [Training] 13/113 [==>...........................] - ETA: 28s  batch_loss: 0.1643 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 0.1658 [Training] 15/113 [==>...........................] - ETA: 27s  batch_loss: 0.1654 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 0.1646 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.1636 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 0.1609 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.1605 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.1606 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 0.1595 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.1596 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.1593 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 0.1618 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.1601 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.1611 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 0.1633 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.1637 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1633 [Training] 30/113 [======>.......................] - ETA: 21s  batch_loss: 0.1636 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.1651 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1646 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1643 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.1632 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1627 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1616 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.1618 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.1614 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1625 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1636 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.1643 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1645 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1644 [Training] 44/113 [==========>...................] - ETA: 17s  batch_loss: 0.1639 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.1633 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1634 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1630 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.1626 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1635 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1635 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1635 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.1633 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1622 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1616 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.1620 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.1616 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1612 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1613 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 0.1613 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1607 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1604 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1606 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.1602 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1606 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1601 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1598 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.1595 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1599 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1601 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1603 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1612 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1616 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1615 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.1614 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1617 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1620 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1624 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 0.1627 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1633 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1632 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1634 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.1638 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1639 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1642 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1638 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1637 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1636 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1642 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1649 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1650 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1649 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1643 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1638 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1644 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1647 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1643 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1641 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1645 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1647 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1647 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1645 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1642 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1642 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1639 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1638 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1637 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1638 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1638 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1638 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1636 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1638 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1634 [Training] 113/113 [==============================] 255.8ms/step  batch_loss: 0.1635 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:33:50 - INFO - root -   The F1-score is 0.6409670042469781
10/25/2022 04:33:50 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:16  batch_loss: 0.1609 [Training] 2/113 [..............................] - ETA: 51s  batch_loss: 0.1524 [Training] 3/113 [..............................] - ETA: 43s  batch_loss: 0.1470 [Training] 4/113 [>.............................] - ETA: 39s  batch_loss: 0.1500 [Training] 5/113 [>.............................] - ETA: 36s  batch_loss: 0.1419 [Training] 6/113 [>.............................] - ETA: 34s  batch_loss: 0.1409 [Training] 7/113 [>.............................] - ETA: 33s  batch_loss: 0.1431 [Training] 8/113 [=>............................] - ETA: 32s  batch_loss: 0.1465 [Training] 9/113 [=>............................] - ETA: 31s  batch_loss: 0.1458 [Training] 10/113 [=>............................] - ETA: 30s  batch_loss: 0.1449 [Training] 11/113 [=>............................] - ETA: 29s  batch_loss: 0.1437 [Training] 12/113 [==>...........................] - ETA: 29s  batch_loss: 0.1476 [Training] 13/113 [==>...........................] - ETA: 28s  batch_loss: 0.1524 [Training] 14/113 [==>...........................] - ETA: 28s  batch_loss: 0.1529 [Training] 15/113 [==>...........................] - ETA: 27s  batch_loss: 0.1547 [Training] 16/113 [===>..........................] - ETA: 27s  batch_loss: 0.1556 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.1551 [Training] 18/113 [===>..........................] - ETA: 26s  batch_loss: 0.1552 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.1571 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.1562 [Training] 21/113 [====>.........................] - ETA: 25s  batch_loss: 0.1559 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.1549 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.1533 [Training] 24/113 [=====>........................] - ETA: 24s  batch_loss: 0.1538 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.1537 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.1551 [Training] 27/113 [======>.......................] - ETA: 23s  batch_loss: 0.1564 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.1570 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1556 [Training] 30/113 [======>.......................] - ETA: 22s  batch_loss: 0.1556 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.1539 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1546 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1537 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.1542 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1539 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1537 [Training] 37/113 [========>.....................] - ETA: 20s  batch_loss: 0.1524 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.1516 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1524 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1546 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.1575 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1573 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1573 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.1580 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.1582 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1592 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1590 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.1595 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1591 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1588 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1592 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.1588 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1584 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1582 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.1582 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.1584 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1577 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1579 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.1579 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1576 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1577 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1577 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.1569 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1572 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1571 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1571 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.1567 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1565 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1566 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1564 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1567 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1563 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1571 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.1573 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1573 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1573 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1569 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.1570 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1572 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1573 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1575 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.1570 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1563 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1568 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1570 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1568 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1570 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1568 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1568 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1573 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1577 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1578 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1580 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1575 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1572 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1570 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1569 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1570 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1566 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1564 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1572 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1568 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1567 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1562 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1567 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1564 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1564 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1566 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1566 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1563 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1563 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1571 [Training] 113/113 [==============================] 258.0ms/step  batch_loss: 0.1570 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:34:25 - INFO - root -   The F1-score is 0.6494629290018338
10/25/2022 04:34:25 - INFO - root -   the best eval f1 is 0.6495, saving model !!
10/25/2022 04:34:28 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:12  batch_loss: 0.1364 [Training] 2/113 [..............................] - ETA: 50s  batch_loss: 0.1312 [Training] 3/113 [..............................] - ETA: 43s  batch_loss: 0.1481 [Training] 4/113 [>.............................] - ETA: 39s  batch_loss: 0.1381 [Training] 5/113 [>.............................] - ETA: 36s  batch_loss: 0.1511 [Training] 6/113 [>.............................] - ETA: 34s  batch_loss: 0.1539 [Training] 7/113 [>.............................] - ETA: 33s  batch_loss: 0.1543 [Training] 8/113 [=>............................] - ETA: 32s  batch_loss: 0.1519 [Training] 9/113 [=>............................] - ETA: 31s  batch_loss: 0.1507 [Training] 10/113 [=>............................] - ETA: 30s  batch_loss: 0.1454 [Training] 11/113 [=>............................] - ETA: 29s  batch_loss: 0.1465 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 0.1433 [Training] 13/113 [==>...........................] - ETA: 28s  batch_loss: 0.1454 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 0.1424 [Training] 15/113 [==>...........................] - ETA: 27s  batch_loss: 0.1424 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 0.1434 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.1456 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 0.1468 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.1476 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.1464 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 0.1448 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.1463 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.1447 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 0.1439 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.1427 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.1430 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 0.1436 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.1433 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1432 [Training] 30/113 [======>.......................] - ETA: 21s  batch_loss: 0.1430 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.1429 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1429 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1436 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.1449 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1449 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1451 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.1454 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.1460 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1457 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1450 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.1450 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1446 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1454 [Training] 44/113 [==========>...................] - ETA: 18s  batch_loss: 0.1467 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.1464 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1465 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1467 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.1471 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1475 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1491 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1493 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.1493 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1487 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1485 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.1493 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.1497 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1495 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1494 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 0.1499 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1498 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1508 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1514 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.1512 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1510 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1503 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1501 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.1506 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1500 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1497 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1498 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1496 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1497 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1495 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.1492 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1491 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1493 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1498 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 0.1501 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1505 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1505 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1507 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.1507 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1506 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1508 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1507 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1509 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1508 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1511 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1513 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1514 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1511 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1513 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1510 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1507 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1506 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1506 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1507 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1510 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1508 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1509 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1510 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1512 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1510 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1516 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1516 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1514 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1515 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1512 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1510 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1513 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1510 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1511 [Training] 113/113 [==============================] 255.0ms/step  batch_loss: 0.1510 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:35:03 - INFO - root -   The F1-score is 0.6491468915473623
10/25/2022 04:35:03 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:11  batch_loss: 0.1322 [Training] 2/113 [..............................] - ETA: 49s  batch_loss: 0.1153 [Training] 3/113 [..............................] - ETA: 41s  batch_loss: 0.1474 [Training] 4/113 [>.............................] - ETA: 37s  batch_loss: 0.1398 [Training] 5/113 [>.............................] - ETA: 35s  batch_loss: 0.1373 [Training] 6/113 [>.............................] - ETA: 33s  batch_loss: 0.1344 [Training] 7/113 [>.............................] - ETA: 32s  batch_loss: 0.1376 [Training] 8/113 [=>............................] - ETA: 31s  batch_loss: 0.1384 [Training] 9/113 [=>............................] - ETA: 30s  batch_loss: 0.1375 [Training] 10/113 [=>............................] - ETA: 29s  batch_loss: 0.1390 [Training] 11/113 [=>............................] - ETA: 29s  batch_loss: 0.1367 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 0.1360 [Training] 13/113 [==>...........................] - ETA: 27s  batch_loss: 0.1335 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 0.1350 [Training] 15/113 [==>...........................] - ETA: 27s  batch_loss: 0.1349 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 0.1340 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.1334 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 0.1333 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.1332 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.1335 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 0.1336 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.1357 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.1355 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 0.1364 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.1372 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.1369 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 0.1382 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.1378 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1378 [Training] 30/113 [======>.......................] - ETA: 21s  batch_loss: 0.1374 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.1373 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1373 [Training] 33/113 [=======>......................] - ETA: 20s  batch_loss: 0.1368 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.1364 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1354 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1352 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.1355 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.1359 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1363 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1376 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.1371 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1378 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1380 [Training] 44/113 [==========>...................] - ETA: 17s  batch_loss: 0.1384 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.1389 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1395 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1400 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.1403 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1403 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1404 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1404 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.1403 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1400 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1398 [Training] 55/113 [=============>................] - ETA: 14s  batch_loss: 0.1397 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.1395 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1407 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1409 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 0.1406 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1411 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1416 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1415 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.1416 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1420 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1419 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1418 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.1417 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1422 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1422 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1429 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1428 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1424 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1428 [Training] 74/113 [==================>...........] - ETA: 9s  batch_loss: 0.1423 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1419 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1419 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1418 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 0.1418 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1416 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1414 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1412 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.1411 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1409 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1411 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1411 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1418 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1418 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1417 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1414 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1424 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1424 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1423 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1425 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1420 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1421 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1421 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1430 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1430 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1432 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1432 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1439 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1443 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1442 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1442 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1449 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1449 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1447 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1452 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1452 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1447 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1446 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1443 [Training] 113/113 [==============================] 255.1ms/step  batch_loss: 0.1444 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:35:37 - INFO - root -   The F1-score is 0.6439954657598187
10/25/2022 04:35:37 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:07  batch_loss: 0.1402 [Training] 2/113 [..............................] - ETA: 48s  batch_loss: 0.1110 [Training] 3/113 [..............................] - ETA: 41s  batch_loss: 0.1168 [Training] 4/113 [>.............................] - ETA: 37s  batch_loss: 0.1406 [Training] 5/113 [>.............................] - ETA: 35s  batch_loss: 0.1332 [Training] 6/113 [>.............................] - ETA: 33s  batch_loss: 0.1292 [Training] 7/113 [>.............................] - ETA: 32s  batch_loss: 0.1298 [Training] 8/113 [=>............................] - ETA: 31s  batch_loss: 0.1278 [Training] 9/113 [=>............................] - ETA: 30s  batch_loss: 0.1258 [Training] 10/113 [=>............................] - ETA: 29s  batch_loss: 0.1313 [Training] 11/113 [=>............................] - ETA: 29s  batch_loss: 0.1289 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 0.1312 [Training] 13/113 [==>...........................] - ETA: 27s  batch_loss: 0.1304 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 0.1307 [Training] 15/113 [==>...........................] - ETA: 26s  batch_loss: 0.1295 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 0.1286 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.1295 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 0.1296 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.1293 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.1298 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 0.1300 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.1310 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.1315 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 0.1315 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.1312 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.1307 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 0.1294 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.1297 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1303 [Training] 30/113 [======>.......................] - ETA: 21s  batch_loss: 0.1312 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.1309 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1302 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1303 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.1311 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1319 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1324 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.1325 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.1321 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1327 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1326 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.1333 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1331 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1323 [Training] 44/113 [==========>...................] - ETA: 17s  batch_loss: 0.1320 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.1338 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1333 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1342 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.1346 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1350 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1356 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1351 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.1355 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1365 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1361 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.1364 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.1372 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1369 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1371 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 0.1367 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1364 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1365 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1363 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.1367 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1363 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1363 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1367 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.1364 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1362 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1364 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1365 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1368 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1369 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1375 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.1379 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1378 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1379 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1379 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 0.1382 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1384 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1385 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1384 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.1381 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1386 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1388 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1395 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1397 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1394 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1404 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1399 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1402 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1401 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1399 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1400 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1400 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1402 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1406 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1401 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1402 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1401 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1400 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1398 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1397 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1401 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1401 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1404 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1404 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1401 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1401 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1401 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1399 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1395 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1395 [Training] 113/113 [==============================] 255.6ms/step  batch_loss: 0.1398 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:36:13 - INFO - root -   The F1-score is 0.6445182724252492
10/25/2022 04:36:13 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:02  batch_loss: 0.1439 [Training] 2/113 [..............................] - ETA: 45s  batch_loss: 0.1421 [Training] 3/113 [..............................] - ETA: 39s  batch_loss: 0.1353 [Training] 4/113 [>.............................] - ETA: 35s  batch_loss: 0.1418 [Training] 5/113 [>.............................] - ETA: 33s  batch_loss: 0.1432 [Training] 6/113 [>.............................] - ETA: 32s  batch_loss: 0.1381 [Training] 7/113 [>.............................] - ETA: 31s  batch_loss: 0.1389 [Training] 8/113 [=>............................] - ETA: 30s  batch_loss: 0.1384 [Training] 9/113 [=>............................] - ETA: 29s  batch_loss: 0.1364 [Training] 10/113 [=>............................] - ETA: 29s  batch_loss: 0.1323 [Training] 11/113 [=>............................] - ETA: 28s  batch_loss: 0.1296 [Training] 12/113 [==>...........................] - ETA: 27s  batch_loss: 0.1330 [Training] 13/113 [==>...........................] - ETA: 27s  batch_loss: 0.1316 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 0.1321 [Training] 15/113 [==>...........................] - ETA: 26s  batch_loss: 0.1326 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 0.1359 [Training] 17/113 [===>..........................] - ETA: 25s  batch_loss: 0.1362 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 0.1354 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.1359 [Training] 20/113 [====>.........................] - ETA: 24s  batch_loss: 0.1361 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 0.1350 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.1361 [Training] 23/113 [=====>........................] - ETA: 23s  batch_loss: 0.1347 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 0.1351 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.1342 [Training] 26/113 [=====>........................] - ETA: 22s  batch_loss: 0.1339 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 0.1350 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.1369 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1361 [Training] 30/113 [======>.......................] - ETA: 21s  batch_loss: 0.1378 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.1379 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1394 [Training] 33/113 [=======>......................] - ETA: 20s  batch_loss: 0.1395 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.1395 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1405 [Training] 36/113 [========>.....................] - ETA: 19s  batch_loss: 0.1400 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.1395 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.1392 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1390 [Training] 40/113 [=========>....................] - ETA: 18s  batch_loss: 0.1381 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.1378 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1380 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1395 [Training] 44/113 [==========>...................] - ETA: 17s  batch_loss: 0.1393 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.1389 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1383 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1374 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.1372 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1379 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1373 [Training] 51/113 [============>.................] - ETA: 15s  batch_loss: 0.1375 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.1371 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1363 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1369 [Training] 55/113 [=============>................] - ETA: 14s  batch_loss: 0.1365 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.1357 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1362 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1360 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 0.1362 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1354 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1350 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1355 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.1354 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1354 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1358 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1355 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.1354 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1363 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1362 [Training] 70/113 [=================>............] - ETA: 10s  batch_loss: 0.1361 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1354 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1355 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1355 [Training] 74/113 [==================>...........] - ETA: 9s  batch_loss: 0.1352 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1352 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1353 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1352 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 0.1347 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1347 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1342 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1340 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.1339 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1337 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1337 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1335 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1337 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1336 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1335 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1338 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1333 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1331 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1333 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1334 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1334 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1335 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1334 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1339 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1340 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1340 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1340 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1337 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1338 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1336 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1335 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1338 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1337 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1336 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1339 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1337 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1339 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1338 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1338 [Training] 113/113 [==============================] 254.6ms/step  batch_loss: 0.1339 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:36:48 - INFO - root -   The F1-score is 0.6473746235432762
10/25/2022 04:36:48 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:08  batch_loss: 0.1297 [Training] 2/113 [..............................] - ETA: 47s  batch_loss: 0.1483 [Training] 3/113 [..............................] - ETA: 41s  batch_loss: 0.1311 [Training] 4/113 [>.............................] - ETA: 37s  batch_loss: 0.1276 [Training] 5/113 [>.............................] - ETA: 35s  batch_loss: 0.1216 [Training] 6/113 [>.............................] - ETA: 33s  batch_loss: 0.1186 [Training] 7/113 [>.............................] - ETA: 32s  batch_loss: 0.1186 [Training] 8/113 [=>............................] - ETA: 31s  batch_loss: 0.1182 [Training] 9/113 [=>............................] - ETA: 30s  batch_loss: 0.1160 [Training] 10/113 [=>............................] - ETA: 29s  batch_loss: 0.1145 [Training] 11/113 [=>............................] - ETA: 29s  batch_loss: 0.1154 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 0.1142 [Training] 13/113 [==>...........................] - ETA: 27s  batch_loss: 0.1200 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 0.1193 [Training] 15/113 [==>...........................] - ETA: 27s  batch_loss: 0.1186 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 0.1191 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.1175 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 0.1186 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.1211 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.1221 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 0.1220 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.1233 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.1223 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 0.1224 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.1237 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.1247 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 0.1246 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.1242 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1245 [Training] 30/113 [======>.......................] - ETA: 21s  batch_loss: 0.1246 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.1238 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1247 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1243 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.1232 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1228 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1217 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.1214 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.1210 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1212 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1204 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.1207 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1216 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1223 [Training] 44/113 [==========>...................] - ETA: 17s  batch_loss: 0.1227 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.1231 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1236 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1242 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.1237 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1244 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1246 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1251 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.1250 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1249 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1246 [Training] 55/113 [=============>................] - ETA: 15s  batch_loss: 0.1247 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.1245 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1246 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1252 [Training] 59/113 [==============>...............] - ETA: 14s  batch_loss: 0.1250 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1254 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1252 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1257 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.1257 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1259 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1254 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1252 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.1257 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1259 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1260 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1256 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1253 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1254 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1252 [Training] 74/113 [==================>...........] - ETA: 10s  batch_loss: 0.1257 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1263 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1268 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1266 [Training] 78/113 [===================>..........] - ETA: 9s  batch_loss: 0.1264 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1261 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1264 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1273 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.1282 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1279 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1280 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1283 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1283 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1279 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1279 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1278 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1278 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1280 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1279 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1282 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1279 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1280 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1280 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1279 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1280 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1282 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1284 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1281 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1278 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1282 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1282 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1282 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1282 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1282 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1282 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1296 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1299 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1299 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1298 [Training] 113/113 [==============================] 255.5ms/step  batch_loss: 0.1304 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:37:23 - INFO - root -   The F1-score is 0.6495168451292765
10/25/2022 04:37:23 - INFO - root -   the best eval f1 is 0.6495, saving model !!
10/25/2022 04:37:25 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/113 [..............................] - ETA: 1:11  batch_loss: 0.1253 [Training] 2/113 [..............................] - ETA: 49s  batch_loss: 0.1246 [Training] 3/113 [..............................] - ETA: 42s  batch_loss: 0.1197 [Training] 4/113 [>.............................] - ETA: 38s  batch_loss: 0.1159 [Training] 5/113 [>.............................] - ETA: 35s  batch_loss: 0.1199 [Training] 6/113 [>.............................] - ETA: 33s  batch_loss: 0.1217 [Training] 7/113 [>.............................] - ETA: 32s  batch_loss: 0.1179 [Training] 8/113 [=>............................] - ETA: 31s  batch_loss: 0.1211 [Training] 9/113 [=>............................] - ETA: 30s  batch_loss: 0.1204 [Training] 10/113 [=>............................] - ETA: 29s  batch_loss: 0.1185 [Training] 11/113 [=>............................] - ETA: 29s  batch_loss: 0.1187 [Training] 12/113 [==>...........................] - ETA: 28s  batch_loss: 0.1179 [Training] 13/113 [==>...........................] - ETA: 28s  batch_loss: 0.1189 [Training] 14/113 [==>...........................] - ETA: 27s  batch_loss: 0.1205 [Training] 15/113 [==>...........................] - ETA: 27s  batch_loss: 0.1175 [Training] 16/113 [===>..........................] - ETA: 26s  batch_loss: 0.1189 [Training] 17/113 [===>..........................] - ETA: 26s  batch_loss: 0.1178 [Training] 18/113 [===>..........................] - ETA: 25s  batch_loss: 0.1191 [Training] 19/113 [====>.........................] - ETA: 25s  batch_loss: 0.1176 [Training] 20/113 [====>.........................] - ETA: 25s  batch_loss: 0.1169 [Training] 21/113 [====>.........................] - ETA: 24s  batch_loss: 0.1183 [Training] 22/113 [====>.........................] - ETA: 24s  batch_loss: 0.1173 [Training] 23/113 [=====>........................] - ETA: 24s  batch_loss: 0.1178 [Training] 24/113 [=====>........................] - ETA: 23s  batch_loss: 0.1210 [Training] 25/113 [=====>........................] - ETA: 23s  batch_loss: 0.1212 [Training] 26/113 [=====>........................] - ETA: 23s  batch_loss: 0.1225 [Training] 27/113 [======>.......................] - ETA: 22s  batch_loss: 0.1215 [Training] 28/113 [======>.......................] - ETA: 22s  batch_loss: 0.1206 [Training] 29/113 [======>.......................] - ETA: 22s  batch_loss: 0.1221 [Training] 30/113 [======>.......................] - ETA: 21s  batch_loss: 0.1227 [Training] 31/113 [=======>......................] - ETA: 21s  batch_loss: 0.1233 [Training] 32/113 [=======>......................] - ETA: 21s  batch_loss: 0.1226 [Training] 33/113 [=======>......................] - ETA: 21s  batch_loss: 0.1234 [Training] 34/113 [========>.....................] - ETA: 20s  batch_loss: 0.1237 [Training] 35/113 [========>.....................] - ETA: 20s  batch_loss: 0.1249 [Training] 36/113 [========>.....................] - ETA: 20s  batch_loss: 0.1248 [Training] 37/113 [========>.....................] - ETA: 19s  batch_loss: 0.1248 [Training] 38/113 [=========>....................] - ETA: 19s  batch_loss: 0.1244 [Training] 39/113 [=========>....................] - ETA: 19s  batch_loss: 0.1241 [Training] 40/113 [=========>....................] - ETA: 19s  batch_loss: 0.1243 [Training] 41/113 [=========>....................] - ETA: 18s  batch_loss: 0.1248 [Training] 42/113 [==========>...................] - ETA: 18s  batch_loss: 0.1243 [Training] 43/113 [==========>...................] - ETA: 18s  batch_loss: 0.1237 [Training] 44/113 [==========>...................] - ETA: 17s  batch_loss: 0.1236 [Training] 45/113 [==========>...................] - ETA: 17s  batch_loss: 0.1232 [Training] 46/113 [===========>..................] - ETA: 17s  batch_loss: 0.1233 [Training] 47/113 [===========>..................] - ETA: 17s  batch_loss: 0.1235 [Training] 48/113 [===========>..................] - ETA: 16s  batch_loss: 0.1235 [Training] 49/113 [============>.................] - ETA: 16s  batch_loss: 0.1236 [Training] 50/113 [============>.................] - ETA: 16s  batch_loss: 0.1234 [Training] 51/113 [============>.................] - ETA: 16s  batch_loss: 0.1238 [Training] 52/113 [============>.................] - ETA: 15s  batch_loss: 0.1234 [Training] 53/113 [=============>................] - ETA: 15s  batch_loss: 0.1237 [Training] 54/113 [=============>................] - ETA: 15s  batch_loss: 0.1236 [Training] 55/113 [=============>................] - ETA: 14s  batch_loss: 0.1235 [Training] 56/113 [=============>................] - ETA: 14s  batch_loss: 0.1242 [Training] 57/113 [==============>...............] - ETA: 14s  batch_loss: 0.1244 [Training] 58/113 [==============>...............] - ETA: 14s  batch_loss: 0.1241 [Training] 59/113 [==============>...............] - ETA: 13s  batch_loss: 0.1261 [Training] 60/113 [==============>...............] - ETA: 13s  batch_loss: 0.1255 [Training] 61/113 [===============>..............] - ETA: 13s  batch_loss: 0.1253 [Training] 62/113 [===============>..............] - ETA: 13s  batch_loss: 0.1254 [Training] 63/113 [===============>..............] - ETA: 12s  batch_loss: 0.1253 [Training] 64/113 [===============>..............] - ETA: 12s  batch_loss: 0.1248 [Training] 65/113 [================>.............] - ETA: 12s  batch_loss: 0.1253 [Training] 66/113 [================>.............] - ETA: 12s  batch_loss: 0.1260 [Training] 67/113 [================>.............] - ETA: 11s  batch_loss: 0.1256 [Training] 68/113 [=================>............] - ETA: 11s  batch_loss: 0.1261 [Training] 69/113 [=================>............] - ETA: 11s  batch_loss: 0.1255 [Training] 70/113 [=================>............] - ETA: 11s  batch_loss: 0.1252 [Training] 71/113 [=================>............] - ETA: 10s  batch_loss: 0.1249 [Training] 72/113 [==================>...........] - ETA: 10s  batch_loss: 0.1247 [Training] 73/113 [==================>...........] - ETA: 10s  batch_loss: 0.1249 [Training] 74/113 [==================>...........] - ETA: 9s  batch_loss: 0.1243 [Training] 75/113 [==================>...........] - ETA: 9s  batch_loss: 0.1247 [Training] 76/113 [===================>..........] - ETA: 9s  batch_loss: 0.1262 [Training] 77/113 [===================>..........] - ETA: 9s  batch_loss: 0.1262 [Training] 78/113 [===================>..........] - ETA: 8s  batch_loss: 0.1265 [Training] 79/113 [===================>..........] - ETA: 8s  batch_loss: 0.1268 [Training] 80/113 [====================>.........] - ETA: 8s  batch_loss: 0.1268 [Training] 81/113 [====================>.........] - ETA: 8s  batch_loss: 0.1271 [Training] 82/113 [====================>.........] - ETA: 7s  batch_loss: 0.1265 [Training] 83/113 [=====================>........] - ETA: 7s  batch_loss: 0.1263 [Training] 84/113 [=====================>........] - ETA: 7s  batch_loss: 0.1258 [Training] 85/113 [=====================>........] - ETA: 7s  batch_loss: 0.1258 [Training] 86/113 [=====================>........] - ETA: 6s  batch_loss: 0.1254 [Training] 87/113 [======================>.......] - ETA: 6s  batch_loss: 0.1253 [Training] 88/113 [======================>.......] - ETA: 6s  batch_loss: 0.1255 [Training] 89/113 [======================>.......] - ETA: 6s  batch_loss: 0.1253 [Training] 90/113 [======================>.......] - ETA: 5s  batch_loss: 0.1253 [Training] 91/113 [=======================>......] - ETA: 5s  batch_loss: 0.1248 [Training] 92/113 [=======================>......] - ETA: 5s  batch_loss: 0.1249 [Training] 93/113 [=======================>......] - ETA: 5s  batch_loss: 0.1248 [Training] 94/113 [=======================>......] - ETA: 4s  batch_loss: 0.1247 [Training] 95/113 [========================>.....] - ETA: 4s  batch_loss: 0.1247 [Training] 96/113 [========================>.....] - ETA: 4s  batch_loss: 0.1246 [Training] 97/113 [========================>.....] - ETA: 4s  batch_loss: 0.1248 [Training] 98/113 [=========================>....] - ETA: 3s  batch_loss: 0.1245 [Training] 99/113 [=========================>....] - ETA: 3s  batch_loss: 0.1243 [Training] 100/113 [=========================>....] - ETA: 3s  batch_loss: 0.1245 [Training] 101/113 [=========================>....] - ETA: 3s  batch_loss: 0.1242 [Training] 102/113 [==========================>...] - ETA: 2s  batch_loss: 0.1243 [Training] 103/113 [==========================>...] - ETA: 2s  batch_loss: 0.1240 [Training] 104/113 [==========================>...] - ETA: 2s  batch_loss: 0.1241 [Training] 105/113 [==========================>...] - ETA: 2s  batch_loss: 0.1244 [Training] 106/113 [===========================>..] - ETA: 1s  batch_loss: 0.1244 [Training] 107/113 [===========================>..] - ETA: 1s  batch_loss: 0.1247 [Training] 108/113 [===========================>..] - ETA: 1s  batch_loss: 0.1245 [Training] 109/113 [===========================>..] - ETA: 1s  batch_loss: 0.1244 [Training] 110/113 [============================>.] - ETA: 0s  batch_loss: 0.1244 [Training] 111/113 [============================>.] - ETA: 0s  batch_loss: 0.1247 [Training] 112/113 [============================>.] - ETA: 0s  batch_loss: 0.1247 [Training] 113/113 [==============================] 254.7ms/step  batch_loss: 0.1249 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
10/25/2022 04:38:00 - INFO - root -   The F1-score is 0.6582616236408512
10/25/2022 04:38:00 - INFO - root -   the best eval f1 is 0.6583, saving model !!
********** predict start ***********
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 8, 
    "event_type": "role", 
    "fine_tunning_model_path": "./output/DuEE1.0/role/best_model.pkl", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 0.001, 
    "max_len": 250, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 50.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 128, 
    "per_gpu_train_batch_size": 8, 
    "predict_save_path": "./output/DuEE1.0/role/test_result.json", 
    "seed": 66, 
    "stride": 100, 
    "test_json": "./data/DuEE1.0/subor_news.json", 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/147540 [00:00<?, ?it/s]tokenizing...:   0%|          | 162/147540 [00:00<01:31, 1611.61it/s]tokenizing...:   0%|          | 328/147540 [00:00<01:29, 1637.85it/s]tokenizing...:   0%|          | 547/147540 [00:00<01:17, 1887.89it/s]tokenizing...:   1%|          | 808/147540 [00:00<01:09, 2116.17it/s]tokenizing...:   1%|          | 1047/147540 [00:00<01:06, 2212.35it/s]tokenizing...:   1%|          | 1268/147540 [00:00<01:12, 2010.62it/s]tokenizing...:   1%|          | 1514/147540 [00:00<01:08, 2147.02it/s]tokenizing...:   1%|          | 1770/147540 [00:00<01:04, 2269.98it/s]tokenizing...:   1%|▏         | 2000/147540 [00:01<02:41, 902.95it/s] tokenizing...:   2%|▏         | 2227/147540 [00:01<02:11, 1101.44it/s]tokenizing...:   2%|▏         | 2456/147540 [00:01<01:51, 1305.79it/s]tokenizing...:   2%|▏         | 2667/147540 [00:01<01:39, 1462.92it/s]tokenizing...:   2%|▏         | 2891/147540 [00:01<01:28, 1632.76it/s]tokenizing...:   2%|▏         | 3107/147540 [00:01<01:22, 1753.89it/s]tokenizing...:   2%|▏         | 3340/147540 [00:02<01:15, 1899.67it/s]tokenizing...:   2%|▏         | 3582/147540 [00:02<01:10, 2035.32it/s]tokenizing...:   3%|▎         | 3824/147540 [00:02<01:07, 2138.48it/s]tokenizing...:   3%|▎         | 4054/147540 [00:02<01:06, 2149.34it/s]tokenizing...:   3%|▎         | 4280/147540 [00:02<01:06, 2160.25it/s]tokenizing...:   3%|▎         | 4509/147540 [00:02<01:05, 2196.62it/s]tokenizing...:   3%|▎         | 4735/147540 [00:02<01:07, 2118.73it/s]tokenizing...:   3%|▎         | 4962/147540 [00:02<01:05, 2160.84it/s]tokenizing...:   4%|▎         | 5182/147540 [00:02<01:10, 2010.39it/s]tokenizing...:   4%|▎         | 5388/147540 [00:03<01:13, 1944.89it/s]tokenizing...:   4%|▍         | 5586/147540 [00:03<01:15, 1888.41it/s]tokenizing...:   4%|▍         | 5777/147540 [00:03<01:15, 1871.54it/s]tokenizing...:   4%|▍         | 5966/147540 [00:03<01:16, 1846.20it/s]tokenizing...:   4%|▍         | 6152/147540 [00:03<01:18, 1799.78it/s]tokenizing...:   4%|▍         | 6333/147540 [00:03<01:18, 1801.16it/s]tokenizing...:   4%|▍         | 6515/147540 [00:03<01:18, 1805.70it/s]tokenizing...:   5%|▍         | 6696/147540 [00:03<01:18, 1798.40it/s]tokenizing...:   5%|▍         | 6877/147540 [00:03<01:19, 1775.75it/s]tokenizing...:   5%|▍         | 7058/147540 [00:03<01:18, 1783.72it/s]tokenizing...:   5%|▍         | 7237/147540 [00:04<01:20, 1746.65it/s]tokenizing...:   5%|▌         | 7417/147540 [00:04<01:19, 1760.96it/s]tokenizing...:   5%|▌         | 7594/147540 [00:04<01:19, 1760.44it/s]tokenizing...:   5%|▌         | 7775/147540 [00:04<01:18, 1774.90it/s]tokenizing...:   5%|▌         | 7959/147540 [00:04<01:17, 1792.44it/s]tokenizing...:   6%|▌         | 8139/147540 [00:04<01:18, 1771.19it/s]tokenizing...:   6%|▌         | 8317/147540 [00:04<01:19, 1740.81it/s]tokenizing...:   6%|▌         | 8498/147540 [00:04<01:18, 1760.80it/s]tokenizing...:   6%|▌         | 8679/147540 [00:04<01:18, 1773.64it/s]tokenizing...:   6%|▌         | 8912/147540 [00:04<01:11, 1935.44it/s]tokenizing...:   6%|▌         | 9138/147540 [00:05<01:08, 2028.12it/s]tokenizing...:   6%|▋         | 9342/147540 [00:05<01:11, 1939.46it/s]tokenizing...:   6%|▋         | 9582/147540 [00:05<01:06, 2071.08it/s]tokenizing...:   7%|▋         | 9793/147540 [00:05<01:06, 2081.08it/s]tokenizing...:   7%|▋         | 10002/147540 [00:05<01:09, 1979.65it/s]tokenizing...:   7%|▋         | 10202/147540 [00:05<01:10, 1945.80it/s]tokenizing...:   7%|▋         | 10398/147540 [00:05<01:12, 1888.45it/s]tokenizing...:   7%|▋         | 10660/147540 [00:05<01:05, 2092.43it/s]tokenizing...:   7%|▋         | 10880/147540 [00:05<01:04, 2122.14it/s]tokenizing...:   8%|▊         | 11094/147540 [00:06<01:12, 1882.01it/s]tokenizing...:   8%|▊         | 11288/147540 [00:06<01:14, 1832.88it/s]tokenizing...:   8%|▊         | 11476/147540 [00:06<01:16, 1772.70it/s]tokenizing...:   8%|▊         | 11694/147540 [00:06<01:12, 1881.88it/s]tokenizing...:   8%|▊         | 11911/147540 [00:06<01:09, 1960.14it/s]tokenizing...:   8%|▊         | 12116/147540 [00:06<01:08, 1981.25it/s]tokenizing...:   8%|▊         | 12317/147540 [00:06<01:08, 1988.38it/s]tokenizing...:   8%|▊         | 12518/147540 [00:06<01:14, 1813.44it/s]tokenizing...:   9%|▊         | 12704/147540 [00:06<01:18, 1724.24it/s]tokenizing...:   9%|▊         | 12898/147540 [00:07<01:15, 1782.10it/s]tokenizing...:   9%|▉         | 13120/147540 [00:07<01:10, 1903.64it/s]tokenizing...:   9%|▉         | 13322/147540 [00:07<01:09, 1935.12it/s]tokenizing...:   9%|▉         | 13543/147540 [00:07<01:06, 2012.78it/s]tokenizing...:   9%|▉         | 13769/147540 [00:07<01:04, 2084.65it/s]tokenizing...:   9%|▉         | 14003/147540 [00:07<01:01, 2156.03it/s]tokenizing...:  10%|▉         | 14220/147540 [00:07<01:16, 1748.61it/s]tokenizing...:  10%|▉         | 14408/147540 [00:07<01:14, 1776.91it/s]tokenizing...:  10%|▉         | 14596/147540 [00:07<01:21, 1634.95it/s]tokenizing...:  10%|█         | 14768/147540 [00:08<01:24, 1578.61it/s]tokenizing...:  10%|█         | 14951/147540 [00:08<01:20, 1640.98it/s]tokenizing...:  10%|█         | 15156/147540 [00:08<01:15, 1750.71it/s]tokenizing...:  10%|█         | 15365/147540 [00:08<01:11, 1842.55it/s]tokenizing...:  11%|█         | 15554/147540 [00:08<01:18, 1677.38it/s]tokenizing...:  11%|█         | 15727/147540 [00:08<01:20, 1638.53it/s]tokenizing...:  11%|█         | 15906/147540 [00:08<01:18, 1678.11it/s]tokenizing...:  11%|█         | 16091/147540 [00:08<01:16, 1723.70it/s]tokenizing...:  11%|█         | 16324/147540 [00:08<01:09, 1893.74it/s]tokenizing...:  11%|█         | 16516/147540 [00:09<01:11, 1840.30it/s]tokenizing...:  11%|█▏        | 16702/147540 [00:09<01:19, 1648.84it/s]tokenizing...:  11%|█▏        | 16872/147540 [00:09<01:21, 1603.28it/s]tokenizing...:  12%|█▏        | 17039/147540 [00:09<01:20, 1620.37it/s]tokenizing...:  12%|█▏        | 17204/147540 [00:09<01:23, 1570.22it/s]tokenizing...:  12%|█▏        | 17379/147540 [00:09<01:20, 1619.00it/s]tokenizing...:  12%|█▏        | 17559/147540 [00:09<01:17, 1669.09it/s]tokenizing...:  12%|█▏        | 17728/147540 [00:09<01:19, 1642.51it/s]tokenizing...:  12%|█▏        | 17908/147540 [00:09<01:16, 1687.32it/s]tokenizing...:  12%|█▏        | 18086/147540 [00:10<01:15, 1710.00it/s]tokenizing...:  12%|█▏        | 18266/147540 [00:10<01:14, 1732.54it/s]tokenizing...:  13%|█▎        | 18465/147540 [00:10<01:11, 1808.39it/s]tokenizing...:  13%|█▎        | 18647/147540 [00:10<01:16, 1674.24it/s]tokenizing...:  13%|█▎        | 18829/147540 [00:10<01:15, 1715.11it/s]tokenizing...:  13%|█▎        | 19015/147540 [00:10<01:13, 1755.51it/s]tokenizing...:  13%|█▎        | 19197/147540 [00:10<01:12, 1771.92it/s]tokenizing...:  13%|█▎        | 19377/147540 [00:10<01:12, 1776.66it/s]tokenizing...:  13%|█▎        | 19557/147540 [00:10<01:11, 1782.76it/s]tokenizing...:  13%|█▎        | 19736/147540 [00:10<01:12, 1756.27it/s]tokenizing...:  13%|█▎        | 19913/147540 [00:11<01:12, 1750.55it/s]tokenizing...:  14%|█▎        | 20129/147540 [00:11<01:08, 1866.97it/s]tokenizing...:  14%|█▍        | 20364/147540 [00:11<01:03, 2008.30it/s]tokenizing...:  14%|█▍        | 20571/147540 [00:11<01:02, 2025.19it/s]tokenizing...:  14%|█▍        | 20774/147540 [00:11<01:07, 1879.22it/s]tokenizing...:  14%|█▍        | 20965/147540 [00:12<03:46, 558.40it/s] tokenizing...:  14%|█▍        | 21145/147540 [00:12<03:03, 690.14it/s]tokenizing...:  14%|█▍        | 21313/147540 [00:12<02:33, 820.21it/s]tokenizing...:  15%|█▍        | 21495/147540 [00:12<02:08, 978.37it/s]tokenizing...:  15%|█▍        | 21667/147540 [00:12<01:52, 1114.16it/s]tokenizing...:  15%|█▍        | 21840/147540 [00:12<01:41, 1242.62it/s]tokenizing...:  15%|█▍        | 22008/147540 [00:13<01:35, 1317.51it/s]tokenizing...:  15%|█▌        | 22176/147540 [00:13<01:29, 1405.48it/s]tokenizing...:  15%|█▌        | 22351/147540 [00:13<01:23, 1491.23it/s]tokenizing...:  15%|█▌        | 22522/147540 [00:13<01:20, 1549.63it/s]tokenizing...:  15%|█▌        | 22698/147540 [00:13<01:17, 1607.04it/s]tokenizing...:  16%|█▌        | 22882/147540 [00:13<01:14, 1670.85it/s]tokenizing...:  16%|█▌        | 23057/147540 [00:13<01:15, 1658.74it/s]tokenizing...:  16%|█▌        | 23229/147540 [00:13<01:14, 1673.46it/s]tokenizing...:  16%|█▌        | 23404/147540 [00:13<01:13, 1694.80it/s]tokenizing...:  16%|█▌        | 23577/147540 [00:13<01:13, 1691.40it/s]tokenizing...:  16%|█▌        | 23751/147540 [00:14<01:12, 1705.31it/s]tokenizing...:  16%|█▌        | 23923/147540 [00:14<01:12, 1701.58it/s]tokenizing...:  16%|█▋        | 24095/147540 [00:14<01:13, 1688.32it/s]tokenizing...:  16%|█▋        | 24269/147540 [00:14<01:12, 1703.27it/s]tokenizing...:  17%|█▋        | 24451/147540 [00:14<01:10, 1734.87it/s]tokenizing...:  17%|█▋        | 24670/147540 [00:14<01:05, 1861.98it/s]tokenizing...:  17%|█▋        | 24857/147540 [00:14<01:08, 1803.32it/s]tokenizing...:  17%|█▋        | 25038/147540 [00:14<01:10, 1743.37it/s]tokenizing...:  17%|█▋        | 25224/147540 [00:14<01:08, 1775.78it/s]tokenizing...:  17%|█▋        | 25404/147540 [00:15<01:08, 1782.67it/s]tokenizing...:  17%|█▋        | 25618/147540 [00:15<01:04, 1885.60it/s]tokenizing...:  17%|█▋        | 25808/147540 [00:15<01:07, 1792.66it/s]tokenizing...:  18%|█▊        | 25989/147540 [00:15<01:11, 1701.49it/s]tokenizing...:  18%|█▊        | 26161/147540 [00:15<01:15, 1601.43it/s]tokenizing...:  18%|█▊        | 26351/147540 [00:15<01:12, 1681.39it/s]tokenizing...:  18%|█▊        | 26586/147540 [00:15<01:04, 1866.25it/s]tokenizing...:  18%|█▊        | 26811/147540 [00:15<01:01, 1973.81it/s]tokenizing...:  18%|█▊        | 27053/147540 [00:15<00:57, 2102.07it/s]tokenizing...:  18%|█▊        | 27272/147540 [00:15<00:56, 2127.59it/s]tokenizing...:  19%|█▊        | 27503/147540 [00:16<00:55, 2178.10it/s]tokenizing...:  19%|█▉        | 27725/147540 [00:16<00:54, 2185.02it/s]tokenizing...:  19%|█▉        | 27953/147540 [00:16<00:58, 2059.30it/s]tokenizing...:  19%|█▉        | 28162/147540 [00:16<01:07, 1757.38it/s]tokenizing...:  19%|█▉        | 28347/147540 [00:16<01:08, 1742.39it/s]tokenizing...:  19%|█▉        | 28550/147540 [00:16<01:05, 1818.04it/s]tokenizing...:  20%|█▉        | 28783/147540 [00:16<01:00, 1957.56it/s]tokenizing...:  20%|█▉        | 29014/147540 [00:16<00:57, 2053.34it/s]tokenizing...:  20%|█▉        | 29236/147540 [00:16<00:56, 2097.73it/s]tokenizing...:  20%|█▉        | 29464/147540 [00:17<00:54, 2148.87it/s]tokenizing...:  20%|██        | 29685/147540 [00:17<00:54, 2163.01it/s]tokenizing...:  20%|██        | 29913/147540 [00:17<00:53, 2190.31it/s]tokenizing...:  20%|██        | 30153/147540 [00:17<00:52, 2251.58it/s]tokenizing...:  21%|██        | 30380/147540 [00:17<00:52, 2211.34it/s]tokenizing...:  21%|██        | 30608/147540 [00:17<00:52, 2227.59it/s]tokenizing...:  21%|██        | 30848/147540 [00:17<00:51, 2276.20it/s]tokenizing...:  21%|██        | 31086/147540 [00:17<00:50, 2304.32it/s]tokenizing...:  21%|██        | 31317/147540 [00:17<00:50, 2278.97it/s]tokenizing...:  21%|██▏       | 31564/147540 [00:17<00:49, 2335.09it/s]tokenizing...:  22%|██▏       | 31798/147540 [00:18<00:54, 2119.99it/s]tokenizing...:  22%|██▏       | 32014/147540 [00:18<00:59, 1939.05it/s]tokenizing...:  22%|██▏       | 32213/147540 [00:18<01:03, 1811.24it/s]tokenizing...:  22%|██▏       | 32399/147540 [00:18<01:06, 1729.47it/s]tokenizing...:  22%|██▏       | 32603/147540 [00:18<01:03, 1810.02it/s]tokenizing...:  22%|██▏       | 32818/147540 [00:18<01:00, 1900.11it/s]tokenizing...:  22%|██▏       | 33012/147540 [00:18<01:01, 1871.32it/s]tokenizing...:  23%|██▎       | 33212/147540 [00:18<00:59, 1907.22it/s]tokenizing...:  23%|██▎       | 33405/147540 [00:19<01:02, 1828.45it/s]tokenizing...:  23%|██▎       | 33590/147540 [00:19<01:02, 1820.10it/s]tokenizing...:  23%|██▎       | 33803/147540 [00:19<00:59, 1907.66it/s]tokenizing...:  23%|██▎       | 33996/147540 [00:19<00:59, 1908.37it/s]tokenizing...:  23%|██▎       | 34188/147540 [00:19<00:59, 1897.96it/s]tokenizing...:  23%|██▎       | 34379/147540 [00:19<01:01, 1827.90it/s]tokenizing...:  23%|██▎       | 34563/147540 [00:19<01:03, 1773.02it/s]tokenizing...:  24%|██▎       | 34751/147540 [00:19<01:02, 1802.59it/s]tokenizing...:  24%|██▎       | 34932/147540 [00:19<01:02, 1797.55it/s]tokenizing...:  24%|██▍       | 35113/147540 [00:19<01:02, 1793.99it/s]tokenizing...:  24%|██▍       | 35293/147540 [00:20<01:03, 1773.39it/s]tokenizing...:  24%|██▍       | 35471/147540 [00:20<01:04, 1732.02it/s]tokenizing...:  24%|██▍       | 35652/147540 [00:20<01:03, 1752.92it/s]tokenizing...:  24%|██▍       | 35828/147540 [00:20<01:03, 1747.52it/s]tokenizing...:  24%|██▍       | 36003/147540 [00:20<01:03, 1744.07it/s]tokenizing...:  25%|██▍       | 36182/147540 [00:20<01:03, 1757.16it/s]tokenizing...:  25%|██▍       | 36375/147540 [00:20<01:01, 1807.67it/s]tokenizing...:  25%|██▍       | 36579/147540 [00:20<00:59, 1875.98it/s]tokenizing...:  25%|██▍       | 36797/147540 [00:20<00:56, 1965.36it/s]tokenizing...:  25%|██▌       | 36994/147540 [00:20<00:56, 1961.12it/s]tokenizing...:  25%|██▌       | 37191/147540 [00:21<00:58, 1876.83it/s]tokenizing...:  25%|██▌       | 37380/147540 [00:21<01:00, 1824.26it/s]tokenizing...:  25%|██▌       | 37564/147540 [00:21<01:01, 1775.85it/s]tokenizing...:  26%|██▌       | 37749/147540 [00:21<01:01, 1795.77it/s]tokenizing...:  26%|██▌       | 37930/147540 [00:21<01:01, 1793.14it/s]tokenizing...:  26%|██▌       | 38112/147540 [00:21<01:00, 1800.24it/s]tokenizing...:  26%|██▌       | 38293/147540 [00:21<01:00, 1798.53it/s]tokenizing...:  26%|██▌       | 38474/147540 [00:21<01:00, 1791.18it/s]tokenizing...:  26%|██▌       | 38654/147540 [00:21<01:02, 1736.56it/s]tokenizing...:  26%|██▋       | 38836/147540 [00:22<01:01, 1756.90it/s]tokenizing...:  26%|██▋       | 39016/147540 [00:22<01:01, 1769.31it/s]tokenizing...:  27%|██▋       | 39194/147540 [00:22<01:01, 1769.70it/s]tokenizing...:  27%|██▋       | 39372/147540 [00:22<01:01, 1765.20it/s]tokenizing...:  27%|██▋       | 39550/147540 [00:22<01:01, 1768.99it/s]tokenizing...:  27%|██▋       | 39727/147540 [00:22<01:03, 1710.32it/s]tokenizing...:  27%|██▋       | 39910/147540 [00:22<01:01, 1742.05it/s]tokenizing...:  27%|██▋       | 40089/147540 [00:22<01:01, 1755.35it/s]tokenizing...:  27%|██▋       | 40269/147540 [00:22<01:00, 1767.36it/s]tokenizing...:  27%|██▋       | 40447/147540 [00:22<01:00, 1769.44it/s]tokenizing...:  28%|██▊       | 40625/147540 [00:23<01:00, 1759.75it/s]tokenizing...:  28%|██▊       | 40802/147540 [00:23<01:00, 1752.52it/s]tokenizing...:  28%|██▊       | 41019/147540 [00:23<00:56, 1874.63it/s]tokenizing...:  28%|██▊       | 41211/147540 [00:23<00:56, 1887.22it/s]tokenizing...:  28%|██▊       | 41400/147540 [00:23<00:56, 1886.47it/s]tokenizing...:  28%|██▊       | 41594/147540 [00:23<00:55, 1899.11it/s]tokenizing...:  28%|██▊       | 41784/147540 [00:23<01:02, 1705.17it/s]tokenizing...:  28%|██▊       | 41959/147540 [00:23<01:02, 1695.39it/s]tokenizing...:  29%|██▊       | 42132/147540 [00:23<01:03, 1655.15it/s]tokenizing...:  29%|██▊       | 42300/147540 [00:24<01:03, 1654.88it/s]tokenizing...:  29%|██▉       | 42522/147540 [00:24<00:57, 1815.78it/s]tokenizing...:  29%|██▉       | 42706/147540 [00:24<01:01, 1710.67it/s]tokenizing...:  29%|██▉       | 42880/147540 [00:24<01:03, 1660.46it/s]tokenizing...:  29%|██▉       | 43048/147540 [00:24<01:05, 1584.14it/s]tokenizing...:  29%|██▉       | 43273/147540 [00:24<00:59, 1766.77it/s]tokenizing...:  29%|██▉       | 43513/147540 [00:24<00:53, 1942.20it/s]tokenizing...:  30%|██▉       | 43711/147540 [00:24<00:54, 1895.34it/s]tokenizing...:  30%|██▉       | 43903/147540 [00:25<03:45, 458.97it/s] tokenizing...:  30%|██▉       | 44151/147540 [00:26<02:41, 639.24it/s]tokenizing...:  30%|███       | 44398/147540 [00:26<02:01, 845.96it/s]tokenizing...:  30%|███       | 44617/147540 [00:26<01:39, 1032.13it/s]tokenizing...:  30%|███       | 44819/147540 [00:26<01:34, 1086.92it/s]tokenizing...:  30%|███       | 44998/147540 [00:26<01:25, 1192.70it/s]tokenizing...:  31%|███       | 45250/147540 [00:26<01:10, 1455.35it/s]tokenizing...:  31%|███       | 45495/147540 [00:26<01:00, 1672.90it/s]tokenizing...:  31%|███       | 45737/147540 [00:26<00:54, 1851.29it/s]tokenizing...:  31%|███       | 45958/147540 [00:26<00:53, 1902.12it/s]tokenizing...:  31%|███▏      | 46174/147540 [00:27<00:56, 1782.44it/s]tokenizing...:  31%|███▏      | 46371/147540 [00:27<00:58, 1717.21it/s]tokenizing...:  32%|███▏      | 46608/147540 [00:27<00:53, 1882.65it/s]tokenizing...:  32%|███▏      | 46860/147540 [00:27<00:49, 2051.50it/s]tokenizing...:  32%|███▏      | 47109/147540 [00:27<00:46, 2171.40it/s]tokenizing...:  32%|███▏      | 47381/147540 [00:27<00:43, 2324.75it/s]tokenizing...:  32%|███▏      | 47621/147540 [00:27<00:47, 2087.08it/s]tokenizing...:  32%|███▏      | 47839/147540 [00:27<01:00, 1643.51it/s]tokenizing...:  33%|███▎      | 48024/147540 [00:28<01:01, 1618.68it/s]tokenizing...:  33%|███▎      | 48200/147540 [00:28<01:02, 1580.91it/s]tokenizing...:  33%|███▎      | 48368/147540 [00:28<01:02, 1581.82it/s]tokenizing...:  33%|███▎      | 48565/147540 [00:28<00:58, 1681.28it/s]tokenizing...:  33%|███▎      | 48762/147540 [00:28<00:56, 1757.33it/s]tokenizing...:  33%|███▎      | 49000/147540 [00:28<00:51, 1928.70it/s]tokenizing...:  33%|███▎      | 49210/147540 [00:28<00:49, 1974.24it/s]tokenizing...:  34%|███▎      | 49435/147540 [00:28<00:52, 1863.33it/s]tokenizing...:  34%|███▎      | 49662/147540 [00:28<00:49, 1972.91it/s]tokenizing...:  34%|███▍      | 49884/147540 [00:29<00:47, 2041.08it/s]tokenizing...:  34%|███▍      | 50111/147540 [00:29<00:46, 2102.94it/s]tokenizing...:  34%|███▍      | 50324/147540 [00:29<00:51, 1870.83it/s]tokenizing...:  34%|███▍      | 50553/147540 [00:29<00:48, 1982.90it/s]tokenizing...:  34%|███▍      | 50757/147540 [00:29<00:48, 1991.83it/s]tokenizing...:  35%|███▍      | 50967/147540 [00:29<00:47, 2021.92it/s]tokenizing...:  35%|███▍      | 51173/147540 [00:29<00:50, 1909.03it/s]tokenizing...:  35%|███▍      | 51394/147540 [00:29<00:48, 1992.80it/s]tokenizing...:  35%|███▍      | 51597/147540 [00:29<00:48, 1976.81it/s]tokenizing...:  35%|███▌      | 51797/147540 [00:30<00:50, 1886.54it/s]tokenizing...:  35%|███▌      | 52011/147540 [00:30<00:48, 1957.14it/s]tokenizing...:  35%|███▌      | 52209/147540 [00:30<00:50, 1891.08it/s]tokenizing...:  36%|███▌      | 52400/147540 [00:30<00:52, 1806.05it/s]tokenizing...:  36%|███▌      | 52592/147540 [00:30<00:51, 1837.59it/s]tokenizing...:  36%|███▌      | 52787/147540 [00:30<00:50, 1868.57it/s]tokenizing...:  36%|███▌      | 52975/147540 [00:30<00:51, 1824.21it/s]tokenizing...:  36%|███▌      | 53159/147540 [00:30<00:52, 1807.95it/s]tokenizing...:  36%|███▌      | 53341/147540 [00:30<00:54, 1739.00it/s]tokenizing...:  36%|███▋      | 53516/147540 [00:31<00:54, 1736.48it/s]tokenizing...:  36%|███▋      | 53721/147540 [00:31<00:51, 1824.13it/s]tokenizing...:  37%|███▋      | 53905/147540 [00:31<00:51, 1818.84it/s]tokenizing...:  37%|███▋      | 54088/147540 [00:31<00:52, 1781.57it/s]tokenizing...:  37%|███▋      | 54271/147540 [00:31<00:52, 1791.99it/s]tokenizing...:  37%|███▋      | 54451/147540 [00:31<00:53, 1755.46it/s]tokenizing...:  37%|███▋      | 54632/147540 [00:31<00:52, 1770.07it/s]tokenizing...:  37%|███▋      | 54814/147540 [00:31<00:51, 1784.57it/s]tokenizing...:  37%|███▋      | 54993/147540 [00:31<00:52, 1753.22it/s]tokenizing...:  37%|███▋      | 55206/147540 [00:31<00:49, 1862.87it/s]tokenizing...:  38%|███▊      | 55406/147540 [00:32<00:48, 1902.02it/s]tokenizing...:  38%|███▊      | 55622/147540 [00:32<00:46, 1978.31it/s]tokenizing...:  38%|███▊      | 55821/147540 [00:32<00:54, 1685.92it/s]tokenizing...:  38%|███▊      | 55998/147540 [00:32<00:58, 1560.19it/s]tokenizing...:  38%|███▊      | 56161/147540 [00:32<01:01, 1492.35it/s]tokenizing...:  38%|███▊      | 56315/147540 [00:32<01:01, 1490.80it/s]tokenizing...:  38%|███▊      | 56482/147540 [00:32<00:59, 1538.09it/s]tokenizing...:  38%|███▊      | 56661/147540 [00:32<00:56, 1605.36it/s]tokenizing...:  39%|███▊      | 56845/147540 [00:32<00:54, 1670.88it/s]tokenizing...:  39%|███▊      | 57017/147540 [00:33<00:53, 1684.90it/s]tokenizing...:  39%|███▉      | 57197/147540 [00:33<00:52, 1717.56it/s]tokenizing...:  39%|███▉      | 57380/147540 [00:33<00:51, 1748.63it/s]tokenizing...:  39%|███▉      | 57556/147540 [00:33<00:52, 1707.43it/s]tokenizing...:  39%|███▉      | 57728/147540 [00:33<00:52, 1695.80it/s]tokenizing...:  39%|███▉      | 57909/147540 [00:33<00:51, 1726.48it/s]tokenizing...:  39%|███▉      | 58090/147540 [00:33<00:51, 1749.88it/s]tokenizing...:  39%|███▉      | 58269/147540 [00:33<00:50, 1760.34it/s]tokenizing...:  40%|███▉      | 58451/147540 [00:33<00:50, 1775.62it/s]tokenizing...:  40%|███▉      | 58629/147540 [00:33<00:50, 1753.17it/s]tokenizing...:  40%|███▉      | 58805/147540 [00:34<00:50, 1751.48it/s]tokenizing...:  40%|███▉      | 58986/147540 [00:34<00:50, 1765.22it/s]tokenizing...:  40%|████      | 59169/147540 [00:34<00:49, 1782.34it/s]tokenizing...:  40%|████      | 59350/147540 [00:34<00:49, 1789.94it/s]tokenizing...:  40%|████      | 59537/147540 [00:34<00:48, 1811.49it/s]tokenizing...:  41%|████      | 59762/147540 [00:34<00:45, 1942.23it/s]tokenizing...:  41%|████      | 59972/147540 [00:34<00:44, 1988.62it/s]tokenizing...:  41%|████      | 60171/147540 [00:34<00:48, 1808.06it/s]tokenizing...:  41%|████      | 60355/147540 [00:34<00:49, 1751.34it/s]tokenizing...:  41%|████      | 60561/147540 [00:35<00:47, 1835.83it/s]tokenizing...:  41%|████      | 60747/147540 [00:35<00:52, 1662.71it/s]tokenizing...:  41%|████▏     | 60918/147540 [00:35<00:54, 1590.58it/s]tokenizing...:  41%|████▏     | 61087/147540 [00:35<00:53, 1615.83it/s]tokenizing...:  42%|████▏     | 61276/147540 [00:35<00:51, 1691.16it/s]tokenizing...:  42%|████▏     | 61448/147540 [00:35<00:53, 1605.27it/s]tokenizing...:  42%|████▏     | 61611/147540 [00:35<00:54, 1582.79it/s]tokenizing...:  42%|████▏     | 61801/147540 [00:35<00:51, 1666.27it/s]tokenizing...:  42%|████▏     | 62032/147540 [00:35<00:46, 1847.54it/s]tokenizing...:  42%|████▏     | 62271/147540 [00:36<00:42, 2003.09it/s]tokenizing...:  42%|████▏     | 62520/147540 [00:36<00:39, 2142.59it/s]tokenizing...:  43%|████▎     | 62737/147540 [00:36<00:40, 2072.67it/s]tokenizing...:  43%|████▎     | 62946/147540 [00:36<00:41, 2032.84it/s]tokenizing...:  43%|████▎     | 63180/147540 [00:36<00:39, 2120.74it/s]tokenizing...:  43%|████▎     | 63394/147540 [00:36<00:39, 2119.30it/s]tokenizing...:  43%|████▎     | 63607/147540 [00:36<00:40, 2090.18it/s]tokenizing...:  43%|████▎     | 63817/147540 [00:36<00:41, 2039.60it/s]tokenizing...:  43%|████▎     | 64029/147540 [00:36<00:40, 2059.67it/s]tokenizing...:  44%|████▎     | 64274/147540 [00:36<00:38, 2171.33it/s]tokenizing...:  44%|████▍     | 64584/147540 [00:37<00:33, 2444.21it/s]tokenizing...:  44%|████▍     | 64830/147540 [00:37<00:35, 2316.74it/s]tokenizing...:  44%|████▍     | 65102/147540 [00:37<00:33, 2431.84it/s]tokenizing...:  44%|████▍     | 65351/147540 [00:37<00:33, 2446.98it/s]tokenizing...:  45%|████▍     | 65678/147540 [00:37<00:30, 2687.27it/s]tokenizing...:  45%|████▍     | 65949/147540 [00:37<00:31, 2601.39it/s]tokenizing...:  45%|████▍     | 66211/147540 [00:37<00:32, 2535.61it/s]tokenizing...:  45%|████▌     | 66466/147540 [00:37<00:32, 2491.35it/s]tokenizing...:  45%|████▌     | 66717/147540 [00:37<00:33, 2419.67it/s]tokenizing...:  45%|████▌     | 66960/147540 [00:38<00:34, 2325.20it/s]tokenizing...:  46%|████▌     | 67194/147540 [00:38<00:35, 2291.97it/s]tokenizing...:  46%|████▌     | 67424/147540 [00:38<00:34, 2293.06it/s]tokenizing...:  46%|████▌     | 67654/147540 [00:38<00:34, 2293.42it/s]tokenizing...:  46%|████▌     | 67884/147540 [00:38<00:36, 2186.69it/s]tokenizing...:  46%|████▌     | 68104/147540 [00:38<00:43, 1842.05it/s]tokenizing...:  46%|████▋     | 68298/147540 [00:38<00:48, 1618.13it/s]tokenizing...:  46%|████▋     | 68470/147540 [00:38<00:48, 1615.98it/s]tokenizing...:  47%|████▋     | 68639/147540 [00:38<00:48, 1626.42it/s]tokenizing...:  47%|████▋     | 68807/147540 [00:39<00:48, 1631.96it/s]tokenizing...:  47%|████▋     | 68974/147540 [00:39<00:48, 1627.69it/s]tokenizing...:  47%|████▋     | 69181/147540 [00:39<00:44, 1750.07it/s]tokenizing...:  47%|████▋     | 69359/147540 [00:39<00:48, 1624.34it/s]tokenizing...:  47%|████▋     | 69525/147540 [00:39<00:51, 1525.43it/s]tokenizing...:  47%|████▋     | 69681/147540 [00:39<00:53, 1457.95it/s]tokenizing...:  47%|████▋     | 69829/147540 [00:39<00:54, 1413.92it/s]tokenizing...:  47%|████▋     | 69972/147540 [00:39<00:56, 1369.93it/s]tokenizing...:  48%|████▊     | 70153/147540 [00:39<00:51, 1488.31it/s]tokenizing...:  48%|████▊     | 70352/147540 [00:40<00:47, 1625.81it/s]tokenizing...:  48%|████▊     | 70552/147540 [00:40<00:44, 1731.07it/s]tokenizing...:  48%|████▊     | 70733/147540 [00:40<00:43, 1753.17it/s]tokenizing...:  48%|████▊     | 70914/147540 [00:40<00:43, 1767.22it/s]tokenizing...:  48%|████▊     | 71092/147540 [00:40<00:43, 1737.69it/s]tokenizing...:  48%|████▊     | 71274/147540 [00:40<00:43, 1761.06it/s]tokenizing...:  48%|████▊     | 71456/147540 [00:40<00:42, 1775.79it/s]tokenizing...:  49%|████▊     | 71640/147540 [00:40<00:42, 1791.42it/s]tokenizing...:  49%|████▊     | 71820/147540 [00:40<00:42, 1785.83it/s]tokenizing...:  49%|████▉     | 72006/147540 [00:40<00:41, 1804.73it/s]tokenizing...:  49%|████▉     | 72187/147540 [00:41<00:42, 1768.56it/s]tokenizing...:  49%|████▉     | 72381/147540 [00:41<00:41, 1816.80it/s]tokenizing...:  49%|████▉     | 72564/147540 [00:41<00:41, 1819.37it/s]tokenizing...:  49%|████▉     | 72747/147540 [00:41<00:41, 1811.65it/s]tokenizing...:  49%|████▉     | 72929/147540 [00:41<00:41, 1802.59it/s]tokenizing...:  50%|████▉     | 73110/147540 [00:41<00:43, 1729.92it/s]tokenizing...:  50%|████▉     | 73284/147540 [00:43<03:57, 312.56it/s] tokenizing...:  50%|████▉     | 73465/147540 [00:43<02:58, 416.13it/s]tokenizing...:  50%|████▉     | 73646/147540 [00:43<02:16, 541.28it/s]tokenizing...:  50%|█████     | 73832/147540 [00:43<01:46, 691.21it/s]tokenizing...:  50%|█████     | 74014/147540 [00:43<01:26, 848.96it/s]tokenizing...:  50%|█████     | 74194/147540 [00:43<01:12, 1007.07it/s]tokenizing...:  50%|█████     | 74366/147540 [00:43<01:04, 1137.52it/s]tokenizing...:  51%|█████     | 74549/147540 [00:44<00:56, 1285.84it/s]tokenizing...:  51%|█████     | 74729/147540 [00:44<00:51, 1403.49it/s]tokenizing...:  51%|█████     | 74915/147540 [00:44<00:47, 1517.64it/s]tokenizing...:  51%|█████     | 75094/147540 [00:44<00:45, 1583.48it/s]tokenizing...:  51%|█████     | 75272/147540 [00:44<00:44, 1619.91it/s]tokenizing...:  51%|█████     | 75481/147540 [00:44<00:41, 1750.12it/s]tokenizing...:  51%|█████▏    | 75705/147540 [00:44<00:38, 1886.18it/s]tokenizing...:  51%|█████▏    | 75917/147540 [00:44<00:36, 1950.90it/s]tokenizing...:  52%|█████▏    | 76164/147540 [00:44<00:33, 2100.99it/s]tokenizing...:  52%|█████▏    | 76379/147540 [00:44<00:36, 1933.98it/s]tokenizing...:  52%|█████▏    | 76579/147540 [00:45<00:37, 1890.59it/s]tokenizing...:  52%|█████▏    | 76815/147540 [00:45<00:35, 2019.59it/s]tokenizing...:  52%|█████▏    | 77040/147540 [00:45<00:33, 2084.41it/s]tokenizing...:  52%|█████▏    | 77288/147540 [00:45<00:31, 2196.85it/s]tokenizing...:  53%|█████▎    | 77511/147540 [00:45<00:34, 2012.37it/s]tokenizing...:  53%|█████▎    | 77724/147540 [00:45<00:34, 2043.72it/s]tokenizing...:  53%|█████▎    | 77989/147540 [00:45<00:31, 2213.72it/s]tokenizing...:  53%|█████▎    | 78214/147540 [00:45<00:33, 2092.42it/s]tokenizing...:  53%|█████▎    | 78427/147540 [00:45<00:37, 1844.14it/s]tokenizing...:  53%|█████▎    | 78619/147540 [00:46<00:39, 1764.13it/s]tokenizing...:  53%|█████▎    | 78801/147540 [00:46<00:39, 1741.95it/s]tokenizing...:  54%|█████▎    | 79020/147540 [00:46<00:36, 1859.58it/s]tokenizing...:  54%|█████▎    | 79247/147540 [00:46<00:34, 1970.99it/s]tokenizing...:  54%|█████▍    | 79448/147540 [00:46<00:38, 1759.65it/s]tokenizing...:  54%|█████▍    | 79631/147540 [00:46<00:38, 1763.14it/s]tokenizing...:  54%|█████▍    | 79812/147540 [00:46<00:41, 1614.29it/s]tokenizing...:  54%|█████▍    | 80031/147540 [00:46<00:38, 1759.72it/s]tokenizing...:  54%|█████▍    | 80213/147540 [00:46<00:39, 1716.79it/s]tokenizing...:  54%|█████▍    | 80389/147540 [00:47<00:41, 1608.43it/s]tokenizing...:  55%|█████▍    | 80554/147540 [00:47<00:43, 1544.34it/s]tokenizing...:  55%|█████▍    | 80789/147540 [00:47<00:38, 1756.51it/s]tokenizing...:  55%|█████▍    | 81041/147540 [00:47<00:33, 1966.18it/s]tokenizing...:  55%|█████▌    | 81278/147540 [00:47<00:31, 2077.45it/s]tokenizing...:  55%|█████▌    | 81490/147540 [00:47<00:34, 1908.83it/s]tokenizing...:  55%|█████▌    | 81687/147540 [00:47<00:36, 1818.41it/s]tokenizing...:  56%|█████▌    | 81909/147540 [00:47<00:34, 1925.35it/s]tokenizing...:  56%|█████▌    | 82106/147540 [00:47<00:33, 1932.83it/s]tokenizing...:  56%|█████▌    | 82328/147540 [00:48<00:32, 2006.72it/s]tokenizing...:  56%|█████▌    | 82532/147540 [00:48<00:32, 1975.57it/s]tokenizing...:  56%|█████▌    | 82812/147540 [00:48<00:29, 2209.86it/s]tokenizing...:  56%|█████▋    | 83065/147540 [00:48<00:28, 2301.98it/s]tokenizing...:  56%|█████▋    | 83298/147540 [00:48<00:28, 2292.94it/s]tokenizing...:  57%|█████▋    | 83579/147540 [00:48<00:26, 2442.48it/s]tokenizing...:  57%|█████▋    | 83857/147540 [00:48<00:25, 2541.90it/s]tokenizing...:  57%|█████▋    | 84141/147540 [00:48<00:24, 2624.93it/s]tokenizing...:  57%|█████▋    | 84405/147540 [00:48<00:24, 2551.47it/s]tokenizing...:  57%|█████▋    | 84662/147540 [00:49<00:24, 2525.12it/s]tokenizing...:  58%|█████▊    | 84916/147540 [00:49<00:26, 2406.96it/s]tokenizing...:  58%|█████▊    | 85159/147540 [00:49<00:26, 2311.64it/s]tokenizing...:  58%|█████▊    | 85392/147540 [00:49<00:26, 2307.18it/s]tokenizing...:  58%|█████▊    | 85626/147540 [00:49<00:26, 2313.99it/s]tokenizing...:  58%|█████▊    | 85859/147540 [00:49<00:27, 2275.64it/s]tokenizing...:  58%|█████▊    | 86094/147540 [00:49<00:26, 2295.20it/s]tokenizing...:  59%|█████▊    | 86324/147540 [00:49<00:26, 2272.74it/s]tokenizing...:  59%|█████▊    | 86554/147540 [00:49<00:26, 2278.88it/s]tokenizing...:  59%|█████▉    | 86783/147540 [00:49<00:27, 2205.24it/s]tokenizing...:  59%|█████▉    | 87011/147540 [00:50<00:27, 2224.61it/s]tokenizing...:  59%|█████▉    | 87234/147540 [00:50<00:27, 2199.39it/s]tokenizing...:  59%|█████▉    | 87455/147540 [00:50<00:27, 2197.14it/s]tokenizing...:  59%|█████▉    | 87675/147540 [00:50<00:27, 2143.98it/s]tokenizing...:  60%|█████▉    | 87890/147540 [00:50<00:28, 2075.85it/s]tokenizing...:  60%|█████▉    | 88099/147540 [00:50<00:29, 2035.86it/s]tokenizing...:  60%|█████▉    | 88321/147540 [00:50<00:28, 2083.72it/s]tokenizing...:  60%|██████    | 88530/147540 [00:50<00:29, 2001.46it/s]tokenizing...:  60%|██████    | 88731/147540 [00:50<00:30, 1924.90it/s]tokenizing...:  60%|██████    | 88925/147540 [00:51<00:30, 1909.43it/s]tokenizing...:  60%|██████    | 89117/147540 [00:51<00:30, 1899.38it/s]tokenizing...:  61%|██████    | 89339/147540 [00:51<00:29, 1990.10it/s]tokenizing...:  61%|██████    | 89566/147540 [00:51<00:27, 2071.27it/s]tokenizing...:  61%|██████    | 89790/147540 [00:51<00:27, 2118.47it/s]tokenizing...:  61%|██████    | 90003/147540 [00:51<00:27, 2061.76it/s]tokenizing...:  61%|██████    | 90210/147540 [00:51<00:32, 1777.19it/s]tokenizing...:  61%|██████▏   | 90395/147540 [00:51<00:33, 1693.49it/s]tokenizing...:  61%|██████▏   | 90570/147540 [00:51<00:35, 1586.46it/s]tokenizing...:  61%|██████▏   | 90733/147540 [00:52<00:39, 1442.90it/s]tokenizing...:  62%|██████▏   | 90882/147540 [00:52<00:41, 1359.21it/s]tokenizing...:  62%|██████▏   | 91031/147540 [00:52<00:40, 1391.21it/s]tokenizing...:  62%|██████▏   | 91242/147540 [00:52<00:35, 1579.37it/s]tokenizing...:  62%|██████▏   | 91472/147540 [00:52<00:31, 1773.09it/s]tokenizing...:  62%|██████▏   | 91675/147540 [00:52<00:30, 1843.70it/s]tokenizing...:  62%|██████▏   | 91898/147540 [00:52<00:28, 1953.72it/s]tokenizing...:  62%|██████▏   | 92124/147540 [00:52<00:27, 2041.16it/s]tokenizing...:  63%|██████▎   | 92378/147540 [00:52<00:25, 2184.31it/s]tokenizing...:  63%|██████▎   | 92599/147540 [00:53<00:26, 2037.68it/s]tokenizing...:  63%|██████▎   | 92807/147540 [00:53<00:26, 2032.33it/s]tokenizing...:  63%|██████▎   | 93028/147540 [00:53<00:26, 2082.03it/s]tokenizing...:  63%|██████▎   | 93263/147540 [00:53<00:25, 2158.77it/s]tokenizing...:  63%|██████▎   | 93495/147540 [00:53<00:24, 2204.70it/s]tokenizing...:  64%|██████▎   | 93727/147540 [00:53<00:24, 2237.75it/s]tokenizing...:  64%|██████▎   | 93952/147540 [00:53<00:24, 2228.05it/s]tokenizing...:  64%|██████▍   | 94176/147540 [00:53<00:24, 2198.23it/s]tokenizing...:  64%|██████▍   | 94397/147540 [00:53<00:24, 2194.93it/s]tokenizing...:  64%|██████▍   | 94628/147540 [00:53<00:23, 2227.26it/s]tokenizing...:  64%|██████▍   | 94852/147540 [00:54<00:23, 2226.94it/s]tokenizing...:  64%|██████▍   | 95075/147540 [00:54<00:23, 2200.30it/s]tokenizing...:  65%|██████▍   | 95296/147540 [00:54<00:23, 2202.27it/s]tokenizing...:  65%|██████▍   | 95529/147540 [00:54<00:23, 2238.76it/s]tokenizing...:  65%|██████▍   | 95754/147540 [00:54<00:26, 1985.19it/s]tokenizing...:  65%|██████▌   | 95958/147540 [00:54<00:27, 1894.44it/s]tokenizing...:  65%|██████▌   | 96162/147540 [00:54<00:26, 1932.72it/s]tokenizing...:  65%|██████▌   | 96392/147540 [00:54<00:25, 2022.95it/s]tokenizing...:  65%|██████▌   | 96598/147540 [00:54<00:25, 2027.77it/s]tokenizing...:  66%|██████▌   | 96809/147540 [00:55<00:24, 2049.10it/s]tokenizing...:  66%|██████▌   | 97016/147540 [00:55<00:25, 1960.93it/s]tokenizing...:  66%|██████▌   | 97214/147540 [00:55<00:27, 1818.59it/s]tokenizing...:  66%|██████▌   | 97399/147540 [00:55<00:27, 1813.76it/s]tokenizing...:  66%|██████▌   | 97583/147540 [00:55<00:27, 1786.93it/s]tokenizing...:  66%|██████▋   | 97763/147540 [00:55<00:28, 1767.46it/s]tokenizing...:  66%|██████▋   | 97941/147540 [00:55<00:28, 1745.17it/s]tokenizing...:  67%|██████▋   | 98120/147540 [00:55<00:28, 1757.21it/s]tokenizing...:  67%|██████▋   | 98297/147540 [00:55<00:28, 1723.60it/s]tokenizing...:  67%|██████▋   | 98477/147540 [00:56<00:28, 1744.59it/s]tokenizing...:  67%|██████▋   | 98658/147540 [00:56<00:27, 1761.67it/s]tokenizing...:  67%|██████▋   | 98835/147540 [00:56<00:27, 1755.25it/s]tokenizing...:  67%|██████▋   | 99014/147540 [00:56<00:27, 1764.62it/s]tokenizing...:  67%|██████▋   | 99192/147540 [00:56<00:27, 1765.26it/s]tokenizing...:  67%|██████▋   | 99369/147540 [00:56<00:28, 1703.07it/s]tokenizing...:  67%|██████▋   | 99540/147540 [00:56<00:28, 1695.97it/s]tokenizing...:  68%|██████▊   | 99710/147540 [00:56<00:28, 1696.62it/s]tokenizing...:  68%|██████▊   | 99880/147540 [00:56<00:28, 1684.01it/s]tokenizing...:  68%|██████▊   | 100058/147540 [00:56<00:27, 1710.62it/s]tokenizing...:  68%|██████▊   | 100232/147540 [00:57<00:27, 1717.48it/s]tokenizing...:  68%|██████▊   | 100404/147540 [00:57<00:28, 1670.85it/s]tokenizing...:  68%|██████▊   | 100634/147540 [00:57<00:25, 1853.22it/s]tokenizing...:  68%|██████▊   | 100821/147540 [00:57<00:27, 1729.07it/s]tokenizing...:  68%|██████▊   | 100996/147540 [00:57<00:31, 1483.97it/s]tokenizing...:  69%|██████▊   | 101160/147540 [00:57<00:30, 1522.87it/s]tokenizing...:  69%|██████▊   | 101388/147540 [00:57<00:26, 1723.46it/s]tokenizing...:  69%|██████▉   | 101605/147540 [00:57<00:24, 1844.91it/s]tokenizing...:  69%|██████▉   | 101841/147540 [00:57<00:22, 1987.65it/s]tokenizing...:  69%|██████▉   | 102045/147540 [00:58<00:22, 1994.08it/s]tokenizing...:  69%|██████▉   | 102273/147540 [00:58<00:21, 2076.10it/s]tokenizing...:  69%|██████▉   | 102484/147540 [00:58<00:22, 2004.48it/s]tokenizing...:  70%|██████▉   | 102715/147540 [00:58<00:21, 2091.91it/s]tokenizing...:  70%|██████▉   | 102954/147540 [00:58<00:20, 2176.35it/s]tokenizing...:  70%|██████▉   | 103183/147540 [00:58<00:20, 2208.02it/s]tokenizing...:  70%|███████   | 103408/147540 [00:58<00:19, 2216.86it/s]tokenizing...:  70%|███████   | 103647/147540 [00:58<00:19, 2267.86it/s]tokenizing...:  70%|███████   | 103875/147540 [00:58<00:19, 2208.35it/s]tokenizing...:  71%|███████   | 104097/147540 [00:58<00:20, 2149.99it/s]tokenizing...:  71%|███████   | 104313/147540 [00:59<00:20, 2125.06it/s]tokenizing...:  71%|███████   | 104556/147540 [00:59<00:19, 2211.56it/s]tokenizing...:  71%|███████   | 104778/147540 [00:59<00:19, 2188.53it/s]tokenizing...:  71%|███████   | 105019/147540 [00:59<00:18, 2250.92it/s]tokenizing...:  71%|███████▏  | 105291/147540 [00:59<00:17, 2388.86it/s]tokenizing...:  72%|███████▏  | 105542/147540 [00:59<00:17, 2385.09it/s]tokenizing...:  72%|███████▏  | 105856/147540 [00:59<00:15, 2606.55it/s]tokenizing...:  72%|███████▏  | 106119/147540 [00:59<00:15, 2612.85it/s]tokenizing...:  72%|███████▏  | 106381/147540 [00:59<00:16, 2526.41it/s]tokenizing...:  72%|███████▏  | 106635/147540 [01:00<00:20, 2043.30it/s]tokenizing...:  72%|███████▏  | 106855/147540 [01:00<00:19, 2072.23it/s]tokenizing...:  73%|███████▎  | 107076/147540 [01:00<00:19, 2106.95it/s]tokenizing...:  73%|███████▎  | 107310/147540 [01:00<00:18, 2169.37it/s]tokenizing...:  73%|███████▎  | 107551/147540 [01:00<00:17, 2235.91it/s]tokenizing...:  73%|███████▎  | 107780/147540 [01:00<00:18, 2204.52it/s]tokenizing...:  73%|███████▎  | 108004/147540 [01:00<00:17, 2201.68it/s]tokenizing...:  73%|███████▎  | 108227/147540 [01:00<00:19, 2007.59it/s]tokenizing...:  73%|███████▎  | 108433/147540 [01:00<00:20, 1875.47it/s]tokenizing...:  74%|███████▎  | 108646/147540 [01:01<00:20, 1942.75it/s]tokenizing...:  74%|███████▍  | 108845/147540 [01:01<00:20, 1899.71it/s]tokenizing...:  74%|███████▍  | 109046/147540 [01:01<00:19, 1929.32it/s]tokenizing...:  74%|███████▍  | 109242/147540 [01:01<00:20, 1881.58it/s]tokenizing...:  74%|███████▍  | 109432/147540 [01:01<00:21, 1746.12it/s]tokenizing...:  74%|███████▍  | 109610/147540 [01:01<00:23, 1589.49it/s]tokenizing...:  74%|███████▍  | 109805/147540 [01:04<02:45, 228.12it/s] tokenizing...:  75%|███████▍  | 109975/147540 [01:04<02:05, 298.58it/s]tokenizing...:  75%|███████▍  | 110151/147540 [01:04<01:35, 391.86it/s]tokenizing...:  75%|███████▍  | 110361/147540 [01:04<01:09, 534.81it/s]tokenizing...:  75%|███████▍  | 110565/147540 [01:04<00:53, 695.30it/s]tokenizing...:  75%|███████▌  | 110741/147540 [01:04<00:44, 835.37it/s]tokenizing...:  75%|███████▌  | 110917/147540 [01:04<00:37, 966.65it/s]tokenizing...:  75%|███████▌  | 111090/147540 [01:04<00:32, 1105.39it/s]tokenizing...:  75%|███████▌  | 111262/147540 [01:05<00:29, 1226.74it/s]tokenizing...:  76%|███████▌  | 111460/147540 [01:05<00:25, 1396.67it/s]tokenizing...:  76%|███████▌  | 111645/147540 [01:05<00:23, 1507.19it/s]tokenizing...:  76%|███████▌  | 111826/147540 [01:05<00:22, 1571.55it/s]tokenizing...:  76%|███████▌  | 112005/147540 [01:05<00:22, 1582.21it/s]tokenizing...:  76%|███████▌  | 112179/147540 [01:05<00:21, 1610.07it/s]tokenizing...:  76%|███████▌  | 112352/147540 [01:05<00:21, 1642.05it/s]tokenizing...:  76%|███████▋  | 112525/147540 [01:05<00:21, 1633.16it/s]tokenizing...:  76%|███████▋  | 112706/147540 [01:05<00:20, 1680.81it/s]tokenizing...:  77%|███████▋  | 112879/147540 [01:05<00:20, 1688.45it/s]tokenizing...:  77%|███████▋  | 113051/147540 [01:06<00:20, 1686.00it/s]tokenizing...:  77%|███████▋  | 113280/147540 [01:06<00:18, 1860.65it/s]tokenizing...:  77%|███████▋  | 113468/147540 [01:06<00:19, 1768.89it/s]tokenizing...:  77%|███████▋  | 113648/147540 [01:06<00:20, 1644.63it/s]tokenizing...:  77%|███████▋  | 113827/147540 [01:06<00:20, 1681.19it/s]tokenizing...:  77%|███████▋  | 114000/147540 [01:06<00:19, 1694.19it/s]tokenizing...:  77%|███████▋  | 114187/147540 [01:06<00:19, 1743.69it/s]tokenizing...:  78%|███████▊  | 114364/147540 [01:06<00:18, 1749.15it/s]tokenizing...:  78%|███████▊  | 114543/147540 [01:06<00:18, 1760.36it/s]tokenizing...:  78%|███████▊  | 114721/147540 [01:06<00:18, 1765.08it/s]tokenizing...:  78%|███████▊  | 114899/147540 [01:07<00:18, 1756.04it/s]tokenizing...:  78%|███████▊  | 115075/147540 [01:07<00:18, 1714.24it/s]tokenizing...:  78%|███████▊  | 115263/147540 [01:07<00:18, 1760.70it/s]tokenizing...:  78%|███████▊  | 115444/147540 [01:07<00:18, 1772.40it/s]tokenizing...:  78%|███████▊  | 115624/147540 [01:07<00:17, 1777.12it/s]tokenizing...:  78%|███████▊  | 115802/147540 [01:07<00:17, 1775.12it/s]tokenizing...:  79%|███████▊  | 115981/147540 [01:07<00:17, 1775.57it/s]tokenizing...:  79%|███████▊  | 116159/147540 [01:07<00:18, 1724.95it/s]tokenizing...:  79%|███████▉  | 116334/147540 [01:07<00:18, 1730.89it/s]tokenizing...:  79%|███████▉  | 116509/147540 [01:08<00:17, 1735.39it/s]tokenizing...:  79%|███████▉  | 116683/147540 [01:08<00:17, 1729.88it/s]tokenizing...:  79%|███████▉  | 116857/147540 [01:08<00:17, 1731.29it/s]tokenizing...:  79%|███████▉  | 117034/147540 [01:08<00:17, 1738.74it/s]tokenizing...:  79%|███████▉  | 117208/147540 [01:08<00:17, 1703.84it/s]tokenizing...:  80%|███████▉  | 117379/147540 [01:08<00:17, 1699.94it/s]tokenizing...:  80%|███████▉  | 117551/147540 [01:08<00:17, 1705.44it/s]tokenizing...:  80%|███████▉  | 117722/147540 [01:08<00:18, 1613.16it/s]tokenizing...:  80%|███████▉  | 117912/147540 [01:08<00:17, 1694.62it/s]tokenizing...:  80%|████████  | 118083/147540 [01:08<00:17, 1649.04it/s]tokenizing...:  80%|████████  | 118249/147540 [01:09<00:21, 1391.48it/s]tokenizing...:  80%|████████  | 118434/147540 [01:09<00:19, 1508.59it/s]tokenizing...:  80%|████████  | 118635/147540 [01:09<00:17, 1642.04it/s]tokenizing...:  81%|████████  | 118851/147540 [01:09<00:16, 1782.99it/s]tokenizing...:  81%|████████  | 119062/147540 [01:09<00:15, 1874.06it/s]tokenizing...:  81%|████████  | 119254/147540 [01:09<00:16, 1738.92it/s]tokenizing...:  81%|████████  | 119433/147540 [01:09<00:18, 1557.51it/s]tokenizing...:  81%|████████  | 119657/147540 [01:09<00:16, 1732.78it/s]tokenizing...:  81%|████████▏ | 119888/147540 [01:10<00:14, 1887.31it/s]tokenizing...:  81%|████████▏ | 120084/147540 [01:10<00:14, 1831.56it/s]tokenizing...:  82%|████████▏ | 120288/147540 [01:10<00:14, 1888.77it/s]tokenizing...:  82%|████████▏ | 120519/147540 [01:10<00:13, 2007.52it/s]tokenizing...:  82%|████████▏ | 120748/147540 [01:10<00:12, 2088.75it/s]tokenizing...:  82%|████████▏ | 120989/147540 [01:10<00:12, 2180.63it/s]tokenizing...:  82%|████████▏ | 121236/147540 [01:10<00:11, 2262.93it/s]tokenizing...:  82%|████████▏ | 121464/147540 [01:10<00:12, 2067.26it/s]tokenizing...:  82%|████████▏ | 121696/147540 [01:10<00:12, 2136.81it/s]tokenizing...:  83%|████████▎ | 121918/147540 [01:10<00:11, 2159.31it/s]tokenizing...:  83%|████████▎ | 122152/147540 [01:11<00:11, 2210.63it/s]tokenizing...:  83%|████████▎ | 122376/147540 [01:11<00:12, 1955.42it/s]tokenizing...:  83%|████████▎ | 122579/147540 [01:11<00:12, 1941.10it/s]tokenizing...:  83%|████████▎ | 122834/147540 [01:11<00:11, 2107.67it/s]tokenizing...:  83%|████████▎ | 123130/147540 [01:11<00:10, 2347.14it/s]tokenizing...:  84%|████████▎ | 123370/147540 [01:11<00:10, 2339.86it/s]tokenizing...:  84%|████████▍ | 123608/147540 [01:11<00:10, 2341.02it/s]tokenizing...:  84%|████████▍ | 123878/147540 [01:11<00:09, 2444.34it/s]tokenizing...:  84%|████████▍ | 124146/147540 [01:11<00:09, 2513.43it/s]tokenizing...:  84%|████████▍ | 124455/147540 [01:12<00:08, 2681.36it/s]tokenizing...:  85%|████████▍ | 124725/147540 [01:12<00:08, 2609.56it/s]tokenizing...:  85%|████████▍ | 124988/147540 [01:12<00:09, 2483.64it/s]tokenizing...:  85%|████████▍ | 125239/147540 [01:12<00:09, 2380.68it/s]tokenizing...:  85%|████████▌ | 125479/147540 [01:12<00:09, 2272.36it/s]tokenizing...:  85%|████████▌ | 125709/147540 [01:12<00:09, 2258.02it/s]tokenizing...:  85%|████████▌ | 125936/147540 [01:12<00:09, 2256.65it/s]tokenizing...:  86%|████████▌ | 126164/147540 [01:12<00:09, 2258.93it/s]tokenizing...:  86%|████████▌ | 126396/147540 [01:12<00:09, 2274.09it/s]tokenizing...:  86%|████████▌ | 126624/147540 [01:12<00:09, 2210.26it/s]tokenizing...:  86%|████████▌ | 126846/147540 [01:13<00:09, 2212.09it/s]tokenizing...:  86%|████████▌ | 127068/147540 [01:13<00:09, 2193.97it/s]tokenizing...:  86%|████████▋ | 127288/147540 [01:13<00:09, 2172.92it/s]tokenizing...:  86%|████████▋ | 127506/147540 [01:13<00:09, 2068.99it/s]tokenizing...:  87%|████████▋ | 127714/147540 [01:13<00:09, 2021.89it/s]tokenizing...:  87%|████████▋ | 127917/147540 [01:13<00:11, 1760.93it/s]tokenizing...:  87%|████████▋ | 128129/147540 [01:13<00:10, 1854.40it/s]tokenizing...:  87%|████████▋ | 128320/147540 [01:13<00:10, 1842.45it/s]tokenizing...:  87%|████████▋ | 128509/147540 [01:13<00:10, 1821.63it/s]tokenizing...:  87%|████████▋ | 128722/147540 [01:14<00:09, 1906.95it/s]tokenizing...:  87%|████████▋ | 128916/147540 [01:14<00:09, 1867.39it/s]tokenizing...:  88%|████████▊ | 129105/147540 [01:14<00:10, 1812.29it/s]tokenizing...:  88%|████████▊ | 129289/147540 [01:14<00:10, 1818.03it/s]tokenizing...:  88%|████████▊ | 129472/147540 [01:14<00:10, 1806.63it/s]tokenizing...:  88%|████████▊ | 129654/147540 [01:14<00:10, 1721.43it/s]tokenizing...:  88%|████████▊ | 129828/147540 [01:14<00:11, 1591.66it/s]tokenizing...:  88%|████████▊ | 129990/147540 [01:14<00:11, 1577.51it/s]tokenizing...:  88%|████████▊ | 130161/147540 [01:14<00:10, 1611.89it/s]tokenizing...:  88%|████████▊ | 130342/147540 [01:15<00:10, 1663.82it/s]tokenizing...:  88%|████████▊ | 130522/147540 [01:15<00:10, 1700.88it/s]tokenizing...:  89%|████████▊ | 130694/147540 [01:15<00:10, 1684.38it/s]tokenizing...:  89%|████████▊ | 130874/147540 [01:15<00:09, 1717.41it/s]tokenizing...:  89%|████████▉ | 131052/147540 [01:15<00:09, 1735.07it/s]tokenizing...:  89%|████████▉ | 131240/147540 [01:15<00:09, 1767.73it/s]tokenizing...:  89%|████████▉ | 131450/147540 [01:15<00:08, 1865.24it/s]tokenizing...:  89%|████████▉ | 131637/147540 [01:15<00:08, 1822.01it/s]tokenizing...:  89%|████████▉ | 131820/147540 [01:15<00:09, 1655.82it/s]tokenizing...:  89%|████████▉ | 132036/147540 [01:16<00:08, 1793.44it/s]tokenizing...:  90%|████████▉ | 132219/147540 [01:16<00:09, 1603.55it/s]tokenizing...:  90%|████████▉ | 132385/147540 [01:16<00:09, 1518.56it/s]tokenizing...:  90%|████████▉ | 132542/147540 [01:16<00:10, 1447.38it/s]tokenizing...:  90%|████████▉ | 132690/147540 [01:16<00:10, 1409.36it/s]tokenizing...:  90%|█████████ | 132833/147540 [01:16<00:10, 1388.76it/s]tokenizing...:  90%|█████████ | 133015/147540 [01:16<00:09, 1503.88it/s]tokenizing...:  90%|█████████ | 133194/147540 [01:16<00:09, 1583.09it/s]tokenizing...:  90%|█████████ | 133372/147540 [01:16<00:08, 1637.64it/s]tokenizing...:  91%|█████████ | 133553/147540 [01:17<00:08, 1681.14it/s]tokenizing...:  91%|█████████ | 133726/147540 [01:17<00:08, 1693.20it/s]tokenizing...:  91%|█████████ | 133897/147540 [01:17<00:08, 1671.51it/s]tokenizing...:  91%|█████████ | 134069/147540 [01:17<00:07, 1684.06it/s]tokenizing...:  91%|█████████ | 134246/147540 [01:17<00:07, 1705.00it/s]tokenizing...:  91%|█████████ | 134421/147540 [01:17<00:07, 1717.66it/s]tokenizing...:  91%|█████████ | 134600/147540 [01:17<00:07, 1738.09it/s]tokenizing...:  91%|█████████▏| 134777/147540 [01:17<00:07, 1746.48it/s]tokenizing...:  91%|█████████▏| 134952/147540 [01:17<00:07, 1686.99it/s]tokenizing...:  92%|█████████▏| 135126/147540 [01:17<00:07, 1702.03it/s]tokenizing...:  92%|█████████▏| 135305/147540 [01:18<00:07, 1725.27it/s]tokenizing...:  92%|█████████▏| 135482/147540 [01:18<00:06, 1735.45it/s]tokenizing...:  92%|█████████▏| 135658/147540 [01:18<00:06, 1742.50it/s]tokenizing...:  92%|█████████▏| 135850/147540 [01:18<00:06, 1793.34it/s]tokenizing...:  92%|█████████▏| 136051/147540 [01:18<00:06, 1857.87it/s]tokenizing...:  92%|█████████▏| 136237/147540 [01:18<00:06, 1759.36it/s]tokenizing...:  92%|█████████▏| 136453/147540 [01:18<00:05, 1871.48it/s]tokenizing...:  93%|█████████▎| 136642/147540 [01:18<00:06, 1643.65it/s]tokenizing...:  93%|█████████▎| 136812/147540 [01:18<00:07, 1526.15it/s]tokenizing...:  93%|█████████▎| 136970/147540 [01:19<00:07, 1485.51it/s]tokenizing...:  93%|█████████▎| 137122/147540 [01:19<00:07, 1485.22it/s]tokenizing...:  93%|█████████▎| 137290/147540 [01:19<00:06, 1537.37it/s]tokenizing...:  93%|█████████▎| 137446/147540 [01:19<00:07, 1319.15it/s]tokenizing...:  93%|█████████▎| 137585/147540 [01:19<00:07, 1324.95it/s]tokenizing...:  93%|█████████▎| 137722/147540 [01:19<00:07, 1317.20it/s]tokenizing...:  93%|█████████▎| 137890/147540 [01:19<00:06, 1414.45it/s]tokenizing...:  94%|█████████▎| 138080/147540 [01:19<00:06, 1550.09it/s]tokenizing...:  94%|█████████▍| 138322/147540 [01:19<00:05, 1795.63it/s]tokenizing...:  94%|█████████▍| 138556/147540 [01:20<00:04, 1952.72it/s]tokenizing...:  94%|█████████▍| 138780/147540 [01:20<00:04, 2036.28it/s]tokenizing...:  94%|█████████▍| 139034/147540 [01:20<00:03, 2148.66it/s]tokenizing...:  94%|█████████▍| 139270/147540 [01:20<00:03, 2210.46it/s]tokenizing...:  95%|█████████▍| 139493/147540 [01:20<00:03, 2193.36it/s]tokenizing...:  95%|█████████▍| 139720/147540 [01:20<00:03, 2212.76it/s]tokenizing...:  95%|█████████▍| 139942/147540 [01:20<00:03, 2165.30it/s]tokenizing...:  95%|█████████▌| 140180/147540 [01:20<00:03, 2226.23it/s]tokenizing...:  95%|█████████▌| 140409/147540 [01:20<00:03, 2243.92it/s]tokenizing...:  95%|█████████▌| 140634/147540 [01:20<00:03, 2220.78it/s]tokenizing...:  96%|█████████▌| 140948/147540 [01:21<00:02, 2490.88it/s]tokenizing...:  96%|█████████▌| 141198/147540 [01:21<00:02, 2403.38it/s]tokenizing...:  96%|█████████▌| 141454/147540 [01:21<00:02, 2448.43it/s]tokenizing...:  96%|█████████▌| 141716/147540 [01:21<00:02, 2495.92it/s]tokenizing...:  96%|█████████▋| 142019/147540 [01:21<00:02, 2652.38it/s]tokenizing...:  96%|█████████▋| 142290/147540 [01:21<00:01, 2669.41it/s]tokenizing...:  97%|█████████▋| 142558/147540 [01:21<00:01, 2606.23it/s]tokenizing...:  97%|█████████▋| 142820/147540 [01:21<00:01, 2492.63it/s]tokenizing...:  97%|█████████▋| 143071/147540 [01:21<00:01, 2389.61it/s]tokenizing...:  97%|█████████▋| 143312/147540 [01:22<00:01, 2303.36it/s]tokenizing...:  97%|█████████▋| 143559/147540 [01:22<00:01, 2343.79it/s]tokenizing...:  97%|█████████▋| 143795/147540 [01:22<00:01, 2110.24it/s]tokenizing...:  98%|█████████▊| 144011/147540 [01:22<00:01, 1948.22it/s]tokenizing...:  98%|█████████▊| 144237/147540 [01:22<00:01, 2027.52it/s]tokenizing...:  98%|█████████▊| 144445/147540 [01:22<00:01, 1958.67it/s]tokenizing...:  98%|█████████▊| 144670/147540 [01:22<00:01, 2035.65it/s]tokenizing...:  98%|█████████▊| 144877/147540 [01:22<00:01, 1964.22it/s]tokenizing...:  98%|█████████▊| 145076/147540 [01:22<00:01, 1966.21it/s]tokenizing...:  98%|█████████▊| 145298/147540 [01:23<00:01, 1998.23it/s]tokenizing...:  99%|█████████▊| 145510/147540 [01:23<00:01, 2028.66it/s]tokenizing...:  99%|█████████▉| 145714/147540 [01:23<00:00, 1936.74it/s]tokenizing...:  99%|█████████▉| 145918/147540 [01:23<00:00, 1962.50it/s]tokenizing...:  99%|█████████▉| 146116/147540 [01:23<00:00, 1902.20it/s]tokenizing...:  99%|█████████▉| 146308/147540 [01:23<00:00, 1860.38it/s]tokenizing...:  99%|█████████▉| 146497/147540 [01:23<00:00, 1861.64it/s]tokenizing...:  99%|█████████▉| 146698/147540 [01:23<00:00, 1903.86it/s]tokenizing...: 100%|█████████▉| 146889/147540 [01:23<00:00, 1847.94it/s]tokenizing...: 100%|█████████▉| 147075/147540 [01:23<00:00, 1834.12it/s]tokenizing...: 100%|█████████▉| 147259/147540 [01:24<00:00, 1816.33it/s]tokenizing...: 100%|█████████▉| 147441/147540 [01:24<00:00, 1767.12it/s]tokenizing...: 100%|██████████| 147540/147540 [01:24<00:00, 1751.03it/s]
10/25/2022 04:39:33 - INFO - root -   The nums of the test_dataset features is 147540
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 0/1153 [00:00<?, ?it/s]  0%|          | 1/1153 [00:01<34:46,  1.81s/it]  0%|          | 2/1153 [00:06<1:04:02,  3.34s/it]  0%|          | 3/1153 [00:07<45:50,  2.39s/it]    0%|          | 4/1153 [00:08<36:46,  1.92s/it]  0%|          | 5/1153 [00:09<31:45,  1.66s/it]  1%|          | 6/1153 [00:11<28:16,  1.48s/it]  1%|          | 7/1153 [00:12<26:30,  1.39s/it]  1%|          | 8/1153 [00:13<25:20,  1.33s/it]  1%|          | 9/1153 [00:14<24:33,  1.29s/it]  1%|          | 10/1153 [00:15<24:03,  1.26s/it]  1%|          | 11/1153 [00:17<23:51,  1.25s/it]  1%|          | 12/1153 [00:17<21:48,  1.15s/it]  1%|          | 13/1153 [00:19<21:59,  1.16s/it]  1%|          | 14/1153 [00:20<22:12,  1.17s/it]  1%|▏         | 15/1153 [00:21<22:20,  1.18s/it]  1%|▏         | 16/1153 [00:22<22:17,  1.18s/it]  1%|▏         | 17/1153 [00:23<22:15,  1.18s/it]  2%|▏         | 18/1153 [00:28<39:41,  2.10s/it]  2%|▏         | 19/1153 [00:29<34:48,  1.84s/it]  2%|▏         | 20/1153 [00:30<31:06,  1.65s/it]  2%|▏         | 21/1153 [00:31<28:28,  1.51s/it]  2%|▏         | 22/1153 [00:32<26:36,  1.41s/it]  2%|▏         | 23/1153 [00:34<25:18,  1.34s/it]  2%|▏         | 24/1153 [00:35<24:36,  1.31s/it]  2%|▏         | 25/1153 [00:36<25:20,  1.35s/it]  2%|▏         | 26/1153 [00:37<24:29,  1.30s/it]  2%|▏         | 27/1153 [00:38<22:22,  1.19s/it]  2%|▏         | 28/1153 [00:40<22:14,  1.19s/it]  3%|▎         | 29/1153 [00:41<22:16,  1.19s/it]  3%|▎         | 30/1153 [00:42<22:20,  1.19s/it]  3%|▎         | 31/1153 [00:43<22:31,  1.20s/it]  3%|▎         | 32/1153 [00:44<22:52,  1.22s/it]  3%|▎         | 33/1153 [00:46<22:51,  1.22s/it]  3%|▎         | 34/1153 [00:50<38:40,  2.07s/it]  3%|▎         | 35/1153 [00:51<33:47,  1.81s/it]  3%|▎         | 36/1153 [00:52<30:24,  1.63s/it]  3%|▎         | 37/1153 [00:53<27:57,  1.50s/it]  3%|▎         | 38/1153 [00:55<26:16,  1.41s/it]  3%|▎         | 39/1153 [00:56<25:31,  1.37s/it]  3%|▎         | 40/1153 [00:57<23:35,  1.27s/it]  4%|▎         | 41/1153 [00:58<22:28,  1.21s/it]  4%|▎         | 42/1153 [00:59<21:33,  1.16s/it]  4%|▎         | 43/1153 [01:00<21:03,  1.14s/it]  4%|▍         | 44/1153 [01:01<20:29,  1.11s/it]  4%|▍         | 45/1153 [01:02<20:10,  1.09s/it]  4%|▍         | 46/1153 [01:03<20:14,  1.10s/it]  4%|▍         | 47/1153 [01:04<19:53,  1.08s/it]  4%|▍         | 48/1153 [01:05<19:44,  1.07s/it]  4%|▍         | 49/1153 [01:06<19:40,  1.07s/it]  4%|▍         | 50/1153 [01:11<36:24,  1.98s/it]  4%|▍         | 51/1153 [01:12<31:38,  1.72s/it]  5%|▍         | 52/1153 [01:13<27:57,  1.52s/it]  5%|▍         | 53/1153 [01:14<25:46,  1.41s/it]  5%|▍         | 54/1153 [01:15<24:11,  1.32s/it]  5%|▍         | 55/1153 [01:16<22:46,  1.24s/it]  5%|▍         | 56/1153 [01:17<21:44,  1.19s/it]  5%|▍         | 57/1153 [01:18<21:05,  1.15s/it]  5%|▌         | 58/1153 [01:19<20:35,  1.13s/it]  5%|▌         | 59/1153 [01:20<20:27,  1.12s/it]  5%|▌         | 60/1153 [01:21<20:16,  1.11s/it]  5%|▌         | 61/1153 [01:23<20:02,  1.10s/it]  5%|▌         | 62/1153 [01:24<20:11,  1.11s/it]  5%|▌         | 63/1153 [01:25<19:47,  1.09s/it]  6%|▌         | 64/1153 [01:26<19:51,  1.09s/it]  6%|▌         | 65/1153 [01:27<19:43,  1.09s/it]  6%|▌         | 66/1153 [01:28<19:31,  1.08s/it]  6%|▌         | 67/1153 [01:29<19:26,  1.07s/it]  6%|▌         | 68/1153 [01:33<33:53,  1.87s/it]  6%|▌         | 69/1153 [01:34<30:21,  1.68s/it]  6%|▌         | 70/1153 [01:35<25:24,  1.41s/it]  6%|▌         | 71/1153 [01:36<24:11,  1.34s/it]  6%|▌         | 72/1153 [01:37<23:22,  1.30s/it]  6%|▋         | 73/1153 [01:38<22:51,  1.27s/it]  6%|▋         | 74/1153 [01:39<21:52,  1.22s/it]  7%|▋         | 75/1153 [01:41<21:42,  1.21s/it]  7%|▋         | 76/1153 [01:42<21:49,  1.22s/it]  7%|▋         | 77/1153 [01:43<21:46,  1.21s/it]  7%|▋         | 78/1153 [01:44<21:43,  1.21s/it]  7%|▋         | 79/1153 [01:45<21:36,  1.21s/it]  7%|▋         | 80/1153 [01:47<21:39,  1.21s/it]  7%|▋         | 81/1153 [01:48<20:50,  1.17s/it]  7%|▋         | 82/1153 [01:49<20:57,  1.17s/it]  7%|▋         | 83/1153 [01:50<21:01,  1.18s/it]  7%|▋         | 84/1153 [01:51<21:23,  1.20s/it]  7%|▋         | 85/1153 [01:55<35:46,  2.01s/it]  7%|▋         | 86/1153 [01:57<31:47,  1.79s/it]  8%|▊         | 87/1153 [01:58<29:01,  1.63s/it]  8%|▊         | 88/1153 [01:59<26:40,  1.50s/it]  8%|▊         | 89/1153 [02:00<25:19,  1.43s/it]  8%|▊         | 90/1153 [02:02<24:18,  1.37s/it]  8%|▊         | 91/1153 [02:03<23:29,  1.33s/it]  8%|▊         | 92/1153 [02:04<22:50,  1.29s/it]  8%|▊         | 93/1153 [02:05<22:27,  1.27s/it]  8%|▊         | 94/1153 [02:06<22:13,  1.26s/it]  8%|▊         | 95/1153 [02:08<21:54,  1.24s/it]  8%|▊         | 96/1153 [02:09<21:44,  1.23s/it]  8%|▊         | 97/1153 [02:10<21:53,  1.24s/it]  8%|▊         | 98/1153 [02:11<22:08,  1.26s/it]  9%|▊         | 99/1153 [02:13<22:07,  1.26s/it]  9%|▊         | 100/1153 [02:17<36:00,  2.05s/it]  9%|▉         | 101/1153 [02:18<31:47,  1.81s/it]  9%|▉         | 102/1153 [02:19<28:47,  1.64s/it]  9%|▉         | 103/1153 [02:20<26:48,  1.53s/it]  9%|▉         | 104/1153 [02:22<25:18,  1.45s/it]  9%|▉         | 105/1153 [02:23<24:19,  1.39s/it]  9%|▉         | 106/1153 [02:24<23:19,  1.34s/it]  9%|▉         | 107/1153 [02:25<22:43,  1.30s/it]  9%|▉         | 108/1153 [02:27<22:18,  1.28s/it]  9%|▉         | 109/1153 [02:28<21:03,  1.21s/it] 10%|▉         | 110/1153 [02:29<21:05,  1.21s/it] 10%|▉         | 111/1153 [02:30<21:02,  1.21s/it] 10%|▉         | 112/1153 [02:31<21:05,  1.22s/it] 10%|▉         | 113/1153 [02:32<21:04,  1.22s/it] 10%|▉         | 114/1153 [02:34<20:54,  1.21s/it] 10%|▉         | 115/1153 [02:35<21:22,  1.24s/it] 10%|█         | 116/1153 [02:39<35:47,  2.07s/it] 10%|█         | 117/1153 [02:40<31:03,  1.80s/it] 10%|█         | 118/1153 [02:41<27:57,  1.62s/it] 10%|█         | 119/1153 [02:43<25:49,  1.50s/it] 10%|█         | 120/1153 [02:44<24:27,  1.42s/it] 10%|█         | 121/1153 [02:45<23:26,  1.36s/it] 11%|█         | 122/1153 [02:46<21:58,  1.28s/it] 11%|█         | 123/1153 [02:47<21:46,  1.27s/it] 11%|█         | 124/1153 [02:49<21:34,  1.26s/it] 11%|█         | 125/1153 [02:50<21:19,  1.24s/it] 11%|█         | 126/1153 [02:51<20:45,  1.21s/it] 11%|█         | 127/1153 [02:52<19:59,  1.17s/it] 11%|█         | 128/1153 [02:53<20:05,  1.18s/it] 11%|█         | 129/1153 [02:54<20:30,  1.20s/it] 11%|█▏        | 130/1153 [02:56<20:34,  1.21s/it] 11%|█▏        | 131/1153 [02:57<20:41,  1.21s/it] 11%|█▏        | 132/1153 [03:01<34:18,  2.02s/it] 12%|█▏        | 133/1153 [03:02<29:32,  1.74s/it] 12%|█▏        | 134/1153 [03:03<26:12,  1.54s/it] 12%|█▏        | 135/1153 [03:04<23:55,  1.41s/it] 12%|█▏        | 136/1153 [03:05<21:55,  1.29s/it] 12%|█▏        | 137/1153 [03:06<21:05,  1.25s/it] 12%|█▏        | 138/1153 [03:07<20:15,  1.20s/it] 12%|█▏        | 139/1153 [03:08<19:37,  1.16s/it] 12%|█▏        | 140/1153 [03:09<19:12,  1.14s/it] 12%|█▏        | 141/1153 [03:11<18:54,  1.12s/it] 12%|█▏        | 142/1153 [03:12<18:32,  1.10s/it] 12%|█▏        | 143/1153 [03:13<19:08,  1.14s/it] 12%|█▏        | 144/1153 [03:14<19:37,  1.17s/it] 13%|█▎        | 145/1153 [03:15<19:54,  1.19s/it] 13%|█▎        | 146/1153 [03:16<19:21,  1.15s/it] 13%|█▎        | 147/1153 [03:17<18:59,  1.13s/it] 13%|█▎        | 148/1153 [03:19<18:46,  1.12s/it] 13%|█▎        | 149/1153 [03:20<18:35,  1.11s/it] 13%|█▎        | 150/1153 [03:21<19:32,  1.17s/it] 13%|█▎        | 151/1153 [03:25<34:00,  2.04s/it] 13%|█▎        | 152/1153 [03:26<29:34,  1.77s/it] 13%|█▎        | 153/1153 [03:27<26:12,  1.57s/it] 13%|█▎        | 154/1153 [03:28<23:45,  1.43s/it] 13%|█▎        | 155/1153 [03:29<22:09,  1.33s/it] 14%|█▎        | 156/1153 [03:31<21:41,  1.31s/it] 14%|█▎        | 157/1153 [03:32<21:37,  1.30s/it] 14%|█▎        | 158/1153 [03:33<21:15,  1.28s/it] 14%|█▍        | 159/1153 [03:34<21:01,  1.27s/it] 14%|█▍        | 160/1153 [03:36<20:52,  1.26s/it] 14%|█▍        | 161/1153 [03:37<20:47,  1.26s/it] 14%|█▍        | 162/1153 [03:38<20:15,  1.23s/it] 14%|█▍        | 163/1153 [03:39<20:38,  1.25s/it] 14%|█▍        | 164/1153 [03:40<19:44,  1.20s/it] 14%|█▍        | 165/1153 [03:42<19:01,  1.16s/it] 14%|█▍        | 166/1153 [03:43<18:38,  1.13s/it] 14%|█▍        | 167/1153 [03:44<18:24,  1.12s/it] 15%|█▍        | 168/1153 [03:48<34:52,  2.12s/it] 15%|█▍        | 169/1153 [03:49<29:44,  1.81s/it] 15%|█▍        | 170/1153 [03:50<26:11,  1.60s/it] 15%|█▍        | 171/1153 [03:51<23:49,  1.46s/it] 15%|█▍        | 172/1153 [03:53<21:56,  1.34s/it] 15%|█▌        | 173/1153 [03:54<20:48,  1.27s/it] 15%|█▌        | 174/1153 [03:55<19:54,  1.22s/it] 15%|█▌        | 175/1153 [03:56<19:07,  1.17s/it] 15%|█▌        | 176/1153 [03:57<19:00,  1.17s/it] 15%|█▌        | 177/1153 [03:58<19:01,  1.17s/it] 15%|█▌        | 178/1153 [03:59<18:58,  1.17s/it] 16%|█▌        | 179/1153 [04:00<18:48,  1.16s/it] 16%|█▌        | 180/1153 [04:02<18:24,  1.13s/it] 16%|█▌        | 181/1153 [04:03<18:13,  1.13s/it] 16%|█▌        | 182/1153 [04:04<18:03,  1.12s/it] 16%|█▌        | 183/1153 [04:05<18:05,  1.12s/it] 16%|█▌        | 184/1153 [04:06<18:08,  1.12s/it] 16%|█▌        | 185/1153 [04:07<18:08,  1.12s/it] 16%|█▌        | 186/1153 [04:11<32:49,  2.04s/it] 16%|█▌        | 187/1153 [04:12<28:19,  1.76s/it] 16%|█▋        | 188/1153 [04:13<25:05,  1.56s/it] 16%|█▋        | 189/1153 [04:15<22:49,  1.42s/it] 16%|█▋        | 190/1153 [04:16<21:04,  1.31s/it] 17%|█▋        | 191/1153 [04:17<20:42,  1.29s/it] 17%|█▋        | 192/1153 [04:18<20:27,  1.28s/it] 17%|█▋        | 193/1153 [04:19<20:49,  1.30s/it] 17%|█▋        | 194/1153 [04:21<20:52,  1.31s/it] 17%|█▋        | 195/1153 [04:22<20:54,  1.31s/it] 17%|█▋        | 196/1153 [04:23<20:34,  1.29s/it] 17%|█▋        | 197/1153 [04:25<20:20,  1.28s/it] 17%|█▋        | 198/1153 [04:26<20:28,  1.29s/it] 17%|█▋        | 199/1153 [04:27<20:25,  1.28s/it] 17%|█▋        | 200/1153 [04:28<20:22,  1.28s/it] 17%|█▋        | 201/1153 [04:30<20:09,  1.27s/it] 18%|█▊        | 202/1153 [04:31<20:13,  1.28s/it] 18%|█▊        | 203/1153 [04:35<35:10,  2.22s/it] 18%|█▊        | 204/1153 [04:37<30:39,  1.94s/it] 18%|█▊        | 205/1153 [04:38<27:24,  1.73s/it] 18%|█▊        | 206/1153 [04:39<25:00,  1.58s/it] 18%|█▊        | 207/1153 [04:40<23:27,  1.49s/it] 18%|█▊        | 208/1153 [04:42<22:19,  1.42s/it] 18%|█▊        | 209/1153 [04:43<21:32,  1.37s/it] 18%|█▊        | 210/1153 [04:44<19:41,  1.25s/it] 18%|█▊        | 211/1153 [04:45<19:42,  1.26s/it] 18%|█▊        | 212/1153 [04:46<17:31,  1.12s/it] 18%|█▊        | 213/1153 [04:47<18:04,  1.15s/it] 19%|█▊        | 214/1153 [04:48<18:31,  1.18s/it] 19%|█▊        | 215/1153 [04:50<18:51,  1.21s/it] 19%|█▊        | 216/1153 [04:51<19:13,  1.23s/it] 19%|█▉        | 217/1153 [04:52<19:11,  1.23s/it] 19%|█▉        | 218/1153 [04:53<18:54,  1.21s/it] 19%|█▉        | 219/1153 [04:58<32:36,  2.09s/it] 19%|█▉        | 220/1153 [04:59<28:44,  1.85s/it] 19%|█▉        | 221/1153 [05:00<25:47,  1.66s/it] 19%|█▉        | 222/1153 [05:01<23:41,  1.53s/it] 19%|█▉        | 223/1153 [05:03<22:14,  1.44s/it] 19%|█▉        | 224/1153 [05:04<21:15,  1.37s/it] 20%|█▉        | 225/1153 [05:05<20:33,  1.33s/it] 20%|█▉        | 226/1153 [05:06<20:06,  1.30s/it] 20%|█▉        | 227/1153 [05:07<19:40,  1.27s/it] 20%|█▉        | 228/1153 [05:09<19:23,  1.26s/it] 20%|█▉        | 229/1153 [05:10<18:30,  1.20s/it] 20%|█▉        | 230/1153 [05:11<18:34,  1.21s/it] 20%|██        | 231/1153 [05:12<18:02,  1.17s/it] 20%|██        | 232/1153 [05:13<18:32,  1.21s/it] 20%|██        | 233/1153 [05:15<18:34,  1.21s/it] 20%|██        | 234/1153 [05:16<18:45,  1.22s/it] 20%|██        | 235/1153 [05:17<18:56,  1.24s/it] 20%|██        | 236/1153 [05:21<32:11,  2.11s/it] 21%|██        | 237/1153 [05:22<28:20,  1.86s/it] 21%|██        | 238/1153 [05:24<25:27,  1.67s/it] 21%|██        | 239/1153 [05:25<23:21,  1.53s/it] 21%|██        | 240/1153 [05:26<21:55,  1.44s/it] 21%|██        | 241/1153 [05:27<20:22,  1.34s/it] 21%|██        | 242/1153 [05:28<19:51,  1.31s/it] 21%|██        | 243/1153 [05:30<19:29,  1.29s/it] 21%|██        | 244/1153 [05:31<19:12,  1.27s/it] 21%|██        | 245/1153 [05:32<17:37,  1.16s/it] 21%|██▏       | 246/1153 [05:33<16:59,  1.12s/it] 21%|██▏       | 247/1153 [05:34<17:34,  1.16s/it] 22%|██▏       | 248/1153 [05:35<17:56,  1.19s/it] 22%|██▏       | 249/1153 [05:37<18:18,  1.21s/it] 22%|██▏       | 250/1153 [05:38<18:42,  1.24s/it] 22%|██▏       | 251/1153 [05:39<18:42,  1.24s/it] 22%|██▏       | 252/1153 [05:43<30:29,  2.03s/it] 22%|██▏       | 253/1153 [05:44<27:17,  1.82s/it] 22%|██▏       | 254/1153 [05:46<24:33,  1.64s/it] 22%|██▏       | 255/1153 [05:47<22:05,  1.48s/it] 22%|██▏       | 256/1153 [05:48<20:53,  1.40s/it] 22%|██▏       | 257/1153 [05:49<20:26,  1.37s/it] 22%|██▏       | 258/1153 [05:50<19:29,  1.31s/it] 22%|██▏       | 259/1153 [05:52<19:57,  1.34s/it] 23%|██▎       | 260/1153 [05:53<19:37,  1.32s/it] 23%|██▎       | 261/1153 [05:54<19:16,  1.30s/it] 23%|██▎       | 262/1153 [05:55<18:22,  1.24s/it] 23%|██▎       | 263/1153 [05:57<17:51,  1.20s/it] 23%|██▎       | 264/1153 [05:58<17:39,  1.19s/it] 23%|██▎       | 265/1153 [05:59<17:53,  1.21s/it] 23%|██▎       | 266/1153 [06:00<17:18,  1.17s/it] 23%|██▎       | 267/1153 [06:01<16:57,  1.15s/it] 23%|██▎       | 268/1153 [06:03<17:45,  1.20s/it] 23%|██▎       | 269/1153 [06:04<17:12,  1.17s/it] 23%|██▎       | 270/1153 [06:08<30:10,  2.05s/it] 24%|██▎       | 271/1153 [06:09<25:56,  1.76s/it] 24%|██▎       | 272/1153 [06:10<23:31,  1.60s/it] 24%|██▎       | 273/1153 [06:11<21:16,  1.45s/it] 24%|██▍       | 274/1153 [06:12<20:04,  1.37s/it] 24%|██▍       | 275/1153 [06:13<19:02,  1.30s/it] 24%|██▍       | 276/1153 [06:15<18:07,  1.24s/it] 24%|██▍       | 277/1153 [06:16<17:28,  1.20s/it] 24%|██▍       | 278/1153 [06:17<17:05,  1.17s/it] 24%|██▍       | 279/1153 [06:18<16:45,  1.15s/it] 24%|██▍       | 280/1153 [06:19<16:31,  1.14s/it] 24%|██▍       | 281/1153 [06:20<16:11,  1.11s/it] 24%|██▍       | 282/1153 [06:21<16:23,  1.13s/it] 25%|██▍       | 283/1153 [06:22<16:13,  1.12s/it] 25%|██▍       | 284/1153 [06:24<16:49,  1.16s/it] 25%|██▍       | 285/1153 [06:25<17:19,  1.20s/it] 25%|██▍       | 286/1153 [06:26<17:36,  1.22s/it] 25%|██▍       | 287/1153 [06:27<17:47,  1.23s/it] 25%|██▍       | 288/1153 [06:32<32:12,  2.23s/it] 25%|██▌       | 289/1153 [06:33<28:11,  1.96s/it] 25%|██▌       | 290/1153 [06:35<25:13,  1.75s/it] 25%|██▌       | 291/1153 [06:36<22:35,  1.57s/it] 25%|██▌       | 292/1153 [06:37<20:28,  1.43s/it] 25%|██▌       | 293/1153 [06:38<19:06,  1.33s/it] 25%|██▌       | 294/1153 [06:39<18:00,  1.26s/it] 26%|██▌       | 295/1153 [06:40<17:24,  1.22s/it] 26%|██▌       | 296/1153 [06:41<16:57,  1.19s/it] 26%|██▌       | 297/1153 [06:42<16:47,  1.18s/it] 26%|██▌       | 298/1153 [06:44<16:54,  1.19s/it] 26%|██▌       | 299/1153 [06:45<16:28,  1.16s/it] 26%|██▌       | 300/1153 [06:46<16:20,  1.15s/it] 26%|██▌       | 301/1153 [06:47<16:02,  1.13s/it] 26%|██▌       | 302/1153 [06:48<15:48,  1.11s/it] 26%|██▋       | 303/1153 [06:49<15:42,  1.11s/it] 26%|██▋       | 304/1153 [06:50<15:49,  1.12s/it] 26%|██▋       | 305/1153 [06:51<15:50,  1.12s/it] 27%|██▋       | 306/1153 [06:53<16:13,  1.15s/it] 27%|██▋       | 307/1153 [06:57<29:19,  2.08s/it] 27%|██▋       | 308/1153 [06:58<25:20,  1.80s/it] 27%|██▋       | 309/1153 [06:59<22:25,  1.59s/it] 27%|██▋       | 310/1153 [07:00<20:22,  1.45s/it] 27%|██▋       | 311/1153 [07:01<19:26,  1.39s/it] 27%|██▋       | 312/1153 [07:02<18:16,  1.30s/it] 27%|██▋       | 313/1153 [07:04<17:21,  1.24s/it] 27%|██▋       | 314/1153 [07:05<16:53,  1.21s/it] 27%|██▋       | 315/1153 [07:06<16:26,  1.18s/it] 27%|██▋       | 316/1153 [07:07<16:04,  1.15s/it] 27%|██▋       | 317/1153 [07:08<15:53,  1.14s/it] 28%|██▊       | 318/1153 [07:09<15:46,  1.13s/it] 28%|██▊       | 319/1153 [07:10<16:26,  1.18s/it] 28%|██▊       | 320/1153 [07:12<16:42,  1.20s/it] 28%|██▊       | 321/1153 [07:13<16:23,  1.18s/it] 28%|██▊       | 322/1153 [07:14<16:50,  1.22s/it] 28%|██▊       | 323/1153 [07:15<16:55,  1.22s/it] 28%|██▊       | 324/1153 [07:17<17:02,  1.23s/it] 28%|██▊       | 325/1153 [07:21<29:29,  2.14s/it] 28%|██▊       | 326/1153 [07:22<25:45,  1.87s/it] 28%|██▊       | 327/1153 [07:23<23:05,  1.68s/it] 28%|██▊       | 328/1153 [07:25<21:15,  1.55s/it] 29%|██▊       | 329/1153 [07:26<20:00,  1.46s/it] 29%|██▊       | 330/1153 [07:27<19:24,  1.42s/it] 29%|██▊       | 331/1153 [07:28<18:50,  1.37s/it] 29%|██▉       | 332/1153 [07:30<18:24,  1.35s/it] 29%|██▉       | 333/1153 [07:31<18:11,  1.33s/it] 29%|██▉       | 334/1153 [07:32<17:58,  1.32s/it] 29%|██▉       | 335/1153 [07:34<17:42,  1.30s/it] 29%|██▉       | 336/1153 [07:35<17:36,  1.29s/it] 29%|██▉       | 337/1153 [07:36<17:30,  1.29s/it] 29%|██▉       | 338/1153 [07:37<17:19,  1.28s/it] 29%|██▉       | 339/1153 [07:39<17:26,  1.29s/it] 29%|██▉       | 340/1153 [07:40<17:15,  1.27s/it] 30%|██▉       | 341/1153 [07:41<17:14,  1.27s/it] 30%|██▉       | 342/1153 [07:45<28:52,  2.14s/it] 30%|██▉       | 343/1153 [07:47<25:29,  1.89s/it] 30%|██▉       | 344/1153 [07:48<22:53,  1.70s/it] 30%|██▉       | 345/1153 [07:49<21:13,  1.58s/it] 30%|███       | 346/1153 [07:50<20:00,  1.49s/it] 30%|███       | 347/1153 [07:52<19:06,  1.42s/it] 30%|███       | 348/1153 [07:53<18:31,  1.38s/it] 30%|███       | 349/1153 [07:54<18:15,  1.36s/it] 30%|███       | 350/1153 [07:56<17:46,  1.33s/it] 30%|███       | 351/1153 [07:57<17:25,  1.30s/it] 31%|███       | 352/1153 [07:58<17:23,  1.30s/it] 31%|███       | 353/1153 [07:59<17:11,  1.29s/it] 31%|███       | 354/1153 [08:01<16:34,  1.24s/it] 31%|███       | 355/1153 [08:02<16:42,  1.26s/it] 31%|███       | 356/1153 [08:03<16:42,  1.26s/it] 31%|███       | 357/1153 [08:04<16:43,  1.26s/it] 31%|███       | 358/1153 [08:08<28:17,  2.13s/it] 31%|███       | 359/1153 [08:10<24:44,  1.87s/it] 31%|███       | 360/1153 [08:11<22:13,  1.68s/it] 31%|███▏      | 361/1153 [08:12<20:30,  1.55s/it] 31%|███▏      | 362/1153 [08:14<19:18,  1.47s/it] 31%|███▏      | 363/1153 [08:15<18:29,  1.40s/it] 32%|███▏      | 364/1153 [08:16<18:00,  1.37s/it] 32%|███▏      | 365/1153 [08:17<17:21,  1.32s/it] 32%|███▏      | 366/1153 [08:19<17:05,  1.30s/it] 32%|███▏      | 367/1153 [08:20<16:53,  1.29s/it] 32%|███▏      | 368/1153 [08:21<16:58,  1.30s/it] 32%|███▏      | 369/1153 [08:22<14:36,  1.12s/it] 32%|███▏      | 370/1153 [08:23<15:14,  1.17s/it] 32%|███▏      | 371/1153 [08:24<15:31,  1.19s/it] 32%|███▏      | 372/1153 [08:26<15:48,  1.21s/it] 32%|███▏      | 373/1153 [08:27<15:51,  1.22s/it] 32%|███▏      | 374/1153 [08:28<16:03,  1.24s/it] 33%|███▎      | 375/1153 [08:32<27:03,  2.09s/it] 33%|███▎      | 376/1153 [08:33<23:51,  1.84s/it] 33%|███▎      | 377/1153 [08:35<21:31,  1.66s/it] 33%|███▎      | 378/1153 [08:36<19:57,  1.55s/it] 33%|███▎      | 379/1153 [08:37<18:50,  1.46s/it] 33%|███▎      | 380/1153 [08:39<18:08,  1.41s/it] 33%|███▎      | 381/1153 [08:40<17:32,  1.36s/it] 33%|███▎      | 382/1153 [08:41<17:30,  1.36s/it] 33%|███▎      | 383/1153 [08:42<17:04,  1.33s/it] 33%|███▎      | 384/1153 [08:44<16:47,  1.31s/it] 33%|███▎      | 385/1153 [08:45<16:59,  1.33s/it] 33%|███▎      | 386/1153 [08:46<17:05,  1.34s/it] 34%|███▎      | 387/1153 [08:48<17:09,  1.34s/it] 34%|███▎      | 388/1153 [08:49<16:58,  1.33s/it] 34%|███▎      | 389/1153 [08:50<16:37,  1.31s/it] 34%|███▍      | 390/1153 [08:52<16:25,  1.29s/it] 34%|███▍      | 391/1153 [08:53<16:18,  1.28s/it] 34%|███▍      | 392/1153 [08:57<29:03,  2.29s/it] 34%|███▍      | 393/1153 [08:59<25:14,  1.99s/it] 34%|███▍      | 394/1153 [09:00<22:24,  1.77s/it] 34%|███▍      | 395/1153 [09:01<19:40,  1.56s/it] 34%|███▍      | 396/1153 [09:02<18:29,  1.47s/it] 34%|███▍      | 397/1153 [09:04<17:35,  1.40s/it] 35%|███▍      | 398/1153 [09:05<17:19,  1.38s/it] 35%|███▍      | 399/1153 [09:06<16:57,  1.35s/it] 35%|███▍      | 400/1153 [09:07<16:36,  1.32s/it] 35%|███▍      | 401/1153 [09:09<16:19,  1.30s/it] 35%|███▍      | 402/1153 [09:10<16:27,  1.32s/it] 35%|███▍      | 403/1153 [09:11<15:36,  1.25s/it] 35%|███▌      | 404/1153 [09:12<15:14,  1.22s/it] 35%|███▌      | 405/1153 [09:13<14:56,  1.20s/it] 35%|███▌      | 406/1153 [09:15<15:08,  1.22s/it] 35%|███▌      | 407/1153 [09:16<15:18,  1.23s/it] 35%|███▌      | 408/1153 [09:17<15:09,  1.22s/it] 35%|███▌      | 409/1153 [09:18<14:48,  1.19s/it] 36%|███▌      | 410/1153 [09:23<26:13,  2.12s/it] 36%|███▌      | 411/1153 [09:24<23:11,  1.88s/it] 36%|███▌      | 412/1153 [09:25<20:27,  1.66s/it] 36%|███▌      | 413/1153 [09:26<18:29,  1.50s/it] 36%|███▌      | 414/1153 [09:27<16:59,  1.38s/it] 36%|███▌      | 415/1153 [09:28<16:13,  1.32s/it] 36%|███▌      | 416/1153 [09:30<15:26,  1.26s/it] 36%|███▌      | 417/1153 [09:31<14:53,  1.21s/it] 36%|███▋      | 418/1153 [09:32<14:41,  1.20s/it] 36%|███▋      | 419/1153 [09:33<14:58,  1.22s/it] 36%|███▋      | 420/1153 [09:34<14:28,  1.18s/it] 37%|███▋      | 421/1153 [09:35<14:32,  1.19s/it] 37%|███▋      | 422/1153 [09:37<14:17,  1.17s/it] 37%|███▋      | 423/1153 [09:38<14:38,  1.20s/it] 37%|███▋      | 424/1153 [09:39<14:16,  1.17s/it] 37%|███▋      | 425/1153 [09:40<14:03,  1.16s/it] 37%|███▋      | 426/1153 [09:41<13:52,  1.15s/it] 37%|███▋      | 427/1153 [09:42<14:20,  1.19s/it] 37%|███▋      | 428/1153 [09:47<25:54,  2.14s/it] 37%|███▋      | 429/1153 [09:48<22:09,  1.84s/it] 37%|███▋      | 430/1153 [09:49<19:32,  1.62s/it] 37%|███▋      | 431/1153 [09:50<18:31,  1.54s/it] 37%|███▋      | 432/1153 [09:52<17:27,  1.45s/it] 38%|███▊      | 433/1153 [09:53<16:43,  1.39s/it] 38%|███▊      | 434/1153 [09:54<16:09,  1.35s/it] 38%|███▊      | 435/1153 [09:55<15:53,  1.33s/it] 38%|███▊      | 436/1153 [09:57<15:06,  1.26s/it] 38%|███▊      | 437/1153 [09:58<14:35,  1.22s/it] 38%|███▊      | 438/1153 [09:59<14:13,  1.19s/it] 38%|███▊      | 439/1153 [10:00<14:32,  1.22s/it] 38%|███▊      | 440/1153 [10:01<14:13,  1.20s/it] 38%|███▊      | 441/1153 [10:03<14:31,  1.22s/it] 38%|███▊      | 442/1153 [10:04<14:05,  1.19s/it] 38%|███▊      | 443/1153 [10:05<13:45,  1.16s/it] 39%|███▊      | 444/1153 [10:06<13:46,  1.17s/it] 39%|███▊      | 445/1153 [10:07<14:22,  1.22s/it] 39%|███▊      | 446/1153 [10:11<25:06,  2.13s/it] 39%|███▉      | 447/1153 [10:13<21:31,  1.83s/it] 39%|███▉      | 448/1153 [10:14<19:01,  1.62s/it] 39%|███▉      | 449/1153 [10:15<17:14,  1.47s/it] 39%|███▉      | 450/1153 [10:16<16:00,  1.37s/it] 39%|███▉      | 451/1153 [10:17<15:08,  1.29s/it] 39%|███▉      | 452/1153 [10:18<14:41,  1.26s/it] 39%|███▉      | 453/1153 [10:20<14:46,  1.27s/it] 39%|███▉      | 454/1153 [10:21<14:44,  1.27s/it] 39%|███▉      | 455/1153 [10:22<14:18,  1.23s/it] 40%|███▉      | 456/1153 [10:23<13:53,  1.20s/it] 40%|███▉      | 457/1153 [10:24<13:38,  1.18s/it] 40%|███▉      | 458/1153 [10:25<13:27,  1.16s/it] 40%|███▉      | 459/1153 [10:26<13:23,  1.16s/it] 40%|███▉      | 460/1153 [10:28<13:18,  1.15s/it] 40%|███▉      | 461/1153 [10:29<13:16,  1.15s/it] 40%|████      | 462/1153 [10:30<13:16,  1.15s/it] 40%|████      | 463/1153 [10:31<13:11,  1.15s/it] 40%|████      | 464/1153 [10:32<13:18,  1.16s/it] 40%|████      | 465/1153 [10:36<23:16,  2.03s/it] 40%|████      | 466/1153 [10:38<20:39,  1.80s/it] 41%|████      | 467/1153 [10:39<17:53,  1.56s/it] 41%|████      | 468/1153 [10:40<16:48,  1.47s/it] 41%|████      | 469/1153 [10:41<16:08,  1.42s/it] 41%|████      | 470/1153 [10:42<15:38,  1.37s/it] 41%|████      | 471/1153 [10:44<15:18,  1.35s/it] 41%|████      | 472/1153 [10:45<14:28,  1.27s/it] 41%|████      | 473/1153 [10:46<14:09,  1.25s/it] 41%|████      | 474/1153 [10:47<14:14,  1.26s/it] 41%|████      | 475/1153 [10:49<14:22,  1.27s/it] 41%|████▏     | 476/1153 [10:50<14:22,  1.27s/it] 41%|████▏     | 477/1153 [10:51<14:25,  1.28s/it] 41%|████▏     | 478/1153 [10:52<14:24,  1.28s/it] 42%|████▏     | 479/1153 [10:54<14:25,  1.28s/it] 42%|████▏     | 480/1153 [10:55<14:20,  1.28s/it] 42%|████▏     | 481/1153 [10:56<14:00,  1.25s/it] 42%|████▏     | 482/1153 [10:57<14:06,  1.26s/it] 42%|████▏     | 483/1153 [11:01<23:00,  2.06s/it] 42%|████▏     | 484/1153 [11:03<20:22,  1.83s/it] 42%|████▏     | 485/1153 [11:04<17:25,  1.57s/it] 42%|████▏     | 486/1153 [11:05<15:47,  1.42s/it] 42%|████▏     | 487/1153 [11:06<15:14,  1.37s/it] 42%|████▏     | 488/1153 [11:07<15:09,  1.37s/it] 42%|████▏     | 489/1153 [11:09<15:11,  1.37s/it] 42%|████▏     | 490/1153 [11:10<14:56,  1.35s/it] 43%|████▎     | 491/1153 [11:11<14:44,  1.34s/it] 43%|████▎     | 492/1153 [11:13<14:38,  1.33s/it] 43%|████▎     | 493/1153 [11:14<14:25,  1.31s/it] 43%|████▎     | 494/1153 [11:15<14:07,  1.29s/it] 43%|████▎     | 495/1153 [11:16<14:08,  1.29s/it] 43%|████▎     | 496/1153 [11:18<14:10,  1.29s/it] 43%|████▎     | 497/1153 [11:19<14:24,  1.32s/it] 43%|████▎     | 498/1153 [11:20<14:19,  1.31s/it] 43%|████▎     | 499/1153 [11:22<14:13,  1.30s/it] 43%|████▎     | 500/1153 [11:26<24:25,  2.24s/it] 43%|████▎     | 501/1153 [11:28<21:36,  1.99s/it] 44%|████▎     | 502/1153 [11:29<18:50,  1.74s/it] 44%|████▎     | 503/1153 [11:30<17:20,  1.60s/it] 44%|████▎     | 504/1153 [11:31<14:35,  1.35s/it] 44%|████▍     | 505/1153 [11:32<14:20,  1.33s/it] 44%|████▍     | 506/1153 [11:33<14:12,  1.32s/it] 44%|████▍     | 507/1153 [11:35<14:06,  1.31s/it] 44%|████▍     | 508/1153 [11:36<13:14,  1.23s/it] 44%|████▍     | 509/1153 [11:36<11:06,  1.03s/it] 44%|████▍     | 510/1153 [11:38<11:56,  1.11s/it] 44%|████▍     | 511/1153 [11:38<10:38,  1.01it/s] 44%|████▍     | 512/1153 [11:40<11:37,  1.09s/it] 44%|████▍     | 513/1153 [11:40<10:32,  1.01it/s] 45%|████▍     | 514/1153 [11:41<09:48,  1.09it/s] 45%|████▍     | 515/1153 [11:42<10:54,  1.03s/it] 45%|████▍     | 516/1153 [11:44<11:36,  1.09s/it] 45%|████▍     | 517/1153 [11:45<11:41,  1.10s/it] 45%|████▍     | 518/1153 [11:46<12:13,  1.16s/it] 45%|████▌     | 519/1153 [11:50<21:45,  2.06s/it] 45%|████▌     | 520/1153 [11:52<19:42,  1.87s/it] 45%|████▌     | 521/1153 [11:53<18:08,  1.72s/it] 45%|████▌     | 522/1153 [11:54<16:58,  1.61s/it] 45%|████▌     | 523/1153 [11:56<15:56,  1.52s/it] 45%|████▌     | 524/1153 [11:57<15:10,  1.45s/it] 46%|████▌     | 525/1153 [11:58<14:39,  1.40s/it] 46%|████▌     | 526/1153 [11:59<14:18,  1.37s/it] 46%|████▌     | 527/1153 [12:01<14:05,  1.35s/it] 46%|████▌     | 528/1153 [12:02<14:05,  1.35s/it] 46%|████▌     | 529/1153 [12:03<14:03,  1.35s/it] 46%|████▌     | 530/1153 [12:05<13:49,  1.33s/it] 46%|████▌     | 531/1153 [12:06<13:39,  1.32s/it] 46%|████▌     | 532/1153 [12:07<13:39,  1.32s/it] 46%|████▌     | 533/1153 [12:09<13:01,  1.26s/it] 46%|████▋     | 534/1153 [12:10<12:35,  1.22s/it] 46%|████▋     | 535/1153 [12:11<12:43,  1.24s/it] 46%|████▋     | 536/1153 [12:12<12:36,  1.23s/it] 47%|████▋     | 537/1153 [12:16<22:14,  2.17s/it] 47%|████▋     | 538/1153 [12:18<19:05,  1.86s/it] 47%|████▋     | 539/1153 [12:19<16:41,  1.63s/it] 47%|████▋     | 540/1153 [12:20<15:51,  1.55s/it] 47%|████▋     | 541/1153 [12:21<15:02,  1.47s/it] 47%|████▋     | 542/1153 [12:23<14:19,  1.41s/it] 47%|████▋     | 543/1153 [12:24<13:28,  1.33s/it] 47%|████▋     | 544/1153 [12:25<12:48,  1.26s/it] 47%|████▋     | 545/1153 [12:26<12:24,  1.22s/it] 47%|████▋     | 546/1153 [12:27<12:03,  1.19s/it] 47%|████▋     | 547/1153 [12:28<12:00,  1.19s/it] 48%|████▊     | 548/1153 [12:29<11:52,  1.18s/it] 48%|████▊     | 549/1153 [12:31<11:43,  1.17s/it] 48%|████▊     | 550/1153 [12:32<12:03,  1.20s/it] 48%|████▊     | 551/1153 [12:33<11:51,  1.18s/it] 48%|████▊     | 552/1153 [12:34<11:39,  1.16s/it] 48%|████▊     | 553/1153 [12:35<11:31,  1.15s/it] 48%|████▊     | 554/1153 [12:36<11:31,  1.15s/it] 48%|████▊     | 555/1153 [12:38<11:29,  1.15s/it] 48%|████▊     | 556/1153 [12:42<20:23,  2.05s/it] 48%|████▊     | 557/1153 [12:43<17:37,  1.77s/it] 48%|████▊     | 558/1153 [12:44<15:39,  1.58s/it] 48%|████▊     | 559/1153 [12:45<14:19,  1.45s/it] 49%|████▊     | 560/1153 [12:46<13:31,  1.37s/it] 49%|████▊     | 561/1153 [12:47<12:52,  1.30s/it] 49%|████▊     | 562/1153 [12:49<12:16,  1.25s/it] 49%|████▉     | 563/1153 [12:50<11:57,  1.22s/it] 49%|████▉     | 564/1153 [12:51<11:44,  1.20s/it] 49%|████▉     | 565/1153 [12:52<11:36,  1.18s/it] 49%|████▉     | 566/1153 [12:53<12:03,  1.23s/it] 49%|████▉     | 567/1153 [12:55<11:54,  1.22s/it] 49%|████▉     | 568/1153 [12:56<11:41,  1.20s/it] 49%|████▉     | 569/1153 [12:57<11:28,  1.18s/it] 49%|████▉     | 570/1153 [12:58<11:25,  1.18s/it] 50%|████▉     | 571/1153 [12:59<11:16,  1.16s/it] 50%|████▉     | 572/1153 [13:00<11:06,  1.15s/it] 50%|████▉     | 573/1153 [13:01<11:08,  1.15s/it] 50%|████▉     | 574/1153 [13:03<11:03,  1.15s/it] 50%|████▉     | 575/1153 [13:04<10:59,  1.14s/it] 50%|████▉     | 576/1153 [13:08<19:04,  1.98s/it] 50%|█████     | 577/1153 [13:09<16:41,  1.74s/it] 50%|█████     | 578/1153 [13:10<14:48,  1.55s/it] 50%|█████     | 579/1153 [13:11<13:38,  1.43s/it] 50%|█████     | 580/1153 [13:12<12:48,  1.34s/it] 50%|█████     | 581/1153 [13:13<12:15,  1.29s/it] 50%|█████     | 582/1153 [13:14<11:47,  1.24s/it] 51%|█████     | 583/1153 [13:16<11:25,  1.20s/it] 51%|█████     | 584/1153 [13:17<11:14,  1.19s/it] 51%|█████     | 585/1153 [13:18<11:05,  1.17s/it] 51%|█████     | 586/1153 [13:19<10:55,  1.16s/it] 51%|█████     | 587/1153 [13:20<10:53,  1.15s/it] 51%|█████     | 588/1153 [13:21<10:50,  1.15s/it] 51%|█████     | 589/1153 [13:23<11:15,  1.20s/it] 51%|█████     | 590/1153 [13:24<11:00,  1.17s/it] 51%|█████▏    | 591/1153 [13:25<11:19,  1.21s/it] 51%|█████▏    | 592/1153 [13:26<11:07,  1.19s/it] 51%|█████▏    | 593/1153 [13:27<11:33,  1.24s/it] 52%|█████▏    | 594/1153 [13:29<11:41,  1.25s/it] 52%|█████▏    | 595/1153 [13:30<11:47,  1.27s/it] 52%|█████▏    | 596/1153 [13:34<19:05,  2.06s/it] 52%|█████▏    | 597/1153 [13:35<17:01,  1.84s/it] 52%|█████▏    | 598/1153 [13:37<15:32,  1.68s/it] 52%|█████▏    | 599/1153 [13:38<13:42,  1.48s/it] 52%|█████▏    | 600/1153 [13:39<13:12,  1.43s/it] 52%|█████▏    | 601/1153 [13:40<12:48,  1.39s/it] 52%|█████▏    | 602/1153 [13:42<12:41,  1.38s/it] 52%|█████▏    | 603/1153 [13:43<12:28,  1.36s/it] 52%|█████▏    | 604/1153 [13:44<12:12,  1.33s/it] 52%|█████▏    | 605/1153 [13:45<12:07,  1.33s/it] 53%|█████▎    | 606/1153 [13:47<12:00,  1.32s/it] 53%|█████▎    | 607/1153 [13:48<11:56,  1.31s/it] 53%|█████▎    | 608/1153 [13:49<10:56,  1.21s/it] 53%|█████▎    | 609/1153 [13:50<10:47,  1.19s/it] 53%|█████▎    | 610/1153 [13:52<11:14,  1.24s/it] 53%|█████▎    | 611/1153 [13:53<11:26,  1.27s/it] 53%|█████▎    | 612/1153 [13:54<11:32,  1.28s/it] 53%|█████▎    | 613/1153 [13:55<11:30,  1.28s/it] 53%|█████▎    | 614/1153 [13:59<18:41,  2.08s/it] 53%|█████▎    | 615/1153 [14:01<16:38,  1.86s/it] 53%|█████▎    | 616/1153 [14:02<15:07,  1.69s/it] 54%|█████▎    | 617/1153 [14:03<14:05,  1.58s/it] 54%|█████▎    | 618/1153 [14:05<13:25,  1.51s/it] 54%|█████▎    | 619/1153 [14:06<12:48,  1.44s/it] 54%|█████▍    | 620/1153 [14:07<12:23,  1.39s/it] 54%|█████▍    | 621/1153 [14:09<12:07,  1.37s/it] 54%|█████▍    | 622/1153 [14:10<11:26,  1.29s/it] 54%|█████▍    | 623/1153 [14:11<11:24,  1.29s/it] 54%|█████▍    | 624/1153 [14:12<11:16,  1.28s/it] 54%|█████▍    | 625/1153 [14:14<11:20,  1.29s/it] 54%|█████▍    | 626/1153 [14:15<11:18,  1.29s/it] 54%|█████▍    | 627/1153 [14:16<10:53,  1.24s/it] 54%|█████▍    | 628/1153 [14:17<11:04,  1.26s/it] 55%|█████▍    | 629/1153 [14:19<11:05,  1.27s/it] 55%|█████▍    | 630/1153 [14:20<11:06,  1.27s/it] 55%|█████▍    | 631/1153 [14:21<11:08,  1.28s/it] 55%|█████▍    | 632/1153 [14:25<18:14,  2.10s/it] 55%|█████▍    | 633/1153 [14:26<15:40,  1.81s/it] 55%|█████▍    | 634/1153 [14:27<13:47,  1.59s/it] 55%|█████▌    | 635/1153 [14:29<12:58,  1.50s/it] 55%|█████▌    | 636/1153 [14:30<12:31,  1.45s/it] 55%|█████▌    | 637/1153 [14:31<12:06,  1.41s/it] 55%|█████▌    | 638/1153 [14:33<11:50,  1.38s/it] 55%|█████▌    | 639/1153 [14:34<11:43,  1.37s/it] 56%|█████▌    | 640/1153 [14:35<11:40,  1.37s/it] 56%|█████▌    | 641/1153 [14:37<11:31,  1.35s/it] 56%|█████▌    | 642/1153 [14:38<11:29,  1.35s/it] 56%|█████▌    | 643/1153 [14:39<11:23,  1.34s/it] 56%|█████▌    | 644/1153 [14:41<11:18,  1.33s/it] 56%|█████▌    | 645/1153 [14:42<11:19,  1.34s/it] 56%|█████▌    | 646/1153 [14:43<11:10,  1.32s/it] 56%|█████▌    | 647/1153 [14:44<09:13,  1.09s/it] 56%|█████▌    | 648/1153 [14:45<09:44,  1.16s/it] 56%|█████▋    | 649/1153 [14:47<10:15,  1.22s/it] 56%|█████▋    | 650/1153 [14:50<17:03,  2.03s/it] 56%|█████▋    | 651/1153 [14:52<15:18,  1.83s/it] 57%|█████▋    | 652/1153 [14:52<12:09,  1.46s/it] 57%|█████▋    | 653/1153 [14:54<11:23,  1.37s/it] 57%|█████▋    | 654/1153 [14:55<11:11,  1.35s/it] 57%|█████▋    | 655/1153 [14:55<09:21,  1.13s/it] 57%|█████▋    | 656/1153 [14:56<08:06,  1.02it/s] 57%|█████▋    | 657/1153 [14:57<08:51,  1.07s/it] 57%|█████▋    | 658/1153 [14:59<09:24,  1.14s/it] 57%|█████▋    | 659/1153 [15:00<09:47,  1.19s/it] 57%|█████▋    | 660/1153 [15:01<10:04,  1.23s/it] 57%|█████▋    | 661/1153 [15:03<10:12,  1.25s/it] 57%|█████▋    | 662/1153 [15:04<10:16,  1.26s/it] 58%|█████▊    | 663/1153 [15:05<10:21,  1.27s/it] 58%|█████▊    | 664/1153 [15:06<10:24,  1.28s/it] 58%|█████▊    | 665/1153 [15:08<10:28,  1.29s/it] 58%|█████▊    | 666/1153 [15:09<10:31,  1.30s/it] 58%|█████▊    | 667/1153 [15:10<10:30,  1.30s/it] 58%|█████▊    | 668/1153 [15:12<10:28,  1.30s/it] 58%|█████▊    | 669/1153 [15:16<17:07,  2.12s/it] 58%|█████▊    | 670/1153 [15:17<15:11,  1.89s/it] 58%|█████▊    | 671/1153 [15:18<13:45,  1.71s/it] 58%|█████▊    | 672/1153 [15:20<12:47,  1.60s/it] 58%|█████▊    | 673/1153 [15:21<11:59,  1.50s/it] 58%|█████▊    | 674/1153 [15:22<11:29,  1.44s/it] 59%|█████▊    | 675/1153 [15:24<11:05,  1.39s/it] 59%|█████▊    | 676/1153 [15:25<10:40,  1.34s/it] 59%|█████▊    | 677/1153 [15:26<10:28,  1.32s/it] 59%|█████▉    | 678/1153 [15:27<09:58,  1.26s/it] 59%|█████▉    | 679/1153 [15:28<10:03,  1.27s/it] 59%|█████▉    | 680/1153 [15:30<09:51,  1.25s/it] 59%|█████▉    | 681/1153 [15:31<09:30,  1.21s/it] 59%|█████▉    | 682/1153 [15:32<09:42,  1.24s/it] 59%|█████▉    | 683/1153 [15:33<10:01,  1.28s/it] 59%|█████▉    | 684/1153 [15:35<09:41,  1.24s/it] 59%|█████▉    | 685/1153 [15:36<09:49,  1.26s/it] 59%|█████▉    | 686/1153 [15:37<10:02,  1.29s/it] 60%|█████▉    | 687/1153 [15:38<09:42,  1.25s/it] 60%|█████▉    | 688/1153 [15:42<15:21,  1.98s/it] 60%|█████▉    | 689/1153 [15:43<13:50,  1.79s/it] 60%|█████▉    | 690/1153 [15:45<12:16,  1.59s/it] 60%|█████▉    | 691/1153 [15:46<11:12,  1.46s/it] 60%|██████    | 692/1153 [15:47<10:31,  1.37s/it] 60%|██████    | 693/1153 [15:48<09:53,  1.29s/it] 60%|██████    | 694/1153 [15:49<09:55,  1.30s/it] 60%|██████    | 695/1153 [15:51<09:58,  1.31s/it] 60%|██████    | 696/1153 [15:52<10:01,  1.32s/it] 60%|██████    | 697/1153 [15:53<09:55,  1.31s/it] 61%|██████    | 698/1153 [15:55<09:59,  1.32s/it] 61%|██████    | 699/1153 [15:56<09:54,  1.31s/it] 61%|██████    | 700/1153 [15:57<09:53,  1.31s/it] 61%|██████    | 701/1153 [15:59<09:51,  1.31s/it] 61%|██████    | 702/1153 [16:00<09:48,  1.30s/it] 61%|██████    | 703/1153 [16:01<09:47,  1.31s/it] 61%|██████    | 704/1153 [16:02<09:47,  1.31s/it] 61%|██████    | 705/1153 [16:04<09:44,  1.30s/it] 61%|██████    | 706/1153 [16:08<16:01,  2.15s/it] 61%|██████▏   | 707/1153 [16:09<14:08,  1.90s/it] 61%|██████▏   | 708/1153 [16:10<12:41,  1.71s/it] 61%|██████▏   | 709/1153 [16:12<11:44,  1.59s/it] 62%|██████▏   | 710/1153 [16:13<11:06,  1.51s/it] 62%|██████▏   | 711/1153 [16:14<10:37,  1.44s/it] 62%|██████▏   | 712/1153 [16:16<10:23,  1.41s/it] 62%|██████▏   | 713/1153 [16:17<10:11,  1.39s/it] 62%|██████▏   | 714/1153 [16:18<10:01,  1.37s/it] 62%|██████▏   | 715/1153 [16:20<09:50,  1.35s/it] 62%|██████▏   | 716/1153 [16:21<09:45,  1.34s/it] 62%|██████▏   | 717/1153 [16:22<09:40,  1.33s/it] 62%|██████▏   | 718/1153 [16:24<09:39,  1.33s/it] 62%|██████▏   | 719/1153 [16:25<09:30,  1.32s/it] 62%|██████▏   | 720/1153 [16:26<09:28,  1.31s/it] 63%|██████▎   | 721/1153 [16:28<09:30,  1.32s/it] 63%|██████▎   | 722/1153 [16:29<09:29,  1.32s/it] 63%|██████▎   | 723/1153 [16:33<15:16,  2.13s/it] 63%|██████▎   | 724/1153 [16:34<13:29,  1.89s/it] 63%|██████▎   | 725/1153 [16:35<12:09,  1.70s/it] 63%|██████▎   | 726/1153 [16:37<11:19,  1.59s/it] 63%|██████▎   | 727/1153 [16:38<10:45,  1.52s/it] 63%|██████▎   | 728/1153 [16:39<10:23,  1.47s/it] 63%|██████▎   | 729/1153 [16:41<10:04,  1.43s/it] 63%|██████▎   | 730/1153 [16:42<09:48,  1.39s/it] 63%|██████▎   | 731/1153 [16:43<09:34,  1.36s/it] 63%|██████▎   | 732/1153 [16:45<09:31,  1.36s/it] 64%|██████▎   | 733/1153 [16:46<09:23,  1.34s/it] 64%|██████▎   | 734/1153 [16:47<09:19,  1.33s/it] 64%|██████▎   | 735/1153 [16:49<09:14,  1.33s/it] 64%|██████▍   | 736/1153 [16:50<09:12,  1.32s/it] 64%|██████▍   | 737/1153 [16:51<09:11,  1.33s/it] 64%|██████▍   | 738/1153 [16:53<09:09,  1.32s/it] 64%|██████▍   | 739/1153 [16:54<09:05,  1.32s/it] 64%|██████▍   | 740/1153 [16:55<09:05,  1.32s/it] 64%|██████▍   | 741/1153 [16:59<14:55,  2.17s/it] 64%|██████▍   | 742/1153 [17:01<13:09,  1.92s/it] 64%|██████▍   | 743/1153 [17:02<11:50,  1.73s/it] 65%|██████▍   | 744/1153 [17:03<10:24,  1.53s/it] 65%|██████▍   | 745/1153 [17:04<09:58,  1.47s/it] 65%|██████▍   | 746/1153 [17:06<09:29,  1.40s/it] 65%|██████▍   | 747/1153 [17:07<09:16,  1.37s/it] 65%|██████▍   | 748/1153 [17:08<09:09,  1.36s/it] 65%|██████▍   | 749/1153 [17:10<09:04,  1.35s/it] 65%|██████▌   | 750/1153 [17:11<09:01,  1.34s/it] 65%|██████▌   | 751/1153 [17:12<08:56,  1.34s/it] 65%|██████▌   | 752/1153 [17:14<08:57,  1.34s/it] 65%|██████▌   | 753/1153 [17:15<08:44,  1.31s/it] 65%|██████▌   | 754/1153 [17:16<08:44,  1.31s/it] 65%|██████▌   | 755/1153 [17:18<08:44,  1.32s/it] 66%|██████▌   | 756/1153 [17:19<08:27,  1.28s/it] 66%|██████▌   | 757/1153 [17:20<08:15,  1.25s/it] 66%|██████▌   | 758/1153 [17:21<08:05,  1.23s/it] 66%|██████▌   | 759/1153 [17:22<07:55,  1.21s/it] 66%|██████▌   | 760/1153 [17:26<13:30,  2.06s/it] 66%|██████▌   | 761/1153 [17:28<11:59,  1.84s/it] 66%|██████▌   | 762/1153 [17:29<10:42,  1.64s/it] 66%|██████▌   | 763/1153 [17:30<09:43,  1.50s/it] 66%|██████▋   | 764/1153 [17:31<09:13,  1.42s/it] 66%|██████▋   | 765/1153 [17:32<08:43,  1.35s/it] 66%|██████▋   | 766/1153 [17:34<08:20,  1.29s/it] 67%|██████▋   | 767/1153 [17:35<08:09,  1.27s/it] 67%|██████▋   | 768/1153 [17:36<07:54,  1.23s/it] 67%|██████▋   | 769/1153 [17:37<07:42,  1.21s/it] 67%|██████▋   | 770/1153 [17:38<07:44,  1.21s/it] 67%|██████▋   | 771/1153 [17:40<07:43,  1.21s/it] 67%|██████▋   | 772/1153 [17:41<07:41,  1.21s/it] 67%|██████▋   | 773/1153 [17:42<07:33,  1.19s/it] 67%|██████▋   | 774/1153 [17:43<07:27,  1.18s/it] 67%|██████▋   | 775/1153 [17:44<07:30,  1.19s/it] 67%|██████▋   | 776/1153 [17:45<07:28,  1.19s/it] 67%|██████▋   | 777/1153 [17:47<07:27,  1.19s/it] 67%|██████▋   | 778/1153 [17:48<07:23,  1.18s/it] 68%|██████▊   | 779/1153 [17:49<07:21,  1.18s/it] 68%|██████▊   | 780/1153 [17:50<07:27,  1.20s/it] 68%|██████▊   | 781/1153 [17:54<12:20,  1.99s/it] 68%|██████▊   | 782/1153 [17:55<10:47,  1.74s/it] 68%|██████▊   | 783/1153 [17:56<09:42,  1.57s/it] 68%|██████▊   | 784/1153 [17:58<08:58,  1.46s/it] 68%|██████▊   | 785/1153 [17:59<08:41,  1.42s/it] 68%|██████▊   | 786/1153 [18:00<08:36,  1.41s/it] 68%|██████▊   | 787/1153 [18:02<08:25,  1.38s/it] 68%|██████▊   | 788/1153 [18:03<08:16,  1.36s/it] 68%|██████▊   | 789/1153 [18:04<08:08,  1.34s/it] 69%|██████▊   | 790/1153 [18:06<08:05,  1.34s/it] 69%|██████▊   | 791/1153 [18:07<07:38,  1.27s/it] 69%|██████▊   | 792/1153 [18:08<07:16,  1.21s/it] 69%|██████▉   | 793/1153 [18:09<07:24,  1.24s/it] 69%|██████▉   | 794/1153 [18:10<07:33,  1.26s/it] 69%|██████▉   | 795/1153 [18:12<07:37,  1.28s/it] 69%|██████▉   | 796/1153 [18:13<07:44,  1.30s/it] 69%|██████▉   | 797/1153 [18:14<07:47,  1.31s/it] 69%|██████▉   | 798/1153 [18:16<07:45,  1.31s/it] 69%|██████▉   | 799/1153 [18:17<07:43,  1.31s/it] 69%|██████▉   | 800/1153 [18:21<12:59,  2.21s/it] 69%|██████▉   | 801/1153 [18:23<11:28,  1.96s/it] 70%|██████▉   | 802/1153 [18:24<10:22,  1.77s/it] 70%|██████▉   | 803/1153 [18:25<09:36,  1.65s/it] 70%|██████▉   | 804/1153 [18:27<08:59,  1.55s/it] 70%|██████▉   | 805/1153 [18:28<08:34,  1.48s/it] 70%|██████▉   | 806/1153 [18:29<08:16,  1.43s/it] 70%|██████▉   | 807/1153 [18:31<08:05,  1.40s/it] 70%|███████   | 808/1153 [18:32<07:56,  1.38s/it] 70%|███████   | 809/1153 [18:33<07:54,  1.38s/it] 70%|███████   | 810/1153 [18:35<07:49,  1.37s/it] 70%|███████   | 811/1153 [18:36<07:46,  1.36s/it] 70%|███████   | 812/1153 [18:37<07:41,  1.35s/it] 71%|███████   | 813/1153 [18:39<07:38,  1.35s/it] 71%|███████   | 814/1153 [18:40<07:38,  1.35s/it] 71%|███████   | 815/1153 [18:41<07:36,  1.35s/it] 71%|███████   | 816/1153 [18:42<06:14,  1.11s/it] 71%|███████   | 817/1153 [18:43<06:34,  1.17s/it] 71%|███████   | 818/1153 [18:47<11:04,  1.98s/it] 71%|███████   | 819/1153 [18:49<09:59,  1.79s/it] 71%|███████   | 820/1153 [18:50<09:11,  1.66s/it] 71%|███████   | 821/1153 [18:51<08:26,  1.53s/it] 71%|███████▏  | 822/1153 [18:52<07:08,  1.29s/it] 71%|███████▏  | 823/1153 [18:53<07:05,  1.29s/it] 71%|███████▏  | 824/1153 [18:54<07:08,  1.30s/it] 72%|███████▏  | 825/1153 [18:55<06:40,  1.22s/it] 72%|███████▏  | 826/1153 [18:57<06:49,  1.25s/it] 72%|███████▏  | 827/1153 [18:58<06:42,  1.23s/it] 72%|███████▏  | 828/1153 [18:59<06:52,  1.27s/it] 72%|███████▏  | 829/1153 [19:00<06:18,  1.17s/it] 72%|███████▏  | 830/1153 [19:02<06:45,  1.25s/it] 72%|███████▏  | 831/1153 [19:03<06:49,  1.27s/it] 72%|███████▏  | 832/1153 [19:04<06:53,  1.29s/it] 72%|███████▏  | 833/1153 [19:06<06:57,  1.30s/it] 72%|███████▏  | 834/1153 [19:07<06:59,  1.32s/it] 72%|███████▏  | 835/1153 [19:08<07:01,  1.33s/it] 73%|███████▎  | 836/1153 [19:10<07:04,  1.34s/it] 73%|███████▎  | 837/1153 [19:11<07:02,  1.34s/it] 73%|███████▎  | 838/1153 [19:15<11:46,  2.24s/it] 73%|███████▎  | 839/1153 [19:17<10:21,  1.98s/it] 73%|███████▎  | 840/1153 [19:18<09:18,  1.78s/it] 73%|███████▎  | 841/1153 [19:20<08:37,  1.66s/it] 73%|███████▎  | 842/1153 [19:21<08:03,  1.56s/it] 73%|███████▎  | 843/1153 [19:22<07:40,  1.49s/it] 73%|███████▎  | 844/1153 [19:23<07:24,  1.44s/it] 73%|███████▎  | 845/1153 [19:25<07:13,  1.41s/it] 73%|███████▎  | 846/1153 [19:26<06:48,  1.33s/it] 73%|███████▎  | 847/1153 [19:27<06:47,  1.33s/it] 74%|███████▎  | 848/1153 [19:28<06:27,  1.27s/it] 74%|███████▎  | 849/1153 [19:30<06:22,  1.26s/it] 74%|███████▎  | 850/1153 [19:31<06:32,  1.30s/it] 74%|███████▍  | 851/1153 [19:32<06:36,  1.31s/it] 74%|███████▍  | 852/1153 [19:34<06:40,  1.33s/it] 74%|███████▍  | 853/1153 [19:35<06:27,  1.29s/it] 74%|███████▍  | 854/1153 [19:37<06:53,  1.38s/it] 74%|███████▍  | 855/1153 [19:38<07:25,  1.49s/it] 74%|███████▍  | 856/1153 [19:40<07:22,  1.49s/it] 74%|███████▍  | 857/1153 [19:45<12:46,  2.59s/it] 74%|███████▍  | 858/1153 [19:46<11:07,  2.26s/it] 75%|███████▍  | 859/1153 [19:48<10:02,  2.05s/it] 75%|███████▍  | 860/1153 [19:49<08:54,  1.82s/it] 75%|███████▍  | 861/1153 [19:51<08:00,  1.65s/it] 75%|███████▍  | 862/1153 [19:52<07:35,  1.56s/it] 75%|███████▍  | 863/1153 [19:53<07:24,  1.53s/it] 75%|███████▍  | 864/1153 [19:55<06:53,  1.43s/it] 75%|███████▌  | 865/1153 [19:56<06:32,  1.36s/it] 75%|███████▌  | 866/1153 [19:57<06:17,  1.32s/it] 75%|███████▌  | 867/1153 [19:58<06:17,  1.32s/it] 75%|███████▌  | 868/1153 [19:59<06:04,  1.28s/it] 75%|███████▌  | 869/1153 [20:01<05:55,  1.25s/it] 75%|███████▌  | 870/1153 [20:02<05:48,  1.23s/it] 76%|███████▌  | 871/1153 [20:03<05:55,  1.26s/it] 76%|███████▌  | 872/1153 [20:05<06:06,  1.31s/it] 76%|███████▌  | 873/1153 [20:06<06:00,  1.29s/it] 76%|███████▌  | 874/1153 [20:07<05:51,  1.26s/it] 76%|███████▌  | 875/1153 [20:08<05:44,  1.24s/it] 76%|███████▌  | 876/1153 [20:09<05:36,  1.21s/it] 76%|███████▌  | 877/1153 [20:14<09:45,  2.12s/it] 76%|███████▌  | 878/1153 [20:15<08:27,  1.85s/it] 76%|███████▌  | 879/1153 [20:16<07:30,  1.64s/it] 76%|███████▋  | 880/1153 [20:17<06:52,  1.51s/it] 76%|███████▋  | 881/1153 [20:18<06:25,  1.42s/it] 76%|███████▋  | 882/1153 [20:20<06:03,  1.34s/it] 77%|███████▋  | 883/1153 [20:21<05:51,  1.30s/it] 77%|███████▋  | 884/1153 [20:22<05:55,  1.32s/it] 77%|███████▋  | 885/1153 [20:23<05:58,  1.34s/it] 77%|███████▋  | 886/1153 [20:25<05:56,  1.34s/it] 77%|███████▋  | 887/1153 [20:26<05:55,  1.34s/it] 77%|███████▋  | 888/1153 [20:28<05:55,  1.34s/it] 77%|███████▋  | 889/1153 [20:29<05:40,  1.29s/it] 77%|███████▋  | 890/1153 [20:30<05:35,  1.28s/it] 77%|███████▋  | 891/1153 [20:32<06:02,  1.38s/it] 77%|███████▋  | 892/1153 [20:33<05:53,  1.36s/it] 77%|███████▋  | 893/1153 [20:34<05:48,  1.34s/it] 78%|███████▊  | 894/1153 [20:35<05:39,  1.31s/it] 78%|███████▊  | 895/1153 [20:37<05:29,  1.28s/it] 78%|███████▊  | 896/1153 [20:38<05:19,  1.24s/it] 78%|███████▊  | 897/1153 [20:39<05:15,  1.23s/it] 78%|███████▊  | 898/1153 [20:43<09:25,  2.22s/it] 78%|███████▊  | 899/1153 [20:45<08:10,  1.93s/it] 78%|███████▊  | 900/1153 [20:46<07:16,  1.73s/it] 78%|███████▊  | 901/1153 [20:47<06:34,  1.57s/it] 78%|███████▊  | 902/1153 [20:48<06:04,  1.45s/it] 78%|███████▊  | 903/1153 [20:50<05:46,  1.39s/it] 78%|███████▊  | 904/1153 [20:51<05:31,  1.33s/it] 78%|███████▊  | 905/1153 [20:52<05:19,  1.29s/it] 79%|███████▊  | 906/1153 [20:53<05:14,  1.27s/it] 79%|███████▊  | 907/1153 [20:54<05:05,  1.24s/it] 79%|███████▉  | 908/1153 [20:56<05:01,  1.23s/it] 79%|███████▉  | 909/1153 [20:57<04:59,  1.23s/it] 79%|███████▉  | 910/1153 [20:58<04:56,  1.22s/it] 79%|███████▉  | 911/1153 [20:59<04:55,  1.22s/it] 79%|███████▉  | 912/1153 [21:01<04:55,  1.23s/it] 79%|███████▉  | 913/1153 [21:02<04:54,  1.23s/it] 79%|███████▉  | 914/1153 [21:03<04:54,  1.23s/it] 79%|███████▉  | 915/1153 [21:04<04:52,  1.23s/it] 79%|███████▉  | 916/1153 [21:05<04:50,  1.23s/it] 80%|███████▉  | 917/1153 [21:07<04:47,  1.22s/it] 80%|███████▉  | 918/1153 [21:08<04:44,  1.21s/it] 80%|███████▉  | 919/1153 [21:12<08:46,  2.25s/it] 80%|███████▉  | 920/1153 [21:14<07:44,  1.99s/it] 80%|███████▉  | 921/1153 [21:15<07:01,  1.82s/it] 80%|███████▉  | 922/1153 [21:17<06:28,  1.68s/it] 80%|████████  | 923/1153 [21:18<06:02,  1.58s/it] 80%|████████  | 924/1153 [21:19<05:44,  1.50s/it] 80%|████████  | 925/1153 [21:21<05:33,  1.46s/it] 80%|████████  | 926/1153 [21:22<05:29,  1.45s/it] 80%|████████  | 927/1153 [21:23<05:07,  1.36s/it] 80%|████████  | 928/1153 [21:25<05:06,  1.36s/it] 81%|████████  | 929/1153 [21:26<05:04,  1.36s/it] 81%|████████  | 930/1153 [21:27<05:03,  1.36s/it] 81%|████████  | 931/1153 [21:29<05:00,  1.35s/it] 81%|████████  | 932/1153 [21:30<05:02,  1.37s/it] 81%|████████  | 933/1153 [21:31<05:00,  1.37s/it] 81%|████████  | 934/1153 [21:33<04:58,  1.36s/it] 81%|████████  | 935/1153 [21:34<04:57,  1.37s/it] 81%|████████  | 936/1153 [21:36<04:56,  1.37s/it] 81%|████████▏ | 937/1153 [21:40<08:11,  2.27s/it] 81%|████████▏ | 938/1153 [21:41<07:11,  2.01s/it] 81%|████████▏ | 939/1153 [21:43<06:29,  1.82s/it] 82%|████████▏ | 940/1153 [21:44<05:58,  1.68s/it] 82%|████████▏ | 941/1153 [21:45<05:36,  1.59s/it] 82%|████████▏ | 942/1153 [21:47<05:18,  1.51s/it] 82%|████████▏ | 943/1153 [21:48<05:06,  1.46s/it] 82%|████████▏ | 944/1153 [21:49<04:57,  1.43s/it] 82%|████████▏ | 945/1153 [21:51<04:46,  1.38s/it] 82%|████████▏ | 946/1153 [21:52<04:37,  1.34s/it] 82%|████████▏ | 947/1153 [21:53<04:34,  1.33s/it] 82%|████████▏ | 948/1153 [21:55<04:35,  1.34s/it] 82%|████████▏ | 949/1153 [21:56<04:35,  1.35s/it] 82%|████████▏ | 950/1153 [21:57<04:31,  1.34s/it] 82%|████████▏ | 951/1153 [21:59<04:31,  1.34s/it] 83%|████████▎ | 952/1153 [22:00<04:31,  1.35s/it] 83%|████████▎ | 953/1153 [22:01<04:31,  1.36s/it] 83%|████████▎ | 954/1153 [22:03<04:24,  1.33s/it] 83%|████████▎ | 955/1153 [22:04<04:24,  1.33s/it] 83%|████████▎ | 956/1153 [22:09<07:29,  2.28s/it] 83%|████████▎ | 957/1153 [22:10<06:34,  2.01s/it] 83%|████████▎ | 958/1153 [22:11<05:54,  1.82s/it] 83%|████████▎ | 959/1153 [22:13<05:24,  1.67s/it] 83%|████████▎ | 960/1153 [22:14<05:03,  1.57s/it] 83%|████████▎ | 961/1153 [22:15<04:07,  1.29s/it] 83%|████████▎ | 962/1153 [22:16<04:01,  1.26s/it] 84%|████████▎ | 963/1153 [22:17<04:05,  1.29s/it] 84%|████████▎ | 964/1153 [22:19<04:09,  1.32s/it] 84%|████████▎ | 965/1153 [22:20<04:07,  1.32s/it] 84%|████████▍ | 966/1153 [22:21<04:08,  1.33s/it] 84%|████████▍ | 967/1153 [22:22<04:04,  1.31s/it] 84%|████████▍ | 968/1153 [22:24<04:06,  1.33s/it] 84%|████████▍ | 969/1153 [22:25<04:06,  1.34s/it] 84%|████████▍ | 970/1153 [22:26<03:43,  1.22s/it] 84%|████████▍ | 971/1153 [22:27<02:57,  1.02it/s] 84%|████████▍ | 972/1153 [22:27<02:29,  1.21it/s] 84%|████████▍ | 973/1153 [22:28<02:34,  1.17it/s] 84%|████████▍ | 974/1153 [22:29<03:01,  1.01s/it] 85%|████████▍ | 975/1153 [22:31<03:19,  1.12s/it] 85%|████████▍ | 976/1153 [22:32<03:30,  1.19s/it] 85%|████████▍ | 977/1153 [22:33<03:38,  1.24s/it] 85%|████████▍ | 978/1153 [22:38<06:14,  2.14s/it] 85%|████████▍ | 979/1153 [22:39<05:36,  1.93s/it] 85%|████████▍ | 980/1153 [22:40<05:03,  1.76s/it] 85%|████████▌ | 981/1153 [22:42<04:41,  1.64s/it] 85%|████████▌ | 982/1153 [22:43<04:24,  1.55s/it] 85%|████████▌ | 983/1153 [22:44<04:13,  1.49s/it] 85%|████████▌ | 984/1153 [22:46<03:59,  1.42s/it] 85%|████████▌ | 985/1153 [22:47<03:55,  1.40s/it] 86%|████████▌ | 986/1153 [22:48<03:52,  1.39s/it] 86%|████████▌ | 987/1153 [22:50<03:50,  1.39s/it] 86%|████████▌ | 988/1153 [22:51<03:46,  1.37s/it] 86%|████████▌ | 989/1153 [22:53<03:44,  1.37s/it] 86%|████████▌ | 990/1153 [22:54<03:43,  1.37s/it] 86%|████████▌ | 991/1153 [22:55<03:32,  1.31s/it] 86%|████████▌ | 992/1153 [22:56<03:34,  1.33s/it] 86%|████████▌ | 993/1153 [22:58<03:34,  1.34s/it] 86%|████████▌ | 994/1153 [22:59<03:33,  1.35s/it] 86%|████████▋ | 995/1153 [23:01<03:33,  1.35s/it] 86%|████████▋ | 996/1153 [23:02<03:24,  1.30s/it] 86%|████████▋ | 997/1153 [23:06<05:52,  2.26s/it] 87%|████████▋ | 998/1153 [23:07<05:00,  1.94s/it] 87%|████████▋ | 999/1153 [23:09<04:25,  1.72s/it] 87%|████████▋ | 1000/1153 [23:10<04:06,  1.61s/it] 87%|████████▋ | 1001/1153 [23:11<03:53,  1.54s/it] 87%|████████▋ | 1002/1153 [23:13<03:39,  1.45s/it] 87%|████████▋ | 1003/1153 [23:14<03:28,  1.39s/it] 87%|████████▋ | 1004/1153 [23:15<03:17,  1.33s/it] 87%|████████▋ | 1005/1153 [23:16<03:18,  1.34s/it] 87%|████████▋ | 1006/1153 [23:18<03:18,  1.35s/it] 87%|████████▋ | 1007/1153 [23:19<03:11,  1.31s/it] 87%|████████▋ | 1008/1153 [23:20<03:05,  1.28s/it] 88%|████████▊ | 1009/1153 [23:21<03:02,  1.26s/it] 88%|████████▊ | 1010/1153 [23:23<02:57,  1.24s/it] 88%|████████▊ | 1011/1153 [23:24<02:55,  1.24s/it] 88%|████████▊ | 1012/1153 [23:25<02:53,  1.23s/it] 88%|████████▊ | 1013/1153 [23:26<02:58,  1.28s/it] 88%|████████▊ | 1014/1153 [23:28<02:54,  1.25s/it] 88%|████████▊ | 1015/1153 [23:29<02:52,  1.25s/it] 88%|████████▊ | 1016/1153 [23:30<02:50,  1.25s/it] 88%|████████▊ | 1017/1153 [23:31<02:48,  1.24s/it] 88%|████████▊ | 1018/1153 [23:33<02:47,  1.24s/it] 88%|████████▊ | 1019/1153 [23:37<04:37,  2.07s/it] 88%|████████▊ | 1020/1153 [23:38<04:01,  1.82s/it] 89%|████████▊ | 1021/1153 [23:39<03:36,  1.64s/it] 89%|████████▊ | 1022/1153 [23:40<03:16,  1.50s/it] 89%|████████▊ | 1023/1153 [23:41<03:03,  1.41s/it] 89%|████████▉ | 1024/1153 [23:43<02:54,  1.36s/it] 89%|████████▉ | 1025/1153 [23:44<02:46,  1.30s/it] 89%|████████▉ | 1026/1153 [23:45<02:48,  1.32s/it] 89%|████████▉ | 1027/1153 [23:47<02:48,  1.34s/it] 89%|████████▉ | 1028/1153 [23:48<02:47,  1.34s/it] 89%|████████▉ | 1029/1153 [23:49<02:46,  1.35s/it] 89%|████████▉ | 1030/1153 [23:51<02:46,  1.35s/it] 89%|████████▉ | 1031/1153 [23:52<02:45,  1.36s/it] 90%|████████▉ | 1032/1153 [23:53<02:44,  1.36s/it] 90%|████████▉ | 1033/1153 [23:55<02:37,  1.31s/it] 90%|████████▉ | 1034/1153 [23:56<02:39,  1.34s/it] 90%|████████▉ | 1035/1153 [23:57<02:34,  1.31s/it] 90%|████████▉ | 1036/1153 [23:58<02:31,  1.29s/it] 90%|████████▉ | 1037/1153 [24:00<02:27,  1.27s/it] 90%|█████████ | 1038/1153 [24:04<04:11,  2.19s/it] 90%|█████████ | 1039/1153 [24:05<03:37,  1.91s/it] 90%|█████████ | 1040/1153 [24:07<03:12,  1.71s/it] 90%|█████████ | 1041/1153 [24:08<02:54,  1.56s/it] 90%|█████████ | 1042/1153 [24:09<02:42,  1.46s/it] 90%|█████████ | 1043/1153 [24:10<02:33,  1.40s/it] 91%|█████████ | 1044/1153 [24:11<02:25,  1.34s/it] 91%|█████████ | 1045/1153 [24:13<02:21,  1.31s/it] 91%|█████████ | 1046/1153 [24:14<02:16,  1.28s/it] 91%|█████████ | 1047/1153 [24:15<02:15,  1.28s/it] 91%|█████████ | 1048/1153 [24:16<02:13,  1.27s/it] 91%|█████████ | 1049/1153 [24:18<02:09,  1.25s/it] 91%|█████████ | 1050/1153 [24:19<02:08,  1.24s/it] 91%|█████████ | 1051/1153 [24:20<02:05,  1.23s/it] 91%|█████████ | 1052/1153 [24:21<02:03,  1.22s/it] 91%|█████████▏| 1053/1153 [24:22<02:02,  1.23s/it] 91%|█████████▏| 1054/1153 [24:24<02:00,  1.22s/it] 92%|█████████▏| 1055/1153 [24:25<02:00,  1.23s/it] 92%|█████████▏| 1056/1153 [24:26<01:59,  1.23s/it] 92%|█████████▏| 1057/1153 [24:27<01:58,  1.23s/it] 92%|█████████▏| 1058/1153 [24:29<01:58,  1.24s/it] 92%|█████████▏| 1059/1153 [24:30<01:56,  1.23s/it] 92%|█████████▏| 1060/1153 [24:34<03:15,  2.10s/it] 92%|█████████▏| 1061/1153 [24:35<02:54,  1.89s/it] 92%|█████████▏| 1062/1153 [24:37<02:38,  1.74s/it] 92%|█████████▏| 1063/1153 [24:38<02:27,  1.63s/it] 92%|█████████▏| 1064/1153 [24:40<02:18,  1.56s/it] 92%|█████████▏| 1065/1153 [24:41<02:12,  1.51s/it] 92%|█████████▏| 1066/1153 [24:42<02:07,  1.47s/it] 93%|█████████▎| 1067/1153 [24:44<02:03,  1.44s/it] 93%|█████████▎| 1068/1153 [24:45<02:00,  1.42s/it] 93%|█████████▎| 1069/1153 [24:46<01:58,  1.41s/it] 93%|█████████▎| 1070/1153 [24:48<01:56,  1.40s/it] 93%|█████████▎| 1071/1153 [24:49<01:53,  1.39s/it] 93%|█████████▎| 1072/1153 [24:51<01:51,  1.38s/it] 93%|█████████▎| 1073/1153 [24:52<01:50,  1.38s/it] 93%|█████████▎| 1074/1153 [24:53<01:49,  1.38s/it] 93%|█████████▎| 1075/1153 [24:55<01:47,  1.38s/it] 93%|█████████▎| 1076/1153 [24:56<01:46,  1.39s/it] 93%|█████████▎| 1077/1153 [24:57<01:44,  1.38s/it] 93%|█████████▎| 1078/1153 [24:59<01:43,  1.37s/it] 94%|█████████▎| 1079/1153 [25:00<01:35,  1.30s/it] 94%|█████████▎| 1080/1153 [25:04<02:43,  2.24s/it] 94%|█████████▍| 1081/1153 [25:06<02:22,  1.98s/it] 94%|█████████▍| 1082/1153 [25:07<02:07,  1.80s/it] 94%|█████████▍| 1083/1153 [25:09<01:57,  1.68s/it] 94%|█████████▍| 1084/1153 [25:10<01:49,  1.59s/it] 94%|█████████▍| 1085/1153 [25:11<01:43,  1.52s/it] 94%|█████████▍| 1086/1153 [25:13<01:40,  1.50s/it] 94%|█████████▍| 1087/1153 [25:14<01:33,  1.42s/it] 94%|█████████▍| 1088/1153 [25:15<01:32,  1.42s/it] 94%|█████████▍| 1089/1153 [25:17<01:29,  1.40s/it] 95%|█████████▍| 1090/1153 [25:18<01:29,  1.41s/it] 95%|█████████▍| 1091/1153 [25:20<01:27,  1.41s/it] 95%|█████████▍| 1092/1153 [25:21<01:25,  1.40s/it] 95%|█████████▍| 1093/1153 [25:22<01:23,  1.40s/it] 95%|█████████▍| 1094/1153 [25:24<01:21,  1.39s/it] 95%|█████████▍| 1095/1153 [25:25<01:17,  1.34s/it] 95%|█████████▌| 1096/1153 [25:26<01:17,  1.37s/it] 95%|█████████▌| 1097/1153 [25:28<01:16,  1.37s/it] 95%|█████████▌| 1098/1153 [25:29<01:15,  1.37s/it] 95%|█████████▌| 1099/1153 [25:33<02:01,  2.25s/it] 95%|█████████▌| 1100/1153 [25:34<01:35,  1.79s/it] 95%|█████████▌| 1101/1153 [25:36<01:26,  1.67s/it] 96%|█████████▌| 1102/1153 [25:37<01:20,  1.59s/it] 96%|█████████▌| 1103/1153 [25:38<01:16,  1.53s/it] 96%|█████████▌| 1104/1153 [25:40<01:12,  1.48s/it] 96%|█████████▌| 1105/1153 [25:41<01:03,  1.33s/it] 96%|█████████▌| 1106/1153 [25:42<01:03,  1.35s/it] 96%|█████████▌| 1107/1153 [25:43<00:53,  1.17s/it] 96%|█████████▌| 1108/1153 [25:44<00:55,  1.23s/it] 96%|█████████▌| 1109/1153 [25:45<00:45,  1.03s/it] 96%|█████████▋| 1110/1153 [25:46<00:45,  1.06s/it] 96%|█████████▋| 1111/1153 [25:47<00:48,  1.16s/it] 96%|█████████▋| 1112/1153 [25:49<00:50,  1.23s/it] 97%|█████████▋| 1113/1153 [25:50<00:50,  1.27s/it] 97%|█████████▋| 1114/1153 [25:51<00:50,  1.30s/it] 97%|█████████▋| 1115/1153 [25:53<00:50,  1.32s/it] 97%|█████████▋| 1116/1153 [25:54<00:49,  1.34s/it] 97%|█████████▋| 1117/1153 [25:56<00:48,  1.36s/it] 97%|█████████▋| 1118/1153 [25:57<00:47,  1.37s/it] 97%|█████████▋| 1119/1153 [25:58<00:46,  1.37s/it] 97%|█████████▋| 1120/1153 [26:03<01:15,  2.29s/it] 97%|█████████▋| 1121/1153 [26:04<01:04,  2.02s/it] 97%|█████████▋| 1122/1153 [26:05<00:55,  1.79s/it] 97%|█████████▋| 1123/1153 [26:07<00:49,  1.66s/it] 97%|█████████▋| 1124/1153 [26:08<00:45,  1.58s/it] 98%|█████████▊| 1125/1153 [26:10<00:42,  1.52s/it] 98%|█████████▊| 1126/1153 [26:11<00:39,  1.45s/it] 98%|█████████▊| 1127/1153 [26:12<00:37,  1.44s/it] 98%|█████████▊| 1128/1153 [26:14<00:35,  1.43s/it] 98%|█████████▊| 1129/1153 [26:15<00:34,  1.42s/it] 98%|█████████▊| 1130/1153 [26:16<00:32,  1.42s/it] 98%|█████████▊| 1131/1153 [26:18<00:30,  1.38s/it] 98%|█████████▊| 1132/1153 [26:19<00:28,  1.38s/it] 98%|█████████▊| 1133/1153 [26:21<00:27,  1.38s/it] 98%|█████████▊| 1134/1153 [26:22<00:26,  1.39s/it] 98%|█████████▊| 1135/1153 [26:23<00:24,  1.38s/it] 99%|█████████▊| 1136/1153 [26:25<00:23,  1.38s/it] 99%|█████████▊| 1137/1153 [26:26<00:22,  1.39s/it] 99%|█████████▊| 1138/1153 [26:27<00:20,  1.34s/it] 99%|█████████▉| 1139/1153 [26:29<00:18,  1.32s/it] 99%|█████████▉| 1140/1153 [26:33<00:29,  2.29s/it] 99%|█████████▉| 1141/1153 [26:34<00:23,  1.99s/it] 99%|█████████▉| 1142/1153 [26:36<00:19,  1.76s/it] 99%|█████████▉| 1143/1153 [26:37<00:16,  1.61s/it] 99%|█████████▉| 1144/1153 [26:38<00:13,  1.53s/it] 99%|█████████▉| 1145/1153 [26:40<00:11,  1.49s/it] 99%|█████████▉| 1146/1153 [26:41<00:09,  1.42s/it] 99%|█████████▉| 1147/1153 [26:42<00:08,  1.43s/it]100%|█████████▉| 1148/1153 [26:44<00:06,  1.37s/it]100%|█████████▉| 1149/1153 [26:45<00:05,  1.33s/it]100%|█████████▉| 1150/1153 [26:46<00:03,  1.31s/it]100%|█████████▉| 1151/1153 [26:47<00:02,  1.28s/it]100%|█████████▉| 1152/1153 [26:49<00:01,  1.32s/it]100%|██████████| 1153/1153 [26:50<00:00,  1.20s/it]100%|██████████| 1153/1153 [26:50<00:00,  1.40s/it]
147540
147540
saving data 147540 to ./output/DuEE1.0/role/test_result.json
trigger predict 147540 load from ./output/DuEE1.0/role/test_result.json
role predict 147540 load from ./output/DuEE1.0/role/test_result.json
schema 68 load from ./conf/DuEE1.0/event_schema.json
submit data 147540 save to ./output/DuEE1.0/surbot_news_together.json
********** Done ***************
