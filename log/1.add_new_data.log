nohup: ignoring input
*********** data_prepare *************
********** train start *************
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 5, 
    "event_type": "role", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 1e-05, 
    "max_len": 150, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 30.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 48, 
    "per_gpu_train_batch_size": 64, 
    "seed": 66, 
    "stride": 100, 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/7502 [00:00<?, ?it/s]tokenizing...:   3%|▎         | 243/7502 [00:00<00:02, 2429.31it/s]tokenizing...:   8%|▊         | 620/7502 [00:00<00:02, 3213.42it/s]tokenizing...:  13%|█▎        | 990/7502 [00:00<00:01, 3429.22it/s]tokenizing...:  18%|█▊        | 1333/7502 [00:00<00:02, 3062.09it/s]tokenizing...:  22%|██▏       | 1645/7502 [00:00<00:01, 3027.63it/s]tokenizing...:  26%|██▌       | 1951/7502 [00:00<00:01, 3017.16it/s]tokenizing...:  30%|███       | 2255/7502 [00:00<00:01, 2847.87it/s]tokenizing...:  34%|███▍      | 2543/7502 [00:00<00:01, 2804.21it/s]tokenizing...:  38%|███▊      | 2826/7502 [00:00<00:01, 2749.46it/s]tokenizing...:  41%|████▏     | 3102/7502 [00:01<00:01, 2745.28it/s]tokenizing...:  45%|████▌     | 3378/7502 [00:01<00:01, 2700.69it/s]tokenizing...:  49%|████▉     | 3664/7502 [00:01<00:01, 2742.84it/s]tokenizing...:  53%|█████▎    | 3939/7502 [00:01<00:01, 2712.41it/s]tokenizing...:  56%|█████▌    | 4213/7502 [00:01<00:01, 2717.70it/s]tokenizing...:  60%|█████▉    | 4486/7502 [00:01<00:01, 2647.27it/s]tokenizing...:  64%|██████▎   | 4771/7502 [00:01<00:01, 2704.19it/s]tokenizing...:  67%|██████▋   | 5042/7502 [00:01<00:00, 2695.21it/s]tokenizing...:  71%|███████▏  | 5347/7502 [00:01<00:00, 2799.37it/s]tokenizing...:  75%|███████▌  | 5628/7502 [00:02<00:00, 2069.52it/s]tokenizing...:  78%|███████▊  | 5863/7502 [00:02<00:00, 2120.72it/s]tokenizing...:  81%|████████▏ | 6113/7502 [00:02<00:00, 2215.75it/s]tokenizing...:  86%|████████▌ | 6420/7502 [00:02<00:00, 2440.91it/s]tokenizing...:  89%|████████▉ | 6698/7502 [00:02<00:00, 2532.61it/s]tokenizing...:  93%|█████████▎| 6963/7502 [00:02<00:00, 2382.05it/s]tokenizing...:  96%|█████████▌| 7219/7502 [00:02<00:00, 2430.06it/s]tokenizing...: 100%|██████████| 7502/7502 [00:02<00:00, 2636.43it/s]
tokenizing...:   0%|          | 0/1924 [00:00<?, ?it/s]tokenizing...:  16%|█▌        | 312/1924 [00:00<00:00, 3084.79it/s]tokenizing...:  32%|███▏      | 621/1924 [00:00<00:00, 2974.66it/s]tokenizing...:  48%|████▊     | 919/1924 [00:00<00:00, 2794.68it/s]tokenizing...:  62%|██████▏   | 1200/1924 [00:00<00:00, 2764.06it/s]tokenizing...:  77%|███████▋  | 1477/1924 [00:00<00:00, 2697.49it/s]tokenizing...:  91%|█████████ | 1748/1924 [00:00<00:00, 2549.79it/s]tokenizing...: 100%|██████████| 1924/1924 [00:00<00:00, 2674.42it/s]
09/15/2022 02:34:28 - INFO - root -   The nums of the train_dataset features is 7502
09/15/2022 02:34:28 - INFO - root -   The nums of the eval_dataset features is 1924
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
09/15/2022 02:34:32 - INFO - root -   ***** Running train *****
********************************************************************************
/data/qingyang/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 2:12  batch_loss: 6.2819 [Training] 2/118 [..............................] - ETA: 1:19  batch_loss: 6.3263 [Training] 3/118 [..............................] - ETA: 1:01  batch_loss: 6.3302 [Training] 4/118 [>.............................] - ETA: 53s  batch_loss: 6.3259 [Training] 5/118 [>.............................] - ETA: 48s  batch_loss: 6.2907 [Training] 6/118 [>.............................] - ETA: 44s  batch_loss: 6.2475 [Training] 7/118 [>.............................] - ETA: 41s  batch_loss: 6.1925 [Training] 8/118 [=>............................] - ETA: 39s  batch_loss: 6.1351 [Training] 9/118 [=>............................] - ETA: 37s  batch_loss: 6.0744 [Training] 10/118 [=>............................] - ETA: 36s  batch_loss: 6.0094 [Training] 11/118 [=>............................] - ETA: 34s  batch_loss: 5.9433 [Training] 12/118 [==>...........................] - ETA: 33s  batch_loss: 5.8736 [Training] 13/118 [==>...........................] - ETA: 32s  batch_loss: 5.7996 [Training] 14/118 [==>...........................] - ETA: 32s  batch_loss: 5.7262 [Training] 15/118 [==>...........................] - ETA: 31s  batch_loss: 5.6557 [Training] 16/118 [===>..........................] - ETA: 30s  batch_loss: 5.5783 [Training] 17/118 [===>..........................] - ETA: 30s  batch_loss: 5.5049 [Training] 18/118 [===>..........................] - ETA: 29s  batch_loss: 5.4185 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 5.3296 [Training] 20/118 [====>.........................] - ETA: 28s  batch_loss: 5.2486 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 5.1550 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 5.0732 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 4.9859 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 4.8992 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 4.8171 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 4.7314 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 4.6466 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 4.5696 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 4.4864 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 4.4145 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 4.3551 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 4.2919 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 4.2489 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 4.1909 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 4.1405 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 4.0993 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 4.0588 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 4.0199 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 3.9718 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 3.9418 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 3.9027 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 3.8613 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 3.8247 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 3.7914 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 3.7544 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 3.7250 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 3.7022 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 3.6695 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 3.6409 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 3.6105 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 3.5843 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 3.5626 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 3.5420 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 3.5223 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 3.5025 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 3.4797 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 3.4624 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 3.4365 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 3.4144 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 3.3915 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 3.3741 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 3.3537 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 3.3367 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 3.3195 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 3.2983 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 3.2842 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 3.2681 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 3.2515 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 3.2357 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 3.2151 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 3.2029 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 3.1901 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 3.1758 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 3.1634 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 3.1455 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 3.1321 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 3.1197 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 3.1061 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 3.0953 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 3.0849 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 3.0736 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 3.0659 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 3.0568 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 3.0471 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 3.0384 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 3.0265 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 3.0158 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 3.0057 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 2.9938 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 2.9873 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 2.9754 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 2.9690 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 2.9585 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 2.9493 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 2.9393 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 2.9315 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 2.9222 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 2.9117 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 2.9038 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 2.8958 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 2.8857 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 2.8778 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 2.8700 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 2.8595 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 2.8547 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 2.8453 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 2.8358 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 2.8287 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 2.8185 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 2.8087 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 2.8025 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 2.7955 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 2.7874 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 2.7812 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 2.7738 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 2.7666 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 2.7603 [Training] 118/118 [==============================] 252.9ms/step  batch_loss: 2.7525 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:35:08 - INFO - root -   The F1-score is 0.03587443946188341
09/15/2022 02:35:08 - INFO - root -   the best eval f1 is 0.0359, saving model !!
09/15/2022 02:35:09 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:23  batch_loss: 1.8880 [Training] 2/118 [..............................] - ETA: 55s  batch_loss: 1.9461 [Training] 3/118 [..............................] - ETA: 46s  batch_loss: 2.0349 [Training] 4/118 [>.............................] - ETA: 41s  batch_loss: 1.9429 [Training] 5/118 [>.............................] - ETA: 38s  batch_loss: 1.9046 [Training] 6/118 [>.............................] - ETA: 36s  batch_loss: 1.9592 [Training] 7/118 [>.............................] - ETA: 35s  batch_loss: 1.9411 [Training] 8/118 [=>............................] - ETA: 33s  batch_loss: 1.9664 [Training] 9/118 [=>............................] - ETA: 32s  batch_loss: 1.9573 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 1.9502 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 1.9388 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 1.9270 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 1.9056 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 1.9159 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 1.9215 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 1.9298 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 1.9247 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 1.9268 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 1.9230 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 1.9286 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 1.9197 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 1.9143 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 1.9122 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 1.9089 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 1.9027 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 1.9017 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 1.8868 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 1.8845 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 1.8772 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 1.8791 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 1.8783 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 1.8832 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 1.8780 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 1.8735 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 1.8704 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 1.8765 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 1.8679 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 1.8660 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 1.8623 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 1.8575 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 1.8560 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 1.8508 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 1.8484 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 1.8452 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 1.8433 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 1.8363 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 1.8349 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 1.8347 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 1.8329 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 1.8353 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 1.8344 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 1.8315 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 1.8274 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 1.8243 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 1.8237 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 1.8187 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 1.8129 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 1.8134 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 1.8071 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 1.8085 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 1.8090 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 1.8086 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 1.8083 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 1.8078 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 1.8047 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 1.7999 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 1.7958 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 1.7932 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 1.7911 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 1.7888 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 1.7868 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 1.7847 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 1.7834 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 1.7796 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 1.7798 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 1.7778 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 1.7758 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 1.7732 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 1.7716 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 1.7678 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 1.7678 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 1.7666 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 1.7634 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 1.7582 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 1.7552 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 1.7544 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 1.7511 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 1.7468 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 1.7430 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 1.7399 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 1.7385 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 1.7362 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 1.7372 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 1.7325 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 1.7324 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 1.7308 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 1.7265 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 1.7234 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 1.7213 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 1.7174 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 1.7153 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 1.7107 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 1.7074 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 1.7030 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 1.7024 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 1.7011 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 1.6989 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 1.6962 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 1.6939 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 1.6926 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 1.6903 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 1.6871 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 1.6849 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 1.6841 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 1.6822 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 1.6802 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 1.6758 [Training] 118/118 [==============================] 255.2ms/step  batch_loss: 1.6758 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:35:46 - INFO - root -   The F1-score is 0.1170808368529614
09/15/2022 02:35:46 - INFO - root -   the best eval f1 is 0.1171, saving model !!
09/15/2022 02:35:48 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:27  batch_loss: 1.4233 [Training] 2/118 [..............................] - ETA: 58s  batch_loss: 1.6036 [Training] 3/118 [..............................] - ETA: 48s  batch_loss: 1.5878 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 1.5850 [Training] 5/118 [>.............................] - ETA: 39s  batch_loss: 1.6063 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 1.5802 [Training] 7/118 [>.............................] - ETA: 35s  batch_loss: 1.5543 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 1.5335 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 1.5004 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 1.5064 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 1.4854 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 1.4872 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 1.4791 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 1.4588 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 1.4548 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 1.4449 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 1.4379 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 1.4217 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 1.4173 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 1.4170 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 1.4189 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 1.4074 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 1.4116 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 1.4082 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 1.4053 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 1.3974 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 1.3940 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 1.3864 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 1.3901 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 1.3873 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 1.3841 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 1.3851 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 1.3876 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 1.3859 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 1.3904 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 1.3877 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 1.3885 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 1.3855 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 1.3825 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 1.3785 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 1.3789 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 1.3748 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 1.3729 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 1.3747 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 1.3717 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 1.3712 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 1.3698 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 1.3684 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 1.3664 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 1.3647 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 1.3644 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 1.3598 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 1.3561 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 1.3536 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 1.3512 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 1.3492 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 1.3488 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 1.3444 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 1.3452 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 1.3425 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 1.3412 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 1.3411 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 1.3408 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 1.3368 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 1.3408 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 1.3414 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 1.3395 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 1.3391 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 1.3398 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 1.3387 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 1.3360 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 1.3356 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 1.3370 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 1.3368 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 1.3379 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 1.3389 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 1.3374 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 1.3344 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 1.3340 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 1.3330 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 1.3314 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 1.3309 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 1.3292 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 1.3292 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 1.3264 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 1.3247 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 1.3233 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 1.3205 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 1.3204 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 1.3192 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 1.3169 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 1.3159 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 1.3138 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 1.3125 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 1.3118 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 1.3116 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 1.3123 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 1.3102 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 1.3099 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 1.3095 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 1.3072 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 1.3068 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 1.3056 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 1.3068 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 1.3060 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 1.3041 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 1.3038 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 1.3015 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 1.3001 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 1.2991 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 1.2989 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 1.2969 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 1.2952 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 1.2933 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 1.2921 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 1.2905 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 1.2900 [Training] 118/118 [==============================] 253.8ms/step  batch_loss: 1.2923 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:36:24 - INFO - root -   The F1-score is 0.21211639370329147
09/15/2022 02:36:24 - INFO - root -   the best eval f1 is 0.2121, saving model !!
09/15/2022 02:36:27 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:29  batch_loss: 1.0224 [Training] 2/118 [..............................] - ETA: 59s  batch_loss: 1.0845 [Training] 3/118 [..............................] - ETA: 49s  batch_loss: 1.0860 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 1.1305 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 1.1085 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 1.1231 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 1.1402 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 1.1150 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 1.1134 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 1.1083 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 1.1045 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 1.1029 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 1.0938 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 1.0928 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 1.1097 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 1.1094 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 1.1133 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 1.1180 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 1.1178 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 1.1204 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 1.1201 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 1.1274 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 1.1249 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 1.1281 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 1.1287 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 1.1263 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 1.1246 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 1.1228 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 1.1222 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 1.1248 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 1.1252 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 1.1234 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 1.1237 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 1.1234 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 1.1204 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 1.1195 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 1.1189 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 1.1163 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 1.1125 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 1.1080 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 1.1113 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 1.1093 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 1.1092 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 1.1084 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 1.1069 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 1.1065 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 1.1081 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 1.1074 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 1.1050 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 1.1037 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 1.1046 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 1.1032 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 1.1041 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 1.1034 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 1.1023 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 1.1035 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 1.1015 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 1.1000 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 1.0993 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 1.0984 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 1.0977 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 1.0983 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 1.0978 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 1.0972 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 1.0941 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 1.0915 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 1.0893 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 1.0868 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 1.0839 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 1.0823 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 1.0821 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 1.0804 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 1.0801 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 1.0800 [Training] 75/118 [==================>...........] - ETA: 10s  batch_loss: 1.0766 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 1.0765 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 1.0756 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 1.0745 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 1.0722 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 1.0727 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 1.0720 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 1.0725 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 1.0706 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 1.0695 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 1.0707 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 1.0701 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 1.0696 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 1.0676 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 1.0675 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 1.0653 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 1.0663 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 1.0651 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 1.0637 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 1.0642 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 1.0616 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 1.0619 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 1.0634 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 1.0645 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 1.0641 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 1.0617 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 1.0603 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 1.0592 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 1.0589 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 1.0598 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 1.0587 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 1.0574 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 1.0557 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 1.0553 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 1.0548 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 1.0535 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 1.0526 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 1.0532 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 1.0519 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 1.0521 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 1.0510 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 1.0493 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 1.0484 [Training] 118/118 [==============================] 254.6ms/step  batch_loss: 1.0457 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:37:03 - INFO - root -   The F1-score is 0.3450744558991982
09/15/2022 02:37:03 - INFO - root -   the best eval f1 is 0.3451, saving model !!
09/15/2022 02:37:06 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:29  batch_loss: 0.8603 [Training] 2/118 [..............................] - ETA: 59s  batch_loss: 0.8516 [Training] 3/118 [..............................] - ETA: 48s  batch_loss: 0.8741 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 0.8822 [Training] 5/118 [>.............................] - ETA: 39s  batch_loss: 0.8844 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.8846 [Training] 7/118 [>.............................] - ETA: 35s  batch_loss: 0.8860 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.8971 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.8999 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.8994 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.9077 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.9118 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.9128 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.9128 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.9208 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.9099 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.9068 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.9030 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.9013 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.8937 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.8966 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.8991 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.8974 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.8993 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.9012 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.9016 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.9024 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.9036 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.9079 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.9059 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.9079 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.9045 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.9028 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.9017 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.9015 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.9008 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.9008 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.9044 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.9027 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.9030 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.9022 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.9019 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.9010 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.9006 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.9007 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.9022 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.9000 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.8981 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.8994 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.9014 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.9051 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.9031 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.9018 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.9018 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.9028 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.9040 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.9032 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.9039 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.9024 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.9008 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.8998 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.8986 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.8988 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.8996 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.8993 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.8989 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.8974 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.8978 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.8963 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.8955 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.8950 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.8948 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.8944 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.8926 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.8902 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.8885 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.8890 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.8891 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.8883 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.8881 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.8876 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.8855 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.8852 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.8843 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.8840 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.8843 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.8838 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.8842 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.8830 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.8821 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.8815 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.8806 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.8794 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.8782 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.8787 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.8779 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.8781 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.8787 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.8784 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.8774 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.8764 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.8770 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.8768 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.8759 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.8754 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.8747 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.8744 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.8741 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.8740 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.8740 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.8737 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.8738 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.8734 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.8729 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.8726 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.8722 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.8717 [Training] 118/118 [==============================] 252.4ms/step  batch_loss: 0.8718 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:37:42 - INFO - root -   The F1-score is 0.44776328705651386
09/15/2022 02:37:42 - INFO - root -   the best eval f1 is 0.4478, saving model !!
09/15/2022 02:37:44 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:31  batch_loss: 0.7992 [Training] 2/118 [..............................] - ETA: 59s  batch_loss: 0.7917 [Training] 3/118 [..............................] - ETA: 49s  batch_loss: 0.8193 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 0.8078 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.8014 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.8052 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.7959 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.8031 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.7944 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.8102 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.8058 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.7950 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.7999 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.8008 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.8042 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.8015 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.8023 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.7984 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.7978 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.8030 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.8023 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.7959 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.8008 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.7957 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.7951 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.7897 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.7883 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.7872 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.7876 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.7855 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.7857 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.7827 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.7805 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.7808 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.7826 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.7809 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.7805 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.7816 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.7781 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.7758 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.7737 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.7786 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.7772 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.7758 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.7739 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.7739 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.7733 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.7725 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.7732 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.7717 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.7720 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.7694 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.7687 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.7701 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.7700 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.7696 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.7689 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.7665 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.7663 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.7680 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.7678 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.7675 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.7654 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.7663 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.7660 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.7659 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.7637 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.7636 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.7638 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.7643 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.7637 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.7632 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.7642 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.7651 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.7658 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.7647 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.7645 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.7630 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.7615 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.7603 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.7589 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.7573 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.7558 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.7558 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.7540 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.7529 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.7517 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.7520 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.7507 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.7516 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.7507 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.7511 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.7504 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.7495 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.7491 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.7499 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.7501 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.7507 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.7498 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.7491 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.7488 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.7498 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.7483 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.7491 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.7484 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.7482 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.7465 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.7458 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.7454 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.7440 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.7437 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.7427 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.7423 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.7421 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.7417 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.7418 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.7414 [Training] 118/118 [==============================] 253.5ms/step  batch_loss: 0.7418 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:38:21 - INFO - root -   The F1-score is 0.4806995969119355
09/15/2022 02:38:21 - INFO - root -   the best eval f1 is 0.4807, saving model !!
09/15/2022 02:38:23 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:24  batch_loss: 0.7712 [Training] 2/118 [..............................] - ETA: 57s  batch_loss: 0.7204 [Training] 3/118 [..............................] - ETA: 47s  batch_loss: 0.6998 [Training] 4/118 [>.............................] - ETA: 42s  batch_loss: 0.6831 [Training] 5/118 [>.............................] - ETA: 39s  batch_loss: 0.6731 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.6830 [Training] 7/118 [>.............................] - ETA: 35s  batch_loss: 0.7019 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.6935 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.7063 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.6945 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.6914 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.7004 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.7031 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.6969 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.6888 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.6829 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.6802 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.6780 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.6720 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.6732 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.6688 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.6672 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.6725 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.6731 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.6720 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.6707 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.6695 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.6664 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.6651 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.6636 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.6667 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.6674 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.6682 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.6651 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.6673 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.6716 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.6692 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.6714 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.6714 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.6727 [Training] 41/118 [=========>....................] - ETA: 19s  batch_loss: 0.6730 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.6719 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.6726 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.6725 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.6711 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.6714 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.6699 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.6673 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.6650 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.6639 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.6637 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.6639 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.6617 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.6594 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.6586 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.6568 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.6545 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.6548 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.6545 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.6546 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.6547 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.6534 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.6529 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.6531 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.6529 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.6525 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.6512 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.6497 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.6497 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.6495 [Training] 71/118 [=================>............] - ETA: 11s  batch_loss: 0.6478 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.6482 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.6469 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.6466 [Training] 75/118 [==================>...........] - ETA: 10s  batch_loss: 0.6461 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.6454 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.6442 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.6438 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.6427 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.6427 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.6429 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.6447 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.6450 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.6451 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.6442 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.6433 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.6423 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.6421 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.6411 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.6399 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.6398 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.6392 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.6398 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.6390 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.6387 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.6375 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.6378 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.6376 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.6375 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.6377 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.6380 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.6373 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.6376 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.6371 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.6378 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.6391 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.6389 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.6385 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.6392 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.6391 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.6387 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.6375 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.6366 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.6367 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.6361 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.6355 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.6353 [Training] 118/118 [==============================] 251.9ms/step  batch_loss: 0.6359 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:38:59 - INFO - root -   The F1-score is 0.515163450177235
09/15/2022 02:38:59 - INFO - root -   the best eval f1 is 0.5152, saving model !!
09/15/2022 02:39:02 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:29  batch_loss: 0.4812 [Training] 2/118 [..............................] - ETA: 58s  batch_loss: 0.5283 [Training] 3/118 [..............................] - ETA: 48s  batch_loss: 0.5533 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 0.5402 [Training] 5/118 [>.............................] - ETA: 39s  batch_loss: 0.5610 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.5449 [Training] 7/118 [>.............................] - ETA: 35s  batch_loss: 0.5442 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.5430 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.5412 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.5453 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.5528 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.5613 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.5676 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.5687 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.5666 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.5647 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.5627 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.5644 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.5677 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.5737 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.5720 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.5704 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.5685 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.5674 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.5642 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.5632 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.5618 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.5617 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.5643 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.5665 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.5629 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.5656 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.5649 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.5667 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.5661 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.5641 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.5650 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.5651 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.5639 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.5621 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.5633 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.5624 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.5635 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.5630 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.5639 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.5638 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.5640 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.5632 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.5647 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.5630 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.5620 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.5611 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.5605 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.5598 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.5587 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.5576 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.5579 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.5577 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.5589 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.5587 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.5591 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.5591 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.5581 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.5584 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.5590 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.5592 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.5588 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.5583 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.5596 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.5601 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.5589 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.5581 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.5580 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.5573 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.5569 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.5565 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.5564 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.5548 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.5544 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.5533 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.5533 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.5528 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.5518 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.5514 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.5523 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.5529 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.5538 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.5538 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.5528 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.5522 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.5535 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.5529 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.5529 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.5534 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.5534 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.5526 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.5521 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.5507 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.5496 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.5498 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.5496 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.5504 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.5506 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.5508 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.5500 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.5501 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.5496 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.5497 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.5503 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.5510 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.5515 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.5506 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.5503 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.5492 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.5491 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.5500 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.5504 [Training] 118/118 [==============================] 253.9ms/step  batch_loss: 0.5491 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:39:38 - INFO - root -   The F1-score is 0.5319763624425475
09/15/2022 02:39:38 - INFO - root -   the best eval f1 is 0.5320, saving model !!
09/15/2022 02:39:41 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:28  batch_loss: 0.4916 [Training] 2/118 [..............................] - ETA: 58s  batch_loss: 0.4718 [Training] 3/118 [..............................] - ETA: 48s  batch_loss: 0.4752 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 0.4809 [Training] 5/118 [>.............................] - ETA: 39s  batch_loss: 0.4855 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.4822 [Training] 7/118 [>.............................] - ETA: 35s  batch_loss: 0.4841 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.4859 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.4818 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.4828 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.4850 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.4859 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.4899 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.4849 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.4867 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.4906 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.4960 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.4964 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.4983 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.4984 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.4992 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.5028 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.5074 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.5100 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.5085 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.5109 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.5067 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.5053 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.5068 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.5054 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.5053 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.5024 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.4996 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.5005 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.4971 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.4977 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.4984 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.4990 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.4988 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.4970 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.4963 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.4985 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.4983 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.4965 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.4951 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.4973 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.4962 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.4951 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.4940 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.4920 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.4917 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.4918 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.4907 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.4917 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.4905 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.4907 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.4902 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.4892 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.4884 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.4881 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.4882 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.4875 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.4872 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.4870 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.4871 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.4879 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.4874 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.4870 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.4874 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.4878 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.4866 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.4869 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.4878 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.4867 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.4868 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.4867 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.4879 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.4883 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.4889 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.4891 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.4896 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.4891 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.4893 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.4888 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.4888 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.4875 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.4872 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.4876 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.4867 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.4859 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.4866 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.4867 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.4860 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.4858 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.4858 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.4848 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.4855 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.4849 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.4852 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.4849 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.4847 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.4845 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.4843 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.4844 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.4838 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.4831 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.4824 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.4824 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.4816 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.4816 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.4818 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.4821 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.4815 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.4813 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.4815 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.4816 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.4812 [Training] 118/118 [==============================] 253.9ms/step  batch_loss: 0.4811 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:40:17 - INFO - root -   The F1-score is 0.5698173662287729
09/15/2022 02:40:17 - INFO - root -   the best eval f1 is 0.5698, saving model !!
09/15/2022 02:40:19 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:28  batch_loss: 0.4508 [Training] 2/118 [..............................] - ETA: 58s  batch_loss: 0.4264 [Training] 3/118 [..............................] - ETA: 48s  batch_loss: 0.4405 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 0.4353 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.4261 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.4313 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.4404 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.4411 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.4432 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.4393 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.4449 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.4433 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.4430 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.4451 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.4411 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.4373 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.4402 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.4393 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.4367 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.4347 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.4336 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.4446 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.4452 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.4473 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.4492 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.4471 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.4458 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.4459 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.4439 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.4438 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.4426 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.4436 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.4430 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.4415 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.4382 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.4391 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.4382 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.4378 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.4381 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.4371 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.4362 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.4376 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.4357 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.4355 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.4353 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.4356 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.4352 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.4341 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.4331 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.4316 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.4312 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.4324 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.4311 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.4320 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.4310 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.4305 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.4299 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.4290 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.4303 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.4299 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.4306 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.4316 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.4309 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.4307 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.4317 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.4308 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.4307 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.4304 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.4308 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.4306 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.4315 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.4320 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.4317 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.4310 [Training] 75/118 [==================>...........] - ETA: 10s  batch_loss: 0.4309 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.4309 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.4315 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.4327 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.4339 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.4339 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.4342 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.4344 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.4338 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.4346 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.4345 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.4345 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.4346 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.4347 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.4344 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.4350 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.4356 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.4344 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.4344 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.4344 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.4340 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.4340 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.4330 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.4324 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.4318 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.4316 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.4312 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.4310 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.4299 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.4295 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.4293 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.4290 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.4286 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.4291 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.4287 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.4286 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.4283 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.4279 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.4275 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.4275 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.4279 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.4278 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.4278 [Training] 118/118 [==============================] 252.4ms/step  batch_loss: 0.4288 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:40:56 - INFO - root -   The F1-score is 0.5874951855180381
09/15/2022 02:40:56 - INFO - root -   the best eval f1 is 0.5875, saving model !!
09/15/2022 02:40:58 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:28  batch_loss: 0.4495 [Training] 2/118 [..............................] - ETA: 58s  batch_loss: 0.4188 [Training] 3/118 [..............................] - ETA: 48s  batch_loss: 0.4275 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 0.4253 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.4201 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.4013 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.3959 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.3948 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.3979 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.3901 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.3890 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.3908 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.3938 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.3955 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.3972 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.3932 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.3933 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.3937 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.3916 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.3906 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.3903 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.3885 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.3899 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.3896 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.3906 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.3899 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.3883 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.3891 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.3887 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.3890 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.3876 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.3899 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.3920 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.3901 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.3895 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.3887 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.3885 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.3882 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.3880 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.3884 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.3876 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.3877 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.3869 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.3858 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.3843 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.3836 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.3858 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.3851 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.3856 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.3854 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.3844 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.3843 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.3841 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.3832 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.3820 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.3803 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.3801 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.3794 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.3787 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.3783 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.3796 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.3794 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.3787 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.3782 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.3772 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.3774 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.3767 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.3765 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.3772 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.3767 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.3762 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.3764 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.3765 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.3768 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.3776 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.3784 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.3783 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.3788 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.3787 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.3781 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.3787 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.3788 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.3793 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.3807 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.3806 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.3799 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.3792 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.3801 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.3792 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.3790 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.3785 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.3782 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.3782 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.3790 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.3781 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.3785 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.3780 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.3790 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.3789 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.3790 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.3789 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.3786 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.3793 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.3791 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.3791 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.3791 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.3790 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.3791 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.3791 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.3788 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.3802 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.3803 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.3802 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.3804 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.3804 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.3805 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.3808 [Training] 118/118 [==============================] 257.5ms/step  batch_loss: 0.3817 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:41:35 - INFO - root -   The F1-score is 0.5897368421052632
09/15/2022 02:41:35 - INFO - root -   the best eval f1 is 0.5897, saving model !!
09/15/2022 02:41:37 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:29  batch_loss: 0.3395 [Training] 2/118 [..............................] - ETA: 59s  batch_loss: 0.3422 [Training] 3/118 [..............................] - ETA: 48s  batch_loss: 0.3327 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 0.3390 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.3419 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.3485 [Training] 7/118 [>.............................] - ETA: 35s  batch_loss: 0.3519 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.3522 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.3549 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.3572 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.3577 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.3574 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.3591 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.3580 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.3557 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.3566 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.3518 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.3511 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.3529 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.3520 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.3530 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.3519 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.3515 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.3521 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.3516 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.3507 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.3509 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.3469 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.3465 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.3456 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.3447 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.3449 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.3444 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.3437 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.3421 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.3451 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.3435 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.3432 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.3420 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.3423 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.3429 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.3431 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.3426 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.3416 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.3431 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.3441 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.3437 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.3447 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.3444 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.3433 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.3448 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.3437 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.3446 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.3442 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.3435 [Training] 56/118 [=============>................] - ETA: 15s  batch_loss: 0.3440 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.3440 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.3440 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.3440 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.3430 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.3432 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.3436 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.3441 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.3437 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.3429 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.3430 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.3436 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.3429 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.3429 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.3425 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.3429 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.3430 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.3428 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.3425 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.3437 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.3436 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.3437 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.3431 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.3431 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.3429 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.3424 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.3425 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.3415 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.3416 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.3419 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.3422 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.3419 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.3418 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.3418 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.3413 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.3410 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.3407 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.3404 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.3407 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.3398 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.3400 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.3396 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.3401 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.3401 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.3397 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.3395 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.3398 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.3391 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.3387 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.3389 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.3391 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.3397 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.3401 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.3399 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.3402 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.3409 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.3406 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.3408 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.3419 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.3410 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.3419 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.3430 [Training] 118/118 [==============================] 253.3ms/step  batch_loss: 0.3420 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:42:14 - INFO - root -   The F1-score is 0.5966850828729281
09/15/2022 02:42:14 - INFO - root -   the best eval f1 is 0.5967, saving model !!
09/15/2022 02:42:16 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:28  batch_loss: 0.2957 [Training] 2/118 [..............................] - ETA: 59s  batch_loss: 0.2841 [Training] 3/118 [..............................] - ETA: 48s  batch_loss: 0.2898 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 0.3023 [Training] 5/118 [>.............................] - ETA: 39s  batch_loss: 0.3246 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.3234 [Training] 7/118 [>.............................] - ETA: 35s  batch_loss: 0.3185 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.3212 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.3133 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.3108 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.3160 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.3147 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.3128 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.3142 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.3144 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.3168 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.3150 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.3116 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.3113 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.3124 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.3119 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.3134 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.3133 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.3132 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.3117 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.3112 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.3100 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.3110 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.3129 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.3132 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.3131 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.3117 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.3114 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.3103 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.3097 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.3089 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.3077 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.3101 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.3111 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.3104 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.3101 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.3096 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.3091 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.3086 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.3096 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.3103 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.3105 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.3114 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.3124 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.3139 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.3128 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.3129 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.3132 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.3125 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.3112 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.3107 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.3107 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.3103 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.3100 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.3104 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.3107 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.3101 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.3094 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.3086 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.3084 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.3091 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.3080 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.3078 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.3078 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.3073 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.3081 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.3083 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.3077 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.3078 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.3078 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.3088 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.3092 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.3093 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.3092 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.3095 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.3096 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.3095 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.3093 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.3093 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.3093 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.3097 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.3096 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.3104 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.3101 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.3099 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.3093 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.3085 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.3091 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.3092 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.3090 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.3096 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.3112 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.3114 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.3114 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.3113 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.3112 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.3109 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.3117 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.3120 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.3121 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.3123 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.3123 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.3122 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.3120 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.3119 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.3122 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.3119 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.3119 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.3116 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.3112 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.3108 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.3109 [Training] 118/118 [==============================] 254.0ms/step  batch_loss: 0.3103 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:42:52 - INFO - root -   The F1-score is 0.6096167426333697
09/15/2022 02:42:52 - INFO - root -   the best eval f1 is 0.6096, saving model !!
09/15/2022 02:42:55 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:31  batch_loss: 0.2442 [Training] 2/118 [..............................] - ETA: 59s  batch_loss: 0.2530 [Training] 3/118 [..............................] - ETA: 48s  batch_loss: 0.2727 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 0.2741 [Training] 5/118 [>.............................] - ETA: 39s  batch_loss: 0.2821 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.2855 [Training] 7/118 [>.............................] - ETA: 35s  batch_loss: 0.2878 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.2918 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.2831 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.2773 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.2761 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.2808 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.2824 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.2840 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.2841 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.2863 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.2846 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.2835 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.2838 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.2833 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.2835 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.2822 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.2837 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.2845 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.2844 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.2823 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.2841 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.2856 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.2859 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.2856 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.2863 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.2858 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.2851 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.2833 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.2823 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.2830 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2819 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2815 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.2813 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2835 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2845 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.2842 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.2846 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2850 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2857 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.2849 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2836 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2828 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.2820 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.2814 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2826 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2820 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.2825 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2824 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2833 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.2837 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.2829 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2830 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2828 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.2826 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2827 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2826 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2835 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.2840 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.2841 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2844 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2847 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.2847 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2844 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2846 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.2840 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.2842 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2842 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2845 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.2831 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.2835 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2829 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2840 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.2838 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2842 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2851 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2849 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.2848 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2853 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2854 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2857 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.2856 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2849 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2853 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2858 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.2859 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2855 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2856 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2850 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.2857 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2859 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2861 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2860 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2856 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2858 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2852 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2857 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2852 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2852 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2852 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2849 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2847 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2849 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2863 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2858 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2859 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2861 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2857 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2859 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2861 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2861 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2864 [Training] 118/118 [==============================] 254.3ms/step  batch_loss: 0.2862 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:43:31 - INFO - root -   The F1-score is 0.6215420081967215
09/15/2022 02:43:31 - INFO - root -   the best eval f1 is 0.6215, saving model !!
09/15/2022 02:43:33 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:30  batch_loss: 0.2219 [Training] 2/118 [..............................] - ETA: 59s  batch_loss: 0.2343 [Training] 3/118 [..............................] - ETA: 49s  batch_loss: 0.2514 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 0.2543 [Training] 5/118 [>.............................] - ETA: 41s  batch_loss: 0.2580 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.2564 [Training] 7/118 [>.............................] - ETA: 37s  batch_loss: 0.2567 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.2518 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.2502 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.2490 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.2530 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.2568 [Training] 13/118 [==>...........................] - ETA: 31s  batch_loss: 0.2562 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.2548 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 0.2556 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.2590 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.2571 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.2536 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.2538 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.2549 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.2548 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.2539 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.2555 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.2556 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.2558 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.2559 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.2552 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.2555 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.2581 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.2580 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.2574 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.2586 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.2577 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.2593 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.2598 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.2605 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2608 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2623 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.2617 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2624 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2621 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.2630 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.2634 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2650 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2648 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.2642 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2644 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2642 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.2639 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.2641 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2632 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2626 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.2623 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2623 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2614 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.2618 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.2623 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2626 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2634 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.2625 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2626 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2628 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2621 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.2610 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.2603 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2598 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2607 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.2601 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2600 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2609 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.2608 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.2616 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2608 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2604 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.2614 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.2608 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2621 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2620 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.2626 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2631 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2626 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2629 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.2629 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2634 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2641 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2644 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.2645 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2646 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2643 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2639 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.2635 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2633 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2633 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2636 [Training] 95/118 [=======================>......] - ETA: 6s  batch_loss: 0.2638 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2633 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2630 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2625 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2621 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2619 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2615 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2613 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2617 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2616 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2613 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2610 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2614 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2614 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2611 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2615 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2620 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2620 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2622 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2625 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2624 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2622 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2623 [Training] 118/118 [==============================] 258.1ms/step  batch_loss: 0.2628 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:44:10 - INFO - root -   The F1-score is 0.6223101973054084
09/15/2022 02:44:10 - INFO - root -   the best eval f1 is 0.6223, saving model !!
09/15/2022 02:44:13 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:29  batch_loss: 0.2810 [Training] 2/118 [..............................] - ETA: 1:00  batch_loss: 0.2753 [Training] 3/118 [..............................] - ETA: 49s  batch_loss: 0.2621 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 0.2632 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.2610 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.2584 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.2483 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.2541 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.2478 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.2471 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.2515 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.2480 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.2444 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.2469 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.2466 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.2469 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.2425 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.2411 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.2419 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.2397 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.2403 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.2463 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.2449 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.2453 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.2443 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.2451 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.2440 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.2446 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.2429 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.2415 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.2423 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.2443 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.2427 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.2434 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.2431 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.2451 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2450 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2461 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.2451 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2451 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2447 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.2453 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.2449 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2458 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2455 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.2445 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2445 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2442 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.2439 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.2437 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2434 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2438 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.2428 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2428 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2425 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.2413 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.2415 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2417 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2412 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.2411 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2408 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2415 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2406 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.2413 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.2413 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2424 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2422 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.2418 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2418 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2406 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.2401 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.2397 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2394 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2394 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.2397 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.2396 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2393 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2392 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.2405 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2407 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2410 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2411 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.2415 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2416 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2416 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2413 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.2421 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2416 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2420 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2419 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.2414 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2409 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2409 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2405 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.2415 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2412 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2413 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2413 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2418 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2420 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2417 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2416 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2422 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2414 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2422 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2427 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2428 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2427 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2426 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2424 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2424 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2426 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2426 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2427 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2432 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2430 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2429 [Training] 118/118 [==============================] 254.8ms/step  batch_loss: 0.2420 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:44:49 - INFO - root -   The F1-score is 0.6319385140905209
09/15/2022 02:44:49 - INFO - root -   the best eval f1 is 0.6319, saving model !!
09/15/2022 02:44:52 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:32  batch_loss: 0.2444 [Training] 2/118 [..............................] - ETA: 1:00  batch_loss: 0.2370 [Training] 3/118 [..............................] - ETA: 50s  batch_loss: 0.2410 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 0.2305 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.2364 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.2315 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.2301 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.2256 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.2242 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.2251 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.2304 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.2295 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.2245 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.2242 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.2241 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.2240 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.2225 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.2212 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.2203 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.2193 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.2185 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.2194 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.2203 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.2207 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.2225 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.2218 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.2218 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.2224 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.2221 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.2216 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.2227 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.2219 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.2222 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.2213 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.2240 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.2251 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2244 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2249 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.2264 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2268 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2285 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.2272 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.2280 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2291 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2294 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.2287 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2293 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2285 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.2292 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.2286 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2284 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2284 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.2278 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2273 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2276 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.2277 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.2274 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2272 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2270 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.2277 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2283 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2277 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2286 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.2273 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.2270 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2268 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2257 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.2253 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2253 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2250 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.2253 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.2246 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2251 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2248 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.2247 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.2245 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2241 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2249 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.2247 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2254 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2259 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2261 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.2260 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2262 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2262 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2255 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.2257 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2256 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2255 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2251 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.2252 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2255 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2261 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2258 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.2256 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2258 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2260 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2259 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2265 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2262 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2261 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2259 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2261 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2260 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2258 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2253 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2251 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2253 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2251 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2248 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2250 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2250 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2248 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2251 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2247 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2250 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2251 [Training] 118/118 [==============================] 255.3ms/step  batch_loss: 0.2249 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:45:28 - INFO - root -   The F1-score is 0.6423442654764214
09/15/2022 02:45:28 - INFO - root -   the best eval f1 is 0.6423, saving model !!
09/15/2022 02:45:31 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:28  batch_loss: 0.2561 [Training] 2/118 [..............................] - ETA: 59s  batch_loss: 0.2310 [Training] 3/118 [..............................] - ETA: 49s  batch_loss: 0.2176 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 0.2145 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.2098 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.2147 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.2202 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.2221 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.2232 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.2192 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.2198 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.2208 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.2176 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.2158 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.2161 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.2159 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.2147 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.2157 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.2151 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.2154 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.2148 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.2148 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.2134 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.2132 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.2137 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.2129 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.2128 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.2116 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.2101 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.2103 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.2098 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.2108 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.2112 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.2111 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.2113 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.2105 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2104 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2100 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.2105 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2104 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2114 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.2108 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.2094 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2093 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2095 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.2087 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2099 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2100 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.2104 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.2096 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2092 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2085 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.2087 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2080 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2079 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.2078 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.2076 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2069 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2069 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.2071 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2081 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2081 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2078 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.2077 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.2065 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2078 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2082 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.2078 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2079 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2076 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.2079 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.2083 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2084 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2084 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.2086 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.2092 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2096 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2096 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.2096 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2101 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2103 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2100 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.2095 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2097 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2093 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2092 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.2091 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2086 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2091 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2089 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.2091 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2094 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2091 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2092 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.2094 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2091 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2090 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2088 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2085 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2088 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2085 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2084 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2085 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2080 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2078 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2076 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2085 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2085 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2088 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2088 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.2085 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.2089 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2091 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2092 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.2093 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.2096 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.2093 [Training] 118/118 [==============================] 255.5ms/step  batch_loss: 0.2095 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:46:07 - INFO - root -   The F1-score is 0.6381454711206616
09/15/2022 02:46:07 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:28  batch_loss: 0.2011 [Training] 2/118 [..............................] - ETA: 58s  batch_loss: 0.1770 [Training] 3/118 [..............................] - ETA: 47s  batch_loss: 0.1917 [Training] 4/118 [>.............................] - ETA: 42s  batch_loss: 0.1953 [Training] 5/118 [>.............................] - ETA: 38s  batch_loss: 0.1973 [Training] 6/118 [>.............................] - ETA: 36s  batch_loss: 0.1904 [Training] 7/118 [>.............................] - ETA: 35s  batch_loss: 0.1882 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.1917 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.1919 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.1929 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.1886 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.1867 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.1901 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.1930 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.1934 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.1911 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.1919 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.1929 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.1938 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.1965 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1949 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.1926 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.1961 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1971 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.1963 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.1974 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1971 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.1963 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.1961 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1976 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1986 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.1985 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1981 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1979 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.1998 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.2011 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.2032 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.2035 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.2034 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.2039 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.2040 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.2027 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.2027 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.2027 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.2033 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.2028 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.2029 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.2033 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.2026 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.2026 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.2027 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.2030 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.2028 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.2021 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.2027 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.2035 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.2031 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.2032 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.2027 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.2022 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.2020 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.2020 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.2016 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.2014 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.2016 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.2018 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.2012 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.2009 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.2009 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.2005 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.2007 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.2000 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.2001 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.2004 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.2002 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.2002 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.2004 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.2001 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.2012 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.2015 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.2013 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.2013 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.2011 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.2014 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.2007 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.2010 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.2006 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.2005 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.2002 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.2007 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.2015 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.2009 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.2009 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.2013 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.2012 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.2015 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.2014 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.2010 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.2015 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.2014 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.2011 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.2012 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.2008 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.2005 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.2006 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.2002 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.2007 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.2002 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.2001 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.2000 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1997 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1999 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.2000 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.2002 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1996 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1993 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1989 [Training] 118/118 [==============================] 252.5ms/step  batch_loss: 0.1982 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:46:43 - INFO - root -   The F1-score is 0.6380142204164551
09/15/2022 02:46:43 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:32  batch_loss: 0.1866 [Training] 2/118 [..............................] - ETA: 1:00  batch_loss: 0.1783 [Training] 3/118 [..............................] - ETA: 49s  batch_loss: 0.1878 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 0.1838 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.1729 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.1792 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.1886 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.1837 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.1803 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.1789 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.1802 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.1813 [Training] 13/118 [==>...........................] - ETA: 31s  batch_loss: 0.1777 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.1797 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 0.1834 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.1833 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.1826 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.1835 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.1830 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.1869 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.1865 [Training] 22/118 [====>.........................] - ETA: 27s  batch_loss: 0.1892 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.1884 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.1859 [Training] 25/118 [=====>........................] - ETA: 26s  batch_loss: 0.1864 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1863 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.1872 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.1859 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1848 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.1845 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1867 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1870 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.1862 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1853 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1856 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.1869 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1860 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1857 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1858 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.1859 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1849 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1847 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.1848 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1841 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1843 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.1838 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1834 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1833 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1834 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.1838 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1838 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1839 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1839 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1841 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1848 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1846 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.1845 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1849 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1855 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1854 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1846 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1839 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1837 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1846 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1840 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1842 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1837 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1837 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1834 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1834 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1833 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1833 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1839 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1840 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1838 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1837 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1834 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1833 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1836 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1835 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1836 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1835 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1836 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1837 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1835 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1833 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1838 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1835 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1840 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1843 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1850 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1845 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1852 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1864 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1862 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1861 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1865 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1870 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1869 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1870 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1874 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1872 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1872 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1872 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1869 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1868 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1868 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1865 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1867 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1862 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1864 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1870 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1871 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1870 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1868 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1870 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1871 [Training] 118/118 [==============================] 255.9ms/step  batch_loss: 0.1871 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:47:20 - INFO - root -   The F1-score is 0.6408262144587531
09/15/2022 02:47:20 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:26  batch_loss: 0.1214 [Training] 2/118 [..............................] - ETA: 58s  batch_loss: 0.1472 [Training] 3/118 [..............................] - ETA: 48s  batch_loss: 0.1434 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 0.1469 [Training] 5/118 [>.............................] - ETA: 39s  batch_loss: 0.1459 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.1458 [Training] 7/118 [>.............................] - ETA: 36s  batch_loss: 0.1514 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.1543 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.1664 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.1657 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.1675 [Training] 12/118 [==>...........................] - ETA: 31s  batch_loss: 0.1678 [Training] 13/118 [==>...........................] - ETA: 31s  batch_loss: 0.1669 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.1708 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.1721 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.1712 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.1690 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.1685 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.1675 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.1673 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.1665 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.1678 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.1674 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.1681 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.1701 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1698 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.1700 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.1712 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1702 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.1708 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1725 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1716 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.1718 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 0.1714 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1718 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.1718 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.1710 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1708 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1725 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.1720 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1729 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1740 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.1729 [Training] 44/118 [==========>...................] - ETA: 20s  batch_loss: 0.1724 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1726 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.1717 [Training] 47/118 [==========>...................] - ETA: 19s  batch_loss: 0.1718 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1715 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1706 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.1705 [Training] 51/118 [===========>..................] - ETA: 18s  batch_loss: 0.1712 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1713 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1715 [Training] 54/118 [============>.................] - ETA: 17s  batch_loss: 0.1719 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1719 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1717 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.1713 [Training] 58/118 [=============>................] - ETA: 16s  batch_loss: 0.1714 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1710 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1704 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.1700 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1697 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1700 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1703 [Training] 65/118 [===============>..............] - ETA: 14s  batch_loss: 0.1708 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1704 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1704 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1705 [Training] 69/118 [================>.............] - ETA: 13s  batch_loss: 0.1709 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1706 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1704 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.1714 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1718 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1718 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1726 [Training] 76/118 [==================>...........] - ETA: 11s  batch_loss: 0.1725 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1722 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1723 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1722 [Training] 80/118 [===================>..........] - ETA: 10s  batch_loss: 0.1728 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1728 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1732 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1733 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1737 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1734 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1737 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1739 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1743 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1745 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1741 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.1738 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1734 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1734 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1736 [Training] 95/118 [=======================>......] - ETA: 6s  batch_loss: 0.1737 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1737 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1734 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1736 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1740 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1743 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1740 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1743 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1750 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1752 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1755 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1759 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1761 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1765 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1772 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1770 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1769 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1776 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1776 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1777 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1776 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1775 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1773 [Training] 118/118 [==============================] 258.4ms/step  batch_loss: 0.1776 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:47:57 - INFO - root -   The F1-score is 0.6517298608451423
09/15/2022 02:47:57 - INFO - root -   the best eval f1 is 0.6517, saving model !!
09/15/2022 02:47:59 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:30  batch_loss: 0.1476 [Training] 2/118 [..............................] - ETA: 59s  batch_loss: 0.1438 [Training] 3/118 [..............................] - ETA: 48s  batch_loss: 0.1575 [Training] 4/118 [>.............................] - ETA: 43s  batch_loss: 0.1487 [Training] 5/118 [>.............................] - ETA: 39s  batch_loss: 0.1500 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.1653 [Training] 7/118 [>.............................] - ETA: 35s  batch_loss: 0.1596 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.1632 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.1651 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.1633 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.1632 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.1630 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.1600 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.1636 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.1629 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.1641 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.1631 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.1626 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.1652 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.1658 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1651 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.1650 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.1657 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1654 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.1644 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.1640 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1633 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.1645 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.1634 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1648 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1648 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.1651 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1655 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1656 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1653 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1657 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1663 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1661 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1674 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1674 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1668 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1667 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1671 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1665 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1667 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1666 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1666 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1660 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1664 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1662 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1672 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1666 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.1666 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1663 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1669 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1666 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1663 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1661 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1658 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1649 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1650 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1646 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1644 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.1642 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1642 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1654 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1655 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1661 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1652 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1649 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1654 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1658 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1659 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1661 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1664 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1659 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1659 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1656 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1653 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1657 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1657 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1658 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1659 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1659 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1662 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1664 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1660 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1658 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1663 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1661 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1661 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1657 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1656 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1661 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1663 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1661 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1666 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1663 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1660 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1661 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1668 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1669 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1667 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1661 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1664 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1663 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1667 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1666 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1668 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1666 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1665 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1668 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1667 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1673 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1671 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1671 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1674 [Training] 118/118 [==============================] 254.0ms/step  batch_loss: 0.1681 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:48:36 - INFO - root -   The F1-score is 0.6455133900241147
09/15/2022 02:48:36 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:29  batch_loss: 0.1991 [Training] 2/118 [..............................] - ETA: 59s  batch_loss: 0.1775 [Training] 3/118 [..............................] - ETA: 49s  batch_loss: 0.1596 [Training] 4/118 [>.............................] - ETA: 44s  batch_loss: 0.1536 [Training] 5/118 [>.............................] - ETA: 40s  batch_loss: 0.1611 [Training] 6/118 [>.............................] - ETA: 38s  batch_loss: 0.1572 [Training] 7/118 [>.............................] - ETA: 37s  batch_loss: 0.1578 [Training] 8/118 [=>............................] - ETA: 35s  batch_loss: 0.1545 [Training] 9/118 [=>............................] - ETA: 34s  batch_loss: 0.1547 [Training] 10/118 [=>............................] - ETA: 33s  batch_loss: 0.1556 [Training] 11/118 [=>............................] - ETA: 32s  batch_loss: 0.1560 [Training] 12/118 [==>...........................] - ETA: 32s  batch_loss: 0.1530 [Training] 13/118 [==>...........................] - ETA: 31s  batch_loss: 0.1560 [Training] 14/118 [==>...........................] - ETA: 30s  batch_loss: 0.1571 [Training] 15/118 [==>...........................] - ETA: 30s  batch_loss: 0.1542 [Training] 16/118 [===>..........................] - ETA: 29s  batch_loss: 0.1535 [Training] 17/118 [===>..........................] - ETA: 29s  batch_loss: 0.1527 [Training] 18/118 [===>..........................] - ETA: 28s  batch_loss: 0.1539 [Training] 19/118 [===>..........................] - ETA: 28s  batch_loss: 0.1536 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.1543 [Training] 21/118 [====>.........................] - ETA: 27s  batch_loss: 0.1529 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.1538 [Training] 23/118 [====>.........................] - ETA: 26s  batch_loss: 0.1538 [Training] 24/118 [=====>........................] - ETA: 26s  batch_loss: 0.1540 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.1533 [Training] 26/118 [=====>........................] - ETA: 25s  batch_loss: 0.1537 [Training] 27/118 [=====>........................] - ETA: 25s  batch_loss: 0.1539 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.1531 [Training] 29/118 [======>.......................] - ETA: 24s  batch_loss: 0.1528 [Training] 30/118 [======>.......................] - ETA: 24s  batch_loss: 0.1530 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1524 [Training] 32/118 [=======>......................] - ETA: 23s  batch_loss: 0.1516 [Training] 33/118 [=======>......................] - ETA: 23s  batch_loss: 0.1516 [Training] 34/118 [=======>......................] - ETA: 23s  batch_loss: 0.1514 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1500 [Training] 36/118 [========>.....................] - ETA: 22s  batch_loss: 0.1495 [Training] 37/118 [========>.....................] - ETA: 22s  batch_loss: 0.1500 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1510 [Training] 39/118 [========>.....................] - ETA: 21s  batch_loss: 0.1523 [Training] 40/118 [=========>....................] - ETA: 21s  batch_loss: 0.1518 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1522 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1524 [Training] 43/118 [=========>....................] - ETA: 20s  batch_loss: 0.1524 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1532 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1522 [Training] 46/118 [==========>...................] - ETA: 19s  batch_loss: 0.1529 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1529 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1528 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1529 [Training] 50/118 [===========>..................] - ETA: 18s  batch_loss: 0.1535 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1539 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1545 [Training] 53/118 [============>.................] - ETA: 17s  batch_loss: 0.1540 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1541 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1537 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1549 [Training] 57/118 [=============>................] - ETA: 16s  batch_loss: 0.1552 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1559 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1567 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1564 [Training] 61/118 [==============>...............] - ETA: 15s  batch_loss: 0.1574 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1567 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1567 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1570 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1568 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1566 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1567 [Training] 68/118 [================>.............] - ETA: 13s  batch_loss: 0.1567 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1570 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1575 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1573 [Training] 72/118 [=================>............] - ETA: 12s  batch_loss: 0.1577 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1576 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1583 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1584 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1578 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1576 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1576 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1575 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1577 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1580 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1582 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1585 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1589 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1589 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1586 [Training] 87/118 [=====================>........] - ETA: 8s  batch_loss: 0.1592 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1593 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1594 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1595 [Training] 91/118 [======================>.......] - ETA: 7s  batch_loss: 0.1595 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1593 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1598 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1596 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1600 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1601 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1601 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1603 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1600 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1597 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1597 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1599 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1599 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1599 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1597 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1601 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1604 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1602 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1603 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1605 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1608 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1611 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1613 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1607 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1608 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1610 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1608 [Training] 118/118 [==============================] 256.3ms/step  batch_loss: 0.1607 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:49:13 - INFO - root -   The F1-score is 0.645791025221094
09/15/2022 02:49:13 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:25  batch_loss: 0.1364 [Training] 2/118 [..............................] - ETA: 56s  batch_loss: 0.1405 [Training] 3/118 [..............................] - ETA: 47s  batch_loss: 0.1431 [Training] 4/118 [>.............................] - ETA: 42s  batch_loss: 0.1478 [Training] 5/118 [>.............................] - ETA: 39s  batch_loss: 0.1404 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.1368 [Training] 7/118 [>.............................] - ETA: 35s  batch_loss: 0.1354 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.1354 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.1352 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.1384 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.1378 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.1374 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.1436 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.1451 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.1447 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.1431 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.1458 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.1462 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.1450 [Training] 20/118 [====>.........................] - ETA: 27s  batch_loss: 0.1447 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1450 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.1453 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.1468 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1478 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.1493 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.1501 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1500 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.1501 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.1512 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1522 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1515 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.1514 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1518 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1519 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1519 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1524 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1539 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1543 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1547 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1551 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1546 [Training] 42/118 [=========>....................] - ETA: 20s  batch_loss: 0.1537 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1532 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1526 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1516 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1511 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1509 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1514 [Training] 49/118 [===========>..................] - ETA: 18s  batch_loss: 0.1532 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1533 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1527 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1531 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.1532 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1537 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1541 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1539 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1541 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1535 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1533 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1536 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1534 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1543 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1540 [Training] 64/118 [===============>..............] - ETA: 14s  batch_loss: 0.1536 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1541 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1546 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1548 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1551 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1549 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1548 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1545 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1546 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1543 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1546 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1545 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1545 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1543 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1538 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1544 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1541 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1542 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1546 [Training] 83/118 [====================>.........] - ETA: 9s  batch_loss: 0.1544 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1538 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1538 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1533 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1533 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1533 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1531 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1534 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1531 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1532 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1535 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1536 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1535 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1541 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1539 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1538 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1538 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1533 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1535 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1534 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1532 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1528 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1526 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1527 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1528 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1527 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1530 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1529 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1532 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1530 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1527 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1530 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1528 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1531 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1530 [Training] 118/118 [==============================] 254.9ms/step  batch_loss: 0.1528 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:49:49 - INFO - root -   The F1-score is 0.6490135675427673
09/15/2022 02:49:49 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:26  batch_loss: 0.1021 [Training] 2/118 [..............................] - ETA: 57s  batch_loss: 0.1268 [Training] 3/118 [..............................] - ETA: 47s  batch_loss: 0.1248 [Training] 4/118 [>.............................] - ETA: 42s  batch_loss: 0.1291 [Training] 5/118 [>.............................] - ETA: 39s  batch_loss: 0.1263 [Training] 6/118 [>.............................] - ETA: 37s  batch_loss: 0.1263 [Training] 7/118 [>.............................] - ETA: 35s  batch_loss: 0.1323 [Training] 8/118 [=>............................] - ETA: 34s  batch_loss: 0.1385 [Training] 9/118 [=>............................] - ETA: 33s  batch_loss: 0.1428 [Training] 10/118 [=>............................] - ETA: 32s  batch_loss: 0.1417 [Training] 11/118 [=>............................] - ETA: 31s  batch_loss: 0.1426 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.1447 [Training] 13/118 [==>...........................] - ETA: 30s  batch_loss: 0.1432 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.1406 [Training] 15/118 [==>...........................] - ETA: 29s  batch_loss: 0.1392 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.1394 [Training] 17/118 [===>..........................] - ETA: 28s  batch_loss: 0.1402 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.1394 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.1391 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.1399 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1391 [Training] 22/118 [====>.........................] - ETA: 26s  batch_loss: 0.1385 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.1384 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1387 [Training] 25/118 [=====>........................] - ETA: 25s  batch_loss: 0.1391 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.1382 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1380 [Training] 28/118 [======>.......................] - ETA: 24s  batch_loss: 0.1388 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.1404 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1410 [Training] 31/118 [======>.......................] - ETA: 23s  batch_loss: 0.1407 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.1422 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1414 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1406 [Training] 35/118 [=======>......................] - ETA: 22s  batch_loss: 0.1407 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1414 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1404 [Training] 38/118 [========>.....................] - ETA: 21s  batch_loss: 0.1409 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1409 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1416 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1418 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.1418 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1410 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1412 [Training] 45/118 [==========>...................] - ETA: 19s  batch_loss: 0.1420 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1416 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1414 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1413 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.1413 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1410 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1419 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1423 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.1424 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1421 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1423 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1432 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1431 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1437 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1430 [Training] 60/118 [==============>...............] - ETA: 15s  batch_loss: 0.1435 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1432 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1428 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1429 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.1422 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1430 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1430 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1430 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1429 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1428 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1427 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1430 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1436 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1436 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1436 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1439 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1438 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1442 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1445 [Training] 79/118 [===================>..........] - ETA: 10s  batch_loss: 0.1445 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1446 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1443 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1446 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.1446 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1443 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1444 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1444 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1443 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1441 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1440 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1442 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1444 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1444 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1447 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1451 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1454 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1458 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1467 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1468 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1467 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1466 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1466 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1465 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1466 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1462 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1464 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1468 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1470 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1470 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1469 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1469 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1467 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1470 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1472 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1471 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1467 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1462 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1459 [Training] 118/118 [==============================] 253.8ms/step  batch_loss: 0.1462 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:50:25 - INFO - root -   The F1-score is 0.6423913745238555
09/15/2022 02:50:25 - INFO - root -   ***** Running train *****
********************************************************************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/118 [..............................] - ETA: 1:24  batch_loss: 0.1142 [Training] 2/118 [..............................] - ETA: 56s  batch_loss: 0.1187 [Training] 3/118 [..............................] - ETA: 45s  batch_loss: 0.1206 [Training] 4/118 [>.............................] - ETA: 40s  batch_loss: 0.1273 [Training] 5/118 [>.............................] - ETA: 37s  batch_loss: 0.1195 [Training] 6/118 [>.............................] - ETA: 36s  batch_loss: 0.1205 [Training] 7/118 [>.............................] - ETA: 34s  batch_loss: 0.1297 [Training] 8/118 [=>............................] - ETA: 33s  batch_loss: 0.1287 [Training] 9/118 [=>............................] - ETA: 32s  batch_loss: 0.1287 [Training] 10/118 [=>............................] - ETA: 31s  batch_loss: 0.1302 [Training] 11/118 [=>............................] - ETA: 30s  batch_loss: 0.1282 [Training] 12/118 [==>...........................] - ETA: 30s  batch_loss: 0.1271 [Training] 13/118 [==>...........................] - ETA: 29s  batch_loss: 0.1261 [Training] 14/118 [==>...........................] - ETA: 29s  batch_loss: 0.1262 [Training] 15/118 [==>...........................] - ETA: 28s  batch_loss: 0.1279 [Training] 16/118 [===>..........................] - ETA: 28s  batch_loss: 0.1328 [Training] 17/118 [===>..........................] - ETA: 27s  batch_loss: 0.1302 [Training] 18/118 [===>..........................] - ETA: 27s  batch_loss: 0.1313 [Training] 19/118 [===>..........................] - ETA: 27s  batch_loss: 0.1308 [Training] 20/118 [====>.........................] - ETA: 26s  batch_loss: 0.1313 [Training] 21/118 [====>.........................] - ETA: 26s  batch_loss: 0.1312 [Training] 22/118 [====>.........................] - ETA: 25s  batch_loss: 0.1314 [Training] 23/118 [====>.........................] - ETA: 25s  batch_loss: 0.1316 [Training] 24/118 [=====>........................] - ETA: 25s  batch_loss: 0.1311 [Training] 25/118 [=====>........................] - ETA: 24s  batch_loss: 0.1317 [Training] 26/118 [=====>........................] - ETA: 24s  batch_loss: 0.1325 [Training] 27/118 [=====>........................] - ETA: 24s  batch_loss: 0.1320 [Training] 28/118 [======>.......................] - ETA: 23s  batch_loss: 0.1314 [Training] 29/118 [======>.......................] - ETA: 23s  batch_loss: 0.1310 [Training] 30/118 [======>.......................] - ETA: 23s  batch_loss: 0.1313 [Training] 31/118 [======>.......................] - ETA: 22s  batch_loss: 0.1313 [Training] 32/118 [=======>......................] - ETA: 22s  batch_loss: 0.1320 [Training] 33/118 [=======>......................] - ETA: 22s  batch_loss: 0.1322 [Training] 34/118 [=======>......................] - ETA: 22s  batch_loss: 0.1317 [Training] 35/118 [=======>......................] - ETA: 21s  batch_loss: 0.1315 [Training] 36/118 [========>.....................] - ETA: 21s  batch_loss: 0.1309 [Training] 37/118 [========>.....................] - ETA: 21s  batch_loss: 0.1302 [Training] 38/118 [========>.....................] - ETA: 20s  batch_loss: 0.1298 [Training] 39/118 [========>.....................] - ETA: 20s  batch_loss: 0.1287 [Training] 40/118 [=========>....................] - ETA: 20s  batch_loss: 0.1287 [Training] 41/118 [=========>....................] - ETA: 20s  batch_loss: 0.1296 [Training] 42/118 [=========>....................] - ETA: 19s  batch_loss: 0.1313 [Training] 43/118 [=========>....................] - ETA: 19s  batch_loss: 0.1320 [Training] 44/118 [==========>...................] - ETA: 19s  batch_loss: 0.1330 [Training] 45/118 [==========>...................] - ETA: 18s  batch_loss: 0.1327 [Training] 46/118 [==========>...................] - ETA: 18s  batch_loss: 0.1323 [Training] 47/118 [==========>...................] - ETA: 18s  batch_loss: 0.1323 [Training] 48/118 [===========>..................] - ETA: 18s  batch_loss: 0.1327 [Training] 49/118 [===========>..................] - ETA: 17s  batch_loss: 0.1330 [Training] 50/118 [===========>..................] - ETA: 17s  batch_loss: 0.1335 [Training] 51/118 [===========>..................] - ETA: 17s  batch_loss: 0.1334 [Training] 52/118 [============>.................] - ETA: 17s  batch_loss: 0.1338 [Training] 53/118 [============>.................] - ETA: 16s  batch_loss: 0.1337 [Training] 54/118 [============>.................] - ETA: 16s  batch_loss: 0.1338 [Training] 55/118 [============>.................] - ETA: 16s  batch_loss: 0.1338 [Training] 56/118 [=============>................] - ETA: 16s  batch_loss: 0.1341 [Training] 57/118 [=============>................] - ETA: 15s  batch_loss: 0.1356 [Training] 58/118 [=============>................] - ETA: 15s  batch_loss: 0.1356 [Training] 59/118 [==============>...............] - ETA: 15s  batch_loss: 0.1357 [Training] 60/118 [==============>...............] - ETA: 14s  batch_loss: 0.1362 [Training] 61/118 [==============>...............] - ETA: 14s  batch_loss: 0.1370 [Training] 62/118 [==============>...............] - ETA: 14s  batch_loss: 0.1366 [Training] 63/118 [===============>..............] - ETA: 14s  batch_loss: 0.1362 [Training] 64/118 [===============>..............] - ETA: 13s  batch_loss: 0.1360 [Training] 65/118 [===============>..............] - ETA: 13s  batch_loss: 0.1368 [Training] 66/118 [===============>..............] - ETA: 13s  batch_loss: 0.1367 [Training] 67/118 [================>.............] - ETA: 13s  batch_loss: 0.1386 [Training] 68/118 [================>.............] - ETA: 12s  batch_loss: 0.1387 [Training] 69/118 [================>.............] - ETA: 12s  batch_loss: 0.1386 [Training] 70/118 [================>.............] - ETA: 12s  batch_loss: 0.1385 [Training] 71/118 [=================>............] - ETA: 12s  batch_loss: 0.1379 [Training] 72/118 [=================>............] - ETA: 11s  batch_loss: 0.1378 [Training] 73/118 [=================>............] - ETA: 11s  batch_loss: 0.1376 [Training] 74/118 [=================>............] - ETA: 11s  batch_loss: 0.1374 [Training] 75/118 [==================>...........] - ETA: 11s  batch_loss: 0.1375 [Training] 76/118 [==================>...........] - ETA: 10s  batch_loss: 0.1374 [Training] 77/118 [==================>...........] - ETA: 10s  batch_loss: 0.1377 [Training] 78/118 [==================>...........] - ETA: 10s  batch_loss: 0.1378 [Training] 79/118 [===================>..........] - ETA: 9s  batch_loss: 0.1376 [Training] 80/118 [===================>..........] - ETA: 9s  batch_loss: 0.1371 [Training] 81/118 [===================>..........] - ETA: 9s  batch_loss: 0.1366 [Training] 82/118 [===================>..........] - ETA: 9s  batch_loss: 0.1369 [Training] 83/118 [====================>.........] - ETA: 8s  batch_loss: 0.1366 [Training] 84/118 [====================>.........] - ETA: 8s  batch_loss: 0.1371 [Training] 85/118 [====================>.........] - ETA: 8s  batch_loss: 0.1369 [Training] 86/118 [====================>.........] - ETA: 8s  batch_loss: 0.1369 [Training] 87/118 [=====================>........] - ETA: 7s  batch_loss: 0.1370 [Training] 88/118 [=====================>........] - ETA: 7s  batch_loss: 0.1373 [Training] 89/118 [=====================>........] - ETA: 7s  batch_loss: 0.1374 [Training] 90/118 [=====================>........] - ETA: 7s  batch_loss: 0.1372 [Training] 91/118 [======================>.......] - ETA: 6s  batch_loss: 0.1370 [Training] 92/118 [======================>.......] - ETA: 6s  batch_loss: 0.1371 [Training] 93/118 [======================>.......] - ETA: 6s  batch_loss: 0.1374 [Training] 94/118 [======================>.......] - ETA: 6s  batch_loss: 0.1373 [Training] 95/118 [=======================>......] - ETA: 5s  batch_loss: 0.1371 [Training] 96/118 [=======================>......] - ETA: 5s  batch_loss: 0.1368 [Training] 97/118 [=======================>......] - ETA: 5s  batch_loss: 0.1369 [Training] 98/118 [=======================>......] - ETA: 5s  batch_loss: 0.1370 [Training] 99/118 [========================>.....] - ETA: 4s  batch_loss: 0.1373 [Training] 100/118 [========================>.....] - ETA: 4s  batch_loss: 0.1374 [Training] 101/118 [========================>.....] - ETA: 4s  batch_loss: 0.1375 [Training] 102/118 [========================>.....] - ETA: 4s  batch_loss: 0.1375 [Training] 103/118 [=========================>....] - ETA: 3s  batch_loss: 0.1376 [Training] 104/118 [=========================>....] - ETA: 3s  batch_loss: 0.1374 [Training] 105/118 [=========================>....] - ETA: 3s  batch_loss: 0.1374 [Training] 106/118 [=========================>....] - ETA: 3s  batch_loss: 0.1376 [Training] 107/118 [==========================>...] - ETA: 2s  batch_loss: 0.1376 [Training] 108/118 [==========================>...] - ETA: 2s  batch_loss: 0.1376 [Training] 109/118 [==========================>...] - ETA: 2s  batch_loss: 0.1381 [Training] 110/118 [==========================>...] - ETA: 2s  batch_loss: 0.1381 [Training] 111/118 [===========================>..] - ETA: 1s  batch_loss: 0.1383 [Training] 112/118 [===========================>..] - ETA: 1s  batch_loss: 0.1386 [Training] 113/118 [===========================>..] - ETA: 1s  batch_loss: 0.1395 [Training] 114/118 [===========================>..] - ETA: 1s  batch_loss: 0.1394 [Training] 115/118 [============================>.] - ETA: 0s  batch_loss: 0.1392 [Training] 116/118 [============================>.] - ETA: 0s  batch_loss: 0.1393 [Training] 117/118 [============================>.] - ETA: 0s  batch_loss: 0.1393 [Training] 118/118 [==============================] 254.9ms/step  batch_loss: 0.1388 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/15/2022 02:51:02 - INFO - root -   The F1-score is 0.6440743609604959
09/15/2022 02:51:02 - INFO - root -   Early stop in 25 epoch!
********** predict start ***********
{
    "adam_epsilon": 1e-08, 
    "dataset": "DuEE1.0", 
    "do_distri_train": false, 
    "early_stop": 8, 
    "event_type": "role", 
    "fine_tunning_model_path": "./output/DuEE1.0/role/best_model.pkl", 
    "gradient_accumulation_steps": 1, 
    "learning_rate": 1e-05, 
    "linear_learning_rate": 0.001, 
    "max_len": 250, 
    "model_name_or_path": "/data/qingyang/data/chinese-roberta-wwm-ext", 
    "num_train_epochs": 50.0, 
    "output_dir": "./output", 
    "overwrite_cache": false, 
    "per_gpu_eval_batch_size": 128, 
    "per_gpu_train_batch_size": 8, 
    "predict_save_path": "./output/DuEE1.0/role/test_result.json", 
    "seed": 66, 
    "stride": 100, 
    "test_json": "./data/DuEE1.0/subor_news.json", 
    "weight_decay": 0.01
}
tokenizing...:   0%|          | 0/147540 [00:00<?, ?it/s]tokenizing...:   0%|          | 144/147540 [00:00<01:42, 1435.13it/s]tokenizing...:   0%|          | 304/147540 [00:00<01:36, 1529.94it/s]tokenizing...:   0%|          | 486/147540 [00:00<01:28, 1658.02it/s]tokenizing...:   0%|          | 691/147540 [00:00<01:21, 1812.35it/s]tokenizing...:   1%|          | 912/147540 [00:00<01:15, 1947.37it/s]tokenizing...:   1%|          | 1164/147540 [00:00<01:08, 2140.70it/s]tokenizing...:   1%|          | 1379/147540 [00:00<01:22, 1774.25it/s]tokenizing...:   1%|          | 1567/147540 [00:00<01:21, 1801.87it/s]tokenizing...:   1%|          | 1793/147540 [00:00<01:15, 1929.13it/s]tokenizing...:   1%|▏         | 1992/147540 [00:01<03:09, 769.25it/s] tokenizing...:   2%|▏         | 2215/147540 [00:01<02:29, 973.73it/s]tokenizing...:   2%|▏         | 2442/147540 [00:01<02:02, 1188.15it/s]tokenizing...:   2%|▏         | 2660/147540 [00:01<01:45, 1376.90it/s]tokenizing...:   2%|▏         | 2889/147540 [00:01<01:31, 1573.01it/s]tokenizing...:   2%|▏         | 3107/147540 [00:02<01:24, 1709.81it/s]tokenizing...:   2%|▏         | 3316/147540 [00:02<01:25, 1691.46it/s]tokenizing...:   2%|▏         | 3549/147540 [00:02<01:17, 1850.98it/s]tokenizing...:   3%|▎         | 3795/147540 [00:02<01:11, 2011.02it/s]tokenizing...:   3%|▎         | 4025/147540 [00:02<01:09, 2071.12it/s]tokenizing...:   3%|▎         | 4250/147540 [00:02<01:07, 2120.70it/s]tokenizing...:   3%|▎         | 4487/147540 [00:02<01:05, 2191.17it/s]tokenizing...:   3%|▎         | 4713/147540 [00:02<01:06, 2136.41it/s]tokenizing...:   3%|▎         | 4932/147540 [00:02<01:06, 2146.22it/s]tokenizing...:   3%|▎         | 5151/147540 [00:03<01:10, 2027.28it/s]tokenizing...:   4%|▎         | 5358/147540 [00:03<01:12, 1971.80it/s]tokenizing...:   4%|▍         | 5558/147540 [00:03<01:14, 1904.01it/s]tokenizing...:   4%|▍         | 5751/147540 [00:03<01:16, 1860.90it/s]tokenizing...:   4%|▍         | 5939/147540 [00:03<01:17, 1827.56it/s]tokenizing...:   4%|▍         | 6123/147540 [00:03<01:20, 1761.95it/s]tokenizing...:   4%|▍         | 6300/147540 [00:03<01:20, 1761.68it/s]tokenizing...:   4%|▍         | 6477/147540 [00:03<01:20, 1755.95it/s]tokenizing...:   5%|▍         | 6654/147540 [00:03<01:20, 1756.51it/s]tokenizing...:   5%|▍         | 6831/147540 [00:04<01:20, 1758.15it/s]tokenizing...:   5%|▍         | 7007/147540 [00:04<01:19, 1757.23it/s]tokenizing...:   5%|▍         | 7183/147540 [00:04<01:21, 1723.95it/s]tokenizing...:   5%|▍         | 7367/147540 [00:04<01:19, 1756.37it/s]tokenizing...:   5%|▌         | 7548/147540 [00:04<01:19, 1768.63it/s]tokenizing...:   5%|▌         | 7726/147540 [00:04<01:19, 1758.23it/s]tokenizing...:   5%|▌         | 7911/147540 [00:04<01:18, 1783.08it/s]tokenizing...:   5%|▌         | 8092/147540 [00:04<01:17, 1790.23it/s]tokenizing...:   6%|▌         | 8272/147540 [00:04<01:20, 1725.01it/s]tokenizing...:   6%|▌         | 8454/147540 [00:04<01:19, 1748.94it/s]tokenizing...:   6%|▌         | 8634/147540 [00:05<01:18, 1763.04it/s]tokenizing...:   6%|▌         | 8832/147540 [00:05<01:15, 1825.61it/s]tokenizing...:   6%|▌         | 9069/147540 [00:05<01:09, 1985.87it/s]tokenizing...:   6%|▋         | 9269/147540 [00:05<01:12, 1913.36it/s]tokenizing...:   6%|▋         | 9499/147540 [00:05<01:08, 2015.11it/s]tokenizing...:   7%|▋         | 9755/147540 [00:05<01:03, 2170.41it/s]tokenizing...:   7%|▋         | 9973/147540 [00:05<01:04, 2130.56it/s]tokenizing...:   7%|▋         | 10227/147540 [00:05<01:01, 2246.72it/s]tokenizing...:   7%|▋         | 10453/147540 [00:05<01:01, 2237.76it/s]tokenizing...:   7%|▋         | 10710/147540 [00:05<00:58, 2335.21it/s]tokenizing...:   7%|▋         | 10945/147540 [00:06<00:59, 2314.95it/s]tokenizing...:   8%|▊         | 11193/147540 [00:06<00:57, 2359.99it/s]tokenizing...:   8%|▊         | 11430/147540 [00:06<00:58, 2317.27it/s]tokenizing...:   8%|▊         | 11677/147540 [00:06<00:57, 2361.89it/s]tokenizing...:   8%|▊         | 11915/147540 [00:06<00:57, 2365.62it/s]tokenizing...:   8%|▊         | 12154/147540 [00:06<00:57, 2371.65it/s]tokenizing...:   8%|▊         | 12392/147540 [00:06<00:59, 2286.90it/s]tokenizing...:   9%|▊         | 12625/147540 [00:06<00:58, 2298.09it/s]tokenizing...:   9%|▊         | 12856/147540 [00:06<01:00, 2242.11it/s]tokenizing...:   9%|▉         | 13093/147540 [00:07<00:59, 2276.33it/s]tokenizing...:   9%|▉         | 13322/147540 [00:07<00:59, 2259.83it/s]tokenizing...:   9%|▉         | 13553/147540 [00:07<00:58, 2272.43it/s]tokenizing...:   9%|▉         | 13786/147540 [00:07<00:58, 2289.34it/s]tokenizing...:  10%|▉         | 14024/147540 [00:07<00:57, 2311.10it/s]tokenizing...:  10%|▉         | 14256/147540 [00:07<01:09, 1912.03it/s]tokenizing...:  10%|▉         | 14459/147540 [00:07<01:12, 1846.16it/s]tokenizing...:  10%|▉         | 14653/147540 [00:07<01:11, 1869.75it/s]tokenizing...:  10%|█         | 14846/147540 [00:07<01:15, 1761.17it/s]tokenizing...:  10%|█         | 15027/147540 [00:08<01:14, 1766.93it/s]tokenizing...:  10%|█         | 15208/147540 [00:08<01:15, 1761.25it/s]tokenizing...:  10%|█         | 15387/147540 [00:08<01:16, 1733.28it/s]tokenizing...:  11%|█         | 15566/147540 [00:08<01:15, 1746.57it/s]tokenizing...:  11%|█         | 15777/147540 [00:08<01:11, 1850.57it/s]tokenizing...:  11%|█         | 15990/147540 [00:08<01:08, 1930.79it/s]tokenizing...:  11%|█         | 16208/147540 [00:08<01:05, 2002.87it/s]tokenizing...:  11%|█         | 16417/147540 [00:08<01:04, 2026.47it/s]tokenizing...:  11%|█▏        | 16621/147540 [00:08<01:10, 1849.01it/s]tokenizing...:  11%|█▏        | 16831/147540 [00:08<01:08, 1918.69it/s]tokenizing...:  12%|█▏        | 17026/147540 [00:09<01:08, 1891.52it/s]tokenizing...:  12%|█▏        | 17218/147540 [00:09<01:09, 1871.89it/s]tokenizing...:  12%|█▏        | 17436/147540 [00:09<01:06, 1958.67it/s]tokenizing...:  12%|█▏        | 17634/147540 [00:09<01:09, 1871.48it/s]tokenizing...:  12%|█▏        | 17823/147540 [00:09<01:10, 1852.93it/s]tokenizing...:  12%|█▏        | 18011/147540 [00:09<01:09, 1860.15it/s]tokenizing...:  12%|█▏        | 18198/147540 [00:09<01:10, 1843.25it/s]tokenizing...:  12%|█▏        | 18399/147540 [00:09<01:08, 1890.64it/s]tokenizing...:  13%|█▎        | 18589/147540 [00:09<01:08, 1891.94it/s]tokenizing...:  13%|█▎        | 18779/147540 [00:10<01:10, 1828.63it/s]tokenizing...:  13%|█▎        | 18963/147540 [00:10<01:10, 1830.09it/s]tokenizing...:  13%|█▎        | 19147/147540 [00:10<01:10, 1810.66it/s]tokenizing...:  13%|█▎        | 19329/147540 [00:10<01:10, 1811.07it/s]tokenizing...:  13%|█▎        | 19511/147540 [00:10<01:10, 1810.62it/s]tokenizing...:  13%|█▎        | 19693/147540 [00:10<01:12, 1773.51it/s]tokenizing...:  13%|█▎        | 19874/147540 [00:10<01:11, 1781.99it/s]tokenizing...:  14%|█▎        | 20086/147540 [00:10<01:07, 1879.72it/s]tokenizing...:  14%|█▍        | 20317/147540 [00:10<01:03, 2006.42it/s]tokenizing...:  14%|█▍        | 20521/147540 [00:10<01:03, 2014.07it/s]tokenizing...:  14%|█▍        | 20724/147540 [00:11<01:02, 2018.17it/s]tokenizing...:  14%|█▍        | 20927/147540 [00:11<03:40, 573.65it/s] tokenizing...:  14%|█▍        | 21109/147540 [00:12<02:58, 708.03it/s]tokenizing...:  14%|█▍        | 21291/147540 [00:12<02:27, 856.06it/s]tokenizing...:  15%|█▍        | 21474/147540 [00:12<02:04, 1011.23it/s]tokenizing...:  15%|█▍        | 21655/147540 [00:12<01:48, 1159.09it/s]tokenizing...:  15%|█▍        | 21837/147540 [00:12<01:36, 1297.00it/s]tokenizing...:  15%|█▍        | 22013/147540 [00:12<01:30, 1390.15it/s]tokenizing...:  15%|█▌        | 22193/147540 [00:12<01:24, 1490.48it/s]tokenizing...:  15%|█▌        | 22375/147540 [00:12<01:19, 1574.44it/s]tokenizing...:  15%|█▌        | 22555/147540 [00:12<01:16, 1635.31it/s]tokenizing...:  15%|█▌        | 22737/147540 [00:12<01:14, 1684.51it/s]tokenizing...:  16%|█▌        | 22916/147540 [00:13<01:13, 1689.30it/s]tokenizing...:  16%|█▌        | 23101/147540 [00:13<01:11, 1734.42it/s]tokenizing...:  16%|█▌        | 23283/147540 [00:13<01:10, 1756.85it/s]tokenizing...:  16%|█▌        | 23465/147540 [00:13<01:09, 1773.97it/s]tokenizing...:  16%|█▌        | 23646/147540 [00:13<01:09, 1778.91it/s]tokenizing...:  16%|█▌        | 23827/147540 [00:13<01:09, 1786.24it/s]tokenizing...:  16%|█▋        | 24007/147540 [00:13<01:10, 1757.43it/s]tokenizing...:  16%|█▋        | 24189/147540 [00:13<01:09, 1774.67it/s]tokenizing...:  17%|█▋        | 24370/147540 [00:13<01:09, 1783.30it/s]tokenizing...:  17%|█▋        | 24592/147540 [00:14<01:04, 1912.08it/s]tokenizing...:  17%|█▋        | 24826/147540 [00:14<01:00, 2038.49it/s]tokenizing...:  17%|█▋        | 25031/147540 [00:14<01:00, 2013.72it/s]tokenizing...:  17%|█▋        | 25233/147540 [00:14<01:02, 1972.31it/s]tokenizing...:  17%|█▋        | 25431/147540 [00:14<01:02, 1959.29it/s]tokenizing...:  17%|█▋        | 25656/147540 [00:14<00:59, 2043.49it/s]tokenizing...:  18%|█▊        | 25861/147540 [00:14<01:07, 1810.21it/s]tokenizing...:  18%|█▊        | 26048/147540 [00:14<01:06, 1813.73it/s]tokenizing...:  18%|█▊        | 26268/147540 [00:14<01:03, 1920.99it/s]tokenizing...:  18%|█▊        | 26497/147540 [00:14<00:59, 2023.64it/s]tokenizing...:  18%|█▊        | 26721/147540 [00:15<00:57, 2085.06it/s]tokenizing...:  18%|█▊        | 26972/147540 [00:15<00:54, 2206.08it/s]tokenizing...:  18%|█▊        | 27203/147540 [00:15<00:53, 2234.31it/s]tokenizing...:  19%|█▊        | 27428/147540 [00:15<01:00, 2000.09it/s]tokenizing...:  19%|█▊        | 27643/147540 [00:15<00:58, 2040.45it/s]tokenizing...:  19%|█▉        | 27902/147540 [00:15<00:54, 2192.05it/s]tokenizing...:  19%|█▉        | 28126/147540 [00:15<01:00, 1961.62it/s]tokenizing...:  19%|█▉        | 28329/147540 [00:15<01:02, 1905.34it/s]tokenizing...:  19%|█▉        | 28525/147540 [00:16<01:06, 1791.76it/s]tokenizing...:  19%|█▉        | 28709/147540 [00:16<01:06, 1788.92it/s]tokenizing...:  20%|█▉        | 28941/147540 [00:16<01:01, 1933.00it/s]tokenizing...:  20%|█▉        | 29164/147540 [00:16<00:59, 2001.79it/s]tokenizing...:  20%|█▉        | 29409/147540 [00:16<00:55, 2125.88it/s]tokenizing...:  20%|██        | 29625/147540 [00:16<00:56, 2071.59it/s]tokenizing...:  20%|██        | 29835/147540 [00:16<00:56, 2070.13it/s]tokenizing...:  20%|██        | 30072/147540 [00:16<00:54, 2152.23it/s]tokenizing...:  21%|██        | 30293/147540 [00:16<00:54, 2145.11it/s]tokenizing...:  21%|██        | 30527/147540 [00:16<00:53, 2201.00it/s]tokenizing...:  21%|██        | 30761/147540 [00:17<00:52, 2240.36it/s]tokenizing...:  21%|██        | 31003/147540 [00:17<00:50, 2292.98it/s]tokenizing...:  21%|██        | 31239/147540 [00:17<00:50, 2311.46it/s]tokenizing...:  21%|██▏       | 31471/147540 [00:17<00:50, 2308.88it/s]tokenizing...:  21%|██▏       | 31707/147540 [00:17<00:49, 2321.19it/s]tokenizing...:  22%|██▏       | 31940/147540 [00:17<00:52, 2197.12it/s]tokenizing...:  22%|██▏       | 32162/147540 [00:17<00:53, 2166.78it/s]tokenizing...:  22%|██▏       | 32380/147540 [00:17<00:54, 2114.83it/s]tokenizing...:  22%|██▏       | 32593/147540 [00:17<00:54, 2107.90it/s]tokenizing...:  22%|██▏       | 32818/147540 [00:17<00:53, 2142.70it/s]tokenizing...:  22%|██▏       | 33033/147540 [00:18<00:55, 2050.23it/s]tokenizing...:  23%|██▎       | 33240/147540 [00:18<00:58, 1959.61it/s]tokenizing...:  23%|██▎       | 33438/147540 [00:18<01:06, 1711.62it/s]tokenizing...:  23%|██▎       | 33621/147540 [00:18<01:05, 1741.87it/s]tokenizing...:  23%|██▎       | 33845/147540 [00:18<01:00, 1875.62it/s]tokenizing...:  23%|██▎       | 34038/147540 [00:18<01:00, 1868.55it/s]tokenizing...:  23%|██▎       | 34229/147540 [00:18<01:00, 1861.79it/s]tokenizing...:  23%|██▎       | 34418/147540 [00:18<01:02, 1805.10it/s]tokenizing...:  23%|██▎       | 34601/147540 [00:18<01:02, 1797.47it/s]tokenizing...:  24%|██▎       | 34800/147540 [00:19<01:00, 1850.80it/s]tokenizing...:  24%|██▎       | 34987/147540 [00:19<01:00, 1850.62it/s]tokenizing...:  24%|██▍       | 35173/147540 [00:19<01:01, 1835.42it/s]tokenizing...:  24%|██▍       | 35363/147540 [00:19<01:00, 1853.86it/s]tokenizing...:  24%|██▍       | 35549/147540 [00:19<01:02, 1802.02it/s]tokenizing...:  24%|██▍       | 35732/147540 [00:19<01:01, 1809.20it/s]tokenizing...:  24%|██▍       | 35914/147540 [00:19<01:02, 1792.95it/s]tokenizing...:  24%|██▍       | 36097/147540 [00:19<01:01, 1802.10it/s]tokenizing...:  25%|██▍       | 36279/147540 [00:19<01:01, 1806.71it/s]tokenizing...:  25%|██▍       | 36486/147540 [00:19<00:58, 1882.64it/s]tokenizing...:  25%|██▍       | 36709/147540 [00:20<00:55, 1982.66it/s]tokenizing...:  25%|██▌       | 36930/147540 [00:20<00:53, 2049.60it/s]tokenizing...:  25%|██▌       | 37136/147540 [00:20<01:01, 1780.95it/s]tokenizing...:  25%|██▌       | 37321/147540 [00:20<01:06, 1662.62it/s]tokenizing...:  25%|██▌       | 37496/147540 [00:20<01:05, 1684.41it/s]tokenizing...:  26%|██▌       | 37669/147540 [00:20<01:06, 1661.85it/s]tokenizing...:  26%|██▌       | 37848/147540 [00:20<01:04, 1696.09it/s]tokenizing...:  26%|██▌       | 38022/147540 [00:20<01:04, 1706.16it/s]tokenizing...:  26%|██▌       | 38197/147540 [00:20<01:03, 1715.67it/s]tokenizing...:  26%|██▌       | 38370/147540 [00:21<01:03, 1706.19it/s]tokenizing...:  26%|██▌       | 38548/147540 [00:21<01:03, 1725.80it/s]tokenizing...:  26%|██▌       | 38722/147540 [00:21<01:03, 1705.86it/s]tokenizing...:  26%|██▋       | 38905/147540 [00:21<01:02, 1739.40it/s]tokenizing...:  26%|██▋       | 39085/147540 [00:21<01:01, 1753.96it/s]tokenizing...:  27%|██▋       | 39268/147540 [00:21<01:00, 1776.30it/s]tokenizing...:  27%|██▋       | 39450/147540 [00:21<01:00, 1787.36it/s]tokenizing...:  27%|██▋       | 39629/147540 [00:21<01:01, 1753.72it/s]tokenizing...:  27%|██▋       | 39811/147540 [00:21<01:00, 1768.41it/s]tokenizing...:  27%|██▋       | 39994/147540 [00:22<01:00, 1782.26it/s]tokenizing...:  27%|██▋       | 40175/147540 [00:22<00:59, 1789.52it/s]tokenizing...:  27%|██▋       | 40357/147540 [00:22<00:59, 1795.45it/s]tokenizing...:  27%|██▋       | 40537/147540 [00:22<00:59, 1795.05it/s]tokenizing...:  28%|██▊       | 40717/147540 [00:22<01:01, 1747.63it/s]tokenizing...:  28%|██▊       | 40939/147540 [00:22<00:56, 1885.17it/s]tokenizing...:  28%|██▊       | 41182/147540 [00:22<00:52, 2044.35it/s]tokenizing...:  28%|██▊       | 41402/147540 [00:22<00:50, 2089.70it/s]tokenizing...:  28%|██▊       | 41612/147540 [00:22<00:53, 1975.26it/s]tokenizing...:  28%|██▊       | 41812/147540 [00:22<01:00, 1739.70it/s]tokenizing...:  28%|██▊       | 41992/147540 [00:23<01:00, 1733.08it/s]tokenizing...:  29%|██▊       | 42170/147540 [00:23<01:01, 1710.96it/s]tokenizing...:  29%|██▊       | 42390/147540 [00:23<00:56, 1844.76it/s]tokenizing...:  29%|██▉       | 42606/147540 [00:23<00:54, 1933.11it/s]tokenizing...:  29%|██▉       | 42802/147540 [00:23<00:55, 1900.71it/s]tokenizing...:  29%|██▉       | 42999/147540 [00:23<00:54, 1918.19it/s]tokenizing...:  29%|██▉       | 43229/147540 [00:23<00:51, 2028.01it/s]tokenizing...:  29%|██▉       | 43469/147540 [00:23<00:48, 2135.21it/s]tokenizing...:  30%|██▉       | 43684/147540 [00:23<00:50, 2070.95it/s]tokenizing...:  30%|██▉       | 43893/147540 [00:25<03:37, 475.99it/s] tokenizing...:  30%|██▉       | 44149/147540 [00:25<02:37, 656.37it/s]tokenizing...:  30%|███       | 44401/147540 [00:25<01:59, 861.26it/s]tokenizing...:  30%|███       | 44604/147540 [00:25<01:42, 1008.32it/s]tokenizing...:  30%|███       | 44803/147540 [00:25<01:31, 1124.91it/s]tokenizing...:  30%|███       | 44993/147540 [00:25<01:21, 1265.49it/s]tokenizing...:  31%|███       | 45250/147540 [00:25<01:06, 1533.45it/s]tokenizing...:  31%|███       | 45503/147540 [00:25<00:58, 1758.29it/s]tokenizing...:  31%|███       | 45746/147540 [00:26<00:53, 1920.60it/s]tokenizing...:  31%|███       | 45974/147540 [00:26<00:51, 1971.78it/s]tokenizing...:  31%|███▏      | 46197/147540 [00:26<00:50, 1994.89it/s]tokenizing...:  31%|███▏      | 46415/147540 [00:26<00:49, 2027.00it/s]tokenizing...:  32%|███▏      | 46673/147540 [00:26<00:46, 2177.98it/s]tokenizing...:  32%|███▏      | 46909/147540 [00:26<00:45, 2225.97it/s]tokenizing...:  32%|███▏      | 47196/147540 [00:26<00:41, 2410.85it/s]tokenizing...:  32%|███▏      | 47445/147540 [00:26<00:41, 2432.15it/s]tokenizing...:  32%|███▏      | 47693/147540 [00:26<00:41, 2406.21it/s]tokenizing...:  32%|███▏      | 47937/147540 [00:27<00:50, 1961.05it/s]tokenizing...:  33%|███▎      | 48149/147540 [00:27<00:51, 1930.59it/s]tokenizing...:  33%|███▎      | 48362/147540 [00:27<00:50, 1978.70it/s]tokenizing...:  33%|███▎      | 48568/147540 [00:27<00:50, 1954.73it/s]tokenizing...:  33%|███▎      | 48769/147540 [00:27<00:54, 1810.39it/s]tokenizing...:  33%|███▎      | 48961/147540 [00:27<00:53, 1838.41it/s]tokenizing...:  33%|███▎      | 49149/147540 [00:27<00:56, 1740.51it/s]tokenizing...:  33%|███▎      | 49327/147540 [00:27<00:56, 1733.86it/s]tokenizing...:  34%|███▎      | 49503/147540 [00:27<00:59, 1639.31it/s]tokenizing...:  34%|███▎      | 49712/147540 [00:28<00:55, 1760.48it/s]tokenizing...:  34%|███▍      | 49931/147540 [00:28<00:51, 1878.93it/s]tokenizing...:  34%|███▍      | 50128/147540 [00:28<00:51, 1892.99it/s]tokenizing...:  34%|███▍      | 50320/147540 [00:28<00:56, 1714.65it/s]tokenizing...:  34%|███▍      | 50496/147540 [00:28<00:57, 1688.12it/s]tokenizing...:  34%|███▍      | 50668/147540 [00:28<00:58, 1667.36it/s]tokenizing...:  34%|███▍      | 50857/147540 [00:28<00:55, 1728.35it/s]tokenizing...:  35%|███▍      | 51066/147540 [00:28<00:52, 1828.29it/s]tokenizing...:  35%|███▍      | 51251/147540 [00:28<00:55, 1739.16it/s]tokenizing...:  35%|███▍      | 51427/147540 [00:29<00:55, 1726.64it/s]tokenizing...:  35%|███▍      | 51601/147540 [00:29<00:57, 1662.03it/s]tokenizing...:  35%|███▌      | 51775/147540 [00:29<00:56, 1683.24it/s]tokenizing...:  35%|███▌      | 51974/147540 [00:29<00:53, 1769.93it/s]tokenizing...:  35%|███▌      | 52155/147540 [00:29<00:53, 1778.78it/s]tokenizing...:  35%|███▌      | 52334/147540 [00:29<00:55, 1717.65it/s]tokenizing...:  36%|███▌      | 52508/147540 [00:29<00:55, 1721.78it/s]tokenizing...:  36%|███▌      | 52722/147540 [00:29<00:51, 1841.34it/s]tokenizing...:  36%|███▌      | 52907/147540 [00:29<00:52, 1811.78it/s]tokenizing...:  36%|███▌      | 53089/147540 [00:29<00:53, 1772.23it/s]tokenizing...:  36%|███▌      | 53267/147540 [00:30<00:54, 1722.76it/s]tokenizing...:  36%|███▌      | 53440/147540 [00:30<00:54, 1719.05it/s]tokenizing...:  36%|███▋      | 53638/147540 [00:30<00:52, 1791.76it/s]tokenizing...:  36%|███▋      | 53818/147540 [00:30<00:52, 1778.91it/s]tokenizing...:  37%|███▋      | 54000/147540 [00:30<00:52, 1789.85it/s]tokenizing...:  37%|███▋      | 54180/147540 [00:30<00:58, 1602.01it/s]tokenizing...:  37%|███▋      | 54344/147540 [00:30<01:00, 1540.34it/s]tokenizing...:  37%|███▋      | 54523/147540 [00:30<00:57, 1607.84it/s]tokenizing...:  37%|███▋      | 54703/147540 [00:30<00:55, 1658.20it/s]tokenizing...:  37%|███▋      | 54883/147540 [00:31<00:54, 1695.64it/s]tokenizing...:  37%|███▋      | 55068/147540 [00:31<00:53, 1738.74it/s]tokenizing...:  37%|███▋      | 55286/147540 [00:31<00:49, 1864.65it/s]tokenizing...:  38%|███▊      | 55483/147540 [00:31<00:48, 1895.41it/s]tokenizing...:  38%|███▊      | 55674/147540 [00:31<00:49, 1860.81it/s]tokenizing...:  38%|███▊      | 55861/147540 [00:31<00:54, 1691.86it/s]tokenizing...:  38%|███▊      | 56035/147540 [00:31<00:53, 1704.43it/s]tokenizing...:  38%|███▊      | 56208/147540 [00:31<00:54, 1686.99it/s]tokenizing...:  38%|███▊      | 56382/147540 [00:31<00:53, 1700.55it/s]tokenizing...:  38%|███▊      | 56554/147540 [00:31<00:54, 1668.51it/s]tokenizing...:  38%|███▊      | 56728/147540 [00:32<00:53, 1688.69it/s]tokenizing...:  39%|███▊      | 56901/147540 [00:32<00:53, 1698.63it/s]tokenizing...:  39%|███▊      | 57075/147540 [00:32<00:52, 1709.22it/s]tokenizing...:  39%|███▉      | 57257/147540 [00:32<00:51, 1741.84it/s]tokenizing...:  39%|███▉      | 57435/147540 [00:32<00:52, 1715.83it/s]tokenizing...:  39%|███▉      | 57616/147540 [00:32<00:51, 1742.76it/s]tokenizing...:  39%|███▉      | 57795/147540 [00:32<00:51, 1756.12it/s]tokenizing...:  39%|███▉      | 57978/147540 [00:32<00:50, 1777.59it/s]tokenizing...:  39%|███▉      | 58159/147540 [00:32<00:50, 1787.01it/s]tokenizing...:  40%|███▉      | 58343/147540 [00:32<00:49, 1800.91it/s]tokenizing...:  40%|███▉      | 58524/147540 [00:33<00:50, 1761.80it/s]tokenizing...:  40%|███▉      | 58707/147540 [00:33<00:49, 1778.29it/s]tokenizing...:  40%|███▉      | 58887/147540 [00:33<00:49, 1783.72it/s]tokenizing...:  40%|████      | 59069/147540 [00:33<00:49, 1791.70it/s]tokenizing...:  40%|████      | 59250/147540 [00:33<00:49, 1794.62it/s]tokenizing...:  40%|████      | 59432/147540 [00:33<00:48, 1800.33it/s]tokenizing...:  40%|████      | 59623/147540 [00:33<00:47, 1831.91it/s]tokenizing...:  41%|████      | 59819/147540 [00:33<00:46, 1867.87it/s]tokenizing...:  41%|████      | 60006/147540 [00:33<00:48, 1801.24it/s]tokenizing...:  41%|████      | 60187/147540 [00:34<00:52, 1657.92it/s]tokenizing...:  41%|████      | 60407/147540 [00:34<00:48, 1807.37it/s]tokenizing...:  41%|████      | 60612/147540 [00:34<00:46, 1875.69it/s]tokenizing...:  41%|████      | 60829/147540 [00:34<00:44, 1952.38it/s]tokenizing...:  41%|████▏     | 61027/147540 [00:34<00:44, 1937.93it/s]tokenizing...:  41%|████▏     | 61223/147540 [00:34<00:46, 1872.68it/s]tokenizing...:  42%|████▏     | 61412/147540 [00:34<00:52, 1641.55it/s]tokenizing...:  42%|████▏     | 61582/147540 [00:34<00:51, 1656.75it/s]tokenizing...:  42%|████▏     | 61752/147540 [00:34<00:54, 1580.87it/s]tokenizing...:  42%|████▏     | 61914/147540 [00:35<00:54, 1560.37it/s]tokenizing...:  42%|████▏     | 62140/147540 [00:35<00:48, 1751.41it/s]tokenizing...:  42%|████▏     | 62321/147540 [00:35<00:48, 1766.71it/s]tokenizing...:  42%|████▏     | 62507/147540 [00:35<00:47, 1783.48it/s]tokenizing...:  42%|████▏     | 62697/147540 [00:35<00:46, 1814.06it/s]tokenizing...:  43%|████▎     | 62896/147540 [00:35<00:45, 1864.05it/s]tokenizing...:  43%|████▎     | 63116/147540 [00:35<00:43, 1960.67it/s]tokenizing...:  43%|████▎     | 63337/147540 [00:35<00:41, 2033.49it/s]tokenizing...:  43%|████▎     | 63546/147540 [00:35<00:41, 2019.93it/s]tokenizing...:  43%|████▎     | 63749/147540 [00:35<00:43, 1947.87it/s]tokenizing...:  43%|████▎     | 63980/147540 [00:36<00:40, 2049.30it/s]tokenizing...:  44%|████▎     | 64223/147540 [00:36<00:38, 2157.32it/s]tokenizing...:  44%|████▎     | 64538/147540 [00:36<00:33, 2446.90it/s]tokenizing...:  44%|████▍     | 64784/147540 [00:36<00:35, 2361.32it/s]tokenizing...:  44%|████▍     | 65041/147540 [00:36<00:34, 2420.30it/s]tokenizing...:  44%|████▍     | 65285/147540 [00:36<00:34, 2374.77it/s]tokenizing...:  44%|████▍     | 65524/147540 [00:36<00:35, 2314.92it/s]tokenizing...:  45%|████▍     | 65815/147540 [00:36<00:32, 2484.45it/s]tokenizing...:  45%|████▍     | 66065/147540 [00:36<00:32, 2477.45it/s]tokenizing...:  45%|████▍     | 66314/147540 [00:36<00:33, 2459.73it/s]tokenizing...:  45%|████▌     | 66561/147540 [00:37<00:33, 2383.65it/s]tokenizing...:  45%|████▌     | 66804/147540 [00:37<00:33, 2393.84it/s]tokenizing...:  45%|████▌     | 67044/147540 [00:37<00:34, 2333.03it/s]tokenizing...:  46%|████▌     | 67278/147540 [00:37<00:34, 2310.33it/s]tokenizing...:  46%|████▌     | 67517/147540 [00:37<00:34, 2331.14it/s]tokenizing...:  46%|████▌     | 67751/147540 [00:37<00:34, 2286.26it/s]tokenizing...:  46%|████▌     | 67980/147540 [00:37<00:38, 2080.31it/s]tokenizing...:  46%|████▌     | 68192/147540 [00:37<00:42, 1864.43it/s]tokenizing...:  46%|████▋     | 68385/147540 [00:38<00:43, 1822.53it/s]tokenizing...:  46%|████▋     | 68571/147540 [00:38<00:43, 1823.94it/s]tokenizing...:  47%|████▋     | 68756/147540 [00:38<00:43, 1791.54it/s]tokenizing...:  47%|████▋     | 68937/147540 [00:38<00:45, 1738.69it/s]tokenizing...:  47%|████▋     | 69139/147540 [00:38<00:43, 1816.14it/s]tokenizing...:  47%|████▋     | 69326/147540 [00:38<00:42, 1830.34it/s]tokenizing...:  47%|████▋     | 69511/147540 [00:38<00:43, 1804.06it/s]tokenizing...:  47%|████▋     | 69693/147540 [00:38<00:43, 1773.47it/s]tokenizing...:  47%|████▋     | 69871/147540 [00:38<00:44, 1755.98it/s]tokenizing...:  47%|████▋     | 70047/147540 [00:38<00:44, 1725.54it/s]tokenizing...:  48%|████▊     | 70222/147540 [00:39<00:44, 1730.46it/s]tokenizing...:  48%|████▊     | 70432/147540 [00:39<00:41, 1837.03it/s]tokenizing...:  48%|████▊     | 70617/147540 [00:39<00:42, 1813.95it/s]tokenizing...:  48%|████▊     | 70799/147540 [00:39<00:42, 1790.40it/s]tokenizing...:  48%|████▊     | 70979/147540 [00:39<00:43, 1774.91it/s]tokenizing...:  48%|████▊     | 71157/147540 [00:39<00:44, 1717.67it/s]tokenizing...:  48%|████▊     | 71331/147540 [00:39<00:44, 1723.02it/s]tokenizing...:  48%|████▊     | 71506/147540 [00:39<00:43, 1730.71it/s]tokenizing...:  49%|████▊     | 71681/147540 [00:39<00:43, 1734.67it/s]tokenizing...:  49%|████▊     | 71855/147540 [00:39<00:43, 1734.74it/s]tokenizing...:  49%|████▉     | 72029/147540 [00:40<00:44, 1713.31it/s]tokenizing...:  49%|████▉     | 72201/147540 [00:40<00:44, 1680.54it/s]tokenizing...:  49%|████▉     | 72388/147540 [00:40<00:43, 1732.81it/s]tokenizing...:  49%|████▉     | 72562/147540 [00:40<00:43, 1733.76it/s]tokenizing...:  49%|████▉     | 72737/147540 [00:40<00:43, 1735.68it/s]tokenizing...:  49%|████▉     | 72913/147540 [00:40<00:42, 1740.69it/s]tokenizing...:  50%|████▉     | 73088/147540 [00:40<00:42, 1735.71it/s]tokenizing...:  50%|████▉     | 73262/147540 [00:42<04:14, 291.37it/s] tokenizing...:  50%|████▉     | 73437/147540 [00:42<03:10, 388.31it/s]tokenizing...:  50%|████▉     | 73610/147540 [00:42<02:26, 504.63it/s]tokenizing...:  50%|█████     | 73784/147540 [00:42<01:55, 640.30it/s]tokenizing...:  50%|█████     | 73955/147540 [00:42<01:33, 784.96it/s]tokenizing...:  50%|█████     | 74129/147540 [00:42<01:18, 939.80it/s]tokenizing...:  50%|█████     | 74300/147540 [00:43<01:07, 1084.34it/s]tokenizing...:  50%|█████     | 74483/147540 [00:43<00:58, 1240.81it/s]tokenizing...:  51%|█████     | 74654/147540 [00:43<00:54, 1349.66it/s]tokenizing...:  51%|█████     | 74839/147540 [00:43<00:49, 1473.29it/s]tokenizing...:  51%|█████     | 75020/147540 [00:43<00:46, 1560.75it/s]tokenizing...:  51%|█████     | 75201/147540 [00:43<00:44, 1628.05it/s]tokenizing...:  51%|█████     | 75391/147540 [00:43<00:42, 1701.44it/s]tokenizing...:  51%|█████     | 75594/147540 [00:43<00:40, 1794.08it/s]tokenizing...:  51%|█████▏    | 75820/147540 [00:43<00:37, 1926.12it/s]tokenizing...:  52%|█████▏    | 76055/147540 [00:43<00:34, 2045.93it/s]tokenizing...:  52%|█████▏    | 76280/147540 [00:44<00:33, 2099.87it/s]tokenizing...:  52%|█████▏    | 76494/147540 [00:44<00:39, 1782.33it/s]tokenizing...:  52%|█████▏    | 76698/147540 [00:44<00:38, 1849.93it/s]tokenizing...:  52%|█████▏    | 76930/147540 [00:44<00:35, 1977.77it/s]tokenizing...:  52%|█████▏    | 77165/147540 [00:44<00:33, 2080.29it/s]tokenizing...:  52%|█████▏    | 77384/147540 [00:44<00:33, 2110.63it/s]tokenizing...:  53%|█████▎    | 77600/147540 [00:44<00:39, 1789.33it/s]tokenizing...:  53%|█████▎    | 77834/147540 [00:44<00:36, 1929.34it/s]tokenizing...:  53%|█████▎    | 78079/147540 [00:45<00:33, 2066.64it/s]tokenizing...:  53%|█████▎    | 78295/147540 [00:45<00:37, 1857.53it/s]tokenizing...:  53%|█████▎    | 78491/147540 [00:45<00:36, 1871.64it/s]tokenizing...:  53%|█████▎    | 78695/147540 [00:45<00:35, 1916.48it/s]tokenizing...:  53%|█████▎    | 78921/147540 [00:45<00:34, 2008.88it/s]tokenizing...:  54%|█████▎    | 79138/147540 [00:45<00:33, 2053.84it/s]tokenizing...:  54%|█████▍    | 79347/147540 [00:45<00:35, 1912.27it/s]tokenizing...:  54%|█████▍    | 79543/147540 [00:45<00:38, 1745.08it/s]tokenizing...:  54%|█████▍    | 79723/147540 [00:45<00:42, 1612.52it/s]tokenizing...:  54%|█████▍    | 79962/147540 [00:46<00:37, 1811.23it/s]tokenizing...:  54%|█████▍    | 80157/147540 [00:46<00:36, 1846.58it/s]tokenizing...:  54%|█████▍    | 80368/147540 [00:46<00:35, 1919.00it/s]tokenizing...:  55%|█████▍    | 80564/147540 [00:46<00:36, 1842.58it/s]tokenizing...:  55%|█████▍    | 80810/147540 [00:46<00:33, 2014.23it/s]tokenizing...:  55%|█████▍    | 81069/147540 [00:46<00:30, 2176.84it/s]tokenizing...:  55%|█████▌    | 81308/147540 [00:46<00:29, 2230.50it/s]tokenizing...:  55%|█████▌    | 81534/147540 [00:46<00:34, 1915.08it/s]tokenizing...:  55%|█████▌    | 81735/147540 [00:46<00:34, 1908.50it/s]tokenizing...:  56%|█████▌    | 81967/147540 [00:47<00:32, 2016.08it/s]tokenizing...:  56%|█████▌    | 82175/147540 [00:47<00:32, 1982.34it/s]tokenizing...:  56%|█████▌    | 82378/147540 [00:47<00:33, 1921.35it/s]tokenizing...:  56%|█████▌    | 82573/147540 [00:47<00:35, 1840.52it/s]tokenizing...:  56%|█████▌    | 82852/147540 [00:47<00:30, 2100.28it/s]tokenizing...:  56%|█████▋    | 83088/147540 [00:47<00:29, 2171.33it/s]tokenizing...:  56%|█████▋    | 83329/147540 [00:47<00:28, 2239.52it/s]tokenizing...:  57%|█████▋    | 83601/147540 [00:47<00:26, 2378.68it/s]tokenizing...:  57%|█████▋    | 83881/147540 [00:47<00:25, 2501.23it/s]tokenizing...:  57%|█████▋    | 84156/147540 [00:47<00:24, 2572.57it/s]tokenizing...:  57%|█████▋    | 84415/147540 [00:48<00:24, 2543.54it/s]tokenizing...:  57%|█████▋    | 84671/147540 [00:48<00:25, 2469.40it/s]tokenizing...:  58%|█████▊    | 84920/147540 [00:48<00:25, 2419.31it/s]tokenizing...:  58%|█████▊    | 85163/147540 [00:48<00:26, 2364.48it/s]tokenizing...:  58%|█████▊    | 85401/147540 [00:48<00:26, 2345.28it/s]tokenizing...:  58%|█████▊    | 85636/147540 [00:48<00:26, 2340.08it/s]tokenizing...:  58%|█████▊    | 85871/147540 [00:48<00:26, 2294.54it/s]tokenizing...:  58%|█████▊    | 86101/147540 [00:48<00:26, 2294.24it/s]tokenizing...:  59%|█████▊    | 86331/147540 [00:48<00:27, 2259.88it/s]tokenizing...:  59%|█████▊    | 86565/147540 [00:49<00:26, 2280.30it/s]tokenizing...:  59%|█████▉    | 86794/147540 [00:49<00:27, 2214.74it/s]tokenizing...:  59%|█████▉    | 87035/147540 [00:49<00:26, 2269.85it/s]tokenizing...:  59%|█████▉    | 87263/147540 [00:49<00:27, 2221.50it/s]tokenizing...:  59%|█████▉    | 87486/147540 [00:49<00:27, 2184.48it/s]tokenizing...:  59%|█████▉    | 87705/147540 [00:49<00:27, 2150.66it/s]tokenizing...:  60%|█████▉    | 87921/147540 [00:49<00:28, 2058.79it/s]tokenizing...:  60%|█████▉    | 88128/147540 [00:49<00:29, 2043.87it/s]tokenizing...:  60%|█████▉    | 88348/147540 [00:49<00:28, 2085.65it/s]tokenizing...:  60%|██████    | 88558/147540 [00:49<00:29, 2003.62it/s]tokenizing...:  60%|██████    | 88760/147540 [00:50<00:30, 1946.56it/s]tokenizing...:  60%|██████    | 88968/147540 [00:50<00:29, 1983.76it/s]tokenizing...:  60%|██████    | 89169/147540 [00:50<00:29, 1989.81it/s]tokenizing...:  61%|██████    | 89382/147540 [00:50<00:28, 2027.94it/s]tokenizing...:  61%|██████    | 89610/147540 [00:50<00:27, 2099.90it/s]tokenizing...:  61%|██████    | 89843/147540 [00:50<00:26, 2165.25it/s]tokenizing...:  61%|██████    | 90060/147540 [00:50<00:26, 2138.10it/s]tokenizing...:  61%|██████    | 90275/147540 [00:50<00:26, 2131.90it/s]tokenizing...:  61%|██████▏   | 90489/147540 [00:50<00:30, 1895.44it/s]tokenizing...:  61%|██████▏   | 90684/147540 [00:51<00:31, 1789.56it/s]tokenizing...:  62%|██████▏   | 90867/147540 [00:51<00:35, 1611.54it/s]tokenizing...:  62%|██████▏   | 91034/147540 [00:51<00:35, 1587.07it/s]tokenizing...:  62%|██████▏   | 91208/147540 [00:51<00:34, 1626.73it/s]tokenizing...:  62%|██████▏   | 91449/147540 [00:51<00:30, 1836.68it/s]tokenizing...:  62%|██████▏   | 91655/147540 [00:51<00:29, 1898.92it/s]tokenizing...:  62%|██████▏   | 91868/147540 [00:51<00:28, 1964.05it/s]tokenizing...:  62%|██████▏   | 92087/147540 [00:51<00:27, 2029.14it/s]tokenizing...:  63%|██████▎   | 92327/147540 [00:51<00:25, 2136.56it/s]tokenizing...:  63%|██████▎   | 92551/147540 [00:52<00:25, 2165.98it/s]tokenizing...:  63%|██████▎   | 92774/147540 [00:52<00:25, 2184.90it/s]tokenizing...:  63%|██████▎   | 93015/147540 [00:52<00:24, 2218.32it/s]tokenizing...:  63%|██████▎   | 93256/147540 [00:52<00:23, 2273.30it/s]tokenizing...:  63%|██████▎   | 93492/147540 [00:52<00:23, 2295.64it/s]tokenizing...:  64%|██████▎   | 93724/147540 [00:52<00:23, 2301.70it/s]tokenizing...:  64%|██████▎   | 93955/147540 [00:52<00:23, 2283.26it/s]tokenizing...:  64%|██████▍   | 94184/147540 [00:52<00:23, 2274.07it/s]tokenizing...:  64%|██████▍   | 94412/147540 [00:52<00:23, 2241.47it/s]tokenizing...:  64%|██████▍   | 94649/147540 [00:52<00:23, 2276.01it/s]tokenizing...:  64%|██████▍   | 94886/147540 [00:53<00:22, 2298.84it/s]tokenizing...:  64%|██████▍   | 95117/147540 [00:53<00:23, 2210.87it/s]tokenizing...:  65%|██████▍   | 95364/147540 [00:53<00:22, 2281.58it/s]tokenizing...:  65%|██████▍   | 95593/147540 [00:53<00:22, 2281.81it/s]tokenizing...:  65%|██████▍   | 95822/147540 [00:53<00:25, 1990.32it/s]tokenizing...:  65%|██████▌   | 96065/147540 [00:53<00:24, 2107.49it/s]tokenizing...:  65%|██████▌   | 96291/147540 [00:53<00:23, 2145.92it/s]tokenizing...:  65%|██████▌   | 96512/147540 [00:53<00:23, 2163.66it/s]tokenizing...:  66%|██████▌   | 96732/147540 [00:53<00:23, 2137.43it/s]tokenizing...:  66%|██████▌   | 96949/147540 [00:54<00:24, 2041.89it/s]tokenizing...:  66%|██████▌   | 97156/147540 [00:54<00:25, 1968.10it/s]tokenizing...:  66%|██████▌   | 97355/147540 [00:54<00:26, 1876.44it/s]tokenizing...:  66%|██████▌   | 97545/147540 [00:54<00:26, 1860.79it/s]tokenizing...:  66%|██████▌   | 97733/147540 [00:54<00:26, 1852.13it/s]tokenizing...:  66%|██████▋   | 97919/147540 [00:54<00:27, 1837.67it/s]tokenizing...:  66%|██████▋   | 98104/147540 [00:54<00:27, 1830.62it/s]tokenizing...:  67%|██████▋   | 98288/147540 [00:54<00:27, 1783.11it/s]tokenizing...:  67%|██████▋   | 98471/147540 [00:54<00:27, 1796.11it/s]tokenizing...:  67%|██████▋   | 98652/147540 [00:54<00:27, 1796.18it/s]tokenizing...:  67%|██████▋   | 98834/147540 [00:55<00:27, 1800.66it/s]tokenizing...:  67%|██████▋   | 99018/147540 [00:55<00:26, 1806.04it/s]tokenizing...:  67%|██████▋   | 99201/147540 [00:55<00:26, 1809.09it/s]tokenizing...:  67%|██████▋   | 99382/147540 [00:55<00:27, 1770.90it/s]tokenizing...:  67%|██████▋   | 99564/147540 [00:55<00:26, 1783.84it/s]tokenizing...:  68%|██████▊   | 99747/147540 [00:55<00:26, 1794.29it/s]tokenizing...:  68%|██████▊   | 99930/147540 [00:55<00:26, 1800.52it/s]tokenizing...:  68%|██████▊   | 100113/147540 [00:55<00:26, 1806.40it/s]tokenizing...:  68%|██████▊   | 100294/147540 [00:55<00:26, 1805.31it/s]tokenizing...:  68%|██████▊   | 100487/147540 [00:56<00:25, 1841.05it/s]tokenizing...:  68%|██████▊   | 100724/147540 [00:56<00:23, 1998.46it/s]tokenizing...:  68%|██████▊   | 100924/147540 [00:56<00:27, 1709.39it/s]tokenizing...:  69%|██████▊   | 101129/147540 [00:56<00:25, 1799.69it/s]tokenizing...:  69%|██████▊   | 101367/147540 [00:56<00:23, 1949.68it/s]tokenizing...:  69%|██████▉   | 101575/147540 [00:56<00:23, 1985.20it/s]tokenizing...:  69%|██████▉   | 101803/147540 [00:56<00:22, 2069.71it/s]tokenizing...:  69%|██████▉   | 102014/147540 [00:56<00:22, 2030.47it/s]tokenizing...:  69%|██████▉   | 102250/147540 [00:56<00:21, 2124.55it/s]tokenizing...:  69%|██████▉   | 102465/147540 [00:56<00:21, 2084.66it/s]tokenizing...:  70%|██████▉   | 102696/147540 [00:57<00:20, 2149.49it/s]tokenizing...:  70%|██████▉   | 102934/147540 [00:57<00:20, 2216.40it/s]tokenizing...:  70%|██████▉   | 103166/147540 [00:57<00:19, 2244.91it/s]tokenizing...:  70%|███████   | 103392/147540 [00:57<00:19, 2224.59it/s]tokenizing...:  70%|███████   | 103637/147540 [00:57<00:19, 2285.06it/s]tokenizing...:  70%|███████   | 103866/147540 [00:57<00:19, 2198.73it/s]tokenizing...:  71%|███████   | 104109/147540 [00:57<00:19, 2264.65it/s]tokenizing...:  71%|███████   | 104377/147540 [00:57<00:18, 2385.63it/s]tokenizing...:  71%|███████   | 104617/147540 [00:57<00:18, 2377.72it/s]tokenizing...:  71%|███████   | 104856/147540 [00:58<00:18, 2345.60it/s]tokenizing...:  71%|███████   | 105106/147540 [00:58<00:17, 2388.18it/s]tokenizing...:  71%|███████▏  | 105377/147540 [00:58<00:16, 2480.72it/s]tokenizing...:  72%|███████▏  | 105646/147540 [00:58<00:16, 2539.23it/s]tokenizing...:  72%|███████▏  | 105964/147540 [00:58<00:15, 2727.46it/s]tokenizing...:  72%|███████▏  | 106238/147540 [00:58<00:15, 2680.85it/s]tokenizing...:  72%|███████▏  | 106507/147540 [00:58<00:16, 2446.46it/s]tokenizing...:  72%|███████▏  | 106756/147540 [00:58<00:17, 2358.33it/s]tokenizing...:  73%|███████▎  | 106995/147540 [00:58<00:17, 2343.38it/s]tokenizing...:  73%|███████▎  | 107232/147540 [00:58<00:17, 2332.48it/s]tokenizing...:  73%|███████▎  | 107481/147540 [00:59<00:16, 2374.89it/s]tokenizing...:  73%|███████▎  | 107720/147540 [00:59<00:17, 2315.70it/s]tokenizing...:  73%|███████▎  | 107953/147540 [00:59<00:17, 2307.43it/s]tokenizing...:  73%|███████▎  | 108185/147540 [00:59<00:18, 2107.75it/s]tokenizing...:  73%|███████▎  | 108400/147540 [00:59<00:19, 2038.89it/s]tokenizing...:  74%|███████▎  | 108617/147540 [00:59<00:18, 2073.40it/s]tokenizing...:  74%|███████▍  | 108827/147540 [00:59<00:19, 2010.29it/s]tokenizing...:  74%|███████▍  | 109049/147540 [00:59<00:18, 2063.88it/s]tokenizing...:  74%|███████▍  | 109257/147540 [00:59<00:19, 1995.25it/s]tokenizing...:  74%|███████▍  | 109458/147540 [01:00<00:21, 1779.18it/s]tokenizing...:  74%|███████▍  | 109641/147540 [01:00<00:21, 1767.43it/s]tokenizing...:  74%|███████▍  | 109821/147540 [01:02<02:28, 254.12it/s] tokenizing...:  75%|███████▍  | 110003/147540 [01:02<01:51, 336.22it/s]tokenizing...:  75%|███████▍  | 110180/147540 [01:02<01:25, 436.02it/s]tokenizing...:  75%|███████▍  | 110408/147540 [01:02<01:01, 602.64it/s]tokenizing...:  75%|███████▍  | 110606/147540 [01:02<00:48, 760.19it/s]tokenizing...:  75%|███████▌  | 110789/147540 [01:03<00:40, 909.88it/s]tokenizing...:  75%|███████▌  | 110972/147540 [01:03<00:34, 1045.80it/s]tokenizing...:  75%|███████▌  | 111154/147540 [01:03<00:30, 1192.68it/s]tokenizing...:  75%|███████▌  | 111336/147540 [01:03<00:27, 1324.71it/s]tokenizing...:  76%|███████▌  | 111558/147540 [01:03<00:23, 1532.08it/s]tokenizing...:  76%|███████▌  | 111750/147540 [01:03<00:22, 1600.55it/s]tokenizing...:  76%|███████▌  | 111939/147540 [01:03<00:21, 1629.75it/s]tokenizing...:  76%|███████▌  | 112122/147540 [01:03<00:21, 1676.19it/s]tokenizing...:  76%|███████▌  | 112305/147540 [01:03<00:20, 1710.76it/s]tokenizing...:  76%|███████▌  | 112487/147540 [01:03<00:20, 1722.98it/s]tokenizing...:  76%|███████▋  | 112673/147540 [01:04<00:19, 1757.63it/s]tokenizing...:  76%|███████▋  | 112855/147540 [01:04<00:19, 1771.31it/s]tokenizing...:  77%|███████▋  | 113036/147540 [01:04<00:19, 1759.40it/s]tokenizing...:  77%|███████▋  | 113280/147540 [01:04<00:17, 1957.18it/s]tokenizing...:  77%|███████▋  | 113491/147540 [01:04<00:17, 2001.26it/s]tokenizing...:  77%|███████▋  | 113706/147540 [01:04<00:16, 2045.09it/s]tokenizing...:  77%|███████▋  | 113912/147540 [01:04<00:17, 1971.59it/s]tokenizing...:  77%|███████▋  | 114111/147540 [01:04<00:17, 1952.54it/s]tokenizing...:  77%|███████▋  | 114308/147540 [01:04<00:17, 1916.72it/s]tokenizing...:  78%|███████▊  | 114501/147540 [01:04<00:17, 1886.74it/s]tokenizing...:  78%|███████▊  | 114691/147540 [01:05<00:17, 1865.14it/s]tokenizing...:  78%|███████▊  | 114878/147540 [01:05<00:17, 1843.34it/s]tokenizing...:  78%|███████▊  | 115063/147540 [01:05<00:18, 1787.01it/s]tokenizing...:  78%|███████▊  | 115261/147540 [01:05<00:17, 1838.14it/s]tokenizing...:  78%|███████▊  | 115446/147540 [01:05<00:17, 1840.03it/s]tokenizing...:  78%|███████▊  | 115631/147540 [01:05<00:17, 1824.95it/s]tokenizing...:  78%|███████▊  | 115814/147540 [01:05<00:17, 1821.73it/s]tokenizing...:  79%|███████▊  | 115999/147540 [01:05<00:17, 1826.44it/s]tokenizing...:  79%|███████▊  | 116182/147540 [01:05<00:17, 1776.69it/s]tokenizing...:  79%|███████▉  | 116360/147540 [01:06<00:17, 1773.95it/s]tokenizing...:  79%|███████▉  | 116544/147540 [01:06<00:17, 1790.78it/s]tokenizing...:  79%|███████▉  | 116726/147540 [01:06<00:17, 1798.33it/s]tokenizing...:  79%|███████▉  | 116907/147540 [01:06<00:17, 1801.42it/s]tokenizing...:  79%|███████▉  | 117088/147540 [01:06<00:16, 1796.17it/s]tokenizing...:  79%|███████▉  | 117268/147540 [01:06<00:17, 1763.67it/s]tokenizing...:  80%|███████▉  | 117450/147540 [01:06<00:16, 1779.78it/s]tokenizing...:  80%|███████▉  | 117634/147540 [01:06<00:16, 1794.96it/s]tokenizing...:  80%|███████▉  | 117836/147540 [01:06<00:15, 1861.28it/s]tokenizing...:  80%|████████  | 118032/147540 [01:06<00:15, 1887.00it/s]tokenizing...:  80%|████████  | 118221/147540 [01:07<00:19, 1520.21it/s]tokenizing...:  80%|████████  | 118385/147540 [01:07<00:18, 1549.45it/s]tokenizing...:  80%|████████  | 118549/147540 [01:07<00:18, 1572.32it/s]tokenizing...:  81%|████████  | 118796/147540 [01:07<00:15, 1819.20it/s]tokenizing...:  81%|████████  | 119023/147540 [01:07<00:14, 1944.85it/s]tokenizing...:  81%|████████  | 119223/147540 [01:07<00:14, 1958.76it/s]tokenizing...:  81%|████████  | 119423/147540 [01:07<00:18, 1518.76it/s]tokenizing...:  81%|████████  | 119609/147540 [01:07<00:17, 1600.21it/s]tokenizing...:  81%|████████  | 119849/147540 [01:08<00:15, 1804.80it/s]tokenizing...:  81%|████████▏ | 120043/147540 [01:08<00:15, 1802.11it/s]tokenizing...:  81%|████████▏ | 120244/147540 [01:08<00:14, 1856.23it/s]tokenizing...:  82%|████████▏ | 120474/147540 [01:08<00:13, 1978.87it/s]tokenizing...:  82%|████████▏ | 120698/147540 [01:08<00:13, 2051.86it/s]tokenizing...:  82%|████████▏ | 120942/147540 [01:08<00:12, 2161.84it/s]tokenizing...:  82%|████████▏ | 121197/147540 [01:08<00:11, 2274.15it/s]tokenizing...:  82%|████████▏ | 121428/147540 [01:08<00:11, 2240.45it/s]tokenizing...:  82%|████████▏ | 121668/147540 [01:08<00:11, 2273.68it/s]tokenizing...:  83%|████████▎ | 121902/147540 [01:08<00:11, 2288.80it/s]tokenizing...:  83%|████████▎ | 122140/147540 [01:09<00:10, 2315.46it/s]tokenizing...:  83%|████████▎ | 122373/147540 [01:09<00:11, 2179.77it/s]tokenizing...:  83%|████████▎ | 122594/147540 [01:09<00:11, 2186.68it/s]tokenizing...:  83%|████████▎ | 122870/147540 [01:09<00:10, 2351.51it/s]tokenizing...:  83%|████████▎ | 123164/147540 [01:09<00:09, 2522.04it/s]tokenizing...:  84%|████████▎ | 123418/147540 [01:09<00:09, 2460.71it/s]tokenizing...:  84%|████████▍ | 123687/147540 [01:09<00:09, 2526.50it/s]tokenizing...:  84%|████████▍ | 123958/147540 [01:09<00:09, 2576.81it/s]tokenizing...:  84%|████████▍ | 124259/147540 [01:09<00:08, 2703.42it/s]tokenizing...:  84%|████████▍ | 124558/147540 [01:09<00:08, 2785.98it/s]tokenizing...:  85%|████████▍ | 124838/147540 [01:10<00:08, 2664.74it/s]tokenizing...:  85%|████████▍ | 125107/147540 [01:10<00:08, 2523.47it/s]tokenizing...:  85%|████████▍ | 125362/147540 [01:10<00:09, 2455.31it/s]tokenizing...:  85%|████████▌ | 125610/147540 [01:10<00:09, 2387.59it/s]tokenizing...:  85%|████████▌ | 125854/147540 [01:10<00:09, 2400.56it/s]tokenizing...:  85%|████████▌ | 126095/147540 [01:10<00:08, 2387.04it/s]tokenizing...:  86%|████████▌ | 126347/147540 [01:10<00:08, 2423.96it/s]tokenizing...:  86%|████████▌ | 126590/147540 [01:10<00:08, 2345.05it/s]tokenizing...:  86%|████████▌ | 126826/147540 [01:10<00:08, 2333.36it/s]tokenizing...:  86%|████████▌ | 127060/147540 [01:11<00:08, 2286.68it/s]tokenizing...:  86%|████████▋ | 127290/147540 [01:11<00:09, 2233.79it/s]tokenizing...:  86%|████████▋ | 127514/147540 [01:11<00:09, 2127.75it/s]tokenizing...:  87%|████████▋ | 127728/147540 [01:11<00:09, 2101.38it/s]tokenizing...:  87%|████████▋ | 127939/147540 [01:11<00:09, 2033.62it/s]tokenizing...:  87%|████████▋ | 128151/147540 [01:11<00:09, 2056.94it/s]tokenizing...:  87%|████████▋ | 128358/147540 [01:11<00:09, 1981.39it/s]tokenizing...:  87%|████████▋ | 128557/147540 [01:11<00:09, 1961.94it/s]tokenizing...:  87%|████████▋ | 128758/147540 [01:11<00:09, 1972.66it/s]tokenizing...:  87%|████████▋ | 128956/147540 [01:12<00:09, 1920.92it/s]tokenizing...:  88%|████████▊ | 129149/147540 [01:12<00:09, 1875.56it/s]tokenizing...:  88%|████████▊ | 129337/147540 [01:12<00:09, 1857.67it/s]tokenizing...:  88%|████████▊ | 129523/147540 [01:12<00:09, 1847.74it/s]tokenizing...:  88%|████████▊ | 129708/147540 [01:12<00:09, 1836.86it/s]tokenizing...:  88%|████████▊ | 129892/147540 [01:12<00:09, 1828.87it/s]tokenizing...:  88%|████████▊ | 130075/147540 [01:12<00:09, 1824.40it/s]tokenizing...:  88%|████████▊ | 130258/147540 [01:12<00:09, 1811.21it/s]tokenizing...:  88%|████████▊ | 130440/147540 [01:12<00:09, 1805.63it/s]tokenizing...:  89%|████████▊ | 130621/147540 [01:12<00:09, 1794.74it/s]tokenizing...:  89%|████████▊ | 130801/147540 [01:13<00:09, 1765.47it/s]tokenizing...:  89%|████████▉ | 130980/147540 [01:13<00:09, 1770.56it/s]tokenizing...:  89%|████████▉ | 131164/147540 [01:13<00:09, 1787.70it/s]tokenizing...:  89%|████████▉ | 131343/147540 [01:13<00:09, 1739.12it/s]tokenizing...:  89%|████████▉ | 131552/147540 [01:13<00:08, 1840.65it/s]tokenizing...:  89%|████████▉ | 131758/147540 [01:13<00:08, 1903.04it/s]tokenizing...:  89%|████████▉ | 131966/147540 [01:13<00:07, 1954.81it/s]tokenizing...:  90%|████████▉ | 132162/147540 [01:13<00:08, 1769.47it/s]tokenizing...:  90%|████████▉ | 132343/147540 [01:13<00:08, 1696.96it/s]tokenizing...:  90%|████████▉ | 132516/147540 [01:14<00:08, 1704.45it/s]tokenizing...:  90%|████████▉ | 132690/147540 [01:14<00:08, 1713.34it/s]tokenizing...:  90%|█████████ | 132863/147540 [01:14<00:08, 1710.94it/s]tokenizing...:  90%|█████████ | 133038/147540 [01:14<00:08, 1721.13it/s]tokenizing...:  90%|█████████ | 133211/147540 [01:14<00:08, 1722.60it/s]tokenizing...:  90%|█████████ | 133385/147540 [01:14<00:08, 1727.30it/s]tokenizing...:  91%|█████████ | 133561/147540 [01:14<00:08, 1734.47it/s]tokenizing...:  91%|█████████ | 133736/147540 [01:14<00:07, 1735.47it/s]tokenizing...:  91%|█████████ | 133910/147540 [01:14<00:08, 1693.89it/s]tokenizing...:  91%|█████████ | 134087/147540 [01:14<00:07, 1715.05it/s]tokenizing...:  91%|█████████ | 134267/147540 [01:15<00:07, 1737.55it/s]tokenizing...:  91%|█████████ | 134449/147540 [01:15<00:07, 1760.76it/s]tokenizing...:  91%|█████████ | 134630/147540 [01:15<00:07, 1772.36it/s]tokenizing...:  91%|█████████▏| 134808/147540 [01:15<00:07, 1769.68it/s]tokenizing...:  91%|█████████▏| 134986/147540 [01:15<00:07, 1735.59it/s]tokenizing...:  92%|█████████▏| 135166/147540 [01:15<00:07, 1753.23it/s]tokenizing...:  92%|█████████▏| 135356/147540 [01:15<00:06, 1792.35it/s]tokenizing...:  92%|█████████▏| 135536/147540 [01:15<00:06, 1789.55it/s]tokenizing...:  92%|█████████▏| 135716/147540 [01:15<00:06, 1791.94it/s]tokenizing...:  92%|█████████▏| 135918/147540 [01:15<00:06, 1858.45it/s]tokenizing...:  92%|█████████▏| 136141/147540 [01:16<00:05, 1967.57it/s]tokenizing...:  92%|█████████▏| 136381/147540 [01:16<00:05, 2093.78it/s]tokenizing...:  93%|█████████▎| 136591/147540 [01:16<00:05, 1846.34it/s]tokenizing...:  93%|█████████▎| 136782/147540 [01:16<00:06, 1637.30it/s]tokenizing...:  93%|█████████▎| 136954/147540 [01:16<00:06, 1578.69it/s]tokenizing...:  93%|█████████▎| 137173/147540 [01:16<00:05, 1734.80it/s]tokenizing...:  93%|█████████▎| 137353/147540 [01:16<00:05, 1731.98it/s]tokenizing...:  93%|█████████▎| 137531/147540 [01:16<00:06, 1486.93it/s]tokenizing...:  93%|█████████▎| 137688/147540 [01:17<00:06, 1484.68it/s]tokenizing...:  93%|█████████▎| 137921/147540 [01:17<00:05, 1704.81it/s]tokenizing...:  94%|█████████▎| 138131/147540 [01:17<00:05, 1811.91it/s]tokenizing...:  94%|█████████▍| 138390/147540 [01:17<00:04, 2025.42it/s]tokenizing...:  94%|█████████▍| 138612/147540 [01:17<00:04, 2079.62it/s]tokenizing...:  94%|█████████▍| 138847/147540 [01:17<00:04, 2155.00it/s]tokenizing...:  94%|█████████▍| 139089/147540 [01:17<00:03, 2230.39it/s]tokenizing...:  94%|█████████▍| 139324/147540 [01:17<00:03, 2261.41it/s]tokenizing...:  95%|█████████▍| 139552/147540 [01:17<00:03, 2199.58it/s]tokenizing...:  95%|█████████▍| 139798/147540 [01:17<00:03, 2272.66it/s]tokenizing...:  95%|█████████▍| 140027/147540 [01:18<00:03, 2253.33it/s]tokenizing...:  95%|█████████▌| 140270/147540 [01:18<00:03, 2287.27it/s]tokenizing...:  95%|█████████▌| 140507/147540 [01:18<00:03, 2310.99it/s]tokenizing...:  95%|█████████▌| 140768/147540 [01:18<00:02, 2396.89it/s]tokenizing...:  96%|█████████▌| 141062/147540 [01:18<00:02, 2556.14it/s]tokenizing...:  96%|█████████▌| 141319/147540 [01:18<00:02, 2482.26it/s]tokenizing...:  96%|█████████▌| 141608/147540 [01:18<00:02, 2600.49it/s]tokenizing...:  96%|█████████▌| 141896/147540 [01:18<00:02, 2681.32it/s]tokenizing...:  96%|█████████▋| 142210/147540 [01:18<00:01, 2813.87it/s]tokenizing...:  97%|█████████▋| 142493/147540 [01:18<00:01, 2689.42it/s]tokenizing...:  97%|█████████▋| 142764/147540 [01:19<00:01, 2639.61it/s]tokenizing...:  97%|█████████▋| 143030/147540 [01:19<00:01, 2489.93it/s]tokenizing...:  97%|█████████▋| 143282/147540 [01:19<00:01, 2420.39it/s]tokenizing...:  97%|█████████▋| 143532/147540 [01:19<00:01, 2441.16it/s]tokenizing...:  97%|█████████▋| 143778/147540 [01:19<00:01, 2330.87it/s]tokenizing...:  98%|█████████▊| 144013/147540 [01:19<00:01, 2305.05it/s]tokenizing...:  98%|█████████▊| 144255/147540 [01:19<00:01, 2318.24it/s]tokenizing...:  98%|█████████▊| 144488/147540 [01:19<00:01, 2288.37it/s]tokenizing...:  98%|█████████▊| 144718/147540 [01:19<00:01, 2279.42it/s]tokenizing...:  98%|█████████▊| 144947/147540 [01:20<00:01, 2231.61it/s]tokenizing...:  98%|█████████▊| 145171/147540 [01:20<00:01, 2152.60it/s]tokenizing...:  99%|█████████▊| 145398/147540 [01:20<00:00, 2180.97it/s]tokenizing...:  99%|█████████▊| 145617/147540 [01:20<00:00, 2124.10it/s]tokenizing...:  99%|█████████▉| 145830/147540 [01:20<00:00, 2105.01it/s]tokenizing...:  99%|█████████▉| 146041/147540 [01:20<00:00, 2036.10it/s]tokenizing...:  99%|█████████▉| 146246/147540 [01:20<00:00, 1899.15it/s]tokenizing...:  99%|█████████▉| 146438/147540 [01:20<00:00, 1703.37it/s]tokenizing...:  99%|█████████▉| 146654/147540 [01:20<00:00, 1818.29it/s]tokenizing...: 100%|█████████▉| 146841/147540 [01:21<00:00, 1822.22it/s]tokenizing...: 100%|█████████▉| 147027/147540 [01:21<00:00, 1825.80it/s]tokenizing...: 100%|█████████▉| 147212/147540 [01:21<00:00, 1819.96it/s]tokenizing...: 100%|█████████▉| 147396/147540 [01:21<00:00, 1777.54it/s]tokenizing...: 100%|██████████| 147540/147540 [01:21<00:00, 1811.08it/s]
09/15/2022 02:52:29 - INFO - root -   The nums of the test_dataset features is 147540
Some weights of the model checkpoint at /data/qingyang/data/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 0/1153 [00:00<?, ?it/s]  0%|          | 1/1153 [00:01<33:24,  1.74s/it]  0%|          | 2/1153 [00:06<1:03:33,  3.31s/it]  0%|          | 3/1153 [00:07<45:48,  2.39s/it]    0%|          | 4/1153 [00:08<36:47,  1.92s/it]  0%|          | 5/1153 [00:09<31:39,  1.65s/it]  1%|          | 6/1153 [00:10<27:51,  1.46s/it]  1%|          | 7/1153 [00:12<26:24,  1.38s/it]  1%|          | 8/1153 [00:13<25:35,  1.34s/it]  1%|          | 9/1153 [00:14<24:34,  1.29s/it]  1%|          | 10/1153 [00:15<23:50,  1.25s/it]  1%|          | 11/1153 [00:16<23:23,  1.23s/it]  1%|          | 12/1153 [00:17<21:28,  1.13s/it]  1%|          | 13/1153 [00:19<21:52,  1.15s/it]  1%|          | 14/1153 [00:20<22:23,  1.18s/it]  1%|▏         | 15/1153 [00:21<22:31,  1.19s/it]  1%|▏         | 16/1153 [00:22<22:20,  1.18s/it]  1%|▏         | 17/1153 [00:23<22:15,  1.18s/it]  2%|▏         | 18/1153 [00:27<38:05,  2.01s/it]  2%|▏         | 19/1153 [00:28<33:30,  1.77s/it]  2%|▏         | 20/1153 [00:30<30:17,  1.60s/it]  2%|▏         | 21/1153 [00:31<27:58,  1.48s/it]  2%|▏         | 22/1153 [00:32<26:12,  1.39s/it]  2%|▏         | 23/1153 [00:33<25:18,  1.34s/it]  2%|▏         | 24/1153 [00:34<24:22,  1.30s/it]  2%|▏         | 25/1153 [00:36<23:45,  1.26s/it]  2%|▏         | 26/1153 [00:37<23:22,  1.24s/it]  2%|▏         | 27/1153 [00:38<21:36,  1.15s/it]  2%|▏         | 28/1153 [00:39<21:40,  1.16s/it]  3%|▎         | 29/1153 [00:40<21:44,  1.16s/it]  3%|▎         | 30/1153 [00:41<21:50,  1.17s/it]  3%|▎         | 31/1153 [00:42<21:56,  1.17s/it]  3%|▎         | 32/1153 [00:44<22:03,  1.18s/it]  3%|▎         | 33/1153 [00:45<22:07,  1.19s/it]  3%|▎         | 34/1153 [00:49<36:46,  1.97s/it]  3%|▎         | 35/1153 [00:50<32:23,  1.74s/it]  3%|▎         | 36/1153 [00:51<29:15,  1.57s/it]  3%|▎         | 37/1153 [00:52<27:05,  1.46s/it]  3%|▎         | 38/1153 [00:53<25:36,  1.38s/it]  3%|▎         | 39/1153 [00:55<24:36,  1.33s/it]  3%|▎         | 40/1153 [00:56<22:58,  1.24s/it]  4%|▎         | 41/1153 [00:57<22:00,  1.19s/it]  4%|▎         | 42/1153 [00:58<21:17,  1.15s/it]  4%|▎         | 43/1153 [00:59<20:43,  1.12s/it]  4%|▍         | 44/1153 [01:00<20:16,  1.10s/it]  4%|▍         | 45/1153 [01:01<19:59,  1.08s/it]  4%|▍         | 46/1153 [01:02<19:46,  1.07s/it]  4%|▍         | 47/1153 [01:03<19:46,  1.07s/it]  4%|▍         | 48/1153 [01:04<19:36,  1.06s/it]  4%|▍         | 49/1153 [01:05<19:32,  1.06s/it]  4%|▍         | 50/1153 [01:09<33:38,  1.83s/it]  4%|▍         | 51/1153 [01:10<29:24,  1.60s/it]  5%|▍         | 52/1153 [01:11<26:22,  1.44s/it]  5%|▍         | 53/1153 [01:12<24:14,  1.32s/it]  5%|▍         | 54/1153 [01:13<22:52,  1.25s/it]  5%|▍         | 55/1153 [01:14<21:43,  1.19s/it]  5%|▍         | 56/1153 [01:15<20:50,  1.14s/it]  5%|▍         | 57/1153 [01:16<20:25,  1.12s/it]  5%|▌         | 58/1153 [01:17<20:05,  1.10s/it]  5%|▌         | 59/1153 [01:18<19:54,  1.09s/it]  5%|▌         | 60/1153 [01:19<19:40,  1.08s/it]  5%|▌         | 61/1153 [01:20<19:26,  1.07s/it]  5%|▌         | 62/1153 [01:22<19:37,  1.08s/it]  5%|▌         | 63/1153 [01:23<19:21,  1.07s/it]  6%|▌         | 64/1153 [01:24<19:17,  1.06s/it]  6%|▌         | 65/1153 [01:25<19:16,  1.06s/it]  6%|▌         | 66/1153 [01:26<19:11,  1.06s/it]  6%|▌         | 67/1153 [01:27<20:08,  1.11s/it]  6%|▌         | 68/1153 [01:31<34:35,  1.91s/it]  6%|▌         | 69/1153 [01:32<30:58,  1.71s/it]  6%|▌         | 70/1153 [01:33<25:50,  1.43s/it]  6%|▌         | 71/1153 [01:34<24:43,  1.37s/it]  6%|▌         | 72/1153 [01:35<24:07,  1.34s/it]  6%|▋         | 73/1153 [01:36<23:19,  1.30s/it]  6%|▋         | 74/1153 [01:38<22:11,  1.23s/it]  7%|▋         | 75/1153 [01:39<21:54,  1.22s/it]  7%|▋         | 76/1153 [01:40<21:45,  1.21s/it]  7%|▋         | 77/1153 [01:41<21:57,  1.22s/it]  7%|▋         | 78/1153 [01:42<22:19,  1.25s/it]  7%|▋         | 79/1153 [01:44<22:26,  1.25s/it]  7%|▋         | 80/1153 [01:45<22:07,  1.24s/it]  7%|▋         | 81/1153 [01:46<21:06,  1.18s/it]  7%|▋         | 82/1153 [01:47<21:45,  1.22s/it]  7%|▋         | 83/1153 [01:49<21:34,  1.21s/it]  7%|▋         | 84/1153 [01:50<21:31,  1.21s/it]  7%|▋         | 85/1153 [01:54<37:08,  2.09s/it]  7%|▋         | 86/1153 [01:55<32:52,  1.85s/it]  8%|▊         | 87/1153 [01:56<29:29,  1.66s/it]  8%|▊         | 88/1153 [01:58<26:55,  1.52s/it]  8%|▊         | 89/1153 [01:59<25:10,  1.42s/it]  8%|▊         | 90/1153 [02:00<23:58,  1.35s/it]  8%|▊         | 91/1153 [02:01<23:07,  1.31s/it]  8%|▊         | 92/1153 [02:02<22:34,  1.28s/it]  8%|▊         | 93/1153 [02:04<22:13,  1.26s/it]  8%|▊         | 94/1153 [02:05<21:49,  1.24s/it]  8%|▊         | 95/1153 [02:06<21:33,  1.22s/it]  8%|▊         | 96/1153 [02:07<21:20,  1.21s/it]  8%|▊         | 97/1153 [02:08<21:33,  1.22s/it]  8%|▊         | 98/1153 [02:10<21:30,  1.22s/it]  9%|▊         | 99/1153 [02:11<21:44,  1.24s/it]  9%|▊         | 100/1153 [02:15<36:21,  2.07s/it]  9%|▉         | 101/1153 [02:16<31:47,  1.81s/it]  9%|▉         | 102/1153 [02:17<28:32,  1.63s/it]  9%|▉         | 103/1153 [02:19<26:18,  1.50s/it]  9%|▉         | 104/1153 [02:20<24:46,  1.42s/it]  9%|▉         | 105/1153 [02:21<23:32,  1.35s/it]  9%|▉         | 106/1153 [02:22<22:40,  1.30s/it]  9%|▉         | 107/1153 [02:23<22:04,  1.27s/it]  9%|▉         | 108/1153 [02:24<21:43,  1.25s/it]  9%|▉         | 109/1153 [02:26<20:37,  1.18s/it] 10%|▉         | 110/1153 [02:27<20:38,  1.19s/it] 10%|▉         | 111/1153 [02:28<20:43,  1.19s/it] 10%|▉         | 112/1153 [02:29<20:49,  1.20s/it] 10%|▉         | 113/1153 [02:30<20:54,  1.21s/it] 10%|▉         | 114/1153 [02:32<20:48,  1.20s/it] 10%|▉         | 115/1153 [02:33<20:43,  1.20s/it] 10%|█         | 116/1153 [02:36<33:56,  1.96s/it] 10%|█         | 117/1153 [02:38<29:44,  1.72s/it] 10%|█         | 118/1153 [02:39<26:58,  1.56s/it] 10%|█         | 119/1153 [02:40<25:02,  1.45s/it] 10%|█         | 120/1153 [02:41<23:43,  1.38s/it] 10%|█         | 121/1153 [02:43<23:11,  1.35s/it] 11%|█         | 122/1153 [02:44<21:37,  1.26s/it] 11%|█         | 123/1153 [02:45<21:18,  1.24s/it] 11%|█         | 124/1153 [02:46<21:08,  1.23s/it] 11%|█         | 125/1153 [02:47<20:49,  1.22s/it] 11%|█         | 126/1153 [02:48<20:15,  1.18s/it] 11%|█         | 127/1153 [02:49<19:18,  1.13s/it] 11%|█         | 128/1153 [02:50<19:00,  1.11s/it] 11%|█         | 129/1153 [02:52<19:35,  1.15s/it] 11%|█▏        | 130/1153 [02:53<19:49,  1.16s/it] 11%|█▏        | 131/1153 [02:54<19:58,  1.17s/it] 11%|█▏        | 132/1153 [02:58<33:02,  1.94s/it] 12%|█▏        | 133/1153 [02:59<28:34,  1.68s/it] 12%|█▏        | 134/1153 [03:00<25:25,  1.50s/it] 12%|█▏        | 135/1153 [03:01<23:14,  1.37s/it] 12%|█▏        | 136/1153 [03:02<21:22,  1.26s/it] 12%|█▏        | 137/1153 [03:03<20:31,  1.21s/it] 12%|█▏        | 138/1153 [03:04<19:43,  1.17s/it] 12%|█▏        | 139/1153 [03:05<19:10,  1.13s/it] 12%|█▏        | 140/1153 [03:06<18:45,  1.11s/it] 12%|█▏        | 141/1153 [03:07<18:26,  1.09s/it] 12%|█▏        | 142/1153 [03:08<18:09,  1.08s/it] 12%|█▏        | 143/1153 [03:09<18:44,  1.11s/it] 12%|█▏        | 144/1153 [03:11<19:08,  1.14s/it] 13%|█▎        | 145/1153 [03:12<19:29,  1.16s/it] 13%|█▎        | 146/1153 [03:13<18:56,  1.13s/it] 13%|█▎        | 147/1153 [03:14<18:30,  1.10s/it] 13%|█▎        | 148/1153 [03:15<18:15,  1.09s/it] 13%|█▎        | 149/1153 [03:16<18:05,  1.08s/it] 13%|█▎        | 150/1153 [03:17<18:01,  1.08s/it] 13%|█▎        | 151/1153 [03:21<29:29,  1.77s/it] 13%|█▎        | 152/1153 [03:22<26:09,  1.57s/it] 13%|█▎        | 153/1153 [03:23<23:34,  1.41s/it] 13%|█▎        | 154/1153 [03:24<21:46,  1.31s/it] 13%|█▎        | 155/1153 [03:25<20:34,  1.24s/it] 14%|█▎        | 156/1153 [03:26<20:22,  1.23s/it] 14%|█▎        | 157/1153 [03:27<20:17,  1.22s/it] 14%|█▎        | 158/1153 [03:28<20:15,  1.22s/it] 14%|█▍        | 159/1153 [03:30<20:11,  1.22s/it] 14%|█▍        | 160/1153 [03:31<20:10,  1.22s/it] 14%|█▍        | 161/1153 [03:32<20:10,  1.22s/it] 14%|█▍        | 162/1153 [03:33<19:21,  1.17s/it] 14%|█▍        | 163/1153 [03:34<19:34,  1.19s/it] 14%|█▍        | 164/1153 [03:35<18:56,  1.15s/it] 14%|█▍        | 165/1153 [03:37<18:22,  1.12s/it] 14%|█▍        | 166/1153 [03:38<18:17,  1.11s/it] 14%|█▍        | 167/1153 [03:39<18:06,  1.10s/it] 15%|█▍        | 168/1153 [03:42<29:43,  1.81s/it] 15%|█▍        | 169/1153 [03:43<26:06,  1.59s/it] 15%|█▍        | 170/1153 [03:44<23:33,  1.44s/it] 15%|█▍        | 171/1153 [03:45<21:52,  1.34s/it] 15%|█▍        | 172/1153 [03:46<20:29,  1.25s/it] 15%|█▌        | 173/1153 [03:48<19:37,  1.20s/it] 15%|█▌        | 174/1153 [03:49<18:53,  1.16s/it] 15%|█▌        | 175/1153 [03:50<18:18,  1.12s/it] 15%|█▌        | 176/1153 [03:51<18:00,  1.11s/it] 15%|█▌        | 177/1153 [03:52<17:49,  1.10s/it] 15%|█▌        | 178/1153 [03:53<17:46,  1.09s/it] 16%|█▌        | 179/1153 [03:54<17:50,  1.10s/it] 16%|█▌        | 180/1153 [03:55<17:33,  1.08s/it] 16%|█▌        | 181/1153 [03:56<17:34,  1.08s/it] 16%|█▌        | 182/1153 [03:57<17:25,  1.08s/it] 16%|█▌        | 183/1153 [03:58<17:31,  1.08s/it] 16%|█▌        | 184/1153 [03:59<17:30,  1.08s/it] 16%|█▌        | 185/1153 [04:00<17:22,  1.08s/it] 16%|█▌        | 186/1153 [04:04<29:06,  1.81s/it] 16%|█▌        | 187/1153 [04:05<25:37,  1.59s/it] 16%|█▋        | 188/1153 [04:06<23:03,  1.43s/it] 16%|█▋        | 189/1153 [04:07<21:21,  1.33s/it] 16%|█▋        | 190/1153 [04:08<19:59,  1.25s/it] 17%|█▋        | 191/1153 [04:09<19:51,  1.24s/it] 17%|█▋        | 192/1153 [04:11<19:45,  1.23s/it] 17%|█▋        | 193/1153 [04:12<19:45,  1.23s/it] 17%|█▋        | 194/1153 [04:13<19:47,  1.24s/it] 17%|█▋        | 195/1153 [04:14<19:37,  1.23s/it] 17%|█▋        | 196/1153 [04:16<19:30,  1.22s/it] 17%|█▋        | 197/1153 [04:17<19:26,  1.22s/it] 17%|█▋        | 198/1153 [04:18<19:26,  1.22s/it] 17%|█▋        | 199/1153 [04:19<19:28,  1.22s/it] 17%|█▋        | 200/1153 [04:20<19:30,  1.23s/it] 17%|█▋        | 201/1153 [04:22<19:22,  1.22s/it] 18%|█▊        | 202/1153 [04:23<19:17,  1.22s/it] 18%|█▊        | 203/1153 [04:26<29:44,  1.88s/it] 18%|█▊        | 204/1153 [04:28<26:49,  1.70s/it] 18%|█▊        | 205/1153 [04:29<24:37,  1.56s/it] 18%|█▊        | 206/1153 [04:30<22:55,  1.45s/it] 18%|█▊        | 207/1153 [04:31<21:45,  1.38s/it] 18%|█▊        | 208/1153 [04:32<20:56,  1.33s/it] 18%|█▊        | 209/1153 [04:34<20:24,  1.30s/it] 18%|█▊        | 210/1153 [04:35<18:49,  1.20s/it] 18%|█▊        | 211/1153 [04:36<19:04,  1.21s/it] 18%|█▊        | 212/1153 [04:37<17:01,  1.09s/it] 18%|█▊        | 213/1153 [04:38<17:48,  1.14s/it] 19%|█▊        | 214/1153 [04:39<18:12,  1.16s/it] 19%|█▊        | 215/1153 [04:40<18:34,  1.19s/it] 19%|█▊        | 216/1153 [04:42<18:47,  1.20s/it] 19%|█▉        | 217/1153 [04:43<18:48,  1.21s/it] 19%|█▉        | 218/1153 [04:44<18:28,  1.19s/it] 19%|█▉        | 219/1153 [04:48<30:20,  1.95s/it] 19%|█▉        | 220/1153 [04:49<27:07,  1.74s/it] 19%|█▉        | 221/1153 [04:50<24:36,  1.58s/it] 19%|█▉        | 222/1153 [04:51<22:51,  1.47s/it] 19%|█▉        | 223/1153 [04:53<21:40,  1.40s/it] 19%|█▉        | 224/1153 [04:54<20:54,  1.35s/it] 20%|█▉        | 225/1153 [04:55<20:20,  1.32s/it] 20%|█▉        | 226/1153 [04:56<19:59,  1.29s/it] 20%|█▉        | 227/1153 [04:58<19:34,  1.27s/it] 20%|█▉        | 228/1153 [04:59<19:19,  1.25s/it] 20%|█▉        | 229/1153 [05:00<18:28,  1.20s/it] 20%|█▉        | 230/1153 [05:01<18:32,  1.21s/it] 20%|██        | 231/1153 [05:02<17:54,  1.17s/it] 20%|██        | 232/1153 [05:03<18:16,  1.19s/it] 20%|██        | 233/1153 [05:05<18:23,  1.20s/it] 20%|██        | 234/1153 [05:06<18:27,  1.21s/it] 20%|██        | 235/1153 [05:07<18:32,  1.21s/it] 20%|██        | 236/1153 [05:11<29:45,  1.95s/it] 21%|██        | 237/1153 [05:12<26:38,  1.75s/it] 21%|██        | 238/1153 [05:13<24:11,  1.59s/it] 21%|██        | 239/1153 [05:14<22:28,  1.48s/it] 21%|██        | 240/1153 [05:16<21:18,  1.40s/it] 21%|██        | 241/1153 [05:17<19:56,  1.31s/it] 21%|██        | 242/1153 [05:18<19:29,  1.28s/it] 21%|██        | 243/1153 [05:19<19:10,  1.26s/it] 21%|██        | 244/1153 [05:20<19:02,  1.26s/it] 21%|██        | 245/1153 [05:21<17:38,  1.17s/it] 21%|██▏       | 246/1153 [05:22<17:00,  1.13s/it] 21%|██▏       | 247/1153 [05:24<17:35,  1.16s/it] 22%|██▏       | 248/1153 [05:25<17:59,  1.19s/it] 22%|██▏       | 249/1153 [05:26<18:13,  1.21s/it] 22%|██▏       | 250/1153 [05:27<18:24,  1.22s/it] 22%|██▏       | 251/1153 [05:29<18:23,  1.22s/it] 22%|██▏       | 252/1153 [05:32<28:05,  1.87s/it] 22%|██▏       | 253/1153 [05:33<25:22,  1.69s/it] 22%|██▏       | 254/1153 [05:35<23:21,  1.56s/it] 22%|██▏       | 255/1153 [05:36<21:19,  1.43s/it] 22%|██▏       | 256/1153 [05:37<20:23,  1.36s/it] 22%|██▏       | 257/1153 [05:38<19:43,  1.32s/it] 22%|██▏       | 258/1153 [05:39<18:47,  1.26s/it] 22%|██▏       | 259/1153 [05:40<18:24,  1.24s/it] 23%|██▎       | 260/1153 [05:42<18:23,  1.24s/it] 23%|██▎       | 261/1153 [05:43<18:21,  1.23s/it] 23%|██▎       | 262/1153 [05:44<17:28,  1.18s/it] 23%|██▎       | 263/1153 [05:45<17:05,  1.15s/it] 23%|██▎       | 264/1153 [05:46<16:57,  1.15s/it] 23%|██▎       | 265/1153 [05:47<17:21,  1.17s/it] 23%|██▎       | 266/1153 [05:48<16:54,  1.14s/it] 23%|██▎       | 267/1153 [05:50<16:40,  1.13s/it] 23%|██▎       | 268/1153 [05:51<17:04,  1.16s/it] 23%|██▎       | 269/1153 [05:52<16:44,  1.14s/it] 23%|██▎       | 270/1153 [05:56<27:41,  1.88s/it] 24%|██▎       | 271/1153 [05:57<24:11,  1.65s/it] 24%|██▎       | 272/1153 [05:58<22:18,  1.52s/it] 24%|██▎       | 273/1153 [05:59<20:25,  1.39s/it] 24%|██▍       | 274/1153 [06:00<19:12,  1.31s/it] 24%|██▍       | 275/1153 [06:01<18:08,  1.24s/it] 24%|██▍       | 276/1153 [06:02<17:30,  1.20s/it] 24%|██▍       | 277/1153 [06:03<17:01,  1.17s/it] 24%|██▍       | 278/1153 [06:04<16:47,  1.15s/it] 24%|██▍       | 279/1153 [06:06<16:30,  1.13s/it] 24%|██▍       | 280/1153 [06:07<16:08,  1.11s/it] 24%|██▍       | 281/1153 [06:08<15:56,  1.10s/it] 24%|██▍       | 282/1153 [06:09<15:58,  1.10s/it] 25%|██▍       | 283/1153 [06:10<15:55,  1.10s/it] 25%|██▍       | 284/1153 [06:11<16:31,  1.14s/it] 25%|██▍       | 285/1153 [06:12<16:57,  1.17s/it] 25%|██▍       | 286/1153 [06:14<17:15,  1.19s/it] 25%|██▍       | 287/1153 [06:15<17:29,  1.21s/it] 25%|██▍       | 288/1153 [06:18<27:42,  1.92s/it] 25%|██▌       | 289/1153 [06:20<24:46,  1.72s/it] 25%|██▌       | 290/1153 [06:21<22:46,  1.58s/it] 25%|██▌       | 291/1153 [06:22<20:39,  1.44s/it] 25%|██▌       | 292/1153 [06:23<19:10,  1.34s/it] 25%|██▌       | 293/1153 [06:24<18:07,  1.26s/it] 25%|██▌       | 294/1153 [06:25<17:16,  1.21s/it] 26%|██▌       | 295/1153 [06:26<16:50,  1.18s/it] 26%|██▌       | 296/1153 [06:27<16:26,  1.15s/it] 26%|██▌       | 297/1153 [06:29<16:13,  1.14s/it] 26%|██▌       | 298/1153 [06:30<16:13,  1.14s/it] 26%|██▌       | 299/1153 [06:31<16:14,  1.14s/it] 26%|██▌       | 300/1153 [06:32<16:15,  1.14s/it] 26%|██▌       | 301/1153 [06:33<16:04,  1.13s/it] 26%|██▌       | 302/1153 [06:34<15:49,  1.12s/it] 26%|██▋       | 303/1153 [06:35<15:46,  1.11s/it] 26%|██▋       | 304/1153 [06:36<15:46,  1.11s/it] 26%|██▋       | 305/1153 [06:38<15:48,  1.12s/it] 27%|██▋       | 306/1153 [06:39<15:46,  1.12s/it] 27%|██▋       | 307/1153 [06:42<26:29,  1.88s/it] 27%|██▋       | 308/1153 [06:43<23:21,  1.66s/it] 27%|██▋       | 309/1153 [06:45<21:01,  1.50s/it] 27%|██▋       | 310/1153 [06:46<19:27,  1.38s/it] 27%|██▋       | 311/1153 [06:47<18:49,  1.34s/it] 27%|██▋       | 312/1153 [06:48<17:51,  1.27s/it] 27%|██▋       | 313/1153 [06:49<17:04,  1.22s/it] 27%|██▋       | 314/1153 [06:50<16:41,  1.19s/it] 27%|██▋       | 315/1153 [06:51<16:18,  1.17s/it] 27%|██▋       | 316/1153 [06:53<15:58,  1.15s/it] 27%|██▋       | 317/1153 [06:54<15:48,  1.13s/it] 28%|██▊       | 318/1153 [06:55<15:36,  1.12s/it] 28%|██▊       | 319/1153 [06:56<16:08,  1.16s/it] 28%|██▊       | 320/1153 [06:57<16:30,  1.19s/it] 28%|██▊       | 321/1153 [06:58<16:14,  1.17s/it] 28%|██▊       | 322/1153 [07:00<16:31,  1.19s/it] 28%|██▊       | 323/1153 [07:01<16:43,  1.21s/it] 28%|██▊       | 324/1153 [07:02<16:54,  1.22s/it] 28%|██▊       | 325/1153 [07:06<27:29,  1.99s/it] 28%|██▊       | 326/1153 [07:07<24:26,  1.77s/it] 28%|██▊       | 327/1153 [07:08<22:12,  1.61s/it] 28%|██▊       | 328/1153 [07:10<20:40,  1.50s/it] 29%|██▊       | 329/1153 [07:11<19:36,  1.43s/it] 29%|██▊       | 330/1153 [07:12<18:54,  1.38s/it] 29%|██▊       | 331/1153 [07:13<18:24,  1.34s/it] 29%|██▉       | 332/1153 [07:15<17:57,  1.31s/it] 29%|██▉       | 333/1153 [07:16<17:44,  1.30s/it] 29%|██▉       | 334/1153 [07:17<17:33,  1.29s/it] 29%|██▉       | 335/1153 [07:18<17:27,  1.28s/it] 29%|██▉       | 336/1153 [07:20<17:26,  1.28s/it] 29%|██▉       | 337/1153 [07:21<17:21,  1.28s/it] 29%|██▉       | 338/1153 [07:22<17:10,  1.26s/it] 29%|██▉       | 339/1153 [07:23<17:03,  1.26s/it] 29%|██▉       | 340/1153 [07:25<17:01,  1.26s/it] 30%|██▉       | 341/1153 [07:26<17:00,  1.26s/it] 30%|██▉       | 342/1153 [07:30<26:49,  1.98s/it] 30%|██▉       | 343/1153 [07:31<23:58,  1.78s/it] 30%|██▉       | 344/1153 [07:32<21:46,  1.61s/it] 30%|██▉       | 345/1153 [07:33<20:15,  1.50s/it] 30%|███       | 346/1153 [07:35<19:12,  1.43s/it] 30%|███       | 347/1153 [07:36<18:30,  1.38s/it] 30%|███       | 348/1153 [07:37<18:02,  1.35s/it] 30%|███       | 349/1153 [07:38<17:43,  1.32s/it] 30%|███       | 350/1153 [07:40<17:22,  1.30s/it] 30%|███       | 351/1153 [07:41<17:08,  1.28s/it] 31%|███       | 352/1153 [07:42<16:59,  1.27s/it] 31%|███       | 353/1153 [07:43<16:55,  1.27s/it] 31%|███       | 354/1153 [07:45<16:23,  1.23s/it] 31%|███       | 355/1153 [07:46<16:26,  1.24s/it] 31%|███       | 356/1153 [07:47<16:28,  1.24s/it] 31%|███       | 357/1153 [07:48<16:32,  1.25s/it] 31%|███       | 358/1153 [07:52<26:53,  2.03s/it] 31%|███       | 359/1153 [07:53<23:46,  1.80s/it] 31%|███       | 360/1153 [07:55<21:32,  1.63s/it] 31%|███▏      | 361/1153 [07:56<20:01,  1.52s/it] 31%|███▏      | 362/1153 [07:57<18:58,  1.44s/it] 31%|███▏      | 363/1153 [07:59<18:16,  1.39s/it] 32%|███▏      | 364/1153 [08:00<17:47,  1.35s/it] 32%|███▏      | 365/1153 [08:01<17:04,  1.30s/it] 32%|███▏      | 366/1153 [08:02<16:52,  1.29s/it] 32%|███▏      | 367/1153 [08:03<16:44,  1.28s/it] 32%|███▏      | 368/1153 [08:05<16:39,  1.27s/it] 32%|███▏      | 369/1153 [08:05<14:15,  1.09s/it] 32%|███▏      | 370/1153 [08:07<14:54,  1.14s/it] 32%|███▏      | 371/1153 [08:08<15:15,  1.17s/it] 32%|███▏      | 372/1153 [08:09<15:31,  1.19s/it] 32%|███▏      | 373/1153 [08:10<15:39,  1.20s/it] 32%|███▏      | 374/1153 [08:12<15:52,  1.22s/it] 33%|███▎      | 375/1153 [08:15<25:42,  1.98s/it] 33%|███▎      | 376/1153 [08:17<22:55,  1.77s/it] 33%|███▎      | 377/1153 [08:18<20:54,  1.62s/it] 33%|███▎      | 378/1153 [08:19<19:28,  1.51s/it] 33%|███▎      | 379/1153 [08:20<18:29,  1.43s/it] 33%|███▎      | 380/1153 [08:22<17:49,  1.38s/it] 33%|███▎      | 381/1153 [08:23<17:22,  1.35s/it] 33%|███▎      | 382/1153 [08:24<16:56,  1.32s/it] 33%|███▎      | 383/1153 [08:25<16:39,  1.30s/it] 33%|███▎      | 384/1153 [08:27<16:38,  1.30s/it] 33%|███▎      | 385/1153 [08:28<16:29,  1.29s/it] 33%|███▎      | 386/1153 [08:29<16:26,  1.29s/it] 34%|███▎      | 387/1153 [08:31<16:22,  1.28s/it] 34%|███▎      | 388/1153 [08:32<16:11,  1.27s/it] 34%|███▎      | 389/1153 [08:33<16:04,  1.26s/it] 34%|███▍      | 390/1153 [08:34<16:02,  1.26s/it] 34%|███▍      | 391/1153 [08:36<16:03,  1.26s/it] 34%|███▍      | 392/1153 [08:39<25:58,  2.05s/it] 34%|███▍      | 393/1153 [08:41<23:08,  1.83s/it] 34%|███▍      | 394/1153 [08:42<20:56,  1.66s/it] 34%|███▍      | 395/1153 [08:43<18:38,  1.48s/it] 34%|███▍      | 396/1153 [08:44<17:45,  1.41s/it] 34%|███▍      | 397/1153 [08:46<16:48,  1.33s/it] 35%|███▍      | 398/1153 [08:47<16:31,  1.31s/it] 35%|███▍      | 399/1153 [08:48<16:22,  1.30s/it] 35%|███▍      | 400/1153 [08:49<16:07,  1.28s/it] 35%|███▍      | 401/1153 [08:51<15:56,  1.27s/it] 35%|███▍      | 402/1153 [08:52<15:50,  1.27s/it] 35%|███▍      | 403/1153 [08:53<15:10,  1.21s/it] 35%|███▌      | 404/1153 [08:54<14:48,  1.19s/it] 35%|███▌      | 405/1153 [08:55<14:27,  1.16s/it] 35%|███▌      | 406/1153 [08:56<14:47,  1.19s/it] 35%|███▌      | 407/1153 [08:58<15:01,  1.21s/it] 35%|███▌      | 408/1153 [08:59<14:34,  1.17s/it] 35%|███▌      | 409/1153 [09:00<14:20,  1.16s/it] 36%|███▌      | 410/1153 [09:03<23:26,  1.89s/it] 36%|███▌      | 411/1153 [09:05<21:09,  1.71s/it] 36%|███▌      | 412/1153 [09:06<18:56,  1.53s/it] 36%|███▌      | 413/1153 [09:07<17:15,  1.40s/it] 36%|███▌      | 414/1153 [09:08<16:06,  1.31s/it] 36%|███▌      | 415/1153 [09:09<15:20,  1.25s/it] 36%|███▌      | 416/1153 [09:10<14:45,  1.20s/it] 36%|███▌      | 417/1153 [09:11<14:24,  1.17s/it] 36%|███▋      | 418/1153 [09:12<14:14,  1.16s/it] 36%|███▋      | 419/1153 [09:14<14:40,  1.20s/it] 36%|███▋      | 420/1153 [09:15<14:10,  1.16s/it] 37%|███▋      | 421/1153 [09:16<14:06,  1.16s/it] 37%|███▋      | 422/1153 [09:17<14:00,  1.15s/it] 37%|███▋      | 423/1153 [09:18<14:25,  1.19s/it] 37%|███▋      | 424/1153 [09:19<14:06,  1.16s/it] 37%|███▋      | 425/1153 [09:21<13:55,  1.15s/it] 37%|███▋      | 426/1153 [09:22<13:43,  1.13s/it] 37%|███▋      | 427/1153 [09:23<14:13,  1.18s/it] 37%|███▋      | 428/1153 [09:27<22:55,  1.90s/it] 37%|███▋      | 429/1153 [09:28<20:05,  1.67s/it] 37%|███▋      | 430/1153 [09:29<18:03,  1.50s/it] 37%|███▋      | 431/1153 [09:30<17:17,  1.44s/it] 37%|███▋      | 432/1153 [09:31<16:34,  1.38s/it] 38%|███▊      | 433/1153 [09:33<16:05,  1.34s/it] 38%|███▊      | 434/1153 [09:34<15:29,  1.29s/it] 38%|███▊      | 435/1153 [09:35<15:24,  1.29s/it] 38%|███▊      | 436/1153 [09:36<14:45,  1.23s/it] 38%|███▊      | 437/1153 [09:37<14:16,  1.20s/it] 38%|███▊      | 438/1153 [09:38<13:58,  1.17s/it] 38%|███▊      | 439/1153 [09:40<14:20,  1.21s/it] 38%|███▊      | 440/1153 [09:41<14:03,  1.18s/it] 38%|███▊      | 441/1153 [09:42<14:21,  1.21s/it] 38%|███▊      | 442/1153 [09:43<13:57,  1.18s/it] 38%|███▊      | 443/1153 [09:44<13:36,  1.15s/it] 39%|███▊      | 444/1153 [09:45<13:32,  1.15s/it] 39%|███▊      | 445/1153 [09:47<13:59,  1.19s/it] 39%|███▊      | 446/1153 [09:50<22:46,  1.93s/it] 39%|███▉      | 447/1153 [09:51<19:50,  1.69s/it] 39%|███▉      | 448/1153 [09:53<17:49,  1.52s/it] 39%|███▉      | 449/1153 [09:54<16:22,  1.40s/it] 39%|███▉      | 450/1153 [09:55<15:22,  1.31s/it] 39%|███▉      | 451/1153 [09:56<14:41,  1.26s/it] 39%|███▉      | 452/1153 [09:57<14:10,  1.21s/it] 39%|███▉      | 453/1153 [09:58<14:21,  1.23s/it] 39%|███▉      | 454/1153 [10:00<14:24,  1.24s/it] 39%|███▉      | 455/1153 [10:01<14:02,  1.21s/it] 40%|███▉      | 456/1153 [10:02<13:40,  1.18s/it] 40%|███▉      | 457/1153 [10:03<13:27,  1.16s/it] 40%|███▉      | 458/1153 [10:04<13:17,  1.15s/it] 40%|███▉      | 459/1153 [10:05<13:14,  1.15s/it] 40%|███▉      | 460/1153 [10:06<13:12,  1.14s/it] 40%|███▉      | 461/1153 [10:07<13:04,  1.13s/it] 40%|████      | 462/1153 [10:09<13:03,  1.13s/it] 40%|████      | 463/1153 [10:10<12:59,  1.13s/it] 40%|████      | 464/1153 [10:11<13:12,  1.15s/it] 40%|████      | 465/1153 [10:15<22:00,  1.92s/it] 40%|████      | 466/1153 [10:16<19:48,  1.73s/it] 41%|████      | 467/1153 [10:17<17:16,  1.51s/it] 41%|████      | 468/1153 [10:18<16:22,  1.43s/it] 41%|████      | 469/1153 [10:19<15:46,  1.38s/it] 41%|████      | 470/1153 [10:21<15:21,  1.35s/it] 41%|████      | 471/1153 [10:22<15:07,  1.33s/it] 41%|████      | 472/1153 [10:23<14:22,  1.27s/it] 41%|████      | 473/1153 [10:24<14:02,  1.24s/it] 41%|████      | 474/1153 [10:26<14:12,  1.26s/it] 41%|████      | 475/1153 [10:27<14:15,  1.26s/it] 41%|████▏     | 476/1153 [10:28<14:15,  1.26s/it] 41%|████▏     | 477/1153 [10:29<14:16,  1.27s/it] 41%|████▏     | 478/1153 [10:31<14:16,  1.27s/it] 42%|████▏     | 479/1153 [10:32<14:18,  1.27s/it] 42%|████▏     | 480/1153 [10:33<14:13,  1.27s/it] 42%|████▏     | 481/1153 [10:34<13:54,  1.24s/it] 42%|████▏     | 482/1153 [10:36<14:01,  1.25s/it] 42%|████▏     | 483/1153 [10:40<22:47,  2.04s/it] 42%|████▏     | 484/1153 [10:41<20:13,  1.81s/it] 42%|████▏     | 485/1153 [10:42<17:15,  1.55s/it] 42%|████▏     | 486/1153 [10:43<15:39,  1.41s/it] 42%|████▏     | 487/1153 [10:44<15:08,  1.36s/it] 42%|████▏     | 488/1153 [10:45<14:46,  1.33s/it] 42%|████▏     | 489/1153 [10:47<14:32,  1.31s/it] 42%|████▏     | 490/1153 [10:48<14:24,  1.30s/it] 43%|████▎     | 491/1153 [10:49<14:19,  1.30s/it] 43%|████▎     | 492/1153 [10:50<14:16,  1.30s/it] 43%|████▎     | 493/1153 [10:52<14:08,  1.28s/it] 43%|████▎     | 494/1153 [10:53<13:52,  1.26s/it] 43%|████▎     | 495/1153 [10:54<13:54,  1.27s/it] 43%|████▎     | 496/1153 [10:56<13:56,  1.27s/it] 43%|████▎     | 497/1153 [10:57<13:58,  1.28s/it] 43%|████▎     | 498/1153 [10:58<13:54,  1.27s/it] 43%|████▎     | 499/1153 [10:59<13:50,  1.27s/it] 43%|████▎     | 500/1153 [11:03<22:12,  2.04s/it] 43%|████▎     | 501/1153 [11:04<19:46,  1.82s/it] 44%|████▎     | 502/1153 [11:06<17:30,  1.61s/it] 44%|████▎     | 503/1153 [11:07<16:22,  1.51s/it] 44%|████▎     | 504/1153 [11:08<13:52,  1.28s/it] 44%|████▍     | 505/1153 [11:09<13:47,  1.28s/it] 44%|████▍     | 506/1153 [11:10<13:44,  1.28s/it] 44%|████▍     | 507/1153 [11:11<13:45,  1.28s/it] 44%|████▍     | 508/1153 [11:12<13:02,  1.21s/it] 44%|████▍     | 509/1153 [11:13<10:59,  1.02s/it] 44%|████▍     | 510/1153 [11:14<11:53,  1.11s/it] 44%|████▍     | 511/1153 [11:15<10:37,  1.01it/s] 44%|████▍     | 512/1153 [11:16<11:38,  1.09s/it] 44%|████▍     | 513/1153 [11:17<10:35,  1.01it/s] 45%|████▍     | 514/1153 [11:18<09:48,  1.09it/s] 45%|████▍     | 515/1153 [11:19<10:56,  1.03s/it] 45%|████▍     | 516/1153 [11:20<11:33,  1.09s/it] 45%|████▍     | 517/1153 [11:22<11:40,  1.10s/it] 45%|████▍     | 518/1153 [11:23<12:12,  1.15s/it] 45%|████▌     | 519/1153 [11:27<20:48,  1.97s/it] 45%|████▌     | 520/1153 [11:28<18:44,  1.78s/it] 45%|████▌     | 521/1153 [11:29<17:11,  1.63s/it] 45%|████▌     | 522/1153 [11:31<15:59,  1.52s/it] 45%|████▌     | 523/1153 [11:32<15:09,  1.44s/it] 45%|████▌     | 524/1153 [11:33<14:36,  1.39s/it] 46%|████▌     | 525/1153 [11:34<14:14,  1.36s/it] 46%|████▌     | 526/1153 [11:36<14:00,  1.34s/it] 46%|████▌     | 527/1153 [11:37<13:50,  1.33s/it] 46%|████▌     | 528/1153 [11:38<13:36,  1.31s/it] 46%|████▌     | 529/1153 [11:40<13:28,  1.30s/it] 46%|████▌     | 530/1153 [11:41<13:23,  1.29s/it] 46%|████▌     | 531/1153 [11:42<13:21,  1.29s/it] 46%|████▌     | 532/1153 [11:43<13:20,  1.29s/it] 46%|████▌     | 533/1153 [11:45<12:45,  1.23s/it] 46%|████▋     | 534/1153 [11:46<12:19,  1.19s/it] 46%|████▋     | 535/1153 [11:47<12:32,  1.22s/it] 46%|████▋     | 536/1153 [11:48<12:14,  1.19s/it] 47%|████▋     | 537/1153 [11:52<22:10,  2.16s/it] 47%|████▋     | 538/1153 [11:54<19:08,  1.87s/it] 47%|████▋     | 539/1153 [11:55<17:02,  1.66s/it] 47%|████▋     | 540/1153 [11:56<15:52,  1.55s/it] 47%|████▋     | 541/1153 [11:57<15:02,  1.47s/it] 47%|████▋     | 542/1153 [11:59<14:19,  1.41s/it] 47%|████▋     | 543/1153 [12:00<13:26,  1.32s/it] 47%|████▋     | 544/1153 [12:01<12:46,  1.26s/it] 47%|████▋     | 545/1153 [12:02<12:32,  1.24s/it] 47%|████▋     | 546/1153 [12:03<12:21,  1.22s/it] 47%|████▋     | 547/1153 [12:04<12:14,  1.21s/it] 48%|████▊     | 548/1153 [12:06<12:01,  1.19s/it] 48%|████▊     | 549/1153 [12:07<11:49,  1.18s/it] 48%|████▊     | 550/1153 [12:08<12:09,  1.21s/it] 48%|████▊     | 551/1153 [12:09<11:54,  1.19s/it] 48%|████▊     | 552/1153 [12:10<11:46,  1.17s/it] 48%|████▊     | 553/1153 [12:11<11:33,  1.16s/it] 48%|████▊     | 554/1153 [12:13<11:29,  1.15s/it] 48%|████▊     | 555/1153 [12:14<11:35,  1.16s/it] 48%|████▊     | 556/1153 [12:18<21:32,  2.17s/it] 48%|████▊     | 557/1153 [12:19<18:22,  1.85s/it] 48%|████▊     | 558/1153 [12:20<16:12,  1.63s/it] 48%|████▊     | 559/1153 [12:22<14:39,  1.48s/it] 49%|████▊     | 560/1153 [12:23<13:40,  1.38s/it] 49%|████▊     | 561/1153 [12:24<12:52,  1.30s/it] 49%|████▊     | 562/1153 [12:25<12:18,  1.25s/it] 49%|████▉     | 563/1153 [12:26<11:55,  1.21s/it] 49%|████▉     | 564/1153 [12:27<11:43,  1.19s/it] 49%|████▉     | 565/1153 [12:28<11:27,  1.17s/it] 49%|████▉     | 566/1153 [12:30<11:46,  1.20s/it] 49%|████▉     | 567/1153 [12:31<11:36,  1.19s/it] 49%|████▉     | 568/1153 [12:32<11:26,  1.17s/it] 49%|████▉     | 569/1153 [12:33<11:19,  1.16s/it] 49%|████▉     | 570/1153 [12:34<11:18,  1.16s/it] 50%|████▉     | 571/1153 [12:35<11:11,  1.15s/it] 50%|████▉     | 572/1153 [12:37<11:02,  1.14s/it] 50%|████▉     | 573/1153 [12:38<11:03,  1.14s/it] 50%|████▉     | 574/1153 [12:39<10:59,  1.14s/it] 50%|████▉     | 575/1153 [12:40<10:56,  1.14s/it] 50%|████▉     | 576/1153 [12:44<19:37,  2.04s/it] 50%|█████     | 577/1153 [12:45<17:02,  1.78s/it] 50%|█████     | 578/1153 [12:46<15:03,  1.57s/it] 50%|█████     | 579/1153 [12:47<13:49,  1.45s/it] 50%|█████     | 580/1153 [12:49<13:02,  1.37s/it] 50%|█████     | 581/1153 [12:50<12:31,  1.31s/it] 50%|█████     | 582/1153 [12:51<12:00,  1.26s/it] 51%|█████     | 583/1153 [12:52<11:35,  1.22s/it] 51%|█████     | 584/1153 [12:53<11:22,  1.20s/it] 51%|█████     | 585/1153 [12:54<11:12,  1.18s/it] 51%|█████     | 586/1153 [12:56<11:00,  1.16s/it] 51%|█████     | 587/1153 [12:57<10:58,  1.16s/it] 51%|█████     | 588/1153 [12:58<10:54,  1.16s/it] 51%|█████     | 589/1153 [12:59<11:20,  1.21s/it] 51%|█████     | 590/1153 [13:00<11:02,  1.18s/it] 51%|█████▏    | 591/1153 [13:02<11:22,  1.21s/it] 51%|█████▏    | 592/1153 [13:03<11:10,  1.20s/it] 51%|█████▏    | 593/1153 [13:04<11:26,  1.23s/it] 52%|█████▏    | 594/1153 [13:05<11:37,  1.25s/it] 52%|█████▏    | 595/1153 [13:07<11:46,  1.27s/it] 52%|█████▏    | 596/1153 [13:11<19:47,  2.13s/it] 52%|█████▏    | 597/1153 [13:12<17:27,  1.88s/it] 52%|█████▏    | 598/1153 [13:13<15:46,  1.71s/it] 52%|█████▏    | 599/1153 [13:14<13:48,  1.50s/it] 52%|█████▏    | 600/1153 [13:16<13:14,  1.44s/it] 52%|█████▏    | 601/1153 [13:17<12:51,  1.40s/it] 52%|█████▏    | 602/1153 [13:18<12:35,  1.37s/it] 52%|█████▏    | 603/1153 [13:20<12:25,  1.35s/it] 52%|█████▏    | 604/1153 [13:21<12:11,  1.33s/it] 52%|█████▏    | 605/1153 [13:22<12:03,  1.32s/it] 53%|█████▎    | 606/1153 [13:23<11:59,  1.31s/it] 53%|█████▎    | 607/1153 [13:25<11:56,  1.31s/it] 53%|█████▎    | 608/1153 [13:26<10:57,  1.21s/it] 53%|█████▎    | 609/1153 [13:27<10:45,  1.19s/it] 53%|█████▎    | 610/1153 [13:28<11:04,  1.22s/it] 53%|█████▎    | 611/1153 [13:30<11:17,  1.25s/it] 53%|█████▎    | 612/1153 [13:31<11:27,  1.27s/it] 53%|█████▎    | 613/1153 [13:32<11:29,  1.28s/it] 53%|█████▎    | 614/1153 [13:36<18:29,  2.06s/it] 53%|█████▎    | 615/1153 [13:37<16:33,  1.85s/it] 53%|█████▎    | 616/1153 [13:39<15:06,  1.69s/it] 54%|█████▎    | 617/1153 [13:40<14:05,  1.58s/it] 54%|█████▎    | 618/1153 [13:41<13:21,  1.50s/it] 54%|█████▎    | 619/1153 [13:43<12:44,  1.43s/it] 54%|█████▍    | 620/1153 [13:44<12:20,  1.39s/it] 54%|█████▍    | 621/1153 [13:45<12:04,  1.36s/it] 54%|█████▍    | 622/1153 [13:46<11:20,  1.28s/it] 54%|█████▍    | 623/1153 [13:48<11:23,  1.29s/it] 54%|█████▍    | 624/1153 [13:49<11:24,  1.29s/it] 54%|█████▍    | 625/1153 [13:50<11:25,  1.30s/it] 54%|█████▍    | 626/1153 [13:51<11:21,  1.29s/it] 54%|█████▍    | 627/1153 [13:53<10:56,  1.25s/it] 54%|█████▍    | 628/1153 [13:54<11:05,  1.27s/it] 55%|█████▍    | 629/1153 [13:55<11:06,  1.27s/it] 55%|█████▍    | 630/1153 [13:56<11:07,  1.28s/it] 55%|█████▍    | 631/1153 [13:58<11:08,  1.28s/it] 55%|█████▍    | 632/1153 [14:02<18:12,  2.10s/it] 55%|█████▍    | 633/1153 [14:03<15:42,  1.81s/it] 55%|█████▍    | 634/1153 [14:04<13:47,  1.60s/it] 55%|█████▌    | 635/1153 [14:05<12:58,  1.50s/it] 55%|█████▌    | 636/1153 [14:07<12:25,  1.44s/it] 55%|█████▌    | 637/1153 [14:08<12:02,  1.40s/it] 55%|█████▌    | 638/1153 [14:09<11:47,  1.37s/it] 55%|█████▌    | 639/1153 [14:11<11:37,  1.36s/it] 56%|█████▌    | 640/1153 [14:12<11:25,  1.34s/it] 56%|█████▌    | 641/1153 [14:13<11:17,  1.32s/it] 56%|█████▌    | 642/1153 [14:14<11:12,  1.32s/it] 56%|█████▌    | 643/1153 [14:16<11:09,  1.31s/it] 56%|█████▌    | 644/1153 [14:17<11:08,  1.31s/it] 56%|█████▌    | 645/1153 [14:18<11:07,  1.31s/it] 56%|█████▌    | 646/1153 [14:20<11:01,  1.31s/it] 56%|█████▌    | 647/1153 [14:20<09:06,  1.08s/it] 56%|█████▌    | 648/1153 [14:21<09:38,  1.15s/it] 56%|█████▋    | 649/1153 [14:23<10:02,  1.19s/it] 56%|█████▋    | 650/1153 [14:27<16:53,  2.01s/it] 56%|█████▋    | 651/1153 [14:28<15:03,  1.80s/it] 57%|█████▋    | 652/1153 [14:29<11:57,  1.43s/it] 57%|█████▋    | 653/1153 [14:30<11:14,  1.35s/it] 57%|█████▋    | 654/1153 [14:31<11:07,  1.34s/it] 57%|█████▋    | 655/1153 [14:32<09:18,  1.12s/it] 57%|█████▋    | 656/1153 [14:32<08:06,  1.02it/s] 57%|█████▋    | 657/1153 [14:34<08:57,  1.08s/it] 57%|█████▋    | 658/1153 [14:35<09:28,  1.15s/it] 57%|█████▋    | 659/1153 [14:36<09:53,  1.20s/it] 57%|█████▋    | 660/1153 [14:38<10:09,  1.24s/it] 57%|█████▋    | 661/1153 [14:39<10:15,  1.25s/it] 57%|█████▋    | 662/1153 [14:40<10:20,  1.26s/it] 58%|█████▊    | 663/1153 [14:41<10:24,  1.27s/it] 58%|█████▊    | 664/1153 [14:43<10:27,  1.28s/it] 58%|█████▊    | 665/1153 [14:44<10:33,  1.30s/it] 58%|█████▊    | 666/1153 [14:45<10:40,  1.32s/it] 58%|█████▊    | 667/1153 [14:47<10:34,  1.31s/it] 58%|█████▊    | 668/1153 [14:48<10:31,  1.30s/it] 58%|█████▊    | 669/1153 [14:52<16:59,  2.11s/it] 58%|█████▊    | 670/1153 [14:53<15:03,  1.87s/it] 58%|█████▊    | 671/1153 [14:55<13:41,  1.70s/it] 58%|█████▊    | 672/1153 [14:56<12:44,  1.59s/it] 58%|█████▊    | 673/1153 [14:57<11:59,  1.50s/it] 58%|█████▊    | 674/1153 [14:59<11:29,  1.44s/it] 59%|█████▊    | 675/1153 [15:00<11:08,  1.40s/it] 59%|█████▊    | 676/1153 [15:01<10:43,  1.35s/it] 59%|█████▊    | 677/1153 [15:02<10:33,  1.33s/it] 59%|█████▉    | 678/1153 [15:04<10:01,  1.27s/it] 59%|█████▉    | 679/1153 [15:05<10:08,  1.28s/it] 59%|█████▉    | 680/1153 [15:06<09:55,  1.26s/it] 59%|█████▉    | 681/1153 [15:07<09:36,  1.22s/it] 59%|█████▉    | 682/1153 [15:08<09:47,  1.25s/it] 59%|█████▉    | 683/1153 [15:10<09:55,  1.27s/it] 59%|█████▉    | 684/1153 [15:11<09:35,  1.23s/it] 59%|█████▉    | 685/1153 [15:12<09:45,  1.25s/it] 59%|█████▉    | 686/1153 [15:14<09:53,  1.27s/it] 60%|█████▉    | 687/1153 [15:15<09:36,  1.24s/it] 60%|█████▉    | 688/1153 [15:19<16:01,  2.07s/it] 60%|█████▉    | 689/1153 [15:20<14:19,  1.85s/it] 60%|█████▉    | 690/1153 [15:21<12:37,  1.64s/it] 60%|█████▉    | 691/1153 [15:22<11:31,  1.50s/it] 60%|██████    | 692/1153 [15:24<10:46,  1.40s/it] 60%|██████    | 693/1153 [15:25<10:07,  1.32s/it] 60%|██████    | 694/1153 [15:26<10:07,  1.32s/it] 60%|██████    | 695/1153 [15:27<10:07,  1.33s/it] 60%|██████    | 696/1153 [15:29<10:02,  1.32s/it] 60%|██████    | 697/1153 [15:30<10:02,  1.32s/it] 61%|██████    | 698/1153 [15:31<10:04,  1.33s/it] 61%|██████    | 699/1153 [15:33<10:05,  1.33s/it] 61%|██████    | 700/1153 [15:34<10:04,  1.34s/it] 61%|██████    | 701/1153 [15:35<10:03,  1.33s/it] 61%|██████    | 702/1153 [15:37<09:58,  1.33s/it] 61%|██████    | 703/1153 [15:38<09:53,  1.32s/it] 61%|██████    | 704/1153 [15:39<09:52,  1.32s/it] 61%|██████    | 705/1153 [15:41<09:50,  1.32s/it] 61%|██████    | 706/1153 [15:45<15:53,  2.13s/it] 61%|██████▏   | 707/1153 [15:46<14:08,  1.90s/it] 61%|██████▏   | 708/1153 [15:47<12:40,  1.71s/it] 61%|██████▏   | 709/1153 [15:49<11:45,  1.59s/it] 62%|██████▏   | 710/1153 [15:50<11:08,  1.51s/it] 62%|██████▏   | 711/1153 [15:51<11:00,  1.49s/it] 62%|██████▏   | 712/1153 [15:53<10:39,  1.45s/it] 62%|██████▏   | 713/1153 [15:54<10:19,  1.41s/it] 62%|██████▏   | 714/1153 [15:55<10:25,  1.42s/it] 62%|██████▏   | 715/1153 [15:57<10:09,  1.39s/it] 62%|██████▏   | 716/1153 [15:58<09:59,  1.37s/it] 62%|██████▏   | 717/1153 [15:59<09:53,  1.36s/it] 62%|██████▏   | 718/1153 [16:01<09:49,  1.35s/it] 62%|██████▏   | 719/1153 [16:02<09:56,  1.37s/it] 62%|██████▏   | 720/1153 [16:04<09:52,  1.37s/it] 63%|██████▎   | 721/1153 [16:05<09:44,  1.35s/it] 63%|██████▎   | 722/1153 [16:06<09:39,  1.35s/it] 63%|██████▎   | 723/1153 [16:11<16:35,  2.31s/it] 63%|██████▎   | 724/1153 [16:12<14:26,  2.02s/it] 63%|██████▎   | 725/1153 [16:13<13:01,  1.83s/it] 63%|██████▎   | 726/1153 [16:15<11:54,  1.67s/it] 63%|██████▎   | 727/1153 [16:16<11:09,  1.57s/it] 63%|██████▎   | 728/1153 [16:17<10:41,  1.51s/it] 63%|██████▎   | 729/1153 [16:19<10:16,  1.45s/it] 63%|██████▎   | 730/1153 [16:20<09:55,  1.41s/it] 63%|██████▎   | 731/1153 [16:21<09:40,  1.38s/it] 63%|██████▎   | 732/1153 [16:23<09:30,  1.36s/it] 64%|██████▎   | 733/1153 [16:24<09:23,  1.34s/it] 64%|██████▎   | 734/1153 [16:25<09:19,  1.34s/it] 64%|██████▎   | 735/1153 [16:27<09:17,  1.33s/it] 64%|██████▍   | 736/1153 [16:28<09:11,  1.32s/it] 64%|██████▍   | 737/1153 [16:29<09:08,  1.32s/it] 64%|██████▍   | 738/1153 [16:31<09:06,  1.32s/it] 64%|██████▍   | 739/1153 [16:32<09:04,  1.32s/it] 64%|██████▍   | 740/1153 [16:33<09:04,  1.32s/it] 64%|██████▍   | 741/1153 [16:38<15:45,  2.29s/it] 64%|██████▍   | 742/1153 [16:39<13:42,  2.00s/it] 64%|██████▍   | 743/1153 [16:40<12:14,  1.79s/it] 65%|██████▍   | 744/1153 [16:41<10:42,  1.57s/it] 65%|██████▍   | 745/1153 [16:43<10:16,  1.51s/it] 65%|██████▍   | 746/1153 [16:44<09:44,  1.44s/it] 65%|██████▍   | 747/1153 [16:45<09:26,  1.40s/it] 65%|██████▍   | 748/1153 [16:47<09:14,  1.37s/it] 65%|██████▍   | 749/1153 [16:48<09:06,  1.35s/it] 65%|██████▌   | 750/1153 [16:49<09:07,  1.36s/it] 65%|██████▌   | 751/1153 [16:51<09:03,  1.35s/it] 65%|██████▌   | 752/1153 [16:52<09:00,  1.35s/it] 65%|██████▌   | 753/1153 [16:53<08:47,  1.32s/it] 65%|██████▌   | 754/1153 [16:55<08:46,  1.32s/it] 65%|██████▌   | 755/1153 [16:56<08:45,  1.32s/it] 66%|██████▌   | 756/1153 [16:57<08:23,  1.27s/it] 66%|██████▌   | 757/1153 [16:58<08:16,  1.25s/it] 66%|██████▌   | 758/1153 [17:00<08:06,  1.23s/it] 66%|██████▌   | 759/1153 [17:01<07:57,  1.21s/it] 66%|██████▌   | 760/1153 [17:05<13:34,  2.07s/it] 66%|██████▌   | 761/1153 [17:06<12:04,  1.85s/it] 66%|██████▌   | 762/1153 [17:07<10:44,  1.65s/it] 66%|██████▌   | 763/1153 [17:08<09:44,  1.50s/it] 66%|██████▋   | 764/1153 [17:10<09:07,  1.41s/it] 66%|██████▋   | 765/1153 [17:11<08:40,  1.34s/it] 66%|██████▋   | 766/1153 [17:12<08:16,  1.28s/it] 67%|██████▋   | 767/1153 [17:13<08:04,  1.25s/it] 67%|██████▋   | 768/1153 [17:14<07:51,  1.22s/it] 67%|██████▋   | 769/1153 [17:15<07:39,  1.20s/it] 67%|██████▋   | 770/1153 [17:17<07:38,  1.20s/it] 67%|██████▋   | 771/1153 [17:18<07:33,  1.19s/it] 67%|██████▋   | 772/1153 [17:19<07:33,  1.19s/it] 67%|██████▋   | 773/1153 [17:20<07:29,  1.18s/it] 67%|██████▋   | 774/1153 [17:21<07:25,  1.18s/it] 67%|██████▋   | 775/1153 [17:22<07:25,  1.18s/it] 67%|██████▋   | 776/1153 [17:24<07:22,  1.17s/it] 67%|██████▋   | 777/1153 [17:25<07:21,  1.17s/it] 67%|██████▋   | 778/1153 [17:26<07:19,  1.17s/it] 68%|██████▊   | 779/1153 [17:27<07:16,  1.17s/it] 68%|██████▊   | 780/1153 [17:28<07:18,  1.18s/it] 68%|██████▊   | 781/1153 [17:32<12:13,  1.97s/it] 68%|██████▊   | 782/1153 [17:33<10:43,  1.73s/it] 68%|██████▊   | 783/1153 [17:35<09:46,  1.58s/it] 68%|██████▊   | 784/1153 [17:36<09:03,  1.47s/it] 68%|██████▊   | 785/1153 [17:37<08:49,  1.44s/it] 68%|██████▊   | 786/1153 [17:39<08:37,  1.41s/it] 68%|██████▊   | 787/1153 [17:40<08:28,  1.39s/it] 68%|██████▊   | 788/1153 [17:41<08:18,  1.37s/it] 68%|██████▊   | 789/1153 [17:42<08:11,  1.35s/it] 69%|██████▊   | 790/1153 [17:44<08:09,  1.35s/it] 69%|██████▊   | 791/1153 [17:45<07:41,  1.28s/it] 69%|██████▊   | 792/1153 [17:46<07:46,  1.29s/it] 69%|██████▉   | 793/1153 [17:48<07:50,  1.31s/it] 69%|██████▉   | 794/1153 [17:49<07:52,  1.32s/it] 69%|██████▉   | 795/1153 [17:50<07:53,  1.32s/it] 69%|██████▉   | 796/1153 [17:52<07:54,  1.33s/it] 69%|██████▉   | 797/1153 [17:53<07:56,  1.34s/it] 69%|██████▉   | 798/1153 [17:54<08:00,  1.35s/it] 69%|██████▉   | 799/1153 [17:56<08:03,  1.37s/it] 69%|██████▉   | 800/1153 [18:01<14:00,  2.38s/it] 69%|██████▉   | 801/1153 [18:02<12:14,  2.09s/it] 70%|██████▉   | 802/1153 [18:03<10:53,  1.86s/it] 70%|██████▉   | 803/1153 [18:05<09:58,  1.71s/it] 70%|██████▉   | 804/1153 [18:06<09:15,  1.59s/it] 70%|██████▉   | 805/1153 [18:07<08:46,  1.51s/it] 70%|██████▉   | 806/1153 [18:09<08:25,  1.46s/it] 70%|██████▉   | 807/1153 [18:10<08:13,  1.43s/it] 70%|███████   | 808/1153 [18:11<08:08,  1.42s/it] 70%|███████   | 809/1153 [18:13<08:03,  1.41s/it] 70%|███████   | 810/1153 [18:14<07:53,  1.38s/it] 70%|███████   | 811/1153 [18:15<07:51,  1.38s/it] 70%|███████   | 812/1153 [18:17<07:44,  1.36s/it] 71%|███████   | 813/1153 [18:18<07:39,  1.35s/it] 71%|███████   | 814/1153 [18:19<07:37,  1.35s/it] 71%|███████   | 815/1153 [18:21<07:35,  1.35s/it] 71%|███████   | 816/1153 [18:21<06:14,  1.11s/it] 71%|███████   | 817/1153 [18:23<06:34,  1.18s/it] 71%|███████   | 818/1153 [18:27<12:01,  2.15s/it] 71%|███████   | 819/1153 [18:28<10:37,  1.91s/it] 71%|███████   | 820/1153 [18:30<09:37,  1.73s/it] 71%|███████   | 821/1153 [18:31<08:39,  1.57s/it] 71%|███████▏  | 822/1153 [18:32<07:16,  1.32s/it] 71%|███████▏  | 823/1153 [18:33<07:11,  1.31s/it] 71%|███████▏  | 824/1153 [18:34<07:16,  1.33s/it] 72%|███████▏  | 825/1153 [18:35<06:48,  1.25s/it] 72%|███████▏  | 826/1153 [18:37<07:00,  1.29s/it] 72%|███████▏  | 827/1153 [18:38<06:46,  1.25s/it] 72%|███████▏  | 828/1153 [18:39<06:53,  1.27s/it] 72%|███████▏  | 829/1153 [18:40<06:22,  1.18s/it] 72%|███████▏  | 830/1153 [18:42<06:40,  1.24s/it] 72%|███████▏  | 831/1153 [18:43<06:48,  1.27s/it] 72%|███████▏  | 832/1153 [18:44<06:52,  1.28s/it] 72%|███████▏  | 833/1153 [18:46<06:55,  1.30s/it] 72%|███████▏  | 834/1153 [18:47<06:57,  1.31s/it] 72%|███████▏  | 835/1153 [18:48<06:59,  1.32s/it] 73%|███████▎  | 836/1153 [18:50<07:00,  1.33s/it] 73%|███████▎  | 837/1153 [18:51<06:58,  1.32s/it] 73%|███████▎  | 838/1153 [18:55<11:31,  2.20s/it] 73%|███████▎  | 839/1153 [18:56<10:09,  1.94s/it] 73%|███████▎  | 840/1153 [18:58<09:08,  1.75s/it] 73%|███████▎  | 841/1153 [18:59<08:28,  1.63s/it] 73%|███████▎  | 842/1153 [19:00<07:57,  1.54s/it] 73%|███████▎  | 843/1153 [19:02<07:36,  1.47s/it] 73%|███████▎  | 844/1153 [19:03<07:21,  1.43s/it] 73%|███████▎  | 845/1153 [19:04<07:11,  1.40s/it] 73%|███████▎  | 846/1153 [19:06<06:47,  1.33s/it] 73%|███████▎  | 847/1153 [19:07<06:46,  1.33s/it] 74%|███████▎  | 848/1153 [19:08<06:25,  1.27s/it] 74%|███████▎  | 849/1153 [19:09<06:20,  1.25s/it] 74%|███████▎  | 850/1153 [19:11<06:28,  1.28s/it] 74%|███████▍  | 851/1153 [19:12<06:31,  1.30s/it] 74%|███████▍  | 852/1153 [19:13<06:32,  1.31s/it] 74%|███████▍  | 853/1153 [19:14<06:14,  1.25s/it] 74%|███████▍  | 854/1153 [19:16<06:20,  1.27s/it] 74%|███████▍  | 855/1153 [19:17<06:24,  1.29s/it] 74%|███████▍  | 856/1153 [19:18<06:10,  1.25s/it] 74%|███████▍  | 857/1153 [19:22<10:21,  2.10s/it] 74%|███████▍  | 858/1153 [19:24<09:14,  1.88s/it] 75%|███████▍  | 859/1153 [19:25<08:08,  1.66s/it] 75%|███████▍  | 860/1153 [19:26<07:27,  1.53s/it] 75%|███████▍  | 861/1153 [19:27<06:57,  1.43s/it] 75%|███████▍  | 862/1153 [19:29<06:52,  1.42s/it] 75%|███████▍  | 863/1153 [19:30<06:43,  1.39s/it] 75%|███████▍  | 864/1153 [19:31<06:25,  1.33s/it] 75%|███████▌  | 865/1153 [19:32<06:11,  1.29s/it] 75%|███████▌  | 866/1153 [19:34<06:01,  1.26s/it] 75%|███████▌  | 867/1153 [19:35<06:06,  1.28s/it] 75%|███████▌  | 868/1153 [19:36<05:56,  1.25s/it] 75%|███████▌  | 869/1153 [19:37<05:49,  1.23s/it] 75%|███████▌  | 870/1153 [19:38<05:43,  1.22s/it] 76%|███████▌  | 871/1153 [19:40<05:53,  1.25s/it] 76%|███████▌  | 872/1153 [19:41<06:01,  1.29s/it] 76%|███████▌  | 873/1153 [19:42<05:51,  1.26s/it] 76%|███████▌  | 874/1153 [19:43<05:44,  1.24s/it] 76%|███████▌  | 875/1153 [19:45<05:40,  1.22s/it] 76%|███████▌  | 876/1153 [19:46<05:33,  1.20s/it] 76%|███████▌  | 877/1153 [19:50<09:30,  2.07s/it] 76%|███████▌  | 878/1153 [19:51<08:16,  1.81s/it] 76%|███████▌  | 879/1153 [19:52<07:21,  1.61s/it] 76%|███████▋  | 880/1153 [19:53<06:46,  1.49s/it] 76%|███████▋  | 881/1153 [19:55<06:20,  1.40s/it] 76%|███████▋  | 882/1153 [19:56<05:59,  1.33s/it] 77%|███████▋  | 883/1153 [19:57<05:49,  1.29s/it] 77%|███████▋  | 884/1153 [19:58<05:53,  1.31s/it] 77%|███████▋  | 885/1153 [20:00<05:52,  1.32s/it] 77%|███████▋  | 886/1153 [20:01<05:52,  1.32s/it] 77%|███████▋  | 887/1153 [20:02<05:52,  1.33s/it] 77%|███████▋  | 888/1153 [20:04<05:52,  1.33s/it] 77%|███████▋  | 889/1153 [20:05<05:38,  1.28s/it] 77%|███████▋  | 890/1153 [20:06<05:32,  1.26s/it] 77%|███████▋  | 891/1153 [20:07<05:38,  1.29s/it] 77%|███████▋  | 892/1153 [20:09<05:30,  1.27s/it] 77%|███████▋  | 893/1153 [20:10<05:20,  1.23s/it] 78%|███████▊  | 894/1153 [20:11<05:19,  1.23s/it] 78%|███████▊  | 895/1153 [20:12<05:15,  1.22s/it] 78%|███████▊  | 896/1153 [20:13<05:10,  1.21s/it] 78%|███████▊  | 897/1153 [20:15<05:08,  1.21s/it] 78%|███████▊  | 898/1153 [20:19<08:44,  2.06s/it] 78%|███████▊  | 899/1153 [20:20<07:38,  1.81s/it] 78%|███████▊  | 900/1153 [20:21<06:51,  1.63s/it] 78%|███████▊  | 901/1153 [20:22<06:17,  1.50s/it] 78%|███████▊  | 902/1153 [20:23<05:51,  1.40s/it] 78%|███████▊  | 903/1153 [20:25<05:37,  1.35s/it] 78%|███████▊  | 904/1153 [20:26<05:27,  1.31s/it] 78%|███████▊  | 905/1153 [20:27<05:18,  1.28s/it] 79%|███████▊  | 906/1153 [20:28<05:14,  1.27s/it] 79%|███████▊  | 907/1153 [20:30<05:06,  1.24s/it] 79%|███████▉  | 908/1153 [20:31<05:02,  1.24s/it] 79%|███████▉  | 909/1153 [20:32<04:59,  1.23s/it] 79%|███████▉  | 910/1153 [20:33<04:55,  1.22s/it] 79%|███████▉  | 911/1153 [20:34<04:54,  1.22s/it] 79%|███████▉  | 912/1153 [20:36<04:51,  1.21s/it] 79%|███████▉  | 913/1153 [20:37<04:47,  1.20s/it] 79%|███████▉  | 914/1153 [20:38<04:47,  1.20s/it] 79%|███████▉  | 915/1153 [20:39<04:46,  1.20s/it] 79%|███████▉  | 916/1153 [20:40<04:49,  1.22s/it] 80%|███████▉  | 917/1153 [20:42<04:46,  1.21s/it] 80%|███████▉  | 918/1153 [20:43<04:44,  1.21s/it] 80%|███████▉  | 919/1153 [20:47<08:06,  2.08s/it] 80%|███████▉  | 920/1153 [20:48<07:15,  1.87s/it] 80%|███████▉  | 921/1153 [20:50<06:37,  1.71s/it] 80%|███████▉  | 922/1153 [20:51<06:11,  1.61s/it] 80%|████████  | 923/1153 [20:52<05:50,  1.52s/it] 80%|████████  | 924/1153 [20:54<05:35,  1.47s/it] 80%|████████  | 925/1153 [20:55<05:26,  1.43s/it] 80%|████████  | 926/1153 [20:56<05:19,  1.41s/it] 80%|████████  | 927/1153 [20:58<04:59,  1.33s/it] 80%|████████  | 928/1153 [20:59<04:59,  1.33s/it] 81%|████████  | 929/1153 [21:00<04:59,  1.34s/it] 81%|████████  | 930/1153 [21:02<04:57,  1.33s/it] 81%|████████  | 931/1153 [21:03<04:56,  1.33s/it] 81%|████████  | 932/1153 [21:04<04:56,  1.34s/it] 81%|████████  | 933/1153 [21:06<04:55,  1.34s/it] 81%|████████  | 934/1153 [21:07<04:55,  1.35s/it] 81%|████████  | 935/1153 [21:08<04:54,  1.35s/it] 81%|████████  | 936/1153 [21:10<04:54,  1.36s/it] 81%|████████▏ | 937/1153 [21:14<08:10,  2.27s/it] 81%|████████▏ | 938/1153 [21:15<07:10,  2.00s/it] 81%|████████▏ | 939/1153 [21:17<06:27,  1.81s/it] 82%|████████▏ | 940/1153 [21:18<05:56,  1.67s/it] 82%|████████▏ | 941/1153 [21:20<05:35,  1.58s/it] 82%|████████▏ | 942/1153 [21:21<05:17,  1.51s/it] 82%|████████▏ | 943/1153 [21:22<05:05,  1.46s/it] 82%|████████▏ | 944/1153 [21:24<04:57,  1.42s/it] 82%|████████▏ | 945/1153 [21:25<04:45,  1.37s/it] 82%|████████▏ | 946/1153 [21:26<04:37,  1.34s/it] 82%|████████▏ | 947/1153 [21:27<04:36,  1.34s/it] 82%|████████▏ | 948/1153 [21:29<04:38,  1.36s/it] 82%|████████▏ | 949/1153 [21:30<04:37,  1.36s/it] 82%|████████▏ | 950/1153 [21:32<04:34,  1.35s/it] 82%|████████▏ | 951/1153 [21:33<04:34,  1.36s/it] 83%|████████▎ | 952/1153 [21:34<04:32,  1.36s/it] 83%|████████▎ | 953/1153 [21:36<04:31,  1.36s/it] 83%|████████▎ | 954/1153 [21:37<04:24,  1.33s/it] 83%|████████▎ | 955/1153 [21:38<04:24,  1.33s/it] 83%|████████▎ | 956/1153 [21:42<07:15,  2.21s/it] 83%|████████▎ | 957/1153 [21:44<06:24,  1.96s/it] 83%|████████▎ | 958/1153 [21:45<05:47,  1.78s/it] 83%|████████▎ | 959/1153 [21:47<05:20,  1.65s/it] 83%|████████▎ | 960/1153 [21:48<05:00,  1.56s/it] 83%|████████▎ | 961/1153 [21:48<04:02,  1.27s/it] 83%|████████▎ | 962/1153 [21:50<03:55,  1.23s/it] 84%|████████▎ | 963/1153 [21:51<04:01,  1.27s/it] 84%|████████▎ | 964/1153 [21:52<04:04,  1.30s/it] 84%|████████▎ | 965/1153 [21:54<04:02,  1.29s/it] 84%|████████▍ | 966/1153 [21:55<04:04,  1.31s/it] 84%|████████▍ | 967/1153 [21:56<03:57,  1.28s/it] 84%|████████▍ | 968/1153 [21:58<04:01,  1.31s/it] 84%|████████▍ | 969/1153 [21:59<04:02,  1.32s/it] 84%|████████▍ | 970/1153 [22:00<03:40,  1.21s/it] 84%|████████▍ | 971/1153 [22:00<02:55,  1.04it/s] 84%|████████▍ | 972/1153 [22:01<02:28,  1.22it/s] 84%|████████▍ | 973/1153 [22:02<02:32,  1.18it/s] 84%|████████▍ | 974/1153 [22:03<02:58,  1.00it/s] 85%|████████▍ | 975/1153 [22:04<03:15,  1.10s/it] 85%|████████▍ | 976/1153 [22:06<03:28,  1.18s/it] 85%|████████▍ | 977/1153 [22:07<03:37,  1.23s/it] 85%|████████▍ | 978/1153 [22:11<06:21,  2.18s/it] 85%|████████▍ | 979/1153 [22:13<05:36,  1.93s/it] 85%|████████▍ | 980/1153 [22:14<05:04,  1.76s/it] 85%|████████▌ | 981/1153 [22:16<04:41,  1.64s/it] 85%|████████▌ | 982/1153 [22:17<04:23,  1.54s/it] 85%|████████▌ | 983/1153 [22:18<04:13,  1.49s/it] 85%|████████▌ | 984/1153 [22:19<03:59,  1.42s/it] 85%|████████▌ | 985/1153 [22:21<03:54,  1.40s/it] 86%|████████▌ | 986/1153 [22:22<03:51,  1.39s/it] 86%|████████▌ | 987/1153 [22:24<03:49,  1.38s/it] 86%|████████▌ | 988/1153 [22:25<03:44,  1.36s/it] 86%|████████▌ | 989/1153 [22:26<03:42,  1.36s/it] 86%|████████▌ | 990/1153 [22:28<03:42,  1.36s/it] 86%|████████▌ | 991/1153 [22:29<03:31,  1.31s/it] 86%|████████▌ | 992/1153 [22:30<03:34,  1.33s/it] 86%|████████▌ | 993/1153 [22:32<03:35,  1.34s/it] 86%|████████▌ | 994/1153 [22:33<03:35,  1.36s/it] 86%|████████▋ | 995/1153 [22:34<03:35,  1.36s/it] 86%|████████▋ | 996/1153 [22:35<03:26,  1.31s/it] 86%|████████▋ | 997/1153 [22:40<05:43,  2.20s/it] 87%|████████▋ | 998/1153 [22:41<04:54,  1.90s/it] 87%|████████▋ | 999/1153 [22:42<04:22,  1.70s/it] 87%|████████▋ | 1000/1153 [22:44<04:06,  1.61s/it] 87%|████████▋ | 1001/1153 [22:45<03:52,  1.53s/it] 87%|████████▋ | 1002/1153 [22:46<03:37,  1.44s/it] 87%|████████▋ | 1003/1153 [22:47<03:26,  1.37s/it] 87%|████████▋ | 1004/1153 [22:49<03:16,  1.32s/it] 87%|████████▋ | 1005/1153 [22:50<03:16,  1.33s/it] 87%|████████▋ | 1006/1153 [22:51<03:15,  1.33s/it] 87%|████████▋ | 1007/1153 [22:52<03:08,  1.29s/it] 87%|████████▋ | 1008/1153 [22:54<03:02,  1.26s/it] 88%|████████▊ | 1009/1153 [22:55<02:59,  1.25s/it] 88%|████████▊ | 1010/1153 [22:56<02:55,  1.23s/it] 88%|████████▊ | 1011/1153 [22:57<02:54,  1.23s/it] 88%|████████▊ | 1012/1153 [22:58<02:52,  1.23s/it] 88%|████████▊ | 1013/1153 [23:00<02:58,  1.27s/it] 88%|████████▊ | 1014/1153 [23:01<02:53,  1.25s/it] 88%|████████▊ | 1015/1153 [23:02<02:50,  1.24s/it] 88%|████████▊ | 1016/1153 [23:03<02:48,  1.23s/it] 88%|████████▊ | 1017/1153 [23:05<02:45,  1.22s/it] 88%|████████▊ | 1018/1153 [23:06<02:45,  1.23s/it] 88%|████████▊ | 1019/1153 [23:10<04:42,  2.11s/it] 88%|████████▊ | 1020/1153 [23:11<04:05,  1.84s/it] 89%|████████▊ | 1021/1153 [23:13<03:38,  1.66s/it] 89%|████████▊ | 1022/1153 [23:14<03:19,  1.52s/it] 89%|████████▊ | 1023/1153 [23:15<03:04,  1.42s/it] 89%|████████▉ | 1024/1153 [23:16<02:55,  1.36s/it] 89%|████████▉ | 1025/1153 [23:17<02:46,  1.30s/it] 89%|████████▉ | 1026/1153 [23:19<02:48,  1.33s/it] 89%|████████▉ | 1027/1153 [23:20<02:49,  1.35s/it] 89%|████████▉ | 1028/1153 [23:21<02:48,  1.35s/it] 89%|████████▉ | 1029/1153 [23:23<02:47,  1.35s/it] 89%|████████▉ | 1030/1153 [23:24<02:46,  1.36s/it] 89%|████████▉ | 1031/1153 [23:26<02:46,  1.37s/it] 90%|████████▉ | 1032/1153 [23:27<02:46,  1.38s/it] 90%|████████▉ | 1033/1153 [23:28<02:39,  1.33s/it] 90%|████████▉ | 1034/1153 [23:30<02:41,  1.36s/it] 90%|████████▉ | 1035/1153 [23:31<02:35,  1.32s/it] 90%|████████▉ | 1036/1153 [23:32<02:32,  1.31s/it] 90%|████████▉ | 1037/1153 [23:33<02:27,  1.28s/it] 90%|█████████ | 1038/1153 [23:37<04:02,  2.11s/it] 90%|█████████ | 1039/1153 [23:39<03:31,  1.85s/it] 90%|█████████ | 1040/1153 [23:40<03:07,  1.66s/it] 90%|█████████ | 1041/1153 [23:41<02:51,  1.53s/it] 90%|█████████ | 1042/1153 [23:42<02:41,  1.45s/it] 90%|█████████ | 1043/1153 [23:44<02:32,  1.38s/it] 91%|█████████ | 1044/1153 [23:45<02:23,  1.32s/it] 91%|█████████ | 1045/1153 [23:46<02:19,  1.29s/it] 91%|█████████ | 1046/1153 [23:47<02:26,  1.36s/it] 91%|█████████ | 1047/1153 [23:49<02:21,  1.33s/it] 91%|█████████ | 1048/1153 [23:50<02:16,  1.30s/it] 91%|█████████ | 1049/1153 [23:51<02:11,  1.27s/it] 91%|█████████ | 1050/1153 [23:53<02:14,  1.30s/it] 91%|█████████ | 1051/1153 [23:54<02:10,  1.28s/it] 91%|█████████ | 1052/1153 [23:55<02:06,  1.25s/it] 91%|█████████▏| 1053/1153 [23:56<02:05,  1.25s/it] 91%|█████████▏| 1054/1153 [23:57<02:02,  1.24s/it] 92%|█████████▏| 1055/1153 [23:59<02:02,  1.25s/it] 92%|█████████▏| 1056/1153 [24:00<02:04,  1.28s/it] 92%|█████████▏| 1057/1153 [24:01<02:02,  1.28s/it] 92%|█████████▏| 1058/1153 [24:03<02:01,  1.28s/it] 92%|█████████▏| 1059/1153 [24:04<01:59,  1.27s/it] 92%|█████████▏| 1060/1153 [24:09<03:37,  2.34s/it] 92%|█████████▏| 1061/1153 [24:10<03:10,  2.07s/it] 92%|█████████▏| 1062/1153 [24:12<02:53,  1.90s/it] 92%|█████████▏| 1063/1153 [24:13<02:38,  1.76s/it] 92%|█████████▏| 1064/1153 [24:15<02:29,  1.68s/it] 92%|█████████▏| 1065/1153 [24:16<02:19,  1.59s/it] 92%|█████████▏| 1066/1153 [24:17<02:12,  1.53s/it] 93%|█████████▎| 1067/1153 [24:19<02:07,  1.49s/it] 93%|█████████▎| 1068/1153 [24:20<02:04,  1.46s/it] 93%|█████████▎| 1069/1153 [24:22<02:01,  1.45s/it] 93%|█████████▎| 1070/1153 [24:23<01:59,  1.44s/it] 93%|█████████▎| 1071/1153 [24:24<01:56,  1.42s/it] 93%|█████████▎| 1072/1153 [24:26<01:54,  1.41s/it] 93%|█████████▎| 1073/1153 [24:27<01:52,  1.40s/it] 93%|█████████▎| 1074/1153 [24:28<01:50,  1.40s/it] 93%|█████████▎| 1075/1153 [24:30<01:49,  1.40s/it] 93%|█████████▎| 1076/1153 [24:31<01:47,  1.40s/it] 93%|█████████▎| 1077/1153 [24:33<01:45,  1.39s/it] 93%|█████████▎| 1078/1153 [24:34<01:43,  1.38s/it] 94%|█████████▎| 1079/1153 [24:35<01:36,  1.30s/it] 94%|█████████▎| 1080/1153 [24:40<02:52,  2.36s/it] 94%|█████████▍| 1081/1153 [24:41<02:30,  2.09s/it] 94%|█████████▍| 1082/1153 [24:43<02:14,  1.90s/it] 94%|█████████▍| 1083/1153 [24:44<02:03,  1.77s/it] 94%|█████████▍| 1084/1153 [24:46<01:54,  1.66s/it] 94%|█████████▍| 1085/1153 [24:47<01:46,  1.57s/it] 94%|█████████▍| 1086/1153 [24:48<01:41,  1.51s/it] 94%|█████████▍| 1087/1153 [24:50<01:34,  1.43s/it] 94%|█████████▍| 1088/1153 [24:51<01:32,  1.42s/it] 94%|█████████▍| 1089/1153 [24:53<01:32,  1.45s/it] 95%|█████████▍| 1090/1153 [24:54<01:30,  1.43s/it] 95%|█████████▍| 1091/1153 [24:55<01:28,  1.42s/it] 95%|█████████▍| 1092/1153 [24:57<01:26,  1.41s/it] 95%|█████████▍| 1093/1153 [24:58<01:24,  1.41s/it] 95%|█████████▍| 1094/1153 [25:00<01:22,  1.40s/it] 95%|█████████▍| 1095/1153 [25:01<01:18,  1.35s/it] 95%|█████████▌| 1096/1153 [25:02<01:18,  1.37s/it] 95%|█████████▌| 1097/1153 [25:04<01:18,  1.40s/it] 95%|█████████▌| 1098/1153 [25:05<01:16,  1.39s/it] 95%|█████████▌| 1099/1153 [25:10<02:11,  2.44s/it] 95%|█████████▌| 1100/1153 [25:11<01:42,  1.93s/it] 95%|█████████▌| 1101/1153 [25:12<01:31,  1.77s/it] 96%|█████████▌| 1102/1153 [25:13<01:24,  1.66s/it] 96%|█████████▌| 1103/1153 [25:15<01:18,  1.58s/it] 96%|█████████▌| 1104/1153 [25:16<01:14,  1.51s/it] 96%|█████████▌| 1105/1153 [25:17<01:05,  1.36s/it] 96%|█████████▌| 1106/1153 [25:19<01:04,  1.36s/it] 96%|█████████▌| 1107/1153 [25:19<00:54,  1.18s/it] 96%|█████████▌| 1108/1153 [25:21<00:55,  1.24s/it] 96%|█████████▌| 1109/1153 [25:21<00:45,  1.03s/it] 96%|█████████▋| 1110/1153 [25:22<00:45,  1.07s/it] 96%|█████████▋| 1111/1153 [25:24<00:48,  1.16s/it] 96%|█████████▋| 1112/1153 [25:25<00:50,  1.23s/it] 97%|█████████▋| 1113/1153 [25:27<00:50,  1.27s/it] 97%|█████████▋| 1114/1153 [25:28<00:50,  1.30s/it] 97%|█████████▋| 1115/1153 [25:29<00:50,  1.32s/it] 97%|█████████▋| 1116/1153 [25:31<00:49,  1.35s/it] 97%|█████████▋| 1117/1153 [25:32<00:49,  1.38s/it] 97%|█████████▋| 1118/1153 [25:34<00:48,  1.39s/it] 97%|█████████▋| 1119/1153 [25:35<00:47,  1.39s/it] 97%|█████████▋| 1120/1153 [25:40<01:19,  2.40s/it] 97%|█████████▋| 1121/1153 [25:41<01:06,  2.07s/it] 97%|█████████▋| 1122/1153 [25:42<00:56,  1.83s/it] 97%|█████████▋| 1123/1153 [25:44<00:50,  1.69s/it] 97%|█████████▋| 1124/1153 [25:45<00:46,  1.60s/it] 98%|█████████▊| 1125/1153 [25:46<00:43,  1.54s/it] 98%|█████████▊| 1126/1153 [25:48<00:39,  1.46s/it] 98%|█████████▊| 1127/1153 [25:49<00:37,  1.43s/it] 98%|█████████▊| 1128/1153 [25:50<00:35,  1.42s/it] 98%|█████████▊| 1129/1153 [25:52<00:33,  1.41s/it] 98%|█████████▊| 1130/1153 [25:53<00:32,  1.40s/it] 98%|█████████▊| 1131/1153 [25:55<00:29,  1.35s/it] 98%|█████████▊| 1132/1153 [25:56<00:29,  1.40s/it] 98%|█████████▊| 1133/1153 [25:57<00:28,  1.40s/it] 98%|█████████▊| 1134/1153 [25:59<00:27,  1.42s/it] 98%|█████████▊| 1135/1153 [26:00<00:25,  1.44s/it] 99%|█████████▊| 1136/1153 [26:02<00:24,  1.42s/it] 99%|█████████▊| 1137/1153 [26:03<00:22,  1.41s/it] 99%|█████████▊| 1138/1153 [26:04<00:20,  1.35s/it] 99%|█████████▉| 1139/1153 [26:06<00:18,  1.32s/it] 99%|█████████▉| 1140/1153 [26:11<00:32,  2.47s/it] 99%|█████████▉| 1141/1153 [26:12<00:25,  2.11s/it] 99%|█████████▉| 1142/1153 [26:13<00:20,  1.84s/it] 99%|█████████▉| 1143/1153 [26:14<00:16,  1.66s/it] 99%|█████████▉| 1144/1153 [26:16<00:14,  1.56s/it] 99%|█████████▉| 1145/1153 [26:17<00:12,  1.51s/it] 99%|█████████▉| 1146/1153 [26:18<00:10,  1.43s/it] 99%|█████████▉| 1147/1153 [26:20<00:08,  1.42s/it]100%|█████████▉| 1148/1153 [26:21<00:06,  1.37s/it]100%|█████████▉| 1149/1153 [26:22<00:05,  1.35s/it]100%|█████████▉| 1150/1153 [26:24<00:03,  1.32s/it]100%|█████████▉| 1151/1153 [26:25<00:02,  1.29s/it]100%|█████████▉| 1152/1153 [26:26<00:01,  1.34s/it]100%|██████████| 1153/1153 [26:27<00:00,  1.21s/it]100%|██████████| 1153/1153 [26:27<00:00,  1.38s/it]
147540
147540
saving data 147540 to ./output/DuEE1.0/role/test_result.json
trigger predict 147540 load from ./output/DuEE1.0/role/test_result.json
role predict 147540 load from ./output/DuEE1.0/role/test_result.json
schema 68 load from ./conf/DuEE1.0/event_schema.json
submit data 147540 save to ./output/DuEE1.0/surbot_news_together.json
********** Done ***************
